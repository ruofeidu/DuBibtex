{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10020,
    "startDate": 1539475200000,
    "endDate": 1539734400000,
    "shortName": "UIST",
    "name": "UIST 2018",
    "year": 2018,
    "fullName": "User Interface Software and Technology Symposium",
    "url": "http://uist.acm.org/uist2018/",
    "location": "Berlin, Germany",
    "timeZoneOffset": 120,
    "logoUrl": "https://files.sigchi.org/conference/logo/d4c3bf17-3fa9-f9c8-885d-fbfbde79382b.png",
    "timeZoneName": "Europe/Berlin"
  },
  "sponsors": [],
  "sponsorLevels": [
    {
      "id": 10014,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [
    {
      "id": 10028,
      "name": "Berlin Convention Center - Location in Berlin",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/d9896a1a-263a-b28d-0744-5df4f690fd04.png",
      "roomIds": [
        10109
      ]
    },
    {
      "id": 10030,
      "name": "BCC - Level C",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/087ee859-7330-71ee-160d-3b520041d49b.png",
      "roomIds": [
        10110
      ]
    },
    {
      "id": 10031,
      "name": "BCC - Level B",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/7bdebedc-2115-29af-28dc-69cb3e35e098.png",
      "roomIds": [
        10112,
        10113,
        10111
      ]
    }
  ],
  "rooms": [
    {
      "id": 10112,
      "name": "BCC B08",
      "typeId": 10526,
      "setup": "Special"
    },
    {
      "id": 10113,
      "name": "BCC B09",
      "typeId": 10514,
      "setup": "Special"
    },
    {
      "id": 10110,
      "name": "BCC C01 (Auditorium)",
      "typeId": 10519,
      "setup": "Special"
    },
    {
      "id": 10111,
      "name": "BCC Foyer",
      "typeId": 10524,
      "setup": "Special"
    },
    {
      "id": 10109,
      "name": "Berlin Convention Center Foyer",
      "typeId": 10514,
      "setup": "Special"
    },
    {
      "id": 10160,
      "name": "Hasso Plattner Institute",
      "typeId": 10495,
      "setup": "Special"
    },
    {
      "id": 10163,
      "name": "Kulturbrauerei",
      "typeId": 10520,
      "setup": "Special"
    },
    {
      "id": 10162,
      "name": "On your own",
      "typeId": 10514,
      "setup": "Special"
    }
  ],
  "tracks": [
    {
      "id": 10036,
      "typeId": 10484
    },
    {
      "id": 10037,
      "typeId": 11151
    },
    {
      "id": 10038,
      "typeId": 11170
    },
    {
      "id": 10039,
      "typeId": 11199
    },
    {
      "id": 10040,
      "typeId": 11239
    },
    {
      "id": 10041,
      "typeId": 11253
    }
  ],
  "contentTypes": [
    {
      "id": 10194,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 10191,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 10192,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 10193,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 10195,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Invited Talks"
    },
    {
      "id": 10196,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 10197,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90,
      "displayName": "Panels"
    },
    {
      "id": 10198,
      "name": "Paper",
      "color": "#08519c",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 10199,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 90
    },
    {
      "id": 10200,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 10520,
      "name": "Banquet",
      "duration": 0
    },
    {
      "id": 10514,
      "name": "Break",
      "duration": 0
    },
    {
      "id": 10507,
      "name": "Demo Reception",
      "duration": 0
    },
    {
      "id": 10484,
      "name": "Doctoral Symposium",
      "duration": 0
    },
    {
      "id": 10526,
      "name": "Papers",
      "duration": 0
    },
    {
      "id": 10524,
      "name": "Poster Session & Coffee Break",
      "duration": 0
    },
    {
      "id": 10485,
      "name": "Registration",
      "duration": 0
    },
    {
      "id": 10519,
      "name": "Vision Track",
      "duration": 0
    },
    {
      "id": 10495,
      "name": "Welcome Reception",
      "duration": 0
    },
    {
      "id": 11151,
      "name": "Demonstration",
      "duration": 0
    },
    {
      "id": 11170,
      "name": "Full Paper",
      "duration": 0
    },
    {
      "id": 11253,
      "name": "Journal",
      "duration": 0,
      "displayName": "Journals"
    },
    {
      "id": 11199,
      "name": "Poster",
      "duration": 0,
      "displayName": "Posters"
    },
    {
      "id": 11239,
      "name": "UIST Vision",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 10687,
      "type": "SESSION",
      "startDate": 1539507600000,
      "endDate": 1539540000000
    },
    {
      "id": 10688,
      "type": "SESSION",
      "startDate": 1539538200000,
      "endDate": 1539550800000
    },
    {
      "id": 10703,
      "type": "BREAK",
      "startDate": 1539593100000,
      "endDate": 1539595800000
    },
    {
      "id": 10704,
      "type": "SESSION",
      "startDate": 1539595800000,
      "endDate": 1539597600000
    },
    {
      "id": 10705,
      "type": "SESSION",
      "startDate": 1539597600000,
      "endDate": 1539601200000
    },
    {
      "id": 10706,
      "type": "BREAK",
      "startDate": 1539601200000,
      "endDate": 1539603000000
    },
    {
      "id": 10708,
      "type": "SESSION",
      "startDate": 1539603000000,
      "endDate": 1539607800000
    },
    {
      "id": 10709,
      "type": "BREAK",
      "startDate": 1539607800000,
      "endDate": 1539612600000
    },
    {
      "id": 10710,
      "type": "BREAK",
      "startDate": 1539608400000,
      "endDate": 1539611400000
    },
    {
      "id": 10711,
      "type": "SESSION",
      "startDate": 1539612600000,
      "endDate": 1539618600000
    },
    {
      "id": 10713,
      "type": "BREAK",
      "startDate": 1539618600000,
      "endDate": 1539620400000
    },
    {
      "id": 10715,
      "type": "SESSION",
      "startDate": 1539620400000,
      "endDate": 1539625200000
    },
    {
      "id": 10716,
      "type": "SESSION",
      "startDate": 1539625500000,
      "endDate": 1539626400000
    },
    {
      "id": 10717,
      "type": "SESSION",
      "startDate": 1539626400000,
      "endDate": 1539637200000
    },
    {
      "id": 10718,
      "type": "BREAK",
      "startDate": 1539679500000,
      "endDate": 1539680400000
    },
    {
      "id": 10720,
      "type": "SESSION",
      "startDate": 1539680400000,
      "endDate": 1539686400000
    },
    {
      "id": 10721,
      "type": "BREAK",
      "startDate": 1539686400000,
      "endDate": 1539688200000
    },
    {
      "id": 10723,
      "type": "SESSION",
      "startDate": 1539688200000,
      "endDate": 1539694200000
    },
    {
      "id": 10724,
      "type": "BREAK",
      "startDate": 1539694200000,
      "endDate": 1539699600000
    },
    {
      "id": 10727,
      "type": "SESSION",
      "startDate": 1539699600000,
      "endDate": 1539705600000
    },
    {
      "id": 10728,
      "type": "SESSION",
      "startDate": 1539705600000,
      "endDate": 1539709200000
    },
    {
      "id": 10729,
      "type": "SESSION",
      "startDate": 1539709200000,
      "endDate": 1539712800000
    },
    {
      "id": 10730,
      "type": "SESSION",
      "startDate": 1539716400000,
      "endDate": 1539734340000
    },
    {
      "id": 10731,
      "type": "BREAK",
      "startDate": 1539765900000,
      "endDate": 1539766800000
    },
    {
      "id": 10732,
      "type": "SESSION",
      "startDate": 1539766800000,
      "endDate": 1539772800000
    },
    {
      "id": 10734,
      "type": "SESSION",
      "startDate": 1539771600000,
      "endDate": 1539775200000
    },
    {
      "id": 10735,
      "type": "SESSION",
      "startDate": 1539775200000,
      "endDate": 1539780600000
    },
    {
      "id": 10737,
      "type": "BREAK",
      "startDate": 1539780000000,
      "endDate": 1539784800000
    },
    {
      "id": 10738,
      "type": "SESSION",
      "startDate": 1539783000000,
      "endDate": 1539784800000
    },
    {
      "id": 10739,
      "type": "SESSION",
      "startDate": 1539786000000,
      "endDate": 1539792000000
    },
    {
      "id": 10741,
      "type": "BREAK",
      "startDate": 1539790800000,
      "endDate": 1539793800000
    },
    {
      "id": 10742,
      "type": "SESSION",
      "startDate": 1539793800000,
      "endDate": 1539797400000
    },
    {
      "id": 10743,
      "type": "SESSION",
      "startDate": 1539797400000,
      "endDate": 1539799200000
    }
  ],
  "sessions": [
    {
      "id": 1992,
      "name": "Doctoral Symposium (by invitation only)",
      "typeId": 10484,
      "roomId": 10160,
      "chairIds": [],
      "contentIds": [
        6930,
        5952,
        5692,
        4288,
        8175,
        4919,
        4296,
        3481
      ],
      "timeSlotId": 10687
    },
    {
      "id": 1134,
      "name": "Registration",
      "typeId": 10485,
      "roomId": 10160,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10688
    },
    {
      "id": 2246,
      "name": "Welcome Reception",
      "typeId": 10495,
      "roomId": 10160,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10688
    },
    {
      "id": 2318,
      "name": "Refreshments",
      "typeId": 10514,
      "roomId": 10109,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10703
    },
    {
      "id": 2383,
      "name": "Break",
      "typeId": 10514,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10706
    },
    {
      "id": 1866,
      "name": "Opening Remarks + Awards",
      "typeId": 10199,
      "roomId": 10110,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10704
    },
    {
      "id": 2505,
      "name": "Keynote: Jaime Teevan",
      "typeId": 10199,
      "roomId": 10110,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10705
    },
    {
      "id": 1872,
      "name": "Controlling and Collaborating in VR",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        19572
      ],
      "contentIds": [
        6836,
        8154,
        7494,
        6697
      ],
      "timeSlotId": 10708
    },
    {
      "id": 2519,
      "name": "Human-Robot Symbiosis",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        16612
      ],
      "contentIds": [
        3723,
        2936,
        5566,
        4286
      ],
      "timeSlotId": 10708
    },
    {
      "id": 1326,
      "name": "Lunch",
      "typeId": 10514,
      "roomId": 10162,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10709
    },
    {
      "id": 2302,
      "name": "Becoming a Volunteer at ACM SIGCHI Lunchtime event",
      "typeId": 10514,
      "roomId": 10113,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10710
    },
    {
      "id": 1524,
      "name": "Crowds and Human-AI Partnership",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        20000
      ],
      "contentIds": [
        8149,
        2988,
        6523,
        6937,
        4842
      ],
      "timeSlotId": 10711
    },
    {
      "id": 1903,
      "name": "Fabrication",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        10127
      ],
      "contentIds": [
        3497,
        5256,
        7939,
        4092,
        4034
      ],
      "timeSlotId": 10711
    },
    {
      "id": 2412,
      "name": "Break",
      "typeId": 10514,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10713
    },
    {
      "id": 1395,
      "name": "Sensing and Acoustics",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        8199
      ],
      "contentIds": [
        8172,
        5776,
        7829,
        3771
      ],
      "timeSlotId": 10715
    },
    {
      "id": 1204,
      "name": "Visualizations in 2D and 3D",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        15383
      ],
      "contentIds": [
        7346,
        7094,
        6771,
        5187
      ],
      "timeSlotId": 10715
    },
    {
      "id": 1802,
      "name": "UIST Lasting Impact Award",
      "typeId": 10199,
      "roomId": 10110,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10716
    },
    {
      "id": 1860,
      "name": "Demo Reception",
      "typeId": 10507,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [
        7895,
        7354,
        5822,
        4659,
        6883,
        2966,
        7481,
        5619,
        6794,
        6097,
        4649,
        6986,
        5106,
        4847,
        7297,
        3304,
        4148,
        6715,
        5175,
        6215,
        6308,
        8059,
        5725,
        3624,
        5887,
        6567,
        3181,
        4654,
        6665,
        6873,
        5219,
        5273,
        4837,
        4916,
        4877,
        5313,
        7577,
        2980,
        4233,
        5206,
        4807,
        6349,
        5814,
        3016,
        5741,
        7945,
        3869,
        6271,
        3668,
        6990,
        7849,
        4475,
        4734,
        4503,
        6091,
        3922,
        7729,
        2945,
        3854,
        7274
      ],
      "timeSlotId": 10717
    },
    {
      "id": 2150,
      "name": "Refreshments",
      "typeId": 10514,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10718
    },
    {
      "id": 1028,
      "name": "Authoring, Reading and Writing",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        12162
      ],
      "contentIds": [
        3874,
        3930,
        3159,
        6772,
        8114
      ],
      "timeSlotId": 10720
    },
    {
      "id": 1006,
      "name": "Sensing in the Small Scale",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        19589
      ],
      "contentIds": [
        8159,
        3038,
        4669,
        3101,
        7083
      ],
      "timeSlotId": 10720
    },
    {
      "id": 1238,
      "name": "Break",
      "typeId": 10514,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10721
    },
    {
      "id": 2218,
      "name": "Electronics",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        8737
      ],
      "contentIds": [
        6297,
        2857,
        4304,
        2951,
        5044
      ],
      "timeSlotId": 10723
    },
    {
      "id": 1612,
      "name": "Navigation",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        9294
      ],
      "contentIds": [
        5456,
        6457,
        6232,
        5287,
        5902
      ],
      "timeSlotId": 10723
    },
    {
      "id": 1871,
      "name": "Lunch",
      "typeId": 10514,
      "roomId": 10162,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10724
    },
    {
      "id": 1231,
      "name": "Women of UIST Networking Luncheon (RSVP only)",
      "typeId": 10514,
      "roomId": 10113,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10724
    },
    {
      "id": 1787,
      "name": "Haptics and VR",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        16950
      ],
      "contentIds": [
        6736,
        4407,
        3343,
        7961,
        6481
      ],
      "timeSlotId": 10727
    },
    {
      "id": 1312,
      "name": "Modelling and Animation",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        23637
      ],
      "contentIds": [
        4444,
        4780,
        6539,
        8173,
        2979
      ],
      "timeSlotId": 10727
    },
    {
      "id": 2227,
      "name": "Poster Session & Coffee Break",
      "typeId": 10524,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [
        7217,
        6294,
        5852,
        5292,
        7653,
        7821,
        8130,
        8019,
        4248,
        6333,
        4617,
        6917,
        6234,
        7474,
        7349,
        2852,
        4325,
        2725,
        7471,
        3296,
        3948,
        7438,
        5407,
        7725,
        3075,
        6176,
        2694,
        4132,
        3013,
        6749,
        7396,
        3329,
        3118,
        6987,
        5628,
        3903,
        3180,
        5009,
        8133,
        7036,
        4948,
        4263,
        3230,
        6036
      ],
      "timeSlotId": 10728
    },
    {
      "id": 2162,
      "name": "Vision Track",
      "typeId": 10519,
      "roomId": 10110,
      "chairIds": [
        22789
      ],
      "contentIds": [
        5279,
        7941
      ],
      "timeSlotId": 10729
    },
    {
      "id": 1151,
      "name": "Conference Dinner and Student Innovation Contest",
      "typeId": 10520,
      "roomId": 10163,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10730
    },
    {
      "id": 2186,
      "name": "Refreshments",
      "typeId": 10514,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10731
    },
    {
      "id": 1781,
      "name": "Bodies and Sensing",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        11153
      ],
      "contentIds": [
        6732,
        3836,
        6373,
        3725
      ],
      "timeSlotId": 10732
    },
    {
      "id": 1893,
      "name": "Novel Haptics",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        10226
      ],
      "contentIds": [
        6377,
        3783,
        7809,
        3094
      ],
      "timeSlotId": 10732
    },
    {
      "id": 1236,
      "name": "Poster Session & Coffee Break",
      "typeId": 10524,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10734
    },
    {
      "id": 2277,
      "name": "Touch Interaction",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        9782
      ],
      "contentIds": [
        2865,
        6828,
        5963,
        4666
      ],
      "timeSlotId": 10735
    },
    {
      "id": 2384,
      "name": "VR Interaction Techniques",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        9205
      ],
      "contentIds": [
        4827,
        4229,
        6575,
        7150
      ],
      "timeSlotId": 10735
    },
    {
      "id": 1261,
      "name": "Lunch",
      "typeId": 10514,
      "roomId": 10162,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10737
    },
    {
      "id": 2223,
      "name": "UIST Town hall",
      "typeId": 10199,
      "roomId": 10110,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10738
    },
    {
      "id": 1873,
      "name": "Mobile Interactions",
      "typeId": 10526,
      "roomId": 10112,
      "chairIds": [
        14435
      ],
      "contentIds": [
        5648,
        2681,
        4689,
        6264,
        3380
      ],
      "timeSlotId": 10739
    },
    {
      "id": 1623,
      "name": "Web",
      "typeId": 10526,
      "roomId": 10110,
      "chairIds": [
        11434
      ],
      "contentIds": [
        4149,
        7784,
        7891,
        7144,
        5816
      ],
      "timeSlotId": 10739
    },
    {
      "id": 2178,
      "name": "Break",
      "typeId": 10514,
      "roomId": 10111,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10741
    },
    {
      "id": 1048,
      "name": "Keynote: Selma Šabanović",
      "typeId": 10199,
      "roomId": 10110,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10742
    },
    {
      "id": 1122,
      "name": "Closing Ceremony",
      "typeId": 10199,
      "roomId": 10110,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10743
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 3075,
      "typeId": 11199,
      "title": "Kaleidoscope: An RDF-based Exploratory Data Analysis Tool for Ideation Outcomes",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Evaluating and selecting ideas is a critical and time-consuming step in collaborative ideation, making computational support for this task a desired research goal. However, existing automatic approaches to idea selection might eliminate valuable ideas.\nIn this work we combine automatic approaches with human sensemaking. Kaleidoscope is an exploratory data analytics tool based on semantic technologies. It supports users in exploring and annotating existing ideas interactively. In the following, we present key design principles of Kaleidoscope. Based on qualitative feedback collected on a prototype, we identify potential improvements and describe future work.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Freie Universität Berlin"
            }
          ],
          "personId": 10529
        },
        {
          "affiliations": [
            {
              "institution": "Freie Universität Berlin"
            }
          ],
          "personId": 22729
        },
        {
          "affiliations": [
            {
              "institution": "Freie Universität Berlin"
            }
          ],
          "personId": 9681
        },
        {
          "affiliations": [
            {
              "institution": "Freie Universität Berlin"
            }
          ],
          "personId": 11940
        },
        {
          "affiliations": [
            {
              "institution": "Freie Universität Berlin"
            }
          ],
          "personId": 22205
        },
        {
          "affiliations": [
            {
              "institution": "Freie Universität Berlin"
            }
          ],
          "personId": 10624
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 6665,
      "typeId": 11151,
      "title": "DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present a new haptic device that enables blind users to continuously interact with spatial virtual environments that contain moving objects, as is the case in sports or shooter games. Users interact with DualPanto by operating the me handle with one hand and by holding on to the it handle with the other hand. Each handle is connected to a pantograph haptic input/output device. The key feature is that the two handles are spatially registered with respect to each other. When guiding their avatar through a virtual world using the me handle, spatial registration enables users to track moving objects by having the device guide the output hand. This allows blind players of a 1-on-1 soccer game to race for the ball or evade an opponent; it allows blind players of a shooter game to aim at an opponent and dodge shots. In our user study, blind participants reported very high enjoyment when using the device to play (6.5/7).",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute & University of Waterloo"
            }
          ],
          "personId": 10548
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 18233
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 24389
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 13294
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 8413
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 15092
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 14908
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 22160
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4617,
      "typeId": 11199,
      "title": "OmniEyeball: Spherical Display Equipped With Omnidirectional Camera And Its Application For 360-Degree Video Communication",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We propose OmniEyeball (OEB), which is a novel interactive 360° image I/O system. It integrates the spherical display system with an omnidirectional camera to enable both capturing the 360° panoramic live streaming video as well as displaying it. We also present its unique application for symmetric 360° video communication by utilizing two OEB terminals, which may solve the narrow field-of-view problem in video communication. In addition, we designed a vision-based touch detection technique as well as some features to support 360° video communication.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 22209
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 11330
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 20684
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 8645
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 10342
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5648,
      "typeId": 11170,
      "title": "Ultra-Low-Power Mode for Screenless Mobile Interaction",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphones are now a central technology in the daily lives of billions, but it relies on its battery to perform. Battery optimization is thereby a crucial design constraint in any mobile OS and device. However, even with new low-power methods, the ever-growing touchscreen remains the most power-hungry component. We propose an Ultra-Low-Power Mode (ULPM) for mobile devices that allows for touch interaction without visual feedback and exhibits significant power savings of up to 60\\% while allowing to complete interactive tasks. We demonstrate the effectiveness of the screenless ULPM in text-entry tasks, camera usage, and listening to videos, showing only a small decrease in usability for typical users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 21399
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 10880
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 8989
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 21020
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 12240
        }
      ],
      "sessionIds": [
        1873
      ],
      "eventIds": []
    },
    {
      "id": 3094,
      "typeId": 11170,
      "title": "ElectricItch: Skin Irritation as a Feedback Modality",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Grabbing users' attention is a fundamental aspect of interactive systems. However, there is a disconnect between the ways our devices notify us and how our bodies do so naturally. In this paper, we explore the body's modality of itching as a way to provide such natural feedback. We create itching sensations via low-current electric stimulation, which allows us to quickly generate this sensation on demand. In a first study we explore the design space around itching and how changes in stimulation parameters influence the resulting sensation. In a second study we compare vibration feedback and itching integrated in a smartwatch form factor. We find that we can consistently induce itching sensations and that these are perceived as more activating and interrupting than vibrotactile stimuli.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Copenhagen"
            }
          ],
          "personId": 12453
        },
        {
          "affiliations": [
            {
              "institution": "University of Copenhagen"
            }
          ],
          "personId": 18824
        }
      ],
      "sessionIds": [
        1893
      ],
      "eventIds": []
    },
    {
      "id": 3101,
      "typeId": 11170,
      "title": "FingerArc and FingerChord: Supporting Novice to Expert Transitions with Guided Finger-Aware Shortcuts",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Keyboard shortcuts can be more efficient than graphical input, but they are underused by most users. To alleviate this, we present \"Guided Finger-Aware Shortcuts\" to reduce the gulf between graphical input and shortcut activation. The interaction technique works by recognising when a special hand posture is used to press a key, then allowing secondary finger movements to select among related shortcuts if desired. Novice users can learn the mappings through dynamic visual guidance revealed by holding a key down, but experts can trigger shortcuts directly without pausing. Two variations are described: FingerArc uses the angle of the thumb, and FingerChord uses a second key press. The techniques are motivated by an interview study identifying factors hindering the learning, use, and exploration of keyboard shortcuts. A controlled comparison with conventional keyboard shortcuts shows the techniques encourage overall shortcut usage, make interaction faster, less error-prone, and provide advantages over simply adding visual guidance to standard shortcuts.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Waterloo & Google"
            }
          ],
          "personId": 9332
        },
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 11351
        },
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 20072
        },
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 14435
        }
      ],
      "sessionIds": [
        1006
      ],
      "eventIds": []
    },
    {
      "id": 6176,
      "typeId": 11199,
      "title": "Perceptual Switch for Gaze Selection",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "One of the main drawbacks of the fixation-based gaze interfaces is that they are unable to distinguish top-down attention (or selection, a gaze with a purpose) from stimulus driven bottom-up attention (or navigation, a stare without any intentions) without time durations or unnatural eye movements. We found that using the bistable image called the Necker's cube as a button user interface (UI) helps to remedy the limitation. When users switch two rivaling percepts of the Necker's cube at will, unique eye movements are triggered and these characteristics can be used to indicate a button press or a selecting action. In this paper, we introduce (1) the cognitive phenomenon called \"percept switch\" for gaze interaction, and (2) propose \"perceptual switch\" or the Necker's cube user interface (UI) which uses \"percept switch\" as the indication of a selection. Our preliminary experiment confirms that perceptual switch can be used to distinguish voluntary gaze selection from random navigation, and discusses that the visual elements of the Necker's cube such as size and biased visual cues could be adjusted for the optimal use of individual users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Yonsei University"
            }
          ],
          "personId": 15781
        },
        {
          "affiliations": [
            {
              "institution": "Yonsei University"
            }
          ],
          "personId": 17762
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 4132,
      "typeId": 11199,
      "title": "reMi: Translating Ambient Sounds of Moment into Tangible and Shareable Memories through Animated Paper",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We present a tangible memory notebook--reMi--that records the ambient sounds and translates them into a tangible and shareable memory using animated paper. The paper replays the recorded sounds and deforms its shape to generate synchronized motions with the sounds. Computer-mediated communication interfaces have allowed us to share, record and recall memories easily through visual records. However, those digital visual-cues that are trapped behind the device's 2D screen are not the only means to recall a memory we experienced with more than the sense of vision. To develop a new way to store, recall and share a memory, we investigate how tangible motion of a paper that represents sound can enhance the \"reminiscence\".",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Massacshuetts Institute of Technology"
            }
          ],
          "personId": 9933
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 19125
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 18865
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 9045
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 20129
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3624,
      "typeId": 11151,
      "title": "FacePush: Introducing Normal Force on Face with Head-Mounted Displays",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents FacePush, a Head-Mounted Display (HMD) integrated with a pulley system to generate normal forces on a user’s face in virtual reality (VR). The mechanism of FacePush is obtained by shifting torques provided by two motors that press upon a user’s face via utilization of a pulley system. FacePush can generate normal forces of varying strengths and apply those to the surface of the face. To inform our design of FacePush for noticeable and discernible normal forces in VR applications, we conducted two studies to iden- tify the absolute detection threshold and the discrimination threshold for users’ perception. After further consideration in regard to user comfort, we determined that two levels of force, 2.7 kPa and 3.375 kPa, are ideal for the development of the FacePush experience via implementation with three applications which demonstrate use of discrete and continuous normal force for the actions of boxing, diving, and 360 guidance in virtual reality. In addition, with regards to a virtual boxing application, we conducted a user study evaluating the user experience in terms of enjoyment and realism and collected the user’s feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 21737
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 15049
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 15070
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 22175
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 24154
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 22293
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4649,
      "typeId": 11151,
      "title": "Screen–Camera Communication via Matrix Barcode Utilizing Imperceptible Color Vibration",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Communication between screens and cameras has attracted attention as a ubiquitous information source, motivated by the widespread use of smartphones and the increase of public advertising and information screens. We propose embedding matrix barcodes into images projected on displays by utilizing imperceptible color vibration. This approach maintains the visual experience as the barcodes are imperceptible and can be implemented on almost any display and camera for the technology to be pervasive. In fact, the color vibration can be generated by ordinary 60 Hz LCDs and captured by 120 fps smartphone cameras. To illustrate the technology capabilities, we present scenarios of potential practical applications.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "abe@nae-lab.org"
            }
          ],
          "personId": 10692
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6697,
      "typeId": 11170,
      "title": "Wall-based Space Manipulation Technique for Efficient Placement of Distant Objects in Augmented Reality",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present a wall-based space manipulation (WSM) technique that enables users to efficiently select and move distant objects by dynamically squeezing their surrounding space in augmented reality. Users can bring a target object closer by dragging a solid plane behind the object and squeezing the space between them and the plane so that they can select and move the object more delicately and efficiently. We furthermore discuss the unique design challenges of WSM, including the dimension of space reduction and the recognition of the reduced space in relation to the real space. We conducted a user evaluation to verify how WSM improves the performance of the hand-centered object manipulation technique on the HoloLens for moving near objects far away and vice versa. The results indicate that WSM overall performed consistently well and significantly improved efficiency while alleviating arm fatigue.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 8337
        },
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 21979
        },
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 24085
        }
      ],
      "sessionIds": [
        1872
      ],
      "eventIds": []
    },
    {
      "id": 7725,
      "typeId": 11199,
      "title": "Sense.Seat: Inducing Improved Mood and Cognition through Multisensorial Priming",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "User interface software and technologies have been evolving significantly and rapidly. This poster presents a breakthrough user experience that leverages multisensorial priming and embedded interaction and introduces an interactive piece of furniture called Sense.Seat. Sensory stimuli such as calm colors, lavender and other scents as well as ambient soundscapes have been traditionally used to spark creativity and promote well-being. Sense.Seat is the first computational multisensorial seat that can be digitally controlled and vary the frequency and intensity of visual, auditory and olfactory stimulus. It is a new user interface shaped as a seat or pod that primes the user for inducing improved mood and cognition, therefore improving the work environment.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Madeira-ITI, University of Madeira"
            }
          ],
          "personId": 24329
        },
        {
          "affiliations": [
            {
              "institution": "Madeira-ITI, University of Madeira"
            }
          ],
          "personId": 22102
        },
        {
          "affiliations": [
            {
              "institution": "Madeira-ITI, University of Madeira"
            }
          ],
          "personId": 23183
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3118,
      "typeId": 11199,
      "title": "DisplayBowl: A Bowl-Shaped Display for Omnidirectional Videos",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce DisplayBowl which is a concept of a bowl shaped hemispherical display for showing omnidirectional images. This display provides three-way observation for omnidirectional images. DisplayBowl allows users to observe an omnidirectional image by looking the image from above. In addition, users can see it with a first-person-viewpoint, by looking into the inside of the hemispherical surface from diagonally above. Furthermore, by observing both the inside and the outside of the hemispherical surface at the same time from obliquely above, it is possible to observe it by a pseudo third-person-viewpoint, like watching the drone obliquely from behind. These ways of viewing solve the problem of inability of pilots controlling a remote vehicle such as a drone to notice what happens behind them, which happen with conventional displays such as flat displays and head mounted displays.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 11330
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 14811
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 20684
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 10342
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 4654,
      "typeId": 11151,
      "title": "SilentVoice: Unnoticeable Voice Input by Ingressive Speech",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "SilentVoice is a new voice input interface device that penetrates the speech-based natural user interface (NUI) in daily life. The proposed \"ingressive speech\" method enables placement of a microphone very close to the front of the mouth without suffering from pop-noise, capturing very soft speech sounds with a good S/N ratio. It realizes ultra-small (less than 39dB(A)) voice leakage, allowing us to use voice input without annoying surrounding people in public and mobile situations as well as offices and homes. By measuring airflow direction, SilentVoice can easily be separated from normal utterances with 98.8% accuracy; no activation words are needed. It can be used for voice-activated systems with a specially trained voice recognizer; evaluation results yield word error rates (WERs) of 1.8% (speaker-dependent condition), and 7.0% (speaker-independent condition) with a limited dictionary of 85 command sentences. A whisper-like natural voice can also be used for real-time voice communication.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 16737
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 7217,
      "typeId": 11199,
      "title": "Pushables: A DIY Approach for Fabricating Customizable and Self-Contained Tactile Membrane Dome Switches",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Momentary switches are important building blocks to prototype novel physical user interfaces and enable tactile, explicit and eyes-free interactions. Unfortunately, typical representatives, such as push-buttons or pre-manufactured membrane switches, often do not fulfill individual design requirements and lack customization options for rapid prototyping. With this work, we present Pushables, a DIY fabrication approach for producing thin, bendable and highly customizable membrane dome switches. Therefore, we contribute a three-stage fabrication pipeline that describes the production and assembly on the basis of prototyping methods with different skill levels making our approach suitable for technology-enthusiastic makers, researchers, fab labs and others who require custom membrane switches in small quantities. To demonstrate the wide applicability of Pushables, we present application examples from ubiquitous, mobile and wearable computing.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Technische Universität Dresden"
            }
          ],
          "personId": 9346
        },
        {
          "affiliations": [
            {
              "institution": "Technische Universität Dresden"
            }
          ],
          "personId": 18479
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 7729,
      "typeId": 11151,
      "title": "[Sponsor Demo] Adobe",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4659,
      "typeId": 11151,
      "title": "Face/On: Actuating the Facial Contact Area of a Head-Mounted Display for Increased Immersion",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "In this demonstration, we introduce Face/On, an embedded feedback device that leverages the contact area between the user's face and a virtual reality (VR) head-mounted display (HMD) to provide rich haptic feedback in virtual environments (VEs). Head-worn haptic feedback devices have been explored in previous work to provide directional cues via grids of actuators and localized feedback on the users' skin. Most of these solutions were immersion breaking due to their encumbering and uncomfortable design and build around a single actuator type, thus limiting the overall fidelity and flexibility of the haptic feedback. We present Face/On, a VR HMD face cushion with three types of discreetly embedded actuators that provide rich haptic feedback without encumbering users with invasive instrumentation on the body. By combining vibro-tactile and thermal feedback with electrical muscle stimulation (EMS), Face/On can simulate a wide range of scenarios and benefit from synergy effects between these feedback types.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 19162
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 23295
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 17172
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4148,
      "typeId": 11151,
      "title": "Artificial Motion Guidance: an Intuitive Device based on Pneumatic Gel Muscle (PGM)",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present a wearable soft exoskeleton sleeve based on PGM. The sleeve consists of 4 PGMs is controlled by a computing system and can actuate 4 different movements (hand extension, flexion, pronation and supination). Depending on how strong the actuation is, the user feels a slight force (haptic feedback) or the hand moves (if the users relaxes the muscles). The paper gives details about the system implementation, the interaction space and some ideas about application scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Graduate School of Media Design, Keio University"
            }
          ],
          "personId": 24394
        },
        {
          "affiliations": [
            {
              "institution": "Graduate School of Engineering, Hiroshima University"
            }
          ],
          "personId": 18185
        },
        {
          "affiliations": [
            {
              "institution": "Graduate School of Engineering, Hiroshima University"
            }
          ],
          "personId": 19782
        },
        {
          "affiliations": [
            {
              "institution": "Graduate School of Media Design, Keio University"
            }
          ],
          "personId": 23696
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4149,
      "typeId": 11170,
      "title": "Arboretum and Arbility: Improving Web Accessibility Through a Shared Browsing Architecture",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Many web pages developed today require navigation by visual interaction—seeing,  hovering, pointing,  clicking, and dragging with the mouse over dynamic page content.  These forms of interaction are increasingly popular as developer trends have moved from static, logically structured pages to dynamic, interactive pages. However, they are also often inaccessible to blind web users who tend to rely on keyboard-based  screen  readers  to  navigate  the  web. Despite  existing web accessibility standards, engineering web pages to be equally accessible via both keyboard and visuomotor mouse-based interactions is often not a priority for developers. Improving access to this kind of visual and interactive web content has been a long-standing  goal of HCI researchers, but the barriers have proven to be too varied and unpredictable to be overcome by some of the proposed solutions: promoting  guidelines and  best  practices,  automatically  generating accessible versions of pre-exisiting web pages, or developing human-assisted solutions, such as screen and cursor-sharing, which  tend  to  diminish  an  end  user’s  agency. In this paper we present a real-time, collaborative approach to helping blind web users overcome inaccessible parts of existing web pages.  We introduce *Arboretum*, a new architecture that enables  any  web user to seamlessly hand off controlled parts of their browsing session to remote users, while maintaining control over the interface via a \"propose and accept/reject\" mechanism. We  illustrate  the  benefit of Arboretum by using it to implement *Arbility*, a browser that allows blind users to hand off targeted visual interaction tasks to remote crowd workers. We  evaluate the entire system in a study  with 9 blind web users, showing that Arbility allows them to interact with web content that was previously difficult to access via a screen reader alone.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 12162
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 20202
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 22726
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 20642
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 23449
        }
      ],
      "sessionIds": [
        1623
      ],
      "eventIds": []
    },
    {
      "id": 5175,
      "typeId": 11151,
      "title": "An Interactive Pipeline for Creating Visual Blends",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Visual blends are an advanced graphic design technique to draw users' attention to a message. They blend together two objects in a way that is novel and useful in conveying a message symbolically. This demo presents an interactive pipeline for creating visual blends that follows the iterative design process. Our pipeline decomposes the process into both computational techniques and human microtasks. It allows users to collaboratively generate visual blends with steps involving brainstorming, synthesis, and iteration. Our demo allows individual users to see how existing visual blends were made, edit or improve existing visual blends, and create new visual blends.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Columbia University"
            }
          ],
          "personId": 24187
        },
        {
          "affiliations": [
            {
              "institution": "Columbia University"
            }
          ],
          "personId": 22243
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 17780
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4666,
      "typeId": 11170,
      "title": "Unimanual Pen+Touch Input Using Variations of Precision Grip Postures",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce a new pen input space by forming postures with the same hand that also grips the pen while writing, drawing, or selecting. The postures contact the multitouch surface around the pen to enable detection without special sensors. A formative study investigates the effectiveness, accuracy, and comfort of 33 candidate postures in controlled tasks. The results indicate a useful subset of postures. Using raw capacitive sensor data captured in the study, a convolutional neural network is trained to recognize 10 postures in real time. This recognizer is used to create application demonstrations for pen-based document annotation and vector drawing. A small usability study shows the approach is feasible.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 17569
        },
        {
          "affiliations": [
            {
              "institution": "Preferred Networks Inc. & University of Waterloo"
            }
          ],
          "personId": 13591
        },
        {
          "affiliations": [
            {
              "institution": "Preferred Networks Inc."
            }
          ],
          "personId": 20822
        },
        {
          "affiliations": [
            {
              "institution": "Preferred Networks Inc."
            }
          ],
          "personId": 21827
        },
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 14435
        }
      ],
      "sessionIds": [
        2277
      ],
      "eventIds": []
    },
    {
      "id": 6715,
      "typeId": 11151,
      "title": "A Demonstration of VRSpinning: Exploring the Design Space of a 1D Rotation Platform to Increase the Perception of Self-Motion in VR",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "In this demonstration we introduce \\spinVR, a seated locomotion approach based around stimulating the user's vestibular system using a rotational impulse to induce the perception of linear self-motion. Currently, most approaches for locomotion in VR use either concepts like teleportation for traveling longer distances or present a virtual motion that creates a visual-vestibular conflict, which is assumed to cause simulator sickness. With our platform we evaluated two designs for using the rotation of a motorized swivel chair to alleviate this, \\emph{wiggle} and \\emph{impulse}. Our evaluation showed that \\emph{impulse}, using short rotation bursts matched with the visual acceleration, can significantly reduce simulator sickness and increase the perception of self-motion compared to no physical motion.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 22688
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 12300
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 23720
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 13792
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 23621
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 17172
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 5692,
      "typeId": 10484,
      "title": "Artistic Vision: Providing Contextual Guidance for Capture-Time Decisions",
      "trackId": 10036,
      "tags": [],
      "keywords": [],
      "abstract": "With the increased popularity of cameras, more and more people are interested in learning photography. People are willing to invest in expensive cameras as a medium for their artistic expression, but few have access to in-person classes. Inspired by critique sessions common in in-person art practice classes, we propose design principles for creative learning. My dissertation research focuses on designing new interfaces and interactions that provide contextual in-camera feedback to aid users in learning visual elements of photography. We interactively visualize results of image processing algorithms as additional information for the user to make more informed and intentional decisions during capture. In this paper, we describe our design principles, and apply these principles in the design of two guided photography interfaces: one to explore lighting options for a portrait, and one to refine contents and composition of a photo.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 12124
        }
      ],
      "sessionIds": [
        1992
      ],
      "eventIds": []
    },
    {
      "id": 4669,
      "typeId": 11170,
      "title": "Touch+Finger: Extending Touch-based User Interface Capabilities with “Idle” Finger Gestures in the Air",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present Touch+Finger, a new interaction technique that augments touch input with multi-finger gestures for rich and expressive interaction. The main idea is that while one finger is engaged in a touch event, a user can leverage the remaining fingers, the “idle” fingers, to perform a variety of hand poses or in-air gestures to extend touch-based user interface capabilities. To fully understand the use of these idle fingers, we constructed a design space based on conventional touch gestures (i.e., single- and multi-touch gestures) and inter- action period (i.e., before and during touch). Considering the design space, we investigated the possible movement of the idle fingers and developed a total of 20 Touch+Finger gestures. Using ring-like devices to track the motion of the idle fingers in the air, we evaluated the Touch+Finger gestures on both recognition accuracy and ease of use. They were classified with a recognition accuracy of over 99% and received positive and negative comments from 8 participants. We suggested 8 interaction techniques with Touch+Finger gestures that demonstrate extended touch-based user interface capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 17964
        },
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 23154
        },
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 23374
        },
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 13072
        },
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 16328
        },
        {
          "affiliations": [
            {
              "institution": "Seoul National University"
            }
          ],
          "personId": 23147
        }
      ],
      "sessionIds": [
        1006
      ],
      "eventIds": []
    },
    {
      "id": 5187,
      "typeId": 11170,
      "title": "Turbulence Ahead - A 3D Web-Based Aviation Weather Visualizer",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Although severe aircraft accidents have been reduced in the last decades, the number of injuries and fatalities caused by turbulence is still rising. Current aviation weather products are unable to provide a holistic and intuitive view of the overall weather situation, especially in terms of turbulence forecasts. This work introduces an interactive 3D prototype developed with a user-centered design approach. The prototype focuses on the visualization of significant weather charts, which are utilized during flight preparation. An online user study is conducted to compare the prototype with today's 2D paper maps. A total of 64 pilots from an internationally operating airline participated in the study. Among the major findings of the study is that the prototype significantly decreased the cognitive load, and enhanced spatial awareness and usability. To determine the spatial awareness, a novel similarity measure for spatial configurations of aviation weather data is introduced.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 10851
        },
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 24369
        },
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 9448
        }
      ],
      "sessionIds": [
        1204
      ],
      "eventIds": []
    },
    {
      "id": 6215,
      "typeId": 11151,
      "title": "A Stretch-Flexible Textile Multitouch Sensor for User Input on Inflatable Membrane Structures & Non-Planar Surfaces",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present a textile sensor, capable of detecting multi-touch and multi-pressure input on non-planar surfaces and demonstrate how such sensors can be fabricated and integrated into pressure stabilized membrane envelopes (i.e. inflatables). Our sensor design is both stretchable and flexible/bendable and can thus conform to a large variety of three-dimensional surface geometries, including shape-changing surfaces. We briefly outline an approach for basic signal acquisition from such sensors and how they can be leveraged to measure internal air-pressure of inflatable objects without a need for specialized air-pressure sensors. We further demonstrate how rigid electronic components can be mounted inside malleable inflatable objects without the need for dedicated enclosures for mechanical protection.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "kristian.gohlke@uni-weimar.de"
            }
          ],
          "personId": 9398
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6732,
      "typeId": 11170,
      "title": "Designing Groundless Body Channel Communication Systems: Performance and Implications",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Novel interactions that capacitively couple electromagnetic (EM) fields between devices and the human body are gaining more attention in the human-computer interaction community. One class of these techniques is Body Channel Communication (BCC), a method that overlays physical touch with digital information. Despite the number of published capacitive sensing and communication prototypes, there exists no guideline on how to design such hardware or what are the application limitations and possibilities. Specifically, wearable (groundless) BCC has been proven in the past to be extremely challenging to implement. Additionally, the exact behavior of the human body as an EM-field medium is still not fully understood today. Consequently, the application domain of BCC technology could not be fully explored.\n\nThis paper addresses this problem. Based on a recently published general purpose wearable BCC system, we first present a thorough evaluation of the impact of various technical parameter choices and an exhaustive channel characterization of the human body as a host for BCC. Second, we discuss the implications of these results for the application design space and present guidelines for future wearable BCC systems and their applications. Third, we point out an important observation of the measurements, namely that BCC can employ the whole body as user interface (and not just hands or feet). We sketch several applications with these novel interaction modalities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "ETH Zurich & Disney Research"
            }
          ],
          "personId": 20990
        },
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 23197
        },
        {
          "affiliations": [
            {
              "institution": "ETH Zurich & Disney Research"
            }
          ],
          "personId": 13900
        },
        {
          "affiliations": [
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 12565
        },
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 15561
        }
      ],
      "sessionIds": [
        1781
      ],
      "eventIds": []
    },
    {
      "id": 6736,
      "typeId": 11170,
      "title": "DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present a new haptic device that enables blind users to continuously interact with spatial virtual environments that contain moving objects, as is the case in sports or shooter games. Users interact with DualPanto by operating the me handle with one hand and by holding on to the it handle with the other hand. Each handle is connected to a pantograph haptic input/output device. The key feature is that the two handles are spatially registered with respect to each other. When guiding their avatar through a virtual world using the me handle, spatial registration enables users to track moving objects by having the device guide the output hand. This allows blind players of a 1-on-1 soccer game to race for the ball or evade an opponent; it allows blind players of a shooter game to aim at an opponent and dodge shots. In our user study, blind participants reported very high enjoyment when using the device to play (6.5/7).",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute & University of Waterloo"
            }
          ],
          "personId": 10548
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 18233
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 24389
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 13294
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 8413
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 15092
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 14908
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 22160
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 13798
        }
      ],
      "sessionIds": [
        1787
      ],
      "eventIds": []
    },
    {
      "id": 4689,
      "typeId": 11170,
      "title": "Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present Lip-Interact, an interaction technique that allows users to issue commands on their smartphone through silent speech. Lip-Interact repurposes the front camera to capture the user's mouth movements and recognize the issued commands with an end-to-end deep learning model. Our system supports 44 commands for accessing both system-level functionalities (launching apps, changing system settings, and handling pop-up windows) and application-level functionalities (integrated operations for two apps). We verify the feasibility of Lip-Interact with three user experiments: evaluating the recognition accuracy, comparing with touch on input efficiency, and comparing with voiced commands with regards to personal privacy and social norms. We demonstrate that Lip-Interact can help users access functionality efficiently in one step, enable one-handed input when the other hand is occupied, and assist touch to make interactions more fluent.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tsinghua University"
            }
          ],
          "personId": 19803
        },
        {
          "affiliations": [
            {
              "institution": "Tsinghua University"
            }
          ],
          "personId": 24198
        },
        {
          "affiliations": [
            {
              "institution": "Tsinghua University"
            }
          ],
          "personId": 9830
        },
        {
          "affiliations": [
            {
              "institution": "Tsinghua University"
            }
          ],
          "personId": 22360
        },
        {
          "affiliations": [
            {
              "institution": "Tsinghua University"
            }
          ],
          "personId": 17348
        }
      ],
      "sessionIds": [
        1873
      ],
      "eventIds": []
    },
    {
      "id": 3668,
      "typeId": 11151,
      "title": "MetaArms: Body Remapping Using Feet-Controlled Artificial Arms",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce MetaArms, wearable anthropomorphic robotic arms and hands with six degrees of freedom operated by the user’s legs and feet. Our overall research goal is to re-imagine what our bodies can do with the aid of wearable robotics using a body-remapping approach. To this end, we present an initial exploratory case study. MetaArms’ two robotic arms are controlled by the user’s feet motion, and the robotic hands can grip objects according to the user’s toes bending. Haptic feedback is also presented on the user’s feet that correlate with the touched objects on the robotic hands, creating a closed-loop system. We present formal and informal evaluations of the system, the former using a 2D pointing task according to Fitts’ Law. The overall throughput for 12 users of the system is reported as 1.01 bits/s (std 0.39). We also present informal feedback from over 230 users. We find that MetaArms demonstrate the feasibility of body-remapping approach in designing robotic limbs that may help us re-imagine what the human body could do.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 12572
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11423
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 15687
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 8508
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 16545
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 5206,
      "typeId": 11151,
      "title": "I/O Braid: Scalable Touch-Sensitive Lighted Cords Using Spiraling, Repeating Sensing Textiles and Fiber Optics",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce I/O Braid, an interactive textile cord with embedded sensing and visual feedback. I/O Braid senses proximity, touch, and twist through a spiraling, repeating braiding topology of touch matrices. This sensing topology is uniquely scalable, requiring only a few sensing lines to cover the whole length of a cord. The same topology allows us to embed fiber optic strands to integrate co-located visual feedback.\n\nWe provide an overview of the enabling braiding techniques, design considerations, and approaches to gesture detection. These allow us to derive a set of interaction techniques, which we demonstrate with different form factors and capabilities. Our applications illustrate how I/O Braid can invisibly augment everyday objects, such as  touch-sensitive headphones and interactive drawstrings on garments, while enabling discoverability and feedback through embedded light sources.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 21636
        },
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 11000
        },
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 17131
        },
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 22640
        },
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 16607
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3159,
      "typeId": 11170,
      "title": "Non-Linear Editing of Text-Based Screencasts",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Screencasts, where recordings of a computer screen are broadcast to a large audience on the web, are becoming popular as an online educational tool. To provide rich interactions with the text within screencasts, there are emerging platforms that support text-based screencasts by recording every character insertion and deletion from the creator and reconstructing its playback on the viewer's screen. However, these platforms lack support for non-linear editing of screencasts, which involves manipulating a sequence of text editing operations. Since text editing operations are tightly coupled in sequence, modifying an arbitrary part of the sequence often creates ambiguity that yields multiple possible results that require user's choice for resolution.\nWe present an editing tool with a non-linear editing algorithm for text-based screencasts. The tool allows users to edit any arbitrary part of a text-based screencast while preserving the overall consistency of the screencast. In an exploratory user study, all subjects successfully carried out a variety of screencast editing tasks using our prototype screencast editor.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science and Technology"
            }
          ],
          "personId": 18817
        },
        {
          "affiliations": [
            {
              "institution": "University of Minnesota"
            }
          ],
          "personId": 9811
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science and Technology"
            }
          ],
          "personId": 21486
        }
      ],
      "sessionIds": [
        1028
      ],
      "eventIds": []
    },
    {
      "id": 6232,
      "typeId": 11170,
      "title": "Increasing Walking in VR using Redirected Teleportation",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Teleportation is a popular locomotion technique that lets users safely navigate beyond the confines of available positional tracking space without inducing VR sickness.\nBecause available walking space is limited and teleportation is faster than walking, a risk with using teleportation is that users might end up abandoning walking input and only relying on teleportation, which is considered detrimental to presence. We present redirected teleportation; an improved version of teleportation that uses iterative non-obtrusive reorientation and repositioning using a portal to redirect the user back to the center of the tracking space, where available walking space is larger. A user study compares the effectiveness, accuracy, and usability of redirected teleportation with regular teleportation using a navigation task in three different environments.  Results show that redirected teleportation allows for a better utilization of available tracking space than regular teleportation, as it requires significantly fewer teleportations, while users walk more and use a larger portion of the available tracking space.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Nevada"
            }
          ],
          "personId": 18822
        },
        {
          "affiliations": [
            {
              "institution": "University of Nevada Reno"
            }
          ],
          "personId": 17284
        },
        {
          "affiliations": [
            {
              "institution": "University of Nevada"
            }
          ],
          "personId": 24063
        },
        {
          "affiliations": [
            {
              "institution": "University of Nevada"
            }
          ],
          "personId": 20102
        }
      ],
      "sessionIds": [
        1612
      ],
      "eventIds": []
    },
    {
      "id": 6234,
      "typeId": 11199,
      "title": "Head Pose Classification by using Body-Conducted Sound",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Vibrations generated by human activity have been used for recognizing human behavior and developing user interfaces; however, it is difficult to estimate static poses that do not generate a vibration. This can be solved using active acoustic sensing; however, this method is not suitable for emitting some vibrations around the head in terms of the influence of audition. Therefore, we propose a method for estimating head poses using body-conducted sound naturally and regularly generated in the human body. The support vector classification recognizes vertical and horizontal directions of the head, and we confirmed the feasibility of the proposed method through experiments.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tokai University"
            }
          ],
          "personId": 16508
        },
        {
          "affiliations": [
            {
              "institution": "Tokai University"
            }
          ],
          "personId": 16988
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 6749,
      "typeId": 11199,
      "title": "A WOZ Study of Feedforward Information on an Ambient Display in Autonomous Cars",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We describe the development and user testing of an ambient display for autonomous vehicles. Instead of providing feedback about driving actions, once executed, it communicates driving decisions in advance, via light signals in passengers\" peripheral vision. This ambient display was tested in an WoZ-based on-the-road-driving simulation of a fully autonomous vehicle.  Findings from a preliminary study with 14 participants suggest that such a display might be particularly useful to communicate upcoming inertia changes for passengers.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Bauhaus-Universität Weimar"
            }
          ],
          "personId": 9232
        },
        {
          "affiliations": [
            {
              "institution": "Bauhaus-Universität Weimar"
            }
          ],
          "personId": 20613
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5725,
      "typeId": 11151,
      "title": "Hybrid Watch User Interfaces: Collaboration Between Electro-Mechanical Components and Analog Materials",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce programmable material and electro-mechanical control to enable a set of hybrid watch user interfaces that symbiotically leverage the joint strengths of electro-mechanical hands and a dynamic watch dial. This approach enables computation and connectivity with existing materials to preserve the inherent physical qualities and abilities of traditional analog watches. We augment the watch's mechanical hands with micro-stepper motors for control, positioning and mechanical expressivity. We extend the traditional watch dial with programmable pigments for non-emissive dynamic patterns. Together, these components enable a unique set of interaction techniques and user interfaces beyond their individual capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "olwal@google.com"
            }
          ],
          "personId": 14193
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 5219,
      "typeId": 11151,
      "title": "TrussFormer: 3D Printing Large Kinetic Structures",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present TrussFormer, an integrated end-to-end system that allows users to 3D print large-scale kinetic structures, i.e., structures that involve motion and deal with dynamic forces. TrussFormer builds on TrussFab, from which it inherits the ability to create static large-scale truss structures from 3D printed connectors and PET bottles. TrussFormer adds movement to these structures by placing linear actuators into them: either manually, wrapped in reusable components called assets, or by demonstrating the intended movement. TrussFormer verifies that the resulting structure is mechanically sound and will withstand the dynamic forces resulting from the motion. To fabricate the design, TrussFormer generates the underlying hinge system that can be printed on standard desktop 3D printers.  We demonstrate TrussFormer with several example objects, including a 6 legged walking robot and a 4m tall animatronics dinosaur with 5 degrees of freedom.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 24389
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 14866
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 16612
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 9919
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 11279
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 22938
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 12216
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 11435
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 7784,
      "typeId": 11170,
      "title": "Fusion: Opportunistic Web Prototyping with UI Mashups",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Modern web development is rife with complexity at all layers, ranging from needing to configure backend services to grappling with frontend frameworks and dependencies. To lower these development barriers, we introduce a technique that enables people to prototype opportunistically by borrowing pieces of desired functionality from across the web without needing any access to their underlying codebases, build environments, or server backends. We implemented this technique in a browser extension called Fusion, which lets users create web UI mashups by extracting components from existing unmodified webpages and hooking them together using transclusion and JavaScript glue code. We demonstrate the generality and versatility of Fusion via a case study where we used it to create seven UI mashups in domains such as programming tools, data science, web design, and collaborative work. Our mashups include replicating portions of prior HCI systems (Blueprint for in-situ code search and DS.js for in-browser data science), extending the p5.js IDE for Processing with real-time collaborative editing, and integrating Python Tutor code visualizations into static tutorials. These UI mashups each took less than 15 lines of JavaScript glue code to create with Fusion.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Rochester"
            }
          ],
          "personId": 12021
        },
        {
          "affiliations": [
            {
              "institution": "University of California, San Diego"
            }
          ],
          "personId": 19262
        }
      ],
      "sessionIds": [
        1623
      ],
      "eventIds": []
    },
    {
      "id": 7274,
      "typeId": 11151,
      "title": "[Sponsor Demo] gTec",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3180,
      "typeId": 11199,
      "title": "One Button to Rule Them All: Rendering Arbitrary Force-Displacement Curves",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Physical buttons provide rich force characteristics during the travel range, which are commonly described in the form of force-displacement curves. These force characteristics play an important role in the users' experiences while pressing a button. However, due to lack of proper tools to dynamically render various force-displacement curves, little literature has tried iterative button design improvement. This paper presents Button Simulator, a low-cost 3D printed physical button capable of displaying any force-displacement curves, with limited average error offset around .034 N. By reading the force-displacement curves of existing push-buttons, we can easily replicate the force characteristics from any buttons onto our Button Simulator. One can even go beyond existing buttons and design non-existent ones as the form of arbitrary force-displacement curves; then use Button Simulator to render the sensation. This project will be open-sourced and the implementation details will be released. Our system can be a useful tool for future researchers, designers, and makers to investigate rich and dynamic button\"s force design.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 10944
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 12069
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 22402
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5741,
      "typeId": 11151,
      "title": "Next-Point Prediction for Direct Touch Using Finite-Time Derivative Estimation",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "End-to-end latency in interactive systems is detrimental to performance and usability, and comes from a combination of hardware and software delays. While these delays are steadily addressed by hardware and software improvements, it is at a decelerating pace. In parallel, short-term input prediction has shown promising results in recent years, in both research and industry, as an addition to these efforts. We describe a new prediction algorithm for direct touch devices based on (i) a state-of-the-art finite-time derivative estimator, (ii) a smoothing mechanism based on input speed, and (iii) a post-filtering of the prediction in two steps. Using both a pre-existing dataset of touch input as benchmark, and subjective data from a new user study, we show that this new predictor outperforms the predictors currently available in the literature and industry, based on metrics that model user-defined negative side-effects caused by input prediction. In particular, we show that our predictor can predict up to 2 or 3 times further than existing techniques with minimal negative side-effects.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 16991
        },
        {
          "affiliations": [
            {
              "institution": "CentraleSupelec"
            }
          ],
          "personId": 14633
        },
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 20026
        },
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 21401
        },
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 17794
        },
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 24344
        },
        {
          "affiliations": [
            {
              "institution": "Université de Lille & Inria"
            }
          ],
          "personId": 17069
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3181,
      "typeId": 11151,
      "title": "Sprout: Crowd-Powered Task Design for Crowdsourcing",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "While crowdsourcing enables data collection at scale, ensuring high-quality data remains a challenge. In particular, effective task design underlies nearly every reported crowdsourcing success, yet remains difficult to accomplish. Task design is hard because it involves a costly iterative process: identifying the kind of work output one wants, conveying this information to workers, observing worker performance, understanding what remains ambiguous, revising the instructions, and repeating the process until the resulting output is satisfactory.\n\nTo facilitate this process, we propose a novel meta-workflow that helps requesters optimize crowdsourcing task designs and Sprout, our open-source tool, which implements this workflow. Sprout improves task designs by (1) eliciting points of confusion from crowd workers, (2) enabling requesters to quickly understand these misconceptions and the overall space of questions, and (3) guiding requesters to improve the task design in response. We report the results of a user study with two labeling tasks demonstrating that requesters strongly prefer Sprout and produce higher-rated instructions compared to current best practices for creating gated instructions (instructions plus a workflow for training and testing workers). We also offer a set of design recommendations for future tools that support crowdsourcing task design.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 16936
        },
        {
          "affiliations": [
            {
              "institution": "Indian Institute of Technology, Delhi"
            }
          ],
          "personId": 16072
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 10743
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6771,
      "typeId": 11170,
      "title": "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Instructors of 3D design workshops for children face many challenges, including maintaining awareness of students’ progress, helping students who need additional attention, and creating a fun experience while still achieving learning goals. To help address these challenges, we developed Maestro, a workshop orchestration system that visualizes students’ progress, automatically detects and draws attention to common challenges faced by students, and provides mechanisms to address common student challenges as they occur. We present the design of Maestro, and the results of a case-study evaluation with an experienced facilitator and 13 children. The facilitator appreciated Maestro’s real-time indications of which students were successfully following her tutorial demonstration, and recognized the system’s potential to “extend her reach” while helping struggling students. Participant interaction data from the study provided support for our follow-along detection algorithm, and the capability to remind students to use 3D navigation.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Manitoba & Autodesk Research"
            }
          ],
          "personId": 21289
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research"
            }
          ],
          "personId": 24070
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research & University of Toronto"
            }
          ],
          "personId": 9877
        },
        {
          "affiliations": [
            {
              "institution": "University of Manitoba"
            }
          ],
          "personId": 9161
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research"
            }
          ],
          "personId": 15020
        }
      ],
      "sessionIds": [
        1204
      ],
      "eventIds": []
    },
    {
      "id": 6772,
      "typeId": 11170,
      "title": "Multitasking with Play Write, a Mobile Microproductivity Writing Tool",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile devices offer people the opportunity to get useful tasks done during time previously thought to be unusable. Because mobile devices have small screens and are often used in divided attention scenarios, people are limited to using them for short, simple tasks; complex tasks like editing a document present significant challenges in this environment. In this paper we demonstrate how a complex task requiring focused attention can be adapted to the fragmented way people work while mobile by decomposing the task into smaller, simpler microtasks. We introduce Play Write, a microproductivity tool that allows people to edit Word documents from their phones via such microtasks. When participants used Play Write while simultaneously watching a video, we found that they strongly preferred its microtask-based editing approach to the traditional editing experience offered by Mobile Word. Play Write made participants feel more productive and less stressed, and they completed more edits with it. Our findings suggest microproductivity tools like Play Write can help people be productive in divided attention scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 18747
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 8429
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research & Google"
            }
          ],
          "personId": 9923
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 13649
        }
      ],
      "sessionIds": [
        1028
      ],
      "eventIds": []
    },
    {
      "id": 6264,
      "typeId": 11170,
      "title": "Self-Powered Gesture Recognition with Ambient Light",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present a self-powered module for gesture recognition that utilizes small, low-cost photodiodes for both energy harvesting and gesture sensing. Operating in the photovoltaic mode, photodiodes harvest energy from ambient light. In the meantime, the instantaneously harvested power from individual photodiodes is monitored and exploited as clues for sensing finger gestures in proximity. Harvested power from all photodiodes are aggregated to drive the whole gesture-recognition module including the micro-controller running the recognition algorithm. We design robust, lightweight algorithm to recognize finger gestures in the presence of ambient light fluctuations. We fabricate two prototypes to facilitate user’s interaction with smart glasses and smart watch. Results show 99.7%/98.3% overall precision/recall in recognizing five gestures on glasses and 99.2%/97.5% precision/recall in recognizing seven gestures on the watch. The system consumes 34.6 µW/74.3 µW for the glasses/watch and thus can be powered by the energy harvested from ambient light. We also test system’s robustness under varying light intensities, light directions, and ambient light fluctuations, where the system maintains high recognition accuracy (> 96%) in all tested settings.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 16919
        },
        {
          "affiliations": [
            {
              "institution": "Darmouth College"
            }
          ],
          "personId": 14668
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 20813
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 8791
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 9146
        }
      ],
      "sessionIds": [
        1873
      ],
      "eventIds": []
    },
    {
      "id": 2681,
      "typeId": 11170,
      "title": "Learning Design Semantics for Mobile Apps",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Recently, researchers have developed black-box approaches to mine design and interaction data from mobile apps. Although the data captured during this interaction mining is descriptive, it does not expose the design semantics of UIs: what elements on the screen mean and how they are used. This paper introduces an automatic approach for generating semantic annotations for mobile app UIs. Through an iterative open coding of 73k UI elements and 720 screens, we contribute a lexical database of 25 types of UI components, 197 text button concepts, and 135 icon classes shared across apps. We use this labeled data to learn code-based patterns to detect UI components and to train a convolutional neural network that distinguishes between icon classes with 94% accuracy. To demonstrate the efficacy of our approach at scale, we compute semantic annotations for the 72k unique UIs in the Rico dataset, assigning labels for 78% of the total visible, non-redundant elements.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana–Champaign"
            }
          ],
          "personId": 22848
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana–Champaign"
            }
          ],
          "personId": 23952
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana–Champaign"
            }
          ],
          "personId": 12103
        },
        {
          "affiliations": [
            {
              "institution": "Argo AI"
            }
          ],
          "personId": 23593
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 13995
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 9731
        }
      ],
      "sessionIds": [
        1873
      ],
      "eventIds": []
    },
    {
      "id": 4734,
      "typeId": 11151,
      "title": "[Best of SUI] Cubic Keyboard for Virtual Reality",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 18337
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6271,
      "typeId": 11151,
      "title": "Immersive Trip Reports",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Since the advent of consumer photography, tourists and hikers have made photo records of their trips to share later. Aside from being kept as memories, photo presentations such as slideshows are also shown to others who have not visited the location to try to convey the experience.However, a slideshow alone is limited in conveying the broader spatial context, and thus the feeling of presence in beautiful natural scenery is lost. We address this by presenting the photographs as part of an immersive experience. We introduce an automated pipeline for aligning photographs with a digital terrain model. From this geographic registration, we produce immersive presentations which are viewed either passively as a video, or interactively in virtual reality. Our experimental evaluation verifies that this new mode of presentation successfully conveys the spatial context of the scene and is enjoyable to users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Brno University of Technology & Adobe Research"
            }
          ],
          "personId": 21511
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 9897
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 22696
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 19763
        },
        {
          "affiliations": [
            {
              "institution": "Brno University of Technology"
            }
          ],
          "personId": 17099
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 7809,
      "typeId": 11170,
      "title": "Haptic Feedback to the Palm and Fingers for Improved Tactile Perception of Large Objects",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "When one manipulates a large or bulky object, s/he utilizes tactile information at both fingers and the palm. Our goal is to efficiently convey contact information to a user’s hand during interaction with a virtual object. We propose a haptic system that can provide haptic feedback to thumb/middle finger/index finger and on a palm. Our interface design utilizes a novel compact mechanism to provide haptic information to the palm. Also, we propose a haptic rendering strategy to calculate haptic feedback continuously. We demonstrate that cutaneous feedback on the palm improves the haptic perception of a large virtual object compared to when there is only kinesthetic feedback to the fingers.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Korea Institute of Science and Technology"
            }
          ],
          "personId": 14458
        },
        {
          "affiliations": [
            {
              "institution": "Korea Institute of Science and Technology"
            }
          ],
          "personId": 13035
        }
      ],
      "sessionIds": [
        1893
      ],
      "eventIds": []
    },
    {
      "id": 7297,
      "typeId": 11151,
      "title": "The Immersive Bubble Chart: a Semantic and Virtual Reality Visualization for Big Data",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we introduce the Immersive Bubble Chart, a visualization for hierarchical datasets presented in a virtual reality (VR) world. Users get immersed into the visualization and interact with the bubbles using gestures with a view to overcoming some limitations of 2D visualizations due to the capabilities and interaction affordances of the devices. The technological advances in VR give the possibility to design malleable and extensible representations and more natural and engaging interactions. Using the Oculus Touch controllers, the users can grab and move the bubbles, throw them away or bump two of them for creating a cluster. We have tested the Immersive Bubble Chart with the hierarchical clusters of semantically related terms generated from Twitter.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Universidad Carlos III de Madrid"
            }
          ],
          "personId": 22093
        },
        {
          "affiliations": [
            {
              "institution": "Universidad Carlos III de Madrid"
            }
          ],
          "personId": 13210
        },
        {
          "affiliations": [
            {
              "institution": "Universidad Carlos III de Madrid"
            }
          ],
          "personId": 20144
        },
        {
          "affiliations": [
            {
              "institution": "Universidad Carlos III de Madrid"
            }
          ],
          "personId": 15347
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4229,
      "typeId": 11253,
      "title": "TOCHI: Fallacies of Agreement: A Critical Review of Consensus Assessment Methods for Gesture",
      "trackId": 10041,
      "tags": [],
      "keywords": [],
      "abstract": "Discovering gestures that gain consensus is a key goal of gesture elicitation. To this end, HCI research has developed statistical methods to reason about agreement. We review these methods and identify three major problems. First, we show that raw agreement rates disregard agreement that occurs by chance and do not reliably capture how participants distinguish among referents. Second, we explain why current recommendations on how to interpret agreement scores rely on problematic assumptions. Third, we demonstrate that significance tests for comparing agreement rates, either within or between participants, yield large Type I error rates (>40% for α =.05). As alternatives, we present agreement indices that are routinely used in inter-rater reliability studies. We discuss how to apply them to gesture elicitation studies. We also demonstrate how to use common resampling techniques to support statistical inference with interval estimates. We apply these methods to reanalyze and reinterpret the findings of four gesture elicitation studies.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Inria, Université Paris-Saclay, and Univ. Paris-Sud"
            }
          ],
          "personId": 13492
        }
      ],
      "sessionIds": [
        2384
      ],
      "eventIds": []
    },
    {
      "id": 2694,
      "typeId": 11199,
      "title": "ZEUSSS: Zero Energy Ubiquitous Sound Sensing Surface Leveraging Triboelectric Nanogenerator and Analog Backscatter Communication",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "ZEUSSS (Zero Energy Ubiquitous Sound Sensing Surface), allows physical objects and surfaces to be instrumented with a thin, self-sustainable material that provides acoustic sensing and communication capabilities. We have built a prototype ZEUSSS tag using minimal hardware and flexible electronic components, extending our original self-sustaining SATURN microphone with a printed, flexible antenna to support passive communication via analog backscatter. ZEUSSS enables objects to have ubiquitous wire-free battery-free audio based context sensing, interaction, and surveillance capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 15513
        },
        {
          "affiliations": [
            {
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 12716
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5256,
      "typeId": 11170,
      "title": "TrussFormer: 3D Printing Large Kinetic Structures",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present TrussFormer, an integrated end-to-end system that allows users to 3D print large-scale kinetic structures, i.e., structures that involve motion and deal with dynamic forces. TrussFormer builds on TrussFab, from which it inherits the ability to create static large-scale truss structures from 3D printed connectors and PET bottles. TrussFormer adds movement to these structures by placing linear actuators into them: either manually, wrapped in reusable components called assets, or by demonstrating the intended movement. TrussFormer verifies that the resulting structure is mechanically sound and will withstand the dynamic forces resulting from the motion. To fabricate the design, TrussFormer generates the underlying hinge system that can be printed on standard desktop 3D printers.  We demonstrate TrussFormer with several example objects, including a 6 legged walking robot and a 4m tall animatronics dinosaur with 5 degrees of freedom.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 24389
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 14866
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 16612
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 9919
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 11279
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 22938
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 12216
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 11435
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 21277
        },
        {
          "affiliations": [
            {
              "institution": "Oregon State University"
            }
          ],
          "personId": 8581
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 13798
        }
      ],
      "sessionIds": [
        1903
      ],
      "eventIds": []
    },
    {
      "id": 4233,
      "typeId": 11151,
      "title": "Authoring and Verifying Human-Robot Interactions",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "As social agents, robots designed for human interaction must adhere to human social norms. How can we enable designers, engineers, and roboticists to design robot behaviors that adhere to human social norms and do not result in interaction breakdowns? In this paper, we use automated formal-verification methods to facilitate the encoding of appropriate social norms into the interaction design of social robots and the detection of breakdowns and norm violations in order to prevent them. We have developed an authoring environment that utilizes these methods to provide developers of social-robot applications with feedback at design time and evaluated the benefits of their use in reducing such breakdowns and violations in human-robot interactions. Our evaluation with application developers (N=9) shows that the use of formal-verification methods increases designers' ability to identify and contextualize social-norm violations. We discuss the implications of our approach for the future development of tools for effective design of social-robot applications.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Wisconsin–Madison"
            }
          ],
          "personId": 9714
        },
        {
          "affiliations": [
            {
              "institution": "University of Wisconsin–La Crosse"
            }
          ],
          "personId": 9753
        },
        {
          "affiliations": [
            {
              "institution": "University of Wisconsin–Madison"
            }
          ],
          "personId": 16925
        },
        {
          "affiliations": [
            {
              "institution": "bilge@cs.wisc.edu"
            }
          ],
          "personId": 15713
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6794,
      "typeId": 11151,
      "title": "AccordionFab: Fabricating Inflatable 3D Objects by Laser Cutting and Welding Multi-Layered Sheets",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose a method to create 3D inflatable objects by laminating plastic layers. AccordionFab is a fabrication method in which the user can prototype multi-layered inflatable structures rapidly with a common laser cutter. Our key finding is that it is possible to selectively weld the two uppermost plastic sheets out of the stacked sheets by defocusing the laser and inserting the heat-resistant paper below the desired welding layer.  As the contribution of our research, we investigated the optimal distance between the lens and the workpiece for cutting and welding and developed an attachment which supports welding process. Next, we developed a mechanism of changing the thickness and bending angle of multi-layered objects and created a simulation software. Using these techniques, the user can create various prototypes such as personal furniture that fits user's body and packing containers that fit the contents.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 20201
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 24290
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 10990
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 21914
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11040
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11676
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3723,
      "typeId": 11170,
      "title": "MobiLimb: Augmenting Mobile Devices with a Robotic Limb",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we explore the interaction space of MobiLimb, a small 5-DOF serial robotic manipulator attached to a mobile device. It (1) overcomes some limitations of mobile devices (static, passive, motionless); (2) preserves their form factor and I/O capabilities; (3) can be easily attached to or removed from the device; (4) offers additional I/O capabilities such as physical deformation and (5) can support various modular elements such as sensors, lights or shells. We illustrate its potential through three classes of applications: As a tool, MobiLimb offers tangible affordances and an expressive controller that can be manipulated to control virtual and physical objects. As a partner, it reacts expressively to users’ actions to foster curiosity and engagement or assist users. As a medium, it provides rich haptic feedback such as strokes, pat and other tactile stimuli on the hand or the wrist to convey emotions during mediated multimodal communications.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "LTCI, Télécom ParisTech, Université Paris-Saclay & Sorbonne Université"
            }
          ],
          "personId": 9328
        },
        {
          "affiliations": [
            {
              "institution": "Sorbonne Université, CNRS, ISIR"
            }
          ],
          "personId": 18220
        },
        {
          "affiliations": [
            {
              "institution": "Sorbonne Université, CNRS, ISIR"
            }
          ],
          "personId": 11515
        },
        {
          "affiliations": [
            {
              "institution": "LTCI, Télécom ParisTech, Université Paris-Saclay"
            }
          ],
          "personId": 9685
        }
      ],
      "sessionIds": [
        2519
      ],
      "eventIds": []
    },
    {
      "id": 7821,
      "typeId": 11199,
      "title": "Aalto Interface Metrics (AIM): A Service and Codebase for Computational GUI Evaluation",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Aalto Interface Metrics (AIM) pools several empirically validated models and metrics of user perception and attention into an easy-to-use online service for the evaluation of graphical user interface (GUI) designs. Users input a GUI design via URL, and select from a list of 17 different metrics covering aspects ranging from visual clutter to visual learnability. AIM presents detailed breakdowns, visualizations, and statistical comparisons, enabling designers and practitioners to detect shortcomings and possible improvements. The web service and code repository are available at interfacemetrics.aalto.fi.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 22402
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 21958
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 21662
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 18374
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 17848
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 22917
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 22433
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 10739
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 16115
        },
        {
          "affiliations": [
            {
              "institution": "University of Trento"
            }
          ],
          "personId": 16567
        },
        {
          "affiliations": [
            {
              "institution": "KTH Royal Institute of Technology"
            }
          ],
          "personId": 16494
        },
        {
          "affiliations": [
            {
              "institution": "KTH Royal Institute of Technology"
            }
          ],
          "personId": 23263
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3725,
      "typeId": 11170,
      "title": "Asterisk and Obelisk: Motion Codes for Passive Tagging",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Machine readable passive tags for tagging physical objects are ubiquitous today. We propose Motion Codes, a passive tagging mechanism that is based on the kinesthetic motion of the user's hand. Here, the tag comprises of a visual pattern that is displayed on a physical surface. To scan the tag and receive the encoded information, the user simply traces their finger over the pattern. The user wears an inertial motion sensing (IMU) ring on the finger that records the traced pattern. We design two motion code schemes, Asterisk and Obelisk that rely on directional vector data processed from the IMU. We evaluate both schemes for the effects of orientation, size, and data density on their accuracies. We further conduct an in-depth analysis of the sources of motion deviations in the ring data as compared to the ground truth finger movement data. Overall, Asterisk achieves a 95% accuracy for an information capacity of 16.8 million possible sequences.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 18983
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 24240
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 9202
        }
      ],
      "sessionIds": [
        1781
      ],
      "eventIds": []
    },
    {
      "id": 5776,
      "typeId": 11170,
      "title": "Vibrosight: Long-Range Vibrometry for Smart Environment Sensing",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Smart and responsive environments rely on the ability to detect physical events, such as appliance use and human activities. Currently, to sense these types of events, one must either upgrade to “smart” appliances, or attach aftermarket sensors to existing objects. These approaches can be expensive, intrusive and inflexible. In this work, we present Vibrosight, a new approach to sense activities across entire rooms using long-range laser vibrometry. Unlike a microphone, our approach can sense physical vibrations at one specific point, making it robust to interference from other activities and noisy environments. This property enables detection of simultaneous activities, which has proven challenging in prior work. Through a series of evaluations, we show that Vibrosight can offer high accuracies at long range, allowing our sensor to be placed in an inconspicuous location. We also explore a range of additional uses, including data transmission, sensing user input and modes of appliance operation, and detecting human movement and activities on work surfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 18031
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 19589
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 17904
        }
      ],
      "sessionIds": [
        1395
      ],
      "eventIds": []
    },
    {
      "id": 7829,
      "typeId": 11170,
      "title": "SilentVoice: Unnoticeable Voice Input by Ingressive Speech",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "SilentVoice is a new voice input interface device that penetrates the speech-based natural user interface (NUI) in daily life. The proposed \"ingressive speech\" method enables placement of a microphone very close to the front of the mouth without suffering from pop-noise, capturing very soft speech sounds with a good S/N ratio. It realizes ultra-small (less than 39dB(A)) voice leakage, allowing us to use voice input without annoying surrounding people in public and mobile situations as well as offices and homes. By measuring airflow direction, SilentVoice can easily be separated from normal utterances with 98.8% accuracy; no activation words are needed. It can be used for voice-activated systems with a specially trained voice recognizer; evaluation results yield word error rates (WERs) of 1.8% (speaker-dependent condition), and 7.0% (speaker-independent condition) with a limited dictionary of 85 command sentences. A whisper-like natural voice can also be used for real-time voice communication.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 16737
        }
      ],
      "sessionIds": [
        1395
      ],
      "eventIds": []
    },
    {
      "id": 6294,
      "typeId": 11199,
      "title": "Mindgame: Mediating People’s EEG Alpha Band Power through Reinforcement Learning",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents Mindgame, a reinforcement learning optimized neurofeedback mindfulness system. To avoid the potential bias and difficulties of designing mapping between neural signal and output, we adopt a trial-and-error learning method to explore the preferred mapping. In a pilot study we assess the effectiveness of Mindgame in mediating people’s EEG alpha band. All participants’ alpha band change towards the desired direction.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 18428
        },
        {
          "affiliations": [
            {
              "institution": "Tsinghua University"
            }
          ],
          "personId": 22749
        },
        {
          "affiliations": [
            {
              "institution": "Tsinghua University"
            }
          ],
          "personId": 10868
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 4248,
      "typeId": 11199,
      "title": "Gaze-guided Image Classification for Reflecting Perceptual Class Ambiguity",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Despite advances in machine learning and deep neural networks, there is still a huge gap between machine and human image understanding. One of the causes is the annotation process used to label training images. In most image categorization tasks, there is a fundamental ambiguity between some image categories and the underlying class probability differs from very obvious cases to ambiguous ones. However, current machine learning systems and applications usually work with discrete annotation processes and the training labels do not reflect this ambiguity. To address this issue, we propose an new image annotation framework where labeling incorporates human gaze behavior. In this framework, gaze behavior is used to predict image labeling difficulty. The image classifier is then trained with sample weights defined by the predicted difficulty. We demonstrate our approach's effectiveness on four-class image classification tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 21290
        },
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 16283
        },
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 12455
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5273,
      "typeId": 11151,
      "title": "A Mixed-Initiative Interface for Animating Static Pictures",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present an interactive tool to animate the visual elements of a static picture, based on simple sketch-based markup. While animated images enhance websites, infographics, logos, e-books, and social media, creating such animations from still pictures is difficult for novices and tedious for experts. Creating automatic tools is challenging due to ambiguities in object segmentation, relative depth ordering, and non-existent temporal information. With a few user drawn scribbles as input, our mixed initiative creative interface extracts repetitive texture elements in an image, and supports animating them. Our system also facilitates the creation of multiple layers to enhance depth cues in the animation. Finally, after analyzing the artwork during segmentation, several animation processes automatically generate kinetic textures that are spatio-temporally coherent with the source image. Our results, as well as feedback from our user evaluation, suggest that our system effectively allows illustrators and animators to add life to still images in a broad range of visual styles.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Princeton University"
            }
          ],
          "personId": 15678
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk"
            }
          ],
          "personId": 14972
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk"
            }
          ],
          "personId": 14001
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research"
            }
          ],
          "personId": 15020
        },
        {
          "affiliations": [
            {
              "institution": "Princeton University"
            }
          ],
          "personId": 23103
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research & University of Toronto"
            }
          ],
          "personId": 9877
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6297,
      "typeId": 11170,
      "title": "ElectroTutor : Test-Driven Physical Computing Tutorials",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "A wide variety of tools for creating physical computing systems have been developed, but getting started in this domain remains challenging for novices. In this paper, we introduce test-driven physical computing tutorials, a novel application of interactive tutorial systems to better support users in building and programming physical computing systems. These tutorials inject interactive tests into the tutorial process to help users verify and understand individual steps before proceeding. We begin by presenting a taxonomy of the types of tests that can be incorporated into physical computing tutorials. We then present ElectroTutor, a tutorial system that implements a range of tests for both the software and physical aspects of a physical computing system. A user study suggests that ElectroTutor can improve users’ success and confidence when completing a tutorial, and save them time by reducing the need to backtrack and troubleshoot errors made on previous tutorial steps.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of California, Berkeley"
            }
          ],
          "personId": 8499
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research"
            }
          ],
          "personId": 24070
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research"
            }
          ],
          "personId": 15020
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research & University of Toronto"
            }
          ],
          "personId": 9877
        }
      ],
      "sessionIds": [
        2218
      ],
      "eventIds": []
    },
    {
      "id": 3230,
      "typeId": 11199,
      "title": "Game Design for Users with Constraint: Exergame for Older Adults with Cognitive Impairment",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "In order to design serious games, attention needs to be paid to the target users. One important application of serious games is the design of games for older adults with dementia. Interfaces and activities in games designed for this group of users should be conducted by considering both the cognitive and physical limitations of these people, which may be challenging. We overcome these challenges by using the advantages of new head mounted display virtual reality (HMD-VR) technology and the knowledge of experts. The results of a preliminary three-week exercise involving participants with dementia shows that our design approach has been successful in achieving an interesting environment and could engage participants in the game.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 8638
        },
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 15831
        },
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 16339
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5279,
      "typeId": 11239,
      "title": "The Material for the 21st Century",
      "trackId": 10040,
      "tags": [],
      "keywords": [],
      "abstract": "30 years after Weiser’s inspirational words on ubiquitous computing, I revisit one of the premises of that work. I propose a new era of self-sustainable computing through the development of computational materials that can be truly woven into thefabricofeverydaylifeandcreatedecadesofinspirationfor new researchers across a variety of disciplines. I will deﬁne and demonstrate some initial examples of computational materials and explain why self-sustainable computing provides a compelling vision for computing in a post-Moore’s Law world.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 14547
        }
      ],
      "sessionIds": [
        2162
      ],
      "eventIds": []
    },
    {
      "id": 6308,
      "typeId": 11151,
      "title": "Demonstrating Gamepad with Programmable Haptic Texture Analog Buttons",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We demonstrate a haptic feedback method to generate multiple virtual textures on analog buttons of the gamepad. The method utilizes the haptic illusion evoked from proper haptic cues in respect of the analog button's movement to change the perceived physical property of the button. Two types of analog buttons, joystick and trigger button on the gamepad is augmented with localized haptic feedback. We implemented two virtual textures for each type of analog button, and these textures could be programmatically controlled reflecting the dynamic game situations. We also demonstrate a two-player shooter game to show the dynamic texture representation of customized gamepad could enrich the game experience.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 9441
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 17427
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 2725,
      "typeId": 11199,
      "title": "Juggling 4.0: Learning Complex Motor Skills with Augmented Reality Through the Example of Juggling",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Learning new motor skills is a problem that people are constantly confronted with (e.g. to learn a new kind of sport). In our work, we investigate to which extent the learning\nprocess of a motor sequence can be optimized with the help of Augmented Reality as a technical assistant. Therefore, we propose an approach that divides the problem into three tasks: (1) the tracking of the necessary movements, (2) the creation of a model that calculates possible deviations and (3) the implementation of a visual feedback system. To evaluate our approach, we implemented the idea by using infrared depth sensors and an Augmented Reality head-mounted device (HoloLens). Our results show that the system can give an efficient assistance for the correct height of a throw with one ball. Furthermore, it provides a basis for the support of a complete juggling sequence.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "OFFIS - Institute for Information Technology"
            }
          ],
          "personId": 20460
        },
        {
          "affiliations": [
            {
              "institution": "University of Oldenburg"
            }
          ],
          "personId": 14945
        },
        {
          "affiliations": [
            {
              "institution": "University of Oldenburg"
            }
          ],
          "personId": 23818
        },
        {
          "affiliations": [
            {
              "institution": "University of Oldenburg"
            }
          ],
          "personId": 23311
        },
        {
          "affiliations": [
            {
              "institution": "University of Oldenburg"
            }
          ],
          "personId": 18511
        },
        {
          "affiliations": [
            {
              "institution": "University of Oldenburg"
            }
          ],
          "personId": 20069
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5287,
      "typeId": 11170,
      "title": "Adasa: A Conversational In-Vehicle Digital Assistant for Advanced Driver Assistance Features",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Advanced Driver Assistance Systems (ADAS) come equipped on most modern vehicles and are intended to assist the driver and enhance the driving experience through features such as lane keeping system and adaptive cruise control. However, recent studies show that few people utilize these features for several reasons. First, ADAS features were not common until recently. Second, most users are unfamiliar with these features and do not know what to expect. Finally, the interface for operating these features is not intuitive. To help drivers understand ADAS features, we present a conversational in-vehicle digital assistant that responds to drivers' questions and commands in natural language. With the system prototyped herein, drivers can ask questions or command using unconstrained natural language in the vehicle, and the assistant trained by using advanced machine learning techniques, coupled with access to vehicle signals, responds in real-time based on conversational context. Results of our system prototyped on a production vehicle are presented, demonstrating its effectiveness in improving driver understanding and usability of ADAS.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 8305
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 17377
        },
        {
          "affiliations": [
            {
              "institution": "Ford Motor Company"
            }
          ],
          "personId": 9195
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 15846
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 12162
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 20029
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 17778
        }
      ],
      "sessionIds": [
        1612
      ],
      "eventIds": []
    },
    {
      "id": 4263,
      "typeId": 11199,
      "title": "EyeExpress: Expanding Hands-free Input Vocabulary using Eye Expressions",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "The muscles surrounding the human eye are capable of performing a wide range of expressions such as squinting, blinking, frowning, and raising eyebrows. This work explores the use of these ocular expressions to expand the input vocabularies of hands-free interactions. We conducted a series of user studies: 1) to understand which eye expressions users could consistently perform among all possible expressions, 2) to explore how these expressions can be used for hands-free interactions through a user-defined design process. Our study results showed that most participants could consistently perform 9 of the 18 possible eye expressions. Also, in the user define study the participants used the eye expressions to create hands-free interactions for the state-of-the-art augmented reality (AR) head-mounted displays.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 23961
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 17764
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 8199
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 7849,
      "typeId": 11151,
      "title": "Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 14361
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 20201
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 10639
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 9128
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 12753
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11040
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11676
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4780,
      "typeId": 11170,
      "title": "Blocks-to-CAD : A Cross-Application Bridge from Minecraft to 3D Modeling",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Learning a new software application can be a challenge, requiring the user to enter a new environment where their existing knowledge and skills do not apply, or worse, work against them. To ease this transition, we propose the idea of cross-application bridges that start with the interface of a familiar application, and gradually change their interaction model, tools, conventions, and appearance to resemble that of an application to be learned. To investigate this idea, we developed Blocks-to-CAD, a cross-application bridge from Minecraft-style games to 3D solid modeling. A user study of our system demonstrated that our modifications to the game did not hurt enjoyment or increase cognitive load, and that players could successfully apply knowledge and skills learned in the game to tasks in a popular 3D solid modeling application. The process of developing Blocks-to-CAD also revealed eight design strategies that can be applied to design cross-application bridges for other applications and domains.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Autodesk Research"
            }
          ],
          "personId": 24070
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research & University of Toronto"
            }
          ],
          "personId": 9877
        }
      ],
      "sessionIds": [
        1312
      ],
      "eventIds": []
    },
    {
      "id": 5292,
      "typeId": 11199,
      "title": "SurfaceStreams: A Content-Agnostic Streaming Toolkit for Interactive Surfaces",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We present SurfaceStreams, an open-source toolkit for recording and sharing visual content among multiple heterogeneous display-camera systems. SurfaceStreams clients support on-the-fly background removal and rectification on a range of different capture devices (Kinect & RealSense depth cameras, SUR40 sensor, plain webcam). After preprocessing, the raw data is compressed and sent to the SurfaceStreams server, which can dynamically receive streams from multiple clients, overlay them using the removed background as mask, and deliver the merged result back to the clients for display. We discuss an exemplary usage scenario (3-way shared interactive tabletop surface) and present results from a preliminary performance evaluation.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Bauhaus-Universität Weimar"
            }
          ],
          "personId": 11556
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 6828,
      "typeId": 11170,
      "title": "Next-Point Prediction for Direct Touch Using Finite-Time Derivative Estimation",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "End-to-end latency in interactive systems is detrimental to performance and usability, and comes from a combination of hardware and software delays. While these delays are steadily addressed by hardware and software improvements, it is at a decelerating pace. In parallel, short-term input prediction has shown promising results in recent years, in both research and industry, as an addition to these efforts. We describe a new prediction algorithm for direct touch devices based on (i) a state-of-the-art finite-time derivative estimator, (ii) a smoothing mechanism based on input speed, and (iii) a post-filtering of the prediction in two steps. Using both a pre-existing dataset of touch input as benchmark, and subjective data from a new user study, we show that this new predictor outperforms the predictors currently available in the literature and industry, based on metrics that model user-defined negative side-effects caused by input prediction. In particular, we show that our predictor can predict up to 2 or 3 times further than existing techniques with minimal negative side-effects.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 16991
        },
        {
          "affiliations": [
            {
              "institution": "CentraleSupelec"
            }
          ],
          "personId": 14633
        },
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 20026
        },
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 21401
        },
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 17794
        },
        {
          "affiliations": [
            {
              "institution": "Inria & Université de Lille"
            }
          ],
          "personId": 24344
        },
        {
          "affiliations": [
            {
              "institution": "Université de Lille & Inria"
            }
          ],
          "personId": 17069
        }
      ],
      "sessionIds": [
        2277
      ],
      "eventIds": []
    },
    {
      "id": 7346,
      "typeId": 11170,
      "title": "Vizir: A Domain-Specific Graphical Language for Authoring and Operating Airport Automations",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Automation is one of the key solutions proposed and adopted by international Air Transport research programs to meet the challenges of increasing air traffic. For automation to be safe and usable, it needs to be suitable to the activity it supports, both when authoring it and when operating it. Here we present Vizir, a Domain-Specific Graphical Language and an Environment for authoring and operating airport automations. We used a participatory-design process with Air Traffic Controllers to gather requirements for Vizir and to design its features. Vizir combines visual interaction-oriented programming constructs with activity-related geographic areas and events. Vizir offers explicit human-control constructs, graphical substrates and means to scale-up with multiple automations. We propose a set of guidelines to inspire designers of similar usable hybrid human-automation systems.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "ENAC - University of Toulouse"
            }
          ],
          "personId": 18368
        },
        {
          "affiliations": [
            {
              "institution": "ENAC - University of Toulouse"
            }
          ],
          "personId": 11762
        },
        {
          "affiliations": [
            {
              "institution": "ENAC - University of Toulouse"
            }
          ],
          "personId": 12022
        },
        {
          "affiliations": [
            {
              "institution": "ENAC - University of Toulouse"
            }
          ],
          "personId": 14907
        },
        {
          "affiliations": [
            {
              "institution": "ENAC - University of Toulouse"
            }
          ],
          "personId": 9899
        },
        {
          "affiliations": [
            {
              "institution": "ENAC - University of Toulouse"
            }
          ],
          "personId": 20223
        },
        {
          "affiliations": [
            {
              "institution": "Deep Blue"
            }
          ],
          "personId": 8362
        },
        {
          "affiliations": [
            {
              "institution": "Deep Blue"
            }
          ],
          "personId": 24388
        },
        {
          "affiliations": [
            {
              "institution": "Malta Air Services (MATS)"
            }
          ],
          "personId": 17579
        }
      ],
      "sessionIds": [
        1204
      ],
      "eventIds": []
    },
    {
      "id": 6836,
      "typeId": 11170,
      "title": "PuPoP: Pop-up Prop on Palm for Virtual Reality",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 11521
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 24206
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University of Science and Technology"
            }
          ],
          "personId": 9475
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 17640
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 19827
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 22293
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 10420
        }
      ],
      "sessionIds": [
        1872
      ],
      "eventIds": []
    },
    {
      "id": 7349,
      "typeId": 11199,
      "title": "Scaling Notifications Beyond Alerts: From Subtly Drawing Attention up to Forcing the User to Take Action",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Research has been done in sophisticated notifications, still, devices today mainly stick to a binary level of information, while they are either attention drawing or silent. We propose scalable notifications, which adjust the intensity level reaching from subtle to obtrusive and even going beyond that level while forcing the user to take action. To illustrate the technical feasibility and validity of this concept, we developed three prototypes. The prototypes provided mechano-pressure, thermal, and electrical feedback, which were evaluated in different lab studies. Our first prototype provides subtle poking through to high and frequent pressure on the user’s spine, which significantly improves back posture. In a second scenario, the user is able to perceive the overuse of a drill by an increased temperature on the palm of a hand until the heat is intolerable, forcing the user to eventually put down the tool. The last application comprises of a speed control in a driving simulation, while electric muscle stimulation on the users’ legs, conveys information on changing the car’s speed by a perceived tingling until the system forces the foot to move involuntarily. In conclusion, all studies’ findings support the feasibility of our concept of a scalable notification system, including the system forcing an intervention.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Auckland Bioengineering Institute & Fraunhofer IGD Rostock"
            }
          ],
          "personId": 9343
        },
        {
          "affiliations": [
            {
              "institution": "Fraunhofer IGD Rostock"
            }
          ],
          "personId": 21069
        },
        {
          "affiliations": [
            {
              "institution": "Fraunhofer IGD Rostock"
            }
          ],
          "personId": 23727
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5814,
      "typeId": 11151,
      "title": "FDSense: Estimating Young's Modulus and Stiffness of End Effectors to Facilitate Kinetic Interaction on Touch Surfaces",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We make touch input by physically colliding an end effector (e.g., a body part or a stylus) with a touch surface. Prior studies have examined the use of kinematic variables of collision between objects, such as position, velocity, force, and impact. However, the nature of the collision can be understood more thoroughly by considering the known physical relationships that exist between directly measurable variables (i.e., kinetics). Based on this collision kinetics, this study proposes a novel touch technique called FDSense. By simultaneously observing the force and contact area measured from the touchpad, FDSense allows estimation of the Young’s modulus and stiffness of the object being contacted. Our technical evaluation showed that FDSense could effectively estimate the Young’s modulus of end effectors made of various materials, and the stiffness of each part of the human hand. Two applications using FDSense were demonstrated, for digital painting and digital instruments, where the result of the expression varies significantly depending on the elasticity of the end effector. In a following informal study, participants assessed the technique positively.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Korea Institute of Science and Technology"
            }
          ],
          "personId": 16502
        },
        {
          "affiliations": [
            {
              "institution": "Korea Institute of Science and Technology"
            }
          ],
          "personId": 16935
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 10981
        },
        {
          "affiliations": [
            {
              "institution": "bjlee1985@gmail.com"
            }
          ],
          "personId": 8834
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 5816,
      "typeId": 11170,
      "title": "Ply: A Visual Web Inspector for Learning from Professional Webpages",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "While many online resources teach basic web development, few are designed to help novices learn the CSS concepts and design patterns experts use to implement complex visual features. Professional webpages embed these design patterns and could serve as rich learning materials, but their stylesheets are complex and difficult for novices to understand. This paper presents Ply, a CSS inspection tool that helps novices use their visual intuition to make sense of professional webpages. We introduce a new \\emph{visual relevance testing} technique to identify properties that have visual effects on the page, which Ply uses to hide visually irrelevant code and surface unintuitive relationships between properties. In user studies, Ply helped novice developers replicate complex web features 50% faster than those using Chrome Developer Tools, and allowed novices to recognize and explain unfamiliar concepts. These results show that visual inspection tools can support learning from complex professional webpages, even for novice developers.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Northwestern University"
            }
          ],
          "personId": 16475
        },
        {
          "affiliations": [
            {
              "institution": "Northwestern University"
            }
          ],
          "personId": 23934
        },
        {
          "affiliations": [
            {
              "institution": "Northwestern University"
            }
          ],
          "personId": 16197
        },
        {
          "affiliations": [
            {
              "institution": "Northwestern University"
            }
          ],
          "personId": 11205
        }
      ],
      "sessionIds": [
        1623
      ],
      "eventIds": []
    },
    {
      "id": 7354,
      "typeId": 11151,
      "title": "PrintMotion: Actuating Printed Objects Using Actuators Equipped in a 3D Printer",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce a novel use for desktop 3D printers using actuators equipped in the printers. The actuators control an extruder and a build-plate mounted on a fused deposition modeling (FDM) 3D printer, moving them horizontally or vertically. Our technique enables actuation of 3D-printed objects on the build-plate by controlling the actuators, and people can interact with them by connecting interface devices to the 3D printer. In this work, we describe how to actuate printed objects using the actuators and present several objects illustrated by our technique.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Meiji University"
            }
          ],
          "personId": 12313
        },
        {
          "affiliations": [
            {
              "institution": "Meiji University"
            }
          ],
          "personId": 13902
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3771,
      "typeId": 11170,
      "title": "SoundBender: Dynamic Acoustic Control Behind Obstacles",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Ultrasound manipulation is growing in popularity in the HCI community with applications in haptics, on-body interaction, and levitation-based displays. Most of these applications share two key limitations: a) the complexity of the sound fields that can be produced is limited by the physical size of the transducers, and b) no obstacles can be present between the transducers and the control point. We present SoundBender, a hybrid system that overcomes these limitations by combining the versatility of phased arrays of Transducers (PATs) with the precision of acoustic metamaterials. In this paper, we explain our approach to design and implement such hybrid modulators (i.e. to create complex sound fields) and methods to manipulate the field dynamically (i.e. stretch, steer). We demonstrate our concept using self-bending beams enabling both levitation and tactile feedback around an obstacle and present example applications enabled by SoundBender.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Sussex"
            }
          ],
          "personId": 13996
        },
        {
          "affiliations": [
            {
              "institution": "University of Sussex"
            }
          ],
          "personId": 9294
        },
        {
          "affiliations": [
            {
              "institution": "University of Sussex"
            }
          ],
          "personId": 8248
        },
        {
          "affiliations": [
            {
              "institution": "University of Sussex"
            }
          ],
          "personId": 11794
        },
        {
          "affiliations": [
            {
              "institution": "University of Sussex"
            }
          ],
          "personId": 9938
        },
        {
          "affiliations": [
            {
              "institution": "University of Sussex"
            }
          ],
          "personId": 15939
        }
      ],
      "sessionIds": [
        1395
      ],
      "eventIds": []
    },
    {
      "id": 6333,
      "typeId": 11199,
      "title": "Touch180: Finger Identification on Mobile Touchscreen using Fisheye Camera and Convolutional Neural Network",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We present Touch180, a computer vision based solution for identifying fingers on a mobile touchscreen with a fisheye camera and deep learning algorithm. As a proof-of-concept research, this paper focused on robustness and high accuracy of finger identification. We generated a new dataset for Touch180 configuration, which is named as Fisheye180. We trained a CNN (Convolutional Neural Network)-based network utilizing touch locations as auxiliary inputs. With our novel dataset and deep learning algorithm, finger identification result shows 98.56% accuracy with VGG16 model. Our study will serve as a step stone for finger identification on a mobile touchscreen.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 8498
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 19424
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science and Technology & ETRI"
            }
          ],
          "personId": 22307
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 17427
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 4286,
      "typeId": 11170,
      "title": "GridDrones: A Self-Levitating Physical Voxel Lattice for Interactive 3D Surface Deformations",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present GridDrones, a self-levitating programmable matter platform that can be used for representing 2.5D voxel grid relief maps capable of rendering unsupported structures and 3D transformations. GridDrones consists of cube-shaped nanocopters that can be placed in a volumetric 1xnxn mid-air grid, which is demonstrated here with 15 voxels. The number of voxels and scale is only limited by the size of the room and budget. Grid deformations can be applied interactively to this voxel lattice by manually selecting a set of voxels, then assigning a continuous topological relationship between voxel sets that determines how voxels move in relation to each other and manually drawing out selected voxels from the lattice structure. Using this simple technique, it is possible to create unsupported structures that can be translated and oriented freely in 3D. Shape transformations can also be recorded to allow for simple physical shape morphing animations. This work extends previous work on selection and editing techniques for 3D user interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Queen's University"
            }
          ],
          "personId": 9002
        },
        {
          "affiliations": [
            {
              "institution": "Queen's University"
            }
          ],
          "personId": 16182
        },
        {
          "affiliations": [
            {
              "institution": "Aalborg University"
            }
          ],
          "personId": 21726
        },
        {
          "affiliations": [
            {
              "institution": "Queen's University"
            }
          ],
          "personId": 24192
        }
      ],
      "sessionIds": [
        2519
      ],
      "eventIds": []
    },
    {
      "id": 5822,
      "typeId": 11151,
      "title": "Interactive Tangrami: Rapid Prototyping with Modular Paper-folded Electronics",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Prototyping interactive objects with personal fabrication tools like 3D printers requires the maker to create subsequent design artifacts from scratch which produces unnecessary waste and does not allow to reuse functional components.  We present Interactive Tangrami, paper-folded and reusable building blocks (Tangramis) that can contain various sensor input and visual output capabilities. We propose a digital design toolkit that lets the user plan the shape and functionality of a design piece. The software manages the communication to the physical artifact and streams the interaction data via the Open Sound protocol (OSC) to an application prototyping system (e.g. MaxMSP). The building blocks are fabricated digitally with a rapid and inexpensive ink-jet printing method. Our systems allows to prototype physical user interfaces within minutes and without knowledge of the underlying technologies. We demo its usefulness with two application examples.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "INRIA, Univ Paris-Sud, CNRS, Universite Paris-Saclay"
            }
          ],
          "personId": 14201
        },
        {
          "affiliations": [
            {
              "institution": "Academy of Fine Arts"
            }
          ],
          "personId": 19070
        },
        {
          "affiliations": [
            {
              "institution": "Saarland University"
            }
          ],
          "personId": 21743
        },
        {
          "affiliations": [
            {
              "institution": "Academy of Fine Arts"
            }
          ],
          "personId": 8512
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4288,
      "typeId": 10484,
      "title": "Enabling Single-Handed Interaction in Mobile and Wearable Computing",
      "trackId": 10036,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile and wearable computing are increasingly pervasive as people carry and use personal devices in everyday life. Screen sizes of such devices are becoming larger and smaller to accommodate both intimate and practical uses. Some mobile device screens are becoming larger to accommodate new experiences (e.g., phablet, tablet, eReader), whereas screen sizes on wearable devices are becoming smaller to allow them to fit into more places (e.g., smartwatch, wrist-band and eye-wear). However, these trends are making it difficult to use such devices with only one hand due to their placement, limited thumb reach and the fat-finger problem. This is especially true as there are many occasions when a user's other hand is occupied (encumbered) or not available.\nThis thesis work explores, creates and studies novel interaction techniques that enable effective single-hand usage on mobile and wearable devices, empowering users to achieve more with their smart devices when only one hand is available.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of St Andrews"
            }
          ],
          "personId": 20267
        }
      ],
      "sessionIds": [
        1992
      ],
      "eventIds": []
    },
    {
      "id": 5313,
      "typeId": 11151,
      "title": "Turbulence Ahead - A 3D Web-Based Aviation Weather Visualizer",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Although severe aircraft accidents have been reduced in the last decades, the number of injuries and fatalities caused by turbulence is still rising. Current aviation weather products are unable to provide a holistic and intuitive view of the overall weather situation, especially in terms of turbulence forecasts. This work introduces an interactive 3D prototype developed with a user-centered design approach. The prototype focuses on the visualization of significant weather charts, which are utilized during flight preparation. An online user study is conducted to compare the prototype with today's 2D paper maps. A total of 64 pilots from an internationally operating airline participated in the study. Among the major findings of the study is that the prototype significantly decreased the cognitive load, and enhanced spatial awareness and usability. To determine the spatial awareness, a novel similarity measure for spatial configurations of aviation weather data is introduced.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 10851
        },
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 24369
        },
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 9448
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3783,
      "typeId": 11170,
      "title": "RESi: A Highly Flexible, Pressure-Sensitive, Imperceptible Textile Interface Based on Resistive Yarns",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present RESi (Resistive tExtile Sensor Interfaces), a novel sensing approach enabling a new kind of yarn-based, resistive pressure sensing. The core of RESi builds on a newly designed yarn, which features conductive and resistive properties. We run a technical study to characterize the behaviour of the yarn and to determine the sensing principle. We demonstrate how the yarn can be used as a pressure sensor and discuss how specific issues, such as connecting the soft textile sensor with the rigid electronics can be solved. In addition, we present a platform-independent API that allows rapid prototyping. To show its versatility, we present applications developed with different textile manufacturing techniques, including hand sewing, machine sewing, and weaving. RESi is a novel technology, enabling textile pressure sensing to augment everyday objects with interactive capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Applied Sciences Upper Austria"
            }
          ],
          "personId": 20359
        },
        {
          "affiliations": [
            {
              "institution": "University of Applied Sciences Upper Austria"
            }
          ],
          "personId": 23495
        },
        {
          "affiliations": [
            {
              "institution": "University of Applied Sciences Upper Austria"
            }
          ],
          "personId": 16769
        },
        {
          "affiliations": [
            {
              "institution": "University of Applied Sciences Upper Austria"
            }
          ],
          "personId": 9424
        },
        {
          "affiliations": [
            {
              "institution": "University of Applied Sciences Upper Austria"
            }
          ],
          "personId": 15745
        },
        {
          "affiliations": [
            {
              "institution": "University of Applied Sciences Upper Austria"
            }
          ],
          "personId": 8826
        },
        {
          "affiliations": [
            {
              "institution": "University of Applied Sciences Upper Austria"
            }
          ],
          "personId": 13927
        },
        {
          "affiliations": [
            {
              "institution": "Johannes Kepler University"
            }
          ],
          "personId": 20995
        },
        {
          "affiliations": [
            {
              "institution": "Johannes Kepler University"
            }
          ],
          "personId": 11733
        },
        {
          "affiliations": [
            {
              "institution": "Johannes Kepler University"
            }
          ],
          "personId": 10090
        },
        {
          "affiliations": [
            {
              "institution": "University of Applied Sciences Upper Austria"
            }
          ],
          "personId": 19354
        }
      ],
      "sessionIds": [
        1893
      ],
      "eventIds": []
    },
    {
      "id": 4807,
      "typeId": 11151,
      "title": "CamTrackPoint: Camera-Based Pointing Stick Using Transmitted Light through Finger",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present CamTrackPoint, a new input interface that can be controlled by finger gestures captured by front or rear cameras of a mobile device.\nCamTrackPoint mounts a 3D-printed ring on the camera's bezel, and it senses the movements of the user's finger by tracking the light passed through the finger.\nThe proposed method provides mobile devices with a new input interface that offers physical force feedback like a pointing stick.\nThe cost of our method is low as it needs only a simple ring-shaped part on the camera bezel.\nMoreover, the ring doesn't disturb the functions of the camera, unless a user uses the interface.\nWe implement a prototype for a smartphone; two CamTrackPoint rings are made for the front and rear cameras.\nWe evaluate its performance and characteristics in an experiment.\nThe proposed technique provides smooth scrolling and would give better game experience on the available smartphone.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 14139
        },
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 20414
        },
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 9515
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4296,
      "typeId": 10484,
      "title": "Designing Interactive Behaviours Beyond the Desktop",
      "trackId": 10036,
      "tags": [],
      "keywords": [],
      "abstract": "As interactions move beyond the desktop, interactive behaviours (effects of actions as they happen, or once they happen) are becoming increasingly complex. This complexity is due to the variety of forms that objects might take, and the different inputs and sensors capturing information, and the ability to create nuanced responses to those inputs. Current interaction design tools do not support much of this rich behaviour authoring. In my work I create prototyping tools that examine ways in which designers can create interactive behaviours. Thus far, I have created two prototyping tools: Pineal and Astral, which examine how to create physical forms based on a smart object’s behaviour, and how to reuse existing desktop infrastructures to author different kinds of interactive behaviour. I also contribute conceptual elements, such as how to create smart objects using mobile devices, their sensors and outputs, instead of using custom electronic circuits, as well as devising evaluation strategies used in HCI toolkit research which directly informs my approach to evaluating my tools.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Calgary"
            }
          ],
          "personId": 20568
        }
      ],
      "sessionIds": [
        1992
      ],
      "eventIds": []
    },
    {
      "id": 6349,
      "typeId": 11151,
      "title": "VR Grabbers: Ungrounded Haptic Retargeting for Precision Grabbing Tools",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Haptic feedback in VR is important for realistic simulation in virtual reality.  However, recreating the haptic experience for hand tools in VR traditionally requires hardware with precise actuators, adding complexity to the system.  We propose Ungrounded Haptic Retargeting, an interaction technique that provides a realistic haptic experience for grabbing tools using only passive mechanisms.  This technique leverages the ungrounded feedback inherent in grabbing tools combined with dynamic visual adjustments of their position in virtual reality to create an illusion of physical presence for virtual objects.  To demonstrate the capabilities of this technique, we created VR Grabbers, an exemplary passive VR controller, similar to training chopsticks, with haptic feedback for precise object selection and manipulation.\n\nWe conducted two user studies based on VR Grabbers.  The first study probed the perceptual limits of the illusion; we found that the maximum position difference between the virtual and physical world acceptable to the user is (-1.48, 1.95) cm.  The second study showed that task performance of the VR Grabbers controller with Ungrounded Haptic Retargeting enabled outperforms the same controller with Ungrounded Haptic Retargeting disabled.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 20104
        },
        {
          "affiliations": [
            {
              "institution": "HP Inc."
            }
          ],
          "personId": 13536
        },
        {
          "affiliations": [
            {
              "institution": "HP Inc."
            }
          ],
          "personId": 10300
        },
        {
          "affiliations": [
            {
              "institution": "HP Inc."
            }
          ],
          "personId": 12747
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4304,
      "typeId": 11170,
      "title": "Assembly-aware Design of Printable Electromechanical Devices",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "From smart toys and household appliances to personal robots, electromechanical devices play an increasingly important role in our daily lives. Rather than relying on gadgets that are mass-produced, our goal is to enable casual users to custom-design such devices based on their own needs and preferences. To this end, we present a computational design system that leverages the power of digital fabrication and the emergence of affordable electronics such as sensors and microcontrollers. The input to our system consists of a 3D representation of the desired device's shape, and a set of user-preferred off-the-shelf components. Based on this input, our method generates an optimized, 3D printable enclosure that can house the required components. To create these designs automatically, we formalize a new spatio-temporal model that captures the entire assembly process, including the placement of the components within the device, mounting structures and attachment strategies, the order in which components must be inserted, and collision-free assembly paths. Using this model as a technical core, we then leverage engineering design guidelines and efficient numerical techniques to optimize device designs. In a user study, which also highlights the challenges of designing such devices, we find our system to be effective in reducing the entry barriers faced by casual users in creating such devices. We further demonstrate the versatility of our approach by designing and fabricating three devices with diverse functionalities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 19117
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 13946
        },
        {
          "affiliations": [
            {
              "institution": "ETH Zurich"
            }
          ],
          "personId": 10502
        }
      ],
      "sessionIds": [
        2218
      ],
      "eventIds": []
    },
    {
      "id": 7891,
      "typeId": 11170,
      "title": "Rousillon: Scraping Distributed Hierarchical Web Data",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Programming by Demonstration (PBD) promises to enable data scientists to collect web data.  However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks.  The missing piece is the capability to collect hierarchically-structured data from across many different webpages.  We present Rousillon, a programming system for writing complex web automation scripts by demonstration.  Users demonstrate how to collect the first row of a 'universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows.  To offer this new demonstration model, we developed novel relation selection and generalization algorithms.   In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of California, Berkeley"
            }
          ],
          "personId": 17894
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 10771
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 16493
        }
      ],
      "sessionIds": [
        1623
      ],
      "eventIds": []
    },
    {
      "id": 7895,
      "typeId": 11151,
      "title": "Scout: Mixed-Initiative Exploration of Design Variations through High-Level Design Constraints",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Although the exploration of variations is a key part of interface design, current processes for creating variations are mostly manual. We present Scout, a system that helps designers explore many variations rapidly through mixed-initiative interaction with high-level constraints and design feedback. Past constraint-based layout systems use low-level spatial constraints and mostly produce only a single design. Scout advances upon these systems by introducing high-level constraints based on design concepts (e.g. emphasis). With Scout, we have formalized several high-level constraints into their corresponding low-level spatial constraints to enable rapidly generating many designs through constraint solving and program synthesis.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "amaswea@cs.washington.edu"
            }
          ],
          "personId": 23943
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 15804
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 22291
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6873,
      "typeId": 11151,
      "title": "Rousillon: Scraping Distributed Hierarchical Web Data",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Programming by Demonstration (PBD) promises to enable data scientists to collect web data.  However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks.  The missing piece is the capability to collect hierarchically-structured data from across many different webpages.  We present Rousillon, a programming system for writing complex web automation scripts by demonstration.  Users demonstrate how to collect the first row of a 'universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows.  To offer this new demonstration model, we developed novel relation selection and generalization algorithms.   In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of California, Berkeley"
            }
          ],
          "personId": 17894
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 10771
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 16493
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4827,
      "typeId": 11170,
      "title": "RollingStone: Using Single Slip Taxel for Enhancing Active Finger Exploration with a Virtual Reality Controller",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We propose using a single slip tactile pixel on virtual reality controllers to produce sensations of finger sliding and textures. When a user moves the controller on a virtual surface, we add a slip opposite to the movement, creating an illusion of a finger that is sliding on the surface, while varying the slip feedback changes lateral forces on fingertip. When coupled with hand motion the lateral forces can be used to create perceptions of artificial textures. RollingStone has been implemented as a prototype VR controller consisting of a ball-based slip display positioned under the user’s fingertip. Within the slip display, a pair of motors actuates the ball, which is capable of gener- ating both short- and long-term two-degree-of-freedom slip feedback. An exploratory study was conducted to ensure that changing the relative motion between the finger and the ball could alter the perceptions conveying the properties of a tex- ture. The following two perception-based studies examined the minimum changes in speed of slip and angle of slip that are detectable by users. The results help us to design haptic patterns as well as our prototype applications. Finally, our preliminary user evaluation indicated that participants wel- comed RollingStone as a useful addition to the range of VR controllers.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 15642
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 19827
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University of Science and Technology"
            }
          ],
          "personId": 12548
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University of Science and Technology"
            }
          ],
          "personId": 18852
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 10420
        }
      ],
      "sessionIds": [
        2384
      ],
      "eventIds": []
    },
    {
      "id": 5852,
      "typeId": 11199,
      "title": "SweatSponse: Closing the Loop on Notification Delivery Using Skin Conductance Responses",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Today\"s smartphone notification systems are incapable of determining whether a notification has been successfully perceived without explicit interaction from the user. When the system incorrectly assumes that a notification has not been perceived, it may repeat it redundantly, disrupting the user (e.g., phone ringing). Or, when it assumes that a notification was perceived, and therefore fails to repeat it, the notification will be missed altogether (e.g., text message). We introduce SweatSponse, a feedback loop using skin conductance responses (SCR) to infer the perception of smartphone notifications just after their presentation. Early results from a laboratory study suggest that notifications induce SCR and that they could be used to better infer perception of smartphone notifications in real-time.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "McGill University"
            }
          ],
          "personId": 14170
        },
        {
          "affiliations": [
            {
              "institution": "McGill University"
            }
          ],
          "personId": 12129
        },
        {
          "affiliations": [
            {
              "institution": "McGill University"
            }
          ],
          "personId": 14204
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3296,
      "typeId": 11199,
      "title": "Reversing Voice-Related Biases Through Haptic Reinforcement",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Biased perceptions of others are known to negatively influence the outcomes of social and professional interactions in many regards. Theses biases can be informed by a multitude of non-verbal cues such as voice pitch and voice volume. This project explores how haptic effects, generated from speech, could attenuate listeners' perceived voice-related biases formed from a speaker's voice pitch. Promising preliminary results collected during a decision-making task suggest that the speech to haptic mapping and vibration delivery mechanism employed does attenuate voice-related biases. Accordingly, it is anticipated that such a system could be introduced in the workplace to equalize people's contribution opportunities and to create a more inclusive environment by reversing voice-related biases.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "McGill University"
            }
          ],
          "personId": 17752
        },
        {
          "affiliations": [
            {
              "institution": "McGill University"
            }
          ],
          "personId": 14170
        },
        {
          "affiliations": [
            {
              "institution": "McGill University"
            }
          ],
          "personId": 10593
        },
        {
          "affiliations": [
            {
              "institution": "McGill University"
            }
          ],
          "personId": 12061
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 6883,
      "typeId": 11151,
      "title": "Transparent Mask: Face-Capturing Head-Mounted Display with IR Pass Filters",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual reality (VR) using a head-mounted display (HMD) have been rapidly becoming popular. Lots of HMD products and various VR applications such as games, training tools and communication services have been released in recent years. However, there is a well-known problem that the user's face is covered by the HMD preventing the facial expression from being captured. This strongly restricts VR applications. For example, users wearing HMDs normally cannot exchange their face images. This degrades communication quality in virtual spaces because facial expressions are an important element of human communication.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 11143
        },
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 14139
        },
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 20414
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 7396,
      "typeId": 11199,
      "title": "CrowdMuse: An Adaptive Crowd Brainstorming System",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Online crowds, with their large numbers and diversity, show great potential for creativity, particularly during large-scale brainstorming sessions. Research has explored different ways of augmenting this creativity, such as showing ideators some form of inspiration to get them to explore more categories or generate more ideas. The mechanisms used to select which inspirations are shown to ideators thus far have been focused on characteristics of the inspirations rather than on ideators. This can hinder their effect, as creativity research has shown that ideators have unique cognitive structures and may therefore be better inspired by some ideas rather than others. We introduce CrowdMuse, an adaptive system for supporting large scale brainstorming. The system models ideators based on their past ideas and adapts the system views and inspiration mechanisms accordingly. An evaluation of this system could inform how to better individually support ideators.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Arizona State University"
            }
          ],
          "personId": 19768
        },
        {
          "affiliations": [
            {
              "institution": "Arizona State University"
            }
          ],
          "personId": 16190
        },
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 16853
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 4837,
      "typeId": 11151,
      "title": "Montage: A Video Prototyping System to Reduce Re-Shooting and Increase Re-Usability",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Video prototypes help capture and communicate interaction with paper prototypes in the early stages of design.\nHowever, designers sometimes find it tedious to create stop-motion videos for continuous interactions and to re-shoot clips as the design evolves.\nWe introduce Montage, a proof-of-concept implementation of a computer-assisted process for video prototyping.\nMontage lets designers progressively augment video prototypes with digital sketches, facilitating the creation, reuse and exploration of dynamic interactions.\nMontage uses chroma keying to decouple the prototyped interface from its context of use, letting designers reuse or change them independently.\nWe describe how Montage enhances video prototyping by combining video with digital animated sketches, encourages the exploration of different contexts of use, and supports prototyping of different interaction styles.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Université Paris-Sud, CNRS, Inria, Université Paris-Saclay"
            }
          ],
          "personId": 18247
        },
        {
          "affiliations": [
            {
              "institution": "Université Paris-Sud, CNRS, Inria, Université Paris-Saclay"
            }
          ],
          "personId": 20737
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4325,
      "typeId": 11199,
      "title": "Post-literate Programming: Linking Discussion and Code in Software Development Teams",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "The literate programming paradigm presents a program interleaved with natural language text explaining the code's rationale and logic.  While this is great for program readers, the labor of creating literate programs deters most program authors from providing this text at authoring time. Instead, as we determine through interviews, developers provide their design rationales after the fact, in discussions with collaborators.  We propose to capture these discussions and incorporate them into the code. We have prototyped a tool to link online discussion of code directly to the code it discusses. Incorporating these discussions incrementally creates post-literate programs that convey information to future developers.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Massacshuetts Institute of Technology"
            }
          ],
          "personId": 19139
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 15529
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 13218
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 6373,
      "typeId": 11170,
      "title": "Designing Socially Acceptable Hand-to-Face Input",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable hand-to-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "UNIST"
            }
          ],
          "personId": 8210
        },
        {
          "affiliations": [
            {
              "institution": "UNIST"
            }
          ],
          "personId": 14020
        },
        {
          "affiliations": [
            {
              "institution": "UNIST"
            }
          ],
          "personId": 15610
        },
        {
          "affiliations": [
            {
              "institution": "UNIST"
            }
          ],
          "personId": 10137
        }
      ],
      "sessionIds": [
        1781
      ],
      "eventIds": []
    },
    {
      "id": 3304,
      "typeId": 11151,
      "title": "Collaborative Virtual Reality for Low-Latency Interaction",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Columbia University"
            }
          ],
          "personId": 23895
        },
        {
          "affiliations": [
            {
              "institution": "Columbia University"
            }
          ],
          "personId": 23303
        },
        {
          "affiliations": [
            {
              "institution": "Columbia University"
            }
          ],
          "personId": 14758
        },
        {
          "affiliations": [
            {
              "institution": "Columbia University"
            }
          ],
          "personId": 23637
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6377,
      "typeId": 11170,
      "title": "Magneto-Haptics: Embedding Magnetic Force Feedback for Physical Interactions",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present magneto-haptics, a design approach of haptic sensations powered by the forces present among permanent magnets during active touch. Magnetic force has not been efficiently explored in haptic design because it is not intuitive and there is a lack of methods to associate or visualize magnetic force with haptic sensations, especially for complex magnetic patterns. To represent the haptic sensations of magnetic force intuitively, magneto-haptics formularizes haptic potential from the distribution of magnetic force along the path of motion. It provides a rapid way to compute the relationship between the magnetic phenomena and the haptic mechanism. Thus, we can convert a magnetic force distribution into a haptic sensation model, making the design of magnet-embedded haptic sensations more efficient. We demonstrate three applications of magneto-haptics through interactive interfaces and devices. We further verify our theory by evaluating some magneto-haptic designs through experiments.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)"
            }
          ],
          "personId": 23759
        }
      ],
      "sessionIds": [
        1893
      ],
      "eventIds": []
    },
    {
      "id": 4842,
      "typeId": 11170,
      "title": "Porta: Profiling Software Tutorials Using Operating-System-Wide Activity Tracing",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "It can be hard for tutorial creators to get fine-grained feedback about how learners are actually stepping through their tutorials and which parts lead to the most struggle. To provide such feedback for technical software tutorials, we introduce the idea of tutorial profiling, which is inspired by software code profiling. We prototyped this idea in a system called Porta that automatically tracks how users navigate through a tutorial webpage and what actions they take on their computer such as running shell commands, invoking compilers, and logging into remote servers. Porta surfaces this trace data in the form of profiling visualizations that augment the tutorial with heatmaps of activity hotspots and markers that expand to show event details, error messages, and embedded screencast videos of user actions. We found through a user study of 3 tutorial creators and 12 students who followed their tutorials that Porta enabled both the tutorial creators and the students to provide more specific, targeted, and actionable feedback about how to improve these tutorials. Porta opens up possibilities for performing user testing of technical documentation in a more systematic and scalable way.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of California, San Diego"
            }
          ],
          "personId": 10287
        },
        {
          "affiliations": [
            {
              "institution": "University of California, San Diego"
            }
          ],
          "personId": 19262
        }
      ],
      "sessionIds": [
        1524
      ],
      "eventIds": []
    },
    {
      "id": 4847,
      "typeId": 11151,
      "title": "FTIR-based Touch Pad for Smartphone-based HMD Enhancement",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We propose to equip smartphone-based HMDs (SbHMDs) with an additional touch pad.  SbHMDs are a low cost approach to allowing users to experience virtual reality (VR). Current SbHMDs, however, provide poor input functionality and sometimes external devices are necessary to enhance the VR experience.  Our proposal uses frustrated total internal reflection (FTIR) to realize a touch pad on the external surfaces of the HMD case; no special devices are needed.  As simple FTIR approaches do not suit SbHMDs due to the spatial relation between camera and light, we design an arrangement of acrylic plates and mirror suitable for smartphone's built-in camera and torch-light. It extends the input vocabulary SbHMDs to include touch location, gestures, and also pressure.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 16167
        },
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 14139
        },
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 20414
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3836,
      "typeId": 11170,
      "title": "Orecchio: Extending Body-Language through Actuated Static and Dynamic Auricular Postures",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose using the auricle – the visible part of the ear – as a means of expressive output to extend body language to convey emotional states. With an initial exploratory study, we provide an initial set of dynamic and static auricular postures. Using these results, we examined the relationship between emotions and auricular postures, noting that dynamic postures involving stretching the top helix in fast (e.g., 2Hz) and slow speeds (1Hz) conveyed intense and mild pleasantness while static postures involving bending the side or top helix towards the center of the ear were associated with intense and mild unpleasantness. Based on the results, we developed a prototype (called Orrechio) with miniature motors, custom-made robotic arms and other electronic components. A preliminary user evaluation showed that participants feel more comfortable using expressive auricular postures with people they are familiar with, and that it is a welcome addition to the vocabulary of human body language.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Dartmouth College & National Chiao Tung University"
            }
          ],
          "personId": 13574
        },
        {
          "affiliations": [
            {
              "institution": "University of Calgary"
            }
          ],
          "personId": 20520
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 22328
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 23519
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College & Tsinghua University"
            }
          ],
          "personId": 11946
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College & Tsinghua University"
            }
          ],
          "personId": 22646
        },
        {
          "affiliations": [
            {
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 10179
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 8791
        }
      ],
      "sessionIds": [
        1781
      ],
      "eventIds": []
    },
    {
      "id": 5887,
      "typeId": 11151,
      "title": "MoSculp: Interactive Visualization of Shape and Time",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present a system that visualizes complex human motion via 3D motion sculptures—a representation that conveys the 3D structure swept by a human body as it moves through space. Our system computes a motion sculpture from an input video, and then embeds it back into the scene in a 3D-aware fashion. The user may also explore the sculpture directly in 3D or physically print it. Our interactive interface allows users to customize the sculpture design, for example, by selecting materials and lighting conditions.\n\nTo provide this end-to-end workflow, we introduce an algorithm\nthat estimates a human’s 3D geometry over time from a\nset of 2D images, and develop a 3D-aware image-based rendering\napproach that inserts the sculpture back into the original\nvideo. By automating the process, our system takes motion\nsculpture creation out of the realm of professional artists, and\nmakes it applicable to a wide range of existing video material.\n\nBy conveying 3D information to users, motion sculptures reveal\nspace-time motion information that is difficult to perceive\nwith the naked eye, and allow viewers to interpret how different\nparts of the object interact over time. We validate the\neffectiveness of motion sculptures with user studies, finding\nthat our visualizations are more informative about motion than\nexisting stroboscopic and space-time visualization methods.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 15099
        },
        {
          "affiliations": [
            {
              "institution": "Google Research"
            }
          ],
          "personId": 21795
        },
        {
          "affiliations": [
            {
              "institution": "Google Research"
            }
          ],
          "personId": 21980
        },
        {
          "affiliations": [
            {
              "institution": "University of Calfornia, Berkeley"
            }
          ],
          "personId": 12143
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 22438
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 11320
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 8737
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology & Google Research"
            }
          ],
          "personId": 21631
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3329,
      "typeId": 11199,
      "title": "Active Authentication on Smartphone using Touch Pressure",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphone user authentication is still an open challenge because the balance between both security and usability is indispensable. To balance between them, active authentication is one way to overcome the problem. In this paper, we tackle to improve the accuracy of active authentication by adopting online learning with touch pressure. In recent years, it becomes easy to use the smartphones equipped with pressure sensor so that we have confirmed the effectiveness of adopting the touch pressure as one of the features to authenticate. Our experiments adopting online AROW algorithm with touch pressure show that equal error rate (EER), where the miss rate and false rate are equal, is reduced up to one-fifth by adding touch pressure feature. Moreover, we have confirmed that training with the data from both sitting posture and prone posture archives the best when testing variety of postures including sitting, standing and prone, which achieves EER up to 0.14%.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Waseda University"
            }
          ],
          "personId": 17757
        },
        {
          "affiliations": [
            {
              "institution": "Waseda University"
            }
          ],
          "personId": 12972
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 7939,
      "typeId": 11170,
      "title": "Shape-Aware Material: Interactive Fabrication with ShapeMe",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Makers often create both physical and digital prototypes to explore a design, taking advantage of the subtle feel of physical materials and the precision and power of digital models. We introduce ShapeMe, a novel smart material that captures its own geometry as it is physically cut by an artist or designer. ShapeMe includes a software toolkit that lets its users generate customized, embeddable sensors that can accommodate various object shapes. As the designer works on a physical prototype, the toolkit streams the artist's physical changes to its digital counterpart in a 3D CAD environment.\nWe use a rapid, inexpensive and simple-to-manufacture inkjet printing technique to create embedded sensors. We successfully created a linear predictive model of the sensors' lengths, and our empirical tests of ShapeMe show an average accuracy of 2 to 3 mm. We present two application scenarios for modeling multi-object constructions, such as architectural models, and 3D models consisting of multiple layers stacked one on top of each other. ShapeMe demonstrates a novel technique for integrating digital and physical modeling, and suggests new possibilities for creating shape-aware materials.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Inria"
            },
            {
              "institution": "Univ. Paris-Sud & CNRS (LRI)"
            },
            {
              "institution": "Université Paris-Saclay"
            }
          ],
          "personId": 16212
        },
        {
          "affiliations": [
            {
              "institution": "Inria"
            },
            {
              "institution": "Univ. Paris-Sud & CNRS (LRI)"
            },
            {
              "institution": "Université Paris-Saclay"
            }
          ],
          "personId": 12063
        },
        {
          "affiliations": [
            {
              "institution": "Inria"
            },
            {
              "institution": "Univ. Paris-Sud & CNRS (LRI)"
            },
            {
              "institution": "Université Paris-Saclay"
            }
          ],
          "personId": 9782
        }
      ],
      "sessionIds": [
        1903
      ],
      "eventIds": []
    },
    {
      "id": 6917,
      "typeId": 11199,
      "title": "AmbientLetter: Letter Presentation Method for Discreet Notification of Unknown Spelling when Handwriting",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a technique to support writing activity in a confidential manner with a pen-based device. Autocorrect and predictive conversion do not work when writing by hand, and looking up unknown spelling is sometimes embarrassing. Therefore, we propose AmbientLetter which seamlessly and discretely presents the forgotten spelling to the user in scenarios where handwriting is necessary. In this work, we describe the system structure and the technique used to conceal the user\"s getting the information.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Meiji University"
            }
          ],
          "personId": 17088
        },
        {
          "affiliations": [
            {
              "institution": "Meiji University"
            }
          ],
          "personId": 13902
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 7941,
      "typeId": 11239,
      "title": "The Future Evolution of Language",
      "trackId": 10040,
      "tags": [],
      "keywords": [],
      "abstract": "Suppose we could create interactive visualizations in the air during our day to day conversations? In the next few years, mixed reality technologies will make this possible. When that happens, how might language itself evolve? We describe a plan to help guide that evolution. If the capability to share visual communication under gestural control leads to a change in natural language itself, then future generations of children will grow up in a richer world, with powers of natural language-based communication that we now can only begin to envision. The resulting communicative power-up for coming generations may be as fundamental and paradigm changing as was the development of written language itself 5,000 years ago.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 21253
        }
      ],
      "sessionIds": [
        2162
      ],
      "eventIds": []
    },
    {
      "id": 7945,
      "typeId": 11151,
      "title": "Scenograph: Fitting Real-Walking VR Experiences into Various Tracking Volumes",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "When developing a real-walking virtual reality experience, designers generally create virtual locations to fit a specific tracking volume. Unfortunately, this prevents the resulting experience from running on a smaller or differently shaped tracking volume. To address this, we present a software system called Scenograph. The core of Scenograph is a tracking volume-independent representation of real-walking experiences. Scenograph instantiates the experience to a tracking volume of given size and shape by splitting the locations into smaller ones while maintaining narrative structure. In our user study, participants’ ratings of realism decreased significantly when existing techniques were used to map a 25m2 experience to 9m2 and an L-shaped 8m2 tracking volume. In contrast, ratings did not differ when Scenograph was used to instantiate the experience.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 17729
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 13798
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4877,
      "typeId": 11151,
      "title": "Shape-Aware Material: Interactive Fabrication with ShapeMe",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Makers often create both physical and digital prototypes to explore a design, taking advantage of the subtle feel of physical materials and the precision and power of digital models. We introduce ShapeMe, a novel smart material that captures its own geometry as it is physically cut by an artist or designer. ShapeMe includes a software toolkit that lets its users generate customized, embeddable sensors that can accommodate various object shapes. As the designer works on a physical prototype, the toolkit streams the artist's physical changes to its digital counterpart in a 3D CAD environment.\nWe use a rapid, inexpensive and simple-to-manufacture inkjet printing technique to create embedded sensors. We successfully created a linear predictive model of the sensors' lengths, and our empirical tests of ShapeMe show an average accuracy of 2 to 3 mm. We present two application scenarios for modeling multi-object constructions, such as architectural models, and 3D models consisting of multiple layers stacked one on top of each other. ShapeMe demonstrates a novel technique for integrating digital and physical modeling, and suggests new possibilities for creating shape-aware materials.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Inria"
            },
            {
              "institution": "Univ. Paris-Sud & CNRS (LRI)"
            },
            {
              "institution": "Université Paris-Saclay"
            }
          ],
          "personId": 16212
        },
        {
          "affiliations": [
            {
              "institution": "Inria"
            },
            {
              "institution": "Univ. Paris-Sud & CNRS (LRI)"
            },
            {
              "institution": "Université Paris-Saclay"
            }
          ],
          "personId": 12063
        },
        {
          "affiliations": [
            {
              "institution": "Inria"
            },
            {
              "institution": "Univ. Paris-Sud & CNRS (LRI)"
            },
            {
              "institution": "Université Paris-Saclay"
            }
          ],
          "personId": 9782
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 5902,
      "typeId": 11170,
      "title": "Intelligent Modality Selection for Navigation Systems",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Portable electronic navigation systems are often used for directional guidance when humans need to navigate terrain quickly and accurately. Prior work in this field has focused on using either the visual or haptic sensory modality for providing such guidance, and results have indicated that either option may be preferable depending upon the user's specific needs. However, conventional methods involve selecting a single modality based on which will work best with the task the user is most likely to perform and using this modality throughout the duration of the navigation. In this paper, we describe the design and results of a study intended to evaluate the effectiveness of an adaptive modality selection algorithm that dynamically selects a navigation system's directional guidance modality while considering both task-specific benefits and the time-varying effects of switching cost, stimulus-specific adaptation, and habituation. Our findings indicate that use of this algorithm can improve user performance in the presence of multiple simultaneous tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 11963
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 11533
        }
      ],
      "sessionIds": [
        1612
      ],
      "eventIds": []
    },
    {
      "id": 7438,
      "typeId": 11199,
      "title": "Trans-scale Playground: An Immersive Visual Telexistence System for Human Adaptation",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present a novel telexistence system and design methods for telexistence studies to explore spatialscale deconstruction. There have been studies on the experience of dwarf-sized or giant-sized telepresence have been conducted over a period of many years. In this study, we discuss the scale of movements, image transformation, technical components of telepresence robots, and user experiences of telexistence-based spatial transformations. We implemented two types of telepresence robots with an omnidirectional stereo camera setup for a spatial trans-scale experience, wheeled robots, and quadcopters. These telepresence robots provide users with a trans-scale experience for a distance ranging from 15 cm to 30 m. We conducted user studies for different camera positions on robots and for different image transformation method.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 11382
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 17727
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 14229
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 19102
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 14630
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3854,
      "typeId": 11151,
      "title": "[Sponsor Demo] Facebook Reality Labs",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3343,
      "typeId": 11170,
      "title": "DextrES: Wearable Haptic Feedback for Grasping in VR via a Thin Form-Factor Electrostatic Brake",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce DextrES, a flexible and wearable haptic glove which integrates both kinesthetic and cutaneous feedback in a thin and light form factor (weight is less than 8g).\nOur approach is based on an electrostatic clutch generating up to 20 N of holding force on each finger by modulating the electrostatic attraction between flexible elastic metal strips to generate an electrically-controlled friction force. We harness the resulting braking force to rapidly render on-demand kinesthetic feedback. The electrostatic brake is mounted onto the the index finger and thumb via modular 3D printed articulated guides which allow the metal strips to glide smoothly.  Cutaneous feedback is provided via piezo actuators at the fingertips. We demonstrate that our approach can provide rich haptic feedback under dexterous articulation of the user's hands and provides effective haptic feedback across a variety of different grasps. A controlled experiment indicates that DextrES improves the grasping precision for different types of virtual objects. Finally, we report on results of a psycho-physical study which identifies discrimination thresholds for different levels of holding force.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "EPFL"
            }
          ],
          "personId": 22378
        },
        {
          "affiliations": [
            {
              "institution": "ETH"
            }
          ],
          "personId": 21660
        },
        {
          "affiliations": [
            {
              "institution": "EPFL"
            }
          ],
          "personId": 9609
        },
        {
          "affiliations": [
            {
              "institution": "ETH"
            }
          ],
          "personId": 14264
        }
      ],
      "sessionIds": [
        1787
      ],
      "eventIds": []
    },
    {
      "id": 6930,
      "typeId": 10484,
      "title": "Comfortable and Efficient Travel Techniques in VR",
      "trackId": 10036,
      "tags": [],
      "keywords": [],
      "abstract": "Locomotion,the most basic interaction in Virtual Environments (VE), enables users to move around the virtual world. Locomotion in Virtual Reality (VR) is a problem which has not been solved completely since existing techniques have a specific set of requirements and limitations. In addition, the uncertainty about the impact that virtual cues have on users perception complicates the development of better locomotion interfaces. A broadly applicable locomotion technique that is easy to use and addresses the issues of presence, cybersickness and fatigue has yet to be developed. Though optical flow and vestibular cues are dominant in navigation, other cues such as auditory, arm feedback, wind, etc. play a role. The proposed research aims to evaluate and improve upon a set of locomotion techniques for different modes of locomotion in virtual scenarios, as well as the transitions between them. The outcome measures of the evaluations of the different scenarios are usefulness for spatial orientation, presence, fatigue, cybersickness and user preference. The envisioned contribution of my thesis is research towards the design of a locomotion technique that is easy to use and addresses the shortcomings of current implementations.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Canterbury"
            }
          ],
          "personId": 12593
        }
      ],
      "sessionIds": [
        1992
      ],
      "eventIds": []
    },
    {
      "id": 7961,
      "typeId": 11170,
      "title": "HydroRing: Supporting Mixed Reality Haptics Using Liquid Flow",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Current haptic devices are often bulky and rigid, making them unsuitable for ubiquitous interaction and scenarios where the user must also interact with the real world. To address this gap, we propose HydroRing, an unobtrusive, finger-worn device that can provide the tactile sensations of pressure, vibration, and temperature on the fingertip, enabling mixed-reality haptic interactions. Different from previous explorations, HydroRing in active mode delivers sensations using liquid travelling through a thin, flexible latex tube worn across the fingerpad, and has minimal impact on a user’s dexterity and their perception of stimuli in passive mode. Two studies evaluated participants’ ability to perceive and recognize sensations generated by the device, as well as their ability to perceive physical stimuli while wearing the device. We conclude by exploring several applications leveraging this mixed-reality haptics approach.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Autodesk Research & University of Manitoba"
            }
          ],
          "personId": 13781
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research"
            }
          ],
          "personId": 9205
        },
        {
          "affiliations": [
            {
              "institution": "University of Manitoba"
            }
          ],
          "personId": 15019
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research & University of Toronto"
            }
          ],
          "personId": 9877
        }
      ],
      "sessionIds": [
        1787
      ],
      "eventIds": []
    },
    {
      "id": 6937,
      "typeId": 11170,
      "title": "Believe it or not: Designing a Human-AI Partnership for Mixed-Initiative Fact-Checking",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Fact-checking, the task of assessing the veracity of claims, is an important, timely, and challenging problem. While many automated fact-checking systems have been recently proposed, the human side of the partnership has been largely neglected: how might people understand, interact with, and establish trust with an AI fact-checking system? Does such a system actually help people better assess the factuality of claims? In this paper, we present the design and evaluation of a mixed-initiative approach to fact-checking, blending human knowledge and experience with the efficiency and scalability of automated information retrieval and ML. In a user study in which participants used our system to aid their own assessment of claims, our results suggest that individuals tend to trust the system: participant accuracy assessing claims improved when exposed to correct model predictions. However, this trust perhaps goes too far: when the model was wrong, exposure to its predictions often degraded human accuracy. Participants given the option to interact with these incorrect predictions were often able improve their own performance. This suggests that transparent models are key to facilitating effective human interaction with fallible AI models.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Texas at Austin"
            }
          ],
          "personId": 18618
        },
        {
          "affiliations": [
            {
              "institution": "University of Texas at Austin"
            }
          ],
          "personId": 14759
        },
        {
          "affiliations": [
            {
              "institution": "University of Texas at Austin"
            }
          ],
          "personId": 20118
        },
        {
          "affiliations": [
            {
              "institution": "University of Texas at Austin"
            }
          ],
          "personId": 11129
        },
        {
          "affiliations": [
            {
              "institution": "University of Texas at Austin"
            }
          ],
          "personId": 15759
        },
        {
          "affiliations": [
            {
              "institution": "Northeastern University"
            }
          ],
          "personId": 20860
        },
        {
          "affiliations": [
            {
              "institution": "University of Texas at Austin"
            }
          ],
          "personId": 15516
        }
      ],
      "sessionIds": [
        1524
      ],
      "eventIds": []
    },
    {
      "id": 3869,
      "typeId": 11151,
      "title": "Unimanual Pen+Touch Input Using Variations of Precision Grip Postures",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce a new pen input space by forming postures with the same hand that also grips the pen while writing, drawing, or selecting. The postures contact the multitouch surface around the pen to enable detection without special sensors. A formative study investigates the effectiveness, accuracy, and comfort of 33 candidate postures in controlled tasks. The results indicate a useful subset of postures. Using raw capacitive sensor data captured in the study, a convolutional neural network is trained to recognize 10 postures in real time. This recognizer is used to create application demonstrations for pen-based document annotation and vector drawing. A small usability study shows the approach is feasible.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 17569
        },
        {
          "affiliations": [
            {
              "institution": "Preferred Networks Inc. & University of Waterloo"
            }
          ],
          "personId": 13591
        },
        {
          "affiliations": [
            {
              "institution": "Preferred Networks Inc."
            }
          ],
          "personId": 20822
        },
        {
          "affiliations": [
            {
              "institution": "Preferred Networks Inc."
            }
          ],
          "personId": 21827
        },
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 14435
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 5407,
      "typeId": 11199,
      "title": "Augmenting Human Hearing Through Interactive Auditory Mediated Reality",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "To filter and shut out an increasingly loud environment, many resort to the use of personal audio technology. They drown out unwanted sounds, by wearing headphones. This uniform interaction with all surrounding sounds can have a negative impact on social relations and situational awareness. Leveraging mediation through smarter headphones, users gain more agency over their sense of hearing: For instance by being able to selectively alter the volume and other features of specific sounds, without losing the ability to add media.  In this work, we propose the vision of interactive auditory mediated reality (AMR). To understand users' attitude and requirements, we conducted a week-long event sampling study (n = 12), where users recorded and rated sources (n = 225) which they would like to mute, amplify or turn down. The results indicate that besides muting, a distinct, \"quiet-but-audible\" volume exists. It caters to two requirements at the same time: aesthetics/comfort and information acquisition.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 16188
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 22489
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 12300
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 17172
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3874,
      "typeId": 11170,
      "title": "Extending a Reactive Expression Language with Data Update Actions for End-User Application Authoring",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Mavo is a small extension to the HTML language that empowers non-programmers to create simple web applications. Authors can mark up any normal HTML document with attributes that specify data elements that Mavo makes editable and persists. But while applications authored with Mavo allow users to edit individual data items, they do not offer any programmatic data actions that can act in customizable ways on large collections of data simultaneously or that modify data according to a computation. We explore an extension to the Mavo language that enables non-programmers to author these richer data update actions. We show that it lets authors create a more powerful set of applications than they could previously, while adding little additional complexity to the authoring process. Through user evaluations, we assess how closely our data update syntax matches how novice authors would instinctively express such actions, and how well they are able to use the syntax we provided.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 12473
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 13848
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 17883
        }
      ],
      "sessionIds": [
        1028
      ],
      "eventIds": []
    },
    {
      "id": 2852,
      "typeId": 11199,
      "title": "Companion - A Software Toolkit for Digitally Aided Pen-and-Paper Tabletop Roleplaying",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We present Companion, a software tool tailored towards improving and digitally supporting the pen-and-paper tabletop role-playing experience. Pen-and-paper role-playing games (P&P RPG) are a concept known since the early 1970s. Since then, the genre has attracted a massive community of players while branching out into several genres and P&P RPG systems to choose from. Due to the highly interactive and dynamic nature of the game, a participants individual impact on narrative and interactive aspects of the game is extremely high. The diversity of scenarios within this context unfold a variety of players needs, as well as factors limiting and enhancing game-play. Companion offers an audio management workspace for creation and playback of soundscapes based on visual layouting. It supports interactive image presentation and map exploration which can incorporate input from any device providing TUIO tracking data. Additionally, a mobile app was developed to be used as a remote control for media activation on the desktop host.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Bauhaus-Universität Weimar"
            }
          ],
          "personId": 8929
        },
        {
          "affiliations": [
            {
              "institution": "Bauhaus-Universität Weimar"
            }
          ],
          "personId": 21281
        },
        {
          "affiliations": [
            {
              "institution": "Bauhaus-Universität Weimar"
            }
          ],
          "personId": 11556
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 2857,
      "typeId": 11170,
      "title": "WiFröst: Bridging the Information Gap for Debugging of Networked Embedded Systems",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "The rise in prevalence of Internet of Things (IoT) technologies has encouraged more people to prototype and build custom internet connected devices based on low power microcontrollers. While well-developed tools exist for debugging network communication for desktop and web applications, it can be difficult for developers of networked embedded systems to figure out why their network code is failing due to the limited output affordances of embedded devices. This paper presents WiFröst , a new approach for debugging these systems using instrumentation that spans from the device itself, to its communication API, to the wireless router and back-end server. WiFröst automatically collects this data, displays it in a web-based visualization, and highlights likely issues with an extensible suite of checks based on analysis of recorded execution traces.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stanford University & University of California, Berkeley"
            }
          ],
          "personId": 17586
        },
        {
          "affiliations": [
            {
              "institution": "University of California, Berkeley"
            }
          ],
          "personId": 8499
        },
        {
          "affiliations": [
            {
              "institution": "University of California, Berkeley"
            }
          ],
          "personId": 14092
        },
        {
          "affiliations": [
            {
              "institution": "University of California, Berkeley"
            }
          ],
          "personId": 22807
        },
        {
          "affiliations": [
            {
              "institution": "University of California, Berkeley"
            }
          ],
          "personId": 15701
        },
        {
          "affiliations": [
            {
              "institution": "University of California, Berkeley"
            }
          ],
          "personId": 10073
        }
      ],
      "sessionIds": [
        2218
      ],
      "eventIds": []
    },
    {
      "id": 7471,
      "typeId": 11199,
      "title": "Wearable Kinesthetic I/O Device for Sharing Muscle Compliance",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present a wearable kinesthetic I/O device, which is able to measure and intervene in multiple muscle activities simultaneously through the same electrodes.\nThe developed system includes an I/O module, capable of measuring the electromyogram (EMG) of four muscle tissues, while applying electrical muscle stimulation (EMS) at the same time.\nThe developed wearable system is configured in a scalable manner for achieving 1) high stimulus frequency (up to 70 Hz), 2) wearable dimensions in which the device can be placed along the limbs, and 3) flexibility of the number of I/O electrodes (up to 32 channels).\nIn a pilot user study, which shared the wrist compliance between two persons, participants were able to recognize the level of their confederate's wrist joint compliance using a 4-point Likert scale.\nThe developed system would benefit a physical therapist and a patient, during hand rehabilitation, using a peg board for sharing their wrist compliance and grip force, which are usually difficult to be observed in a visual contact.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 15932
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 21463
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 2865,
      "typeId": 11170,
      "title": "<i>InfiniTouch:</i> Finger-Aware Interaction on Fully Touch Sensitive Smartphones",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphones are the most successful mobile devices and offer intuitive interaction through touchscreens. Current devices treat all fingers equally and only sense touch contacts on the front of the device. In this paper, we present InfiniTouch, the first system that enables touch input on the whole device surface and identifies the fingers touching the device without external sensors while keeping the form factor of a standard smartphone. We first developed a prototype with capacitive sensors on the front, the back and on three sides. We then conducted a study to train a convolutional neural network that identifies fingers with an accuracy of 95.78% while estimating their position with a mean absolute error of 0.74cm. We demonstrate the usefulness of multiple use cases made possible with InfiniTouch, including finger-aware gestures and finger flexion state as an action modifier.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Stuttgart"
            }
          ],
          "personId": 16203
        },
        {
          "affiliations": [
            {
              "institution": "University of Stuttgart"
            }
          ],
          "personId": 13451
        },
        {
          "affiliations": [
            {
              "institution": "University of Stuttgart & University of Regensburg"
            }
          ],
          "personId": 14321
        }
      ],
      "sessionIds": [
        2277
      ],
      "eventIds": []
    },
    {
      "id": 7474,
      "typeId": 11199,
      "title": "cARe: An Augmented Reality Support System for Dementia Patients",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Symptoms of progressing dementia like memory loss, impaired executive function and decreasing motivation can gradually undermine instrumental activities of daily living (IADL) such as cooking.  Assisting technologies in form of augmented reality (AR) have previously been applied to support cognitively impaired users during IADLs.  In most cases, instructions were provided locally via projection or a head-mounted display (HMD) but lacked an incentive mechanism and the flexibility to support a broad range of use-cases. To provide users and therapists with a holistic solution, we propose cARe, a framework that can be easily adapted by therapists to various use-cases without any programming knowledge.  Users are then guided through manual processes with localized visual and auditory cues that are rendered by an HMD. Our ongoing user study indicates that users are more comfortable and successful in cooking with cARe as compared to a printed recipe, which promises a more dignified and autonomous living for dementia patients.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 19162
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 19090
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 12830
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 18680
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 17172
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3380,
      "typeId": 11170,
      "title": "Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Accessibility issues in mobile apps make those apps difficult or impossible to access for many people. Examples include elements that fail to provide alternative text for a screen reader, navigation orders that are difficult, or custom widgets that leave key functionality inaccessible. Social annotation techniques have demonstrated compelling approaches to such accessibility concerns in the web, but have been difficult to apply in mobile apps because of the challenges of robustly annotating interfaces. This research develops methods for robust annotation of mobile app interface elements. Designed for use in runtime interface modification, our methods are based in screen identifiers, element identifiers, and screen equivalence heuristics. We implement initial developer tools for annotating mobile app accessibility metadata, evaluate our current screen equivalence heuristics in a dataset of 2038 screens collected from 50 mobile apps, present three case studies implementing runtime repair of common accessibility issues, and examine repair of real-world accessibility issues in 26 apps. These contributions overall demonstrate strong opportunities for social annotation in mobile accessibility.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 24142
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 10081
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 22291
        }
      ],
      "sessionIds": [
        1873
      ],
      "eventIds": []
    },
    {
      "id": 4916,
      "typeId": 11151,
      "title": "Ultra-Low-Power Mode for Screenless Mobile Interaction",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphones are now a central technology in the daily lives of billions, but it relies on its battery to perform. Battery optimization is thereby a crucial design constraint in any mobile OS and device. However, even with new low-power methods, the ever-growing touchscreen remains the most power-hungry component. We propose an Ultra-Low-Power Mode (ULPM) for mobile devices that allows for touch interaction without visual feedback and exhibits significant power savings of up to 60\\% while allowing to complete interactive tasks. We demonstrate the effectiveness of the screenless ULPM in text-entry tasks, camera usage, and listening to videos, showing only a small decrease in usability for typical users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 21399
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 10880
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 8989
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 21020
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 12240
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4919,
      "typeId": 10484,
      "title": "Fostering Design Process of Shape-Changing Interfaces",
      "trackId": 10036,
      "tags": [],
      "keywords": [],
      "abstract": "Shape-changing interfaces match forms and haptics with functions and bring affordances to devices. I believe that shape-changing interfaces will be increasingly available to end-users in the future. To increase acceptance of shape-changing interfaces by end-users, we need to provide designers with design criteria and framework closely grounded on their current skills and needs. Also, we need to provide them with prototyping tools to enable quick assessment of ideas in the physical world. In this paper, I introduce the three threads of my Ph.D. research in the direction of providing the design tools. First, I advance existing shape-changing interface taxonomies to broaden design vocabulary and systemize design framework, based on the classification of everyday objects. Second, I conduct a study with end-users to suggest interaction techniques and design guidelines for shape-changing interfaces from their current practice. Lastly, I develop a physical prototyping tool for shape-changing interfaces to shorten prototyping iterations based on well-known Lego-like bricks.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Université Grenoble Alpes"
            }
          ],
          "personId": 17868
        }
      ],
      "sessionIds": [
        1992
      ],
      "eventIds": []
    },
    {
      "id": 4407,
      "typeId": 11170,
      "title": "VR Grabbers: Ungrounded Haptic Retargeting for Precision Grabbing Tools",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Haptic feedback in VR is important for realistic simulation in virtual reality.  However, recreating the haptic experience for hand tools in VR traditionally requires hardware with precise actuators, adding complexity to the system.  We propose Ungrounded Haptic Retargeting, an interaction technique that provides a realistic haptic experience for grabbing tools using only passive mechanisms.  This technique leverages the ungrounded feedback inherent in grabbing tools combined with dynamic visual adjustments of their position in virtual reality to create an illusion of physical presence for virtual objects.  To demonstrate the capabilities of this technique, we created VR Grabbers, an exemplary passive VR controller, similar to training chopsticks, with haptic feedback for precise object selection and manipulation.\n\nWe conducted two user studies based on VR Grabbers.  The first study probed the perceptual limits of the illusion; we found that the maximum position difference between the virtual and physical world acceptable to the user is (-1.48, 1.95) cm.  The second study showed that task performance of the VR Grabbers controller with Ungrounded Haptic Retargeting enabled outperforms the same controller with Ungrounded Haptic Retargeting disabled.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 20104
        },
        {
          "affiliations": [
            {
              "institution": "HP Inc."
            }
          ],
          "personId": 13536
        },
        {
          "affiliations": [
            {
              "institution": "HP Inc."
            }
          ],
          "personId": 10300
        },
        {
          "affiliations": [
            {
              "institution": "HP Inc."
            }
          ],
          "personId": 12747
        }
      ],
      "sessionIds": [
        1787
      ],
      "eventIds": []
    },
    {
      "id": 7481,
      "typeId": 11151,
      "title": "Haptopus : Haptic VR Experience Using Suction Mechanism Embedded in Head-mounted Display",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "With the spread of VR experiences using HMD, many proposals have been made to improve the experiences by providing tactile information to the fingertips. However, there are problems, such as difficulty attaching and detaching the devices and hindrances to free finger movement. To solve these issues, we developed “Haptopus,” which embeds a tactile display in the HMD and presents tactile sensations to the face. In this paper, we conducted a preliminary investigation on the best suction pressure and compared Haptopus to conventional tactile presentation approaches. As a result, we confirmed that Haptopus improves the quality of the VR experience.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The University of Electro-Communications"
            }
          ],
          "personId": 15033
        },
        {
          "affiliations": [
            {
              "institution": "The University of Electro-Communications"
            }
          ],
          "personId": 13892
        },
        {
          "affiliations": [
            {
              "institution": "The University of Electro-Communications"
            }
          ],
          "personId": 8715
        },
        {
          "affiliations": [
            {
              "institution": "The University of Electro-Communications"
            }
          ],
          "personId": 9462
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6457,
      "typeId": 11170,
      "title": "Scenograph: Fitting Real-Walking VR Experiences into Various Tracking Volumes",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "When developing a real-walking virtual reality experience, designers generally create virtual locations to fit a specific tracking volume. Unfortunately, this prevents the resulting experience from running on a smaller or differently shaped tracking volume. To address this, we present a software system called Scenograph. The core of Scenograph is a tracking volume-independent representation of real-walking experiences. Scenograph instantiates the experience to a tracking volume of given size and shape by splitting the locations into smaller ones while maintaining narrative structure. In our user study, participants’ ratings of realism decreased significantly when existing techniques were used to map a 25m2 experience to 9m2 and an L-shaped 8m2 tracking volume. In contrast, ratings did not differ when Scenograph was used to instantiate the experience.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 8413
        },
        {
          "affiliations": [
            {
              "institution": "Hasso Plattner Institute"
            }
          ],
          "personId": 13798
        }
      ],
      "sessionIds": [
        1612
      ],
      "eventIds": []
    },
    {
      "id": 3903,
      "typeId": 11199,
      "title": "D-Aquarium: A Digital Aquarium to Reduce Perceived Waiting Time at Children’s Hospital",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Patients waiting for long to use medical services become more physically and psychologically anxious than do people waiting to use general services. Since children feel more anxiety and fear in a hospital, it is necessary to reduce their perceived waiting time by disturbing their awareness of time and dispersing their attention.\nWe present the D-Aquarium, a computer-based digital aquarium that provides psychological stability to pediatric patients and reduces their perceived waiting time by using distractions to alleviate their psychological anxiety and interfere with their perception of time.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Sungkyunkwan University"
            }
          ],
          "personId": 15046
        },
        {
          "affiliations": [
            {
              "institution": "Sungkyunkwan University"
            }
          ],
          "personId": 15055
        },
        {
          "affiliations": [
            {
              "institution": "Sungkyunkwan University"
            }
          ],
          "personId": 17320
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5952,
      "typeId": 10484,
      "title": "The Right Content at the Right Time: Contextual Examples for Just-in-time Creative Learning",
      "trackId": 10036,
      "tags": [],
      "keywords": [],
      "abstract": "People often run into barriers when doing creative tasks with software because it is difficult to translate goals into concrete actions. While expert-made tutorials, examples, and documentation abound online, finding the most relevant content and adapting it to one’s own situation and task is a challenge. My research introduces techniques for exposing relevant examples to novices in the context of their own workflows. These techniques are embodied in three systems. The first, RePlay, helps people find solutions when stuck by automatically locating relevant moments from expert-made videos. The second, DiscoverySpace, helps novices get started by mining and recommending expert-made software macros. The third, CritiqueKit, helps novices improve their work by providing ambient guidance and recommendations. Preliminary experiments with RePlay suggest that contextual video clips help people complete targeted tasks. Controlled experiments with DiscoverySpace and CritiqueKit demonstrate that software macros prevent novices from losing confidence, and ambient guidance improves novice output. My research illustrates the power of user communities to support creative learning.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of California, San Diego"
            }
          ],
          "personId": 19845
        }
      ],
      "sessionIds": [
        1992
      ],
      "eventIds": []
    },
    {
      "id": 7494,
      "typeId": 11170,
      "title": "Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present Ownershift, an interaction technique for easing\noverhead manipulation in virtual reality, while preserving the\nillusion that the virtual hand is the user’s own hand. In contrast\nto previous approaches, this technique does not alter\nthe mapping of the virtual hand position for initial reaching\nmovements towards the target. Instead, the virtual hand space\nis only shifted gradually if interaction with the overhead target\nrequires an extended amount of time. While users perceive\ntheir virtual hand as operating overhead, their physical\nhand moves gradually to a less strained position at waist level.\nWe evaluated the technique in a user study and show that\nOwnershift significantly reduces the physical strain of overhead\ninteractions, while only slightly reducing task performance\nand the sense of body ownership of the virtual hand.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Aarhus University"
            }
          ],
          "personId": 14245
        },
        {
          "affiliations": [
            {
              "institution": "University of Bayreuth"
            }
          ],
          "personId": 10319
        }
      ],
      "sessionIds": [
        1872
      ],
      "eventIds": []
    },
    {
      "id": 6986,
      "typeId": 11151,
      "title": "OptRod: Constructing Interactive Surface with Multiple Functions and Flexible Shape by Projected Image",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "In this demonstration, we propose OptRod, constructing interactive surface with multiple functions and flexible shape by projected image. A PC generates images as control signals and projects them to the bottom of OptRods by a projector or LCD. An OptRod receives the light and converts its brightness into a control signal for the attached output device. By using multiple OptRods, the PC can simultaneously operate many output devices without any signal lines. Moreover, we can arrange surfaces of various shapes easily by combining multiple OptRods. OptRod supports various functions by replacing the device unit connected to OptRod.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 11229
        },
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 17183
        },
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 20297
        },
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 20920
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6987,
      "typeId": 11199,
      "title": "Investigation into Natural Gestures Using EMG for SuperNatural\" Interaction in VR\"",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Can natural interaction requirements be fulfilled while still harnessing the \"supernatural\" fantasy of Virtual Reality (VR)? In this work we used off the shelf Electromyogram (EMG) sensors as an input device which can afford natural gestures to preform the \"supernatural\" task of growing your arm in VR. We recorded 18 participants preforming a simple retrieval task in two phases; an initial and a learning phase where the stretch arm was disabled and enabled respectively. The results show that the gestures used in the initial phase are different than the main gestures used to retrieve an object in our system and that the times taken to complete the learning phase are highly variable across participants.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Technical University of Munich"
            }
          ],
          "personId": 16264
        },
        {
          "affiliations": [
            {
              "institution": "Technical University of Munich"
            }
          ],
          "personId": 16591
        },
        {
          "affiliations": [
            {
              "institution": "Technical University of Munich"
            }
          ],
          "personId": 17976
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 5963,
      "typeId": 11170,
      "title": "FDSense: Estimating Young's Modulus and Stiffness of End Effectors to Facilitate Kinetic Interaction on Touch Surfaces",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We make touch input by physically colliding an end effector (e.g., a body part or a stylus) with a touch surface. Prior studies have examined the use of kinematic variables of collision between objects, such as position, velocity, force, and impact. However, the nature of the collision can be understood more thoroughly by considering the known physical relationships that exist between directly measurable variables (i.e., kinetics). Based on this collision kinetics, this study proposes a novel touch technique called FDSense. By simultaneously observing the force and contact area measured from the touchpad, FDSense allows estimation of the Young’s modulus and stiffness of the object being contacted. Our technical evaluation showed that FDSense could effectively estimate the Young’s modulus of end effectors made of various materials, and the stiffness of each part of the human hand. Two applications using FDSense were demonstrated, for digital painting and digital instruments, where the result of the expression varies significantly depending on the elasticity of the end effector. In a following informal study, participants assessed the technique positively.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Korea Institute of Science and Technology"
            }
          ],
          "personId": 16502
        },
        {
          "affiliations": [
            {
              "institution": "Korea Institute of Science and Technology"
            }
          ],
          "personId": 16935
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 10981
        },
        {
          "affiliations": [
            {
              "institution": "bjlee1985@gmail.com"
            }
          ],
          "personId": 8834
        }
      ],
      "sessionIds": [
        2277
      ],
      "eventIds": []
    },
    {
      "id": 6990,
      "typeId": 11151,
      "title": "DextrES: Wearable Haptic Feedback for Grasping in VR via a Thin Form-Factor Electrostatic Brake",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce DextrES, a flexible and wearable haptic glove which integrates both kinesthetic and cutaneous feedback in a thin and light form factor (weight is less than 8g).\nOur approach is based on an electrostatic clutch generating up to 20 N of holding force on each finger by modulating the electrostatic attraction between flexible elastic metal strips to generate an electrically-controlled friction force. We harness the resulting braking force to rapidly render on-demand kinesthetic feedback. The electrostatic brake is mounted onto the the index finger and thumb via modular 3D printed articulated guides which allow the metal strips to glide smoothly.  Cutaneous feedback is provided via piezo actuators at the fingertips. We demonstrate that our approach can provide rich haptic feedback under dexterous articulation of the user's hands and provides effective haptic feedback across a variety of different grasps. A controlled experiment indicates that DextrES improves the grasping precision for different types of virtual objects. Finally, we report on results of a psycho-physical study which identifies discrimination thresholds for different levels of holding force.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "EPFL"
            }
          ],
          "personId": 22378
        },
        {
          "affiliations": [
            {
              "institution": "ETH"
            }
          ],
          "personId": 21660
        },
        {
          "affiliations": [
            {
              "institution": "EPFL"
            }
          ],
          "personId": 9609
        },
        {
          "affiliations": [
            {
              "institution": "ETH"
            }
          ],
          "personId": 14264
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 5456,
      "typeId": 11170,
      "title": "ShareSpace: Facilitating Shared Use of the Physical Space by both VR Head-Mounted Display and External Users",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Currently, “walkable” virtual reality (VR) is achieved by dedicating a room-sized space for VR activities, which is not shared with non-HMD users engaged in their own activities. To achieve the goal of allowing shared use of space for all users while overcoming the obvious difficulty of integrating use with those immersed in a VR experience, we present ShareSpace, a system that allows external users to communicate their needs for physical space to those wearing an HMD and immersed in their VR experience. ShareSpace works by allowing external users to place “shields” in the virtual environment by using a set of physical shield tools. A pad visualizer helps this process by allowing external users to examine the arrangement of virtual shields. We also discuss interaction techniques that minimize the interference between the respective activities of the HMD wearers and the other users of the same physical space. To evaluate our design, a user study was conducted to collect user feedback from participants in four trial scenarios. The results indicate that our ShareSpace system allows users to perform their respective activities with improved engagement and safety. In addition, this study shows that while the HMD users did perceive a considerable degree of interference due to the internal visual indications from the ShareSpace system, they were still more engaged in their VR experience than when interrupted by direct external physical interference initiated by external users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 18886
        },
        {
          "affiliations": [
            {
              "institution": "Nation Chiao Tung University"
            }
          ],
          "personId": 20662
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 13651
        }
      ],
      "sessionIds": [
        1612
      ],
      "eventIds": []
    },
    {
      "id": 6481,
      "typeId": 11170,
      "title": "FacePush: Introducing Normal Force on Face with Head-Mounted Displays",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents FacePush, a Head-Mounted Display (HMD) integrated with a pulley system to generate normal forces on a user’s face in virtual reality (VR). The mechanism of FacePush is obtained by shifting torques provided by two motors that press upon a user’s face via utilization of a pulley system. FacePush can generate normal forces of varying strengths and apply those to the surface of the face. To inform our design of FacePush for noticeable and discernible normal forces in VR applications, we conducted two studies to iden- tify the absolute detection threshold and the discrimination threshold for users’ perception. After further consideration in regard to user comfort, we determined that two levels of force, 2.7 kPa and 3.375 kPa, are ideal for the development of the FacePush experience via implementation with three applications which demonstrate use of discrete and continuous normal force for the actions of boxing, diving, and 360 guidance in virtual reality. In addition, with regards to a virtual boxing application, we conducted a user study evaluating the user experience in terms of enjoyment and realism and collected the user’s feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 21737
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 15049
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 15070
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 22175
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 24154
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 22293
        }
      ],
      "sessionIds": [
        1787
      ],
      "eventIds": []
    },
    {
      "id": 3922,
      "typeId": 11151,
      "title": "[Sponsor Demo] MakeBlock",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 8019,
      "typeId": 11199,
      "title": "DynamicSlide: Reference-based Interaction Techniques for Slide-based Lecture Videos",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Presentation slides play an important role in online lecture videos. Slides convey the main points of the lecture visually, while the instructor's narration adds detailed verbal explanations to each item in the slide. We call the link between a slide item and the corresponding part of the narration a reference. In order to assess the feasibility of reference-based interaction techniques for watching videos, we introduce DynamicSlide, a video processing system that automatically extracts references from slide-based lecture videos and a video player. The system incorporates a set of reference-based techniques: emphasizing the current item in the slide that is being explained, enabling item-based navigation, and enabling item-based note-taking. Our pipeline correctly finds 79% of the references in a set of five videos with 141 references. Results from a user study suggest that DynamicSlide's features improve the learner's video browsing and navigation experience.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 20086
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 20111
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 12347
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 4948,
      "typeId": 11199,
      "title": "resources2City : Explorer A System for Generating Interactive Walkable Virtual Cities out of File Systems",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "We present Resources2City Explorer (R2CE), a tool for representing ?le systems as interactive, walkable virtual cities. R2CE visualizes ?le systems based on concepts of spatial, 3D information processing. For this purpose, it extends the range of functions of conventional ?le browsers considerably. Visual elements in a city generated by R2CE represent (relations of) objects of the underlying ?le system. The paper describes the functional spectrum of R2CE and illustrates it by visualizing a sample of 940 ?les.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Goethe University Frankfurt"
            }
          ],
          "personId": 14661
        },
        {
          "affiliations": [
            {
              "institution": "Goethe University Frankfurt"
            }
          ],
          "personId": 21630
        },
        {
          "affiliations": [
            {
              "institution": "Goethe University"
            }
          ],
          "personId": 20441
        },
        {
          "affiliations": [
            {
              "institution": "Goethe University Frankfurt"
            }
          ],
          "personId": 15965
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3930,
      "typeId": 11170,
      "title": "Immersive Trip Reports",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Since the advent of consumer photography, tourists and hikers have made photo records of their trips to share later. Aside from being kept as memories, photo presentations such as slideshows are also shown to others who have not visited the location to try to convey the experience.However, a slideshow alone is limited in conveying the broader spatial context, and thus the feeling of presence in beautiful natural scenery is lost. We address this by presenting the photographs as part of an immersive experience. We introduce an automated pipeline for aligning photographs with a digital terrain model. From this geographic registration, we produce immersive presentations which are viewed either passively as a video, or interactively in virtual reality. Our experimental evaluation verifies that this new mode of presentation successfully conveys the spatial context of the scene and is enjoyable to users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Brno University of Technology & Adobe Research"
            }
          ],
          "personId": 21511
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 9897
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 22696
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 19763
        },
        {
          "affiliations": [
            {
              "institution": "Brno University of Technology"
            }
          ],
          "personId": 17099
        }
      ],
      "sessionIds": [
        1028
      ],
      "eventIds": []
    },
    {
      "id": 4444,
      "typeId": 11170,
      "title": "4DMesh: 4D Printing Morphing Non-Developable Mesh Surfaces",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present 4DMesh, a method of combining shrinking and bending thermoplastic actuators with customized geometric algorithms to 4D print and morph centimeter- to meter-sized functional non-developable surfaces. We will share two end-to-end inverse design algorithms. With our tools, users can input CAD models of target surfaces and produce respective printable files. The flat sheet printed can morph into target surfaces when triggered by heat. This system saves shipping and packaging costs, in addition to enabling customizability for the design of relatively large non-developable structures. We designed a few functional artifacts to leverage the advantage of non-developable surfaces for their unique functionalities in aesthetics, mechanical strength, geometric ergonomics and other functionalities. In addition, we demonstrated how this technique can potentially be adapted to customize molds for industrial parts (e.g., car, boat, etc.) in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 11874
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 18546
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 15616
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 17256
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University & Zhejiang University"
            }
          ],
          "personId": 19351
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 8263
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 16314
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 20614
        }
      ],
      "sessionIds": [
        1312
      ],
      "eventIds": []
    },
    {
      "id": 3948,
      "typeId": 11199,
      "title": "Mixed-Reality for Object-Focused Remote Collaboration",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we outline the design of a mixed-reality system to support object-focused remote collaboration. Here, being able to adjust collaborators’ perspectives on the object as well as understand one another’s perspective is essential to support effective collaboration over distance. We propose a low-cost mixed-reality system that allows users to: (1) quickly align and understand each other’s perspective; (2) explore objects independently from one another, and (3) render gestures in the remote’s workspace. In this work, we focus on the expert’s role and we introduce an interaction technique allowing users to quickly manipulation 3D virtual objects in space.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "htw saar"
            }
          ],
          "personId": 11140
        },
        {
          "affiliations": [
            {
              "institution": "University of Calgary"
            }
          ],
          "personId": 15247
        },
        {
          "affiliations": [
            {
              "institution": "University of New Brunswick"
            }
          ],
          "personId": 18156
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 2936,
      "typeId": 11170,
      "title": "MetaArms: Body Remapping Using Feet-Controlled Artificial Arms",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce MetaArms, wearable anthropomorphic robotic arms and hands with six degrees of freedom operated by the user’s legs and feet. Our overall research goal is to re-imagine what our bodies can do with the aid of wearable robotics using a body-remapping approach. To this end, we present an initial exploratory case study. MetaArms’ two robotic arms are controlled by the user’s feet motion, and the robotic hands can grip objects according to the user’s toes bending. Haptic feedback is also presented on the user’s feet that correlate with the touched objects on the robotic hands, creating a closed-loop system. We present formal and informal evaluations of the system, the former using a 2D pointing task according to Fitts’ Law. The overall throughput for 12 users of the system is reported as 1.01 bits/s (std 0.39). We also present informal feedback from over 230 users. We find that MetaArms demonstrate the feasibility of body-remapping approach in designing robotic limbs that may help us re-imagine what the human body could do.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 12572
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11423
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 15687
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 8508
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 16545
        }
      ],
      "sessionIds": [
        2519
      ],
      "eventIds": []
    },
    {
      "id": 8059,
      "typeId": 11151,
      "title": "Knobology 2.0: Giving Shape to the Haptic Force Feedback of Interactive Knobs",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present six rotary knobs, each with a distinct shape, that provide haptic force feedback on rotation. The knob shapes were evaluated in relation to twelve haptic feedback stimuli. The stimuli were designed as a combination of the most relevant perceptual parameters of force feedback; acceleration, friction, detent amplitude and spacing. The results indicate that there is a relationship between the shape of a knob and its haptic feedback. The perceived functionality can be dynamically altered by changing its shape and haptic feedback. This work serves as basis for the design of dynamic interface controls that can adapt their shape and haptic feel to the content that is controlled. \\ In our demonstration, we show the six distinct knobs shapes with the different haptic feedback stimuli. Attendees can experience the interaction with the different knob shapes in relation the stimuli and design stimuli with a graphical editor.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "a.v.oosterhout@cs.au.dk"
            }
          ],
          "personId": 14470
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4475,
      "typeId": 11151,
      "title": "[Best of SUI] Flip-Flop Sticker: Force-to-Motion Type 3DoF Input Device for Capacitive Touch Surface",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 13069
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6523,
      "typeId": 11170,
      "title": "Crowdsourcing Similarity Judgments for Agreement Analysis in End-User Elicitation Studies",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "End-user elicitation studies are a popular design method, but their data require substantial time and effort to analyze. In this paper, we present Crowdsensus, a crowd-powered tool that enables researchers to efficiently analyze the results of elicitation studies using subjective human judgment and automatic clustering algorithms. In addition to our own analysis, we asked six expert researchers with experience running and analyzing elicitation studies to analyze an end-user elicitation dataset of 10 functions for operating a web-browser, each with 43 voice commands elicited from end-users for a total of 430 voice commands. We used Crowdsensus to gather similarity judgments of these same 430 commands from 410 online crowd workers. The crowd outperformed the experts by arriving at the same results for seven of eight functions and resolving a function where the experts failed to agree. Also, using Crowdsensus was about four times faster than using experts.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 12922
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 21129
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 11108
        }
      ],
      "sessionIds": [
        1524
      ],
      "eventIds": []
    },
    {
      "id": 7036,
      "typeId": 11199,
      "title": "DroneCTRL: A Tangible Remote Input Control for Quadcopters",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Recent research has presented quadcopters to enable mid-air interaction. Using quadcopters to provide tactile feedback, navigation, or user input are the current scope of related work. However, most quadcopter steering systems are complicated to use for non-expert users or require an expensive tracking system for autonomous flying. Safety-critical scenarios require trained and expensive personnel to navigate quadcopters through crucial flight paths within narrow spaces. To simplify the input and manual operation of quadcopters, we present DroneCTRL, a tangible pointing device to navigate quadcopters. DroneCTRL resembles a remote control including optional visual feedback by a laser pointer and tangibility to improve the quadcopter control usability for non-expert users. In a preliminary user study, we compare the efficiency of hardware and software-based controller with DroneCTRL. Our results favor the usage of DroneCTRL with and without visual feedback to achieve more precision and accuracy.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 23443
        },
        {
          "affiliations": [
            {
              "institution": "TU Darmstadt"
            }
          ],
          "personId": 8818
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 20162
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 2945,
      "typeId": 11151,
      "title": "[Sponsor Demo] Nureva",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 2951,
      "typeId": 11170,
      "title": "RFIMatch: Distributed Batteryless Near-Field Identification Using RFID-Tagged Magnet-Biased Reed Switches",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents a technique enabling distributed batteryless near-field identification (ID) between two passive radio frequency ID (RFID) tags. Each conventional ultra-high-frequency (UHF) RFID tag is modified by connecting its antenna and chip to a reed switch and then attaching a magnet to one of the reed switch's terminals, thus transforming it into an always-on switch. When the two modules approach each other, the magnets counteract each other and turn off both switches at the same time. The coabsence of IDs thus indicates a unique interaction event. In addition to sensing, the module also provides native haptic feedback through magnetic repulsion force, enabling users to perceive the system's state eyes-free, without physical constraints. Additional visual feedback can be provided through an energy-harvesting module and a light emitting diode. This specific hardware design supports contactless, orientation-invariant sensing, with a form factor compact enough for embedded and wearable use in ubiquitous computing applications.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Eindhoven University of Technology"
            }
          ],
          "personId": 10759
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 22323
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 18044
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 12629
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 10420
        }
      ],
      "sessionIds": [
        2218
      ],
      "eventIds": []
    },
    {
      "id": 6539,
      "typeId": 11170,
      "title": "A Mixed-Initiative Interface for Animating Static Pictures",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present an interactive tool to animate the visual elements of a static picture, based on simple sketch-based markup. While animated images enhance websites, infographics, logos, e-books, and social media, creating such animations from still pictures is difficult for novices and tedious for experts. Creating automatic tools is challenging due to ambiguities in object segmentation, relative depth ordering, and non-existent temporal information. With a few user drawn scribbles as input, our mixed initiative creative interface extracts repetitive texture elements in an image, and supports animating them. Our system also facilitates the creation of multiple layers to enhance depth cues in the animation. Finally, after analyzing the artwork during segmentation, several animation processes automatically generate kinetic textures that are spatio-temporally coherent with the source image. Our results, as well as feedback from our user evaluation, suggest that our system effectively allows illustrators and animators to add life to still images in a broad range of visual styles.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Princeton University"
            }
          ],
          "personId": 15678
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk"
            }
          ],
          "personId": 14972
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk"
            }
          ],
          "personId": 14001
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research"
            }
          ],
          "personId": 15020
        },
        {
          "affiliations": [
            {
              "institution": "Princeton University"
            }
          ],
          "personId": 23103
        },
        {
          "affiliations": [
            {
              "institution": "Autodesk Research & University of Toronto"
            }
          ],
          "personId": 9877
        }
      ],
      "sessionIds": [
        1312
      ],
      "eventIds": []
    },
    {
      "id": 5009,
      "typeId": 11199,
      "title": "Towards a Symbiotic Human-Machine Depth Sensor: Exploring 3D Gaze for Object Reconstruction",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking is expected to become an integral part of future augmented reality (AR) head-mounted displays (HMDs) given that it can easily be integrated into existing hardware and provides a versatile interaction modality. To augment objects in the real world, AR HMDs require a three-dimensional understanding of the scene, which is currently solved using depth cameras. In this work we aim to explore how 3D gaze data can be used to enhance scene understanding for AR HMDs by envisioning a symbiotic human-machine depth camera, fusing depth data with 3D gaze information. We present a first proof of concept, exploring to what extend we are able to recognise what a user is looking at by plotting 3D gaze data. To measure 3D gaze, we implemented a vergence-based algorithm and built an eye tracking setup consisting of a Pupil Labs headset and an OptiTrack motion capture system, allowing us to measure 3D gaze inside a 50x50x50 cm volume. We show first 3D gaze plots of \"gazed-at\" objects and describe our vision of a symbiotic human-machine depth camera that combines a depth camera and human 3D gaze information.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 23720
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 13792
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 12287
        },
        {
          "affiliations": [
            {
              "institution": "University of Stuttgart"
            }
          ],
          "personId": 24186
        },
        {
          "affiliations": [
            {
              "institution": "Ulm University"
            }
          ],
          "personId": 17172
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 6036,
      "typeId": 11199,
      "title": "Pop-up Robotics: Facilitating HRI in Public Spaces",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Human-Robot Interaction (HRI) research in public spaces often encounters delays and restrictions due to several factors, including the need for sophisticated technology, regulatory approvals, and public or community support. To remedy these concerns, we suggest HRI can apply the core philosophy of Tactical Urbanism, a concept from urban planning, to catalyze HRI in public spaces, provide community feedback and information on the feasibility of future implementations of robots in the public, and also create social impact and forge connections with the community while spreading awareness about robots as a public resource. As a case study, we share tactics used and strategies followed to conduct a pop-up style study of 'A robotic mailbox to support and raise awareness about homelessness.' We discuss benefits and challenges of the pop-up approach and recommend using it to enable the social studies of HRI not only to match but to precede, the fast-paced technological advancement and deployment of robots.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Indiana University, Bloomington"
            }
          ],
          "personId": 14694
        },
        {
          "affiliations": [
            {
              "institution": "Indiana University, Bloomington"
            }
          ],
          "personId": 16132
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 2966,
      "typeId": 11151,
      "title": "Wearable Haptic Device that Presents the Haptics Sensation Corresponding to Three Fingers on the Forearm",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "In this demonstration, as an attempt of a new haptic presentation method for objects in virtual reality (VR) environment, we show a device that presents the haptic sensation of the fingertip on the forearm, not on the fingertip. This device adopts a five-bar linkage mechanism and it is possible to present the strength, direction of force. Compared with a fingertip mounted type displays, it is possible to address the issues of their weight and size which hinder the free movement of fingers. We have confirmed that the experiences in the VR environment is improved compared with without haptics cues situation regardless of without presenting haptics information directly to the fingertip.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Electro-Communications"
            }
          ],
          "personId": 21762
        },
        {
          "affiliations": [
            {
              "institution": "University of Electro-Communications"
            }
          ],
          "personId": 18879
        },
        {
          "affiliations": [
            {
              "institution": "University of Electro-Communications"
            }
          ],
          "personId": 20220
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4503,
      "typeId": 11151,
      "title": "[Best of SUI] MagicPAPER: An Integrated Shadow- Art Hardware Device Enabling Touch Interaction on Kraft paper",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 17422
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 7577,
      "typeId": 11151,
      "title": "4DMesh: 4D Printing Morphing Non-Developable Mesh Surfaces",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "We present 4DMesh, a method of combining shrinking and bending thermoplastic actuators with customized geometric algorithms to 4D print and morph centimeter- to meter-sized functional non-developable surfaces. We will share two end-to-end inverse design algorithms. With our tools, users can input CAD models of target surfaces and produce respective printable files. The flat sheet printed can morph into target surfaces when triggered by heat. This system saves shipping and packaging costs, in addition to enabling customizability for the design of relatively large non-developable structures. We designed a few functional artifacts to leverage the advantage of non-developable surfaces for their unique functionalities in aesthetics, mechanical strength, geometric ergonomics and other functionalities. In addition, we demonstrated how this technique can potentially be adapted to customize molds for industrial parts (e.g., car, boat, etc.) in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 11874
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 18546
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 15616
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 17256
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University & Zhejiang University"
            }
          ],
          "personId": 19351
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 8263
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 16314
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 20614
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3481,
      "typeId": 10484,
      "title": "Crowd-AI Systems for Non-Visual Information Access in the Real World",
      "trackId": 10036,
      "tags": [],
      "keywords": [],
      "abstract": "The world is full of information, interfaces and environments that are inaccessible to blind people. When navigating indoors, blind people are often unaware of key visual information, such as posters, signs, and exit doors. When accessing specific interfaces, blind people cannot independently do so without at least first learning their layout and labeling them with sighted assistance. My work investigates interactive systems that integrates computer vision, on-demand crowdsourcing, and wearables to amplify the abilities of blind people, offering solutions for real-time environment and interface navigation. My work provides more options for blind people to access information and increases their freedom in navigating the world.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 12853
        }
      ],
      "sessionIds": [
        1992
      ],
      "eventIds": []
    },
    {
      "id": 2979,
      "typeId": 11170,
      "title": "Montage: A Video Prototyping System to Reduce Re-Shooting and Increase Re-Usability",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Video prototypes help capture and communicate interaction with paper prototypes in the early stages of design.\nHowever, designers sometimes find it tedious to create stop-motion videos for continuous interactions and to re-shoot clips as the design evolves.\nWe introduce Montage, a proof-of-concept implementation of a computer-assisted process for video prototyping.\nMontage lets designers progressively augment video prototypes with digital sketches, facilitating the creation, reuse and exploration of dynamic interactions.\nMontage uses chroma keying to decouple the prototyped interface from its context of use, letting designers reuse or change them independently.\nWe describe how Montage enhances video prototyping by combining video with digital animated sketches, encourages the exploration of different contexts of use, and supports prototyping of different interaction styles.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Université Paris-Sud, CNRS, Inria, Université Paris-Saclay"
            }
          ],
          "personId": 18247
        },
        {
          "affiliations": [
            {
              "institution": "Université Paris-Sud, CNRS, Inria, Université Paris-Saclay"
            }
          ],
          "personId": 20737
        }
      ],
      "sessionIds": [
        1312
      ],
      "eventIds": []
    },
    {
      "id": 2980,
      "typeId": 11151,
      "title": "PuPoP: Pop-up Prop on Palm for Virtual Reality",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 11521
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 24206
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University of Science and Technology"
            }
          ],
          "personId": 9475
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 17640
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 19827
        },
        {
          "affiliations": [
            {
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 22293
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 10420
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6567,
      "typeId": 11151,
      "title": "The Exploratory Labeling Assistant: Mixed-Initiative Label Curation with Large Document Collections",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we define the concept of exploratory labeling: the use of computational and interactive methods to help analysts categorize groups of documents into a set of unknown and evolving labels. While many computational methods exist to analyze data and build models once the data is organized around a set of predefined categories or labels, few methods address the problem of reliably discovering and curating such labels in the first place. In order to move first steps towards bridging this gap, we propose an interactive visual data analysis method that integrates human-driven label ideation, specification and refinement with machine-driven recommendations. The proposed method enables the user to progressively discover and ideate labels in an exploratory fashion and specify rules that can be used to automatically match sets of documents to labels. To support this process of ideation, specification, as well as evaluation of the labels, we use unsupervised machine learning methods that provide suggestions and data summaries. We evaluate our method by applying it to a real-world labeling problem as well as through controlled user studies to identify and reflect on patterns of interaction emerging from exploratory labeling activities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 23408
        },
        {
          "affiliations": [
            {
              "institution": "Pacific Northwest National Lab"
            }
          ],
          "personId": 21370
        },
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 15309
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 3497,
      "typeId": 11170,
      "title": "Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 14361
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 20201
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 10639
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 9128
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 12753
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11040
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11676
        }
      ],
      "sessionIds": [
        1903
      ],
      "eventIds": []
    },
    {
      "id": 7083,
      "typeId": 11170,
      "title": "Tacttoo: A Thin and Feel-Through Tattoo for On-Skin Tactile Output",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "This paper introduces Tacttoo, a feel-through interface for electro-tactile output on the user's skin.  Integrated in a temporary tattoo with a thin and conformal form factor, it can be applied on complex body geometries, including the fingertip, and is scalable to various body locations. At less than 35µm in thickness, it is the thinnest tactile interface for wearable computing to date. Our results show that Tacttoo retains the natural tactile acuity similar to bare skin while delivering high-density tactile output.\nWe present the fabrication of customized Tacttoo tattoos using DIY tools and contribute a mechanism for consistent electro-tactile operation on the skin.\nMoreover, we explore new interactive scenarios that are enabled by Tacttoo. Applications in tactile augmented reality and on-skin interaction benefit from a seamless augmentation of real-world tactile cues with computer-generated stimuli. Applications in virtual reality and private notifications benefit from high-density output in an ergonomic form factor.\nResults from two psychophysical studies and a technical evaluation demonstrate Tacttoo's functionality, feel-through properties and durability.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Saarland University, Saarland Informatics Campus"
            }
          ],
          "personId": 22878
        },
        {
          "affiliations": [
            {
              "institution": "Saarland University, Saarland Informatics Campus"
            }
          ],
          "personId": 8725
        },
        {
          "affiliations": [
            {
              "institution": "Saarland University, Saarland Informatics Campus"
            }
          ],
          "personId": 23653
        }
      ],
      "sessionIds": [
        1006
      ],
      "eventIds": []
    },
    {
      "id": 2988,
      "typeId": 11170,
      "title": "Sprout: Crowd-Powered Task Design for Crowdsourcing",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "While crowdsourcing enables data collection at scale, ensuring high-quality data remains a challenge. In particular, effective task design underlies nearly every reported crowdsourcing success, yet remains difficult to accomplish. Task design is hard because it involves a costly iterative process: identifying the kind of work output one wants, conveying this information to workers, observing worker performance, understanding what remains ambiguous, revising the instructions, and repeating the process until the resulting output is satisfactory.\n\nTo facilitate this process, we propose a novel meta-workflow that helps requesters optimize crowdsourcing task designs and Sprout, our open-source tool, which implements this workflow. Sprout improves task designs by (1) eliciting points of confusion from crowd workers, (2) enabling requesters to quickly understand these misconceptions and the overall space of questions, and (3) guiding requesters to improve the task design in response. We report the results of a user study with two labeling tasks demonstrating that requesters strongly prefer Sprout and produce higher-rated instructions compared to current best practices for creating gated instructions (instructions plus a workflow for training and testing workers). We also offer a set of design recommendations for future tools that support crowdsourcing task design.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 16936
        },
        {
          "affiliations": [
            {
              "institution": "Indian Institute of Technology, Delhi"
            }
          ],
          "personId": 16072
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 10743
        }
      ],
      "sessionIds": [
        1524
      ],
      "eventIds": []
    },
    {
      "id": 6575,
      "typeId": 11170,
      "title": "Spacetime: Enabling Fluid Individual and Collaborative Editing in Virtual Reality",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual Reality enables users to explore content whose physics are only limited by our creativity. Such limitless environments provide us with many opportunities to explore innovative ways to support productivity and collaboration. We present Spacetime, a scene editing tool built from the ground up to explore the novel interaction techniques that empower single user interaction while maintaining fluid multi-user collaboration in immersive virtual environment. We achieve this by introducing three novel interaction concepts: the Container, a new interaction primitive that supports a rich set of object manipulation and environmental navigation techniques, Parallel Objects, which enables parallel manipulation of objects to resolve interaction conflicts and support design workflows, and Avatar Objects, which supports interaction among multiple users while maintaining an individual users’ agency. Evaluated by professional Virtual Reality designers, Spacetime supports powerful individual and fluid collaborative workflows.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 21666
        },
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 9687
        },
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 21253
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 17259
        }
      ],
      "sessionIds": [
        2384
      ],
      "eventIds": []
    },
    {
      "id": 8114,
      "typeId": 11170,
      "title": "Facilitating Document Reading by Linking Text and Tables",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Document authors commonly use tables to support arguments presented in\nthe text. But, because tables are usually separate from the main body\ntext, readers must split their attention between different parts of\nthe document.\nWe present an interactive document reader that automatically links\ndocument text with corresponding table cells.\nReaders can select a\nsentence (or tables cells) and our reader highlights the relevant\ntable cells (or sentences).\nWe provide an automatic pipeline for\nextracting such references between sentence text and table cells for\nexisting PDF documents that combines structural analysis of tables\nwith natural language processing and rule-based matching.\nOn a test corpus of 330 (sentence, table) pairs, our pipeline correctly\nextracts 48.8% of the references. An additional 30.5% contain only\nfalse negatives (FN) errors -- the reference is missing table cells.\nThe remaining 20.7% contain false positives (FP) errors -- the reference\nincludes extraneous table cells and could therefore mislead readers.\nA user study finds that despite such errors, our interactive document\nreader helps readers match sentences with corresponding table\ncells more accurately and quickly than a baseline document reader.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 10325
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 23114
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science and Technology"
            }
          ],
          "personId": 13822
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 17780
        }
      ],
      "sessionIds": [
        1028
      ],
      "eventIds": []
    },
    {
      "id": 5044,
      "typeId": 11170,
      "title": "I/O Braid: Scalable Touch-Sensitive Lighted Cords Using Spiraling, Repeating Sensing Textiles and Fiber Optics",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce I/O Braid, an interactive textile cord with embedded sensing and visual feedback. I/O Braid senses proximity, touch, and twist through a spiraling, repeating braiding topology of touch matrices. This sensing topology is uniquely scalable, requiring only a few sensing lines to cover the whole length of a cord. The same topology allows us to embed fiber optic strands to integrate co-located visual feedback.\n\nWe provide an overview of the enabling braiding techniques, design considerations, and approaches to gesture detection. These allow us to derive a set of interaction techniques, which we demonstrate with different form factors and capabilities. Our applications illustrate how I/O Braid can invisibly augment everyday objects, such as  touch-sensitive headphones and interactive drawstrings on garments, while enabling discoverability and feedback through embedded light sources.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 21636
        },
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 11000
        },
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 17131
        },
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 22640
        },
        {
          "affiliations": [
            {
              "institution": "Google Inc."
            }
          ],
          "personId": 16607
        }
      ],
      "sessionIds": [
        2218
      ],
      "eventIds": []
    },
    {
      "id": 7094,
      "typeId": 11170,
      "title": "MoSculp: Interactive Visualization of Shape and Time",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present a system that visualizes complex human motion via 3D motion sculptures—a representation that conveys the 3D structure swept by a human body as it moves through space. Our system computes a motion sculpture from an input video, and then embeds it back into the scene in a 3D-aware fashion. The user may also explore the sculpture directly in 3D or physically print it. Our interactive interface allows users to customize the sculpture design, for example, by selecting materials and lighting conditions.\n\nTo provide this end-to-end workflow, we introduce an algorithm\nthat estimates a human’s 3D geometry over time from a\nset of 2D images, and develop a 3D-aware image-based rendering\napproach that inserts the sculpture back into the original\nvideo. By automating the process, our system takes motion\nsculpture creation out of the realm of professional artists, and\nmakes it applicable to a wide range of existing video material.\n\nBy conveying 3D information to users, motion sculptures reveal\nspace-time motion information that is difficult to perceive\nwith the naked eye, and allow viewers to interpret how different\nparts of the object interact over time. We validate the\neffectiveness of motion sculptures with user studies, finding\nthat our visualizations are more informative about motion than\nexisting stroboscopic and space-time visualization methods.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 15099
        },
        {
          "affiliations": [
            {
              "institution": "Google Research"
            }
          ],
          "personId": 21795
        },
        {
          "affiliations": [
            {
              "institution": "Google Research"
            }
          ],
          "personId": 21980
        },
        {
          "affiliations": [
            {
              "institution": "University of Calfornia, Berkeley"
            }
          ],
          "personId": 12143
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 22438
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 11320
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 8737
        },
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology & Google Research"
            }
          ],
          "personId": 21631
        }
      ],
      "sessionIds": [
        1204
      ],
      "eventIds": []
    },
    {
      "id": 5566,
      "typeId": 11170,
      "title": "Authoring and Verifying Human-Robot Interactions",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "As social agents, robots designed for human interaction must adhere to human social norms. How can we enable designers, engineers, and roboticists to design robot behaviors that adhere to human social norms and do not result in interaction breakdowns? In this paper, we use automated formal-verification methods to facilitate the encoding of appropriate social norms into the interaction design of social robots and the detection of breakdowns and norm violations in order to prevent them. We have developed an authoring environment that utilizes these methods to provide developers of social-robot applications with feedback at design time and evaluated the benefits of their use in reducing such breakdowns and violations in human-robot interactions. Our evaluation with application developers (N=9) shows that the use of formal-verification methods increases designers' ability to identify and contextualize social-norm violations. We discuss the implications of our approach for the future development of tools for effective design of social-robot applications.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Wisconsin–Madison"
            }
          ],
          "personId": 9714
        },
        {
          "affiliations": [
            {
              "institution": "University of Wisconsin–La Crosse"
            }
          ],
          "personId": 9753
        },
        {
          "affiliations": [
            {
              "institution": "University of Wisconsin–Madison"
            }
          ],
          "personId": 16925
        },
        {
          "affiliations": [
            {
              "institution": "bilge@cs.wisc.edu"
            }
          ],
          "personId": 15713
        }
      ],
      "sessionIds": [
        2519
      ],
      "eventIds": []
    },
    {
      "id": 8130,
      "typeId": 11199,
      "title": "Shared Autonomy for Interactive Systems",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Across many domains, interactive systems either make decisions for us autonomously or yield decision-making authority to us and play a supporting role. However, many settings, such as those in education or the workplace, benefit from sharing this autonomy between the user and the system, and thus from a system that adapts to them over time. In this paper, we pursue two primary research questions: (1) How do we design interfaces to share autonomy between the user and the system? (2) How does shared autonomy alter a user\"s perception of a system? We present SharedKeys, an interactive shared autonomy system for piano instruction that plays different video segments of a piece for students to emulate and practice. Underlying our approach to shared autonomy is a mixed-observability Markov decision process that estimates a user\"s desired autonomy level based on her performance and attentiveness. Pilot studies revealed that students sharing autonomy with the system learned more quickly and perceived the system as more intelligent.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 23134
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 18183
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 12608
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 20000
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 21763
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 4034,
      "typeId": 11253,
      "title": "TOCHI: Automatics: Dynamically Generating Fabrication Tasks to Adapt to Varying Contexts",
      "trackId": 10041,
      "tags": [],
      "keywords": [],
      "abstract": "When fabricating, it is common to follow a prescribed set of steps in a tutorial or how-to. While popular, such explicit knowledge resources have many inconsistencies and omissions, use static illustrations, and cannot adapt to drop-in makers or a maker's mistakes. To overcome many of these issues, this work presents Automatics, a novel explicit knowledge resource system that dynamically generates fabrication activities for one or more makers based on their current environmental and fabrication context. Automatics assigns tasks to makers based on the past tools and components the maker was working with, enables makers to recover from mistakes through model regeneration, suggests alternative tools if a needed tool is unavailable or in use, and allows multiple makers to drop-in throughout a fabrication activity. Initial usage and feedback from novice makers showed that Automatics increases the number of tasks that can be completed compared to paper instructions, decreases frustration, and improves one's understanding of the global context of assigned tasks during fabrication activities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 19238
        },
        {
          "affiliations": [
            {
              "institution": "MishMashMakers"
            }
          ],
          "personId": 23683
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 17259
        }
      ],
      "sessionIds": [
        1903
      ],
      "eventIds": []
    },
    {
      "id": 3013,
      "typeId": 11199,
      "title": "Engagement Learning: Expanding Visual Knowledge by Engaging Online Participants",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Most artificial intelligence (AI) systems to date have focused entirely on performance, and rarely if at all on their social interactions with people and how to balance the AIs\" goals against their human collaborators\". Learning quickly from interactions with people poses both social challenges and is unresolved technically. In this paper, we introduce engagement learning: a training approach that learns to trade off what the AI needs---the knowledge value of a label to the AI---against what people are interested to engage with---the engagement value of the label. We realize our goal with ELIA (Engagement Learning Interaction Agent), a conversational AI agent who\"s goal is to learn new facts about the visual world by asking engaging questions of people about the photos they upload to social media. Our current deployment of ELIA on Instagram receives a response rate of 26%.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 17972
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 12011
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 14610
        },
        {
          "affiliations": [
            {
              "institution": "Stanford University"
            }
          ],
          "personId": 20000
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 8133,
      "typeId": 11199,
      "title": "Phonoscape: Auralization of Photographs using Stereophonic Auditory Icons",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we developed an auditory display method which improves the comprehension of photograph to apply the support system for person with visual impairment. The auralization method is constructed by object recognition, auditory iconization and stereophonic techniques. Through the experiments, the enhancement of intelligibility and discriminability was confirmed compared to the image-to-speech reading machine method.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 9544
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 17611
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 13769
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 10483
        },
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 11728
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 3016,
      "typeId": 11151,
      "title": "Tacttoo: A Thin and Feel-Through Tattoo for On-Skin Tactile Output",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "This paper introduces Tacttoo, a feel-through interface for electro-tactile output on the user's skin.  Integrated in a temporary tattoo with a thin and conformal form factor, it can be applied on complex body geometries, including the fingertip, and is scalable to various body locations. At less than 35µm in thickness, it is the thinnest tactile interface for wearable computing to date. Our results show that Tacttoo retains the natural tactile acuity similar to bare skin while delivering high-density tactile output.\nWe present the fabrication of customized Tacttoo tattoos using DIY tools and contribute a mechanism for consistent electro-tactile operation on the skin.\nMoreover, we explore new interactive scenarios that are enabled by Tacttoo. Applications in tactile augmented reality and on-skin interaction benefit from a seamless augmentation of real-world tactile cues with computer-generated stimuli. Applications in virtual reality and private notifications benefit from high-density output in an ergonomic form factor.\nResults from two psychophysical studies and a technical evaluation demonstrate Tacttoo's functionality, feel-through properties and durability.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Saarland University, Saarland Informatics Campus"
            }
          ],
          "personId": 22878
        },
        {
          "affiliations": [
            {
              "institution": "Saarland University, Saarland Informatics Campus"
            }
          ],
          "personId": 8725
        },
        {
          "affiliations": [
            {
              "institution": "Saarland University, Saarland Informatics Campus"
            }
          ],
          "personId": 23653
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6091,
      "typeId": 11151,
      "title": "[Best of SUI] Using Affective Computing for Proxemic Interactions in Mixed-Reality",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 15872
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 6097,
      "typeId": 11151,
      "title": "HoloRoyale: A Large Scale High Fidelity Augmented Reality Game",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Recent years saw an explosion in Augmented Reality (AR) experiences for consumers. These experiences can be classified based on the scale of the interactive area (room vs city/global scale) , or the fidelity of the experience (high vs low). Experiences that target large areas, such as campus or world scale, commonly have only rudimentary interactions with the physical world, and suffer from registration errors and jitter. We classify these experiences as large scale and low fidelity. On the other hand, various room sized experiences feature realistic interaction of virtual content with the real world. We classify these experiences as small scale and high fidelity. Our work is the first to explore the domain of large scale high fidelity (LSHF) AR experiences. We build upon the small scale high fidelity capabilities of the Microsoft HoloLens to allow LSHF interactions. We demonstrate the capabilities of our system with a game specifically designed for LSHF interactions, handling many challenges and limitations unique to the domain of LSHF AR through the game design. Our contributions are twofold: \\ - The lessons learned during the design and development of a system capable of LSHF AR interactions. \\- Identification of a set of reusable game elements specific to LSHF AR, including mechanisms for addressing spatiotemporal\ninconsistencies and crowd control. \\We believe our contributions will be fully applicable not only to games, but all LSHF AR experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 22478
        },
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 14842
        },
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 14056
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science and Technology"
            }
          ],
          "personId": 19597
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science and Technology"
            }
          ],
          "personId": 17860
        },
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 20159
        },
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 15949
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 8149,
      "typeId": 11170,
      "title": "The Exploratory Labeling Assistant: Mixed-Initiative Label Curation with Large Document Collections",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we define the concept of exploratory labeling: the use of computational and interactive methods to help analysts categorize groups of documents into a set of unknown and evolving labels. While many computational methods exist to analyze data and build models once the data is organized around a set of predefined categories or labels, few methods address the problem of reliably discovering and curating such labels in the first place. In order to move first steps towards bridging this gap, we propose an interactive visual data analysis method that integrates human-driven label ideation, specification and refinement with machine-driven recommendations. The proposed method enables the user to progressively discover and ideate labels in an exploratory fashion and specify rules that can be used to automatically match sets of documents to labels. To support this process of ideation, specification, as well as evaluation of the labels, we use unsupervised machine learning methods that provide suggestions and data summaries. We evaluate our method by applying it to a real-world labeling problem as well as through controlled user studies to identify and reflect on patterns of interaction emerging from exploratory labeling activities.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 23408
        },
        {
          "affiliations": [
            {
              "institution": "Pacific Northwest National Lab"
            }
          ],
          "personId": 21370
        },
        {
          "affiliations": [
            {
              "institution": "New York University"
            }
          ],
          "personId": 15309
        }
      ],
      "sessionIds": [
        1524
      ],
      "eventIds": []
    },
    {
      "id": 8154,
      "typeId": 11170,
      "title": "SynchronizAR: Instant Synchronization for Spontaneous and Spatial Collaborations in Augmented Reality",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present SynchronizAR, an approach to spatially register multiple SLAM devices together without sharing maps or involving external tracking infrastructures. SynchronizAR employs a distance based indirect registration which resolves the transformations between the separate SLAM coordinate systems. We attach an Ultra-Wide Bandwidth~(UWB) based distance measurements module on each of the mobile AR devices which is capable of self-localization with respect to the environment. As users move on independent paths, we collect the positions of the AR devices in their local frames and the corresponding distance measurements. Based on the registration, we support to create a spontaneous collaborative AR environment to spatially coordinate users' interactions. We run both technical evaluation and user studies to investigate the registration accuracy and the usability towards spatial collaborations. Finally, we demonstrate various collaborative AR experience using SynchronizAR.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Purdue University"
            }
          ],
          "personId": 17258
        },
        {
          "affiliations": [
            {
              "institution": "Purdue University"
            }
          ],
          "personId": 9576
        },
        {
          "affiliations": [
            {
              "institution": "Purdue University"
            }
          ],
          "personId": 21433
        },
        {
          "affiliations": [
            {
              "institution": "Purdue University"
            }
          ],
          "personId": 16979
        },
        {
          "affiliations": [
            {
              "institution": "Purdue University"
            }
          ],
          "personId": 9366
        },
        {
          "affiliations": [
            {
              "institution": "Purdue University"
            }
          ],
          "personId": 18510
        }
      ],
      "sessionIds": [
        1872
      ],
      "eventIds": []
    },
    {
      "id": 3038,
      "typeId": 11170,
      "title": "Indutivo: Contact-Based, Object-Driven Interactions with Inductive Sensing",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present Indutivo, a contact-based inductive sensing technique for contextual interactions. Our technique recognizes conductive objects (metallic primarily) that are commonly found in households and daily environments, as well as their individual movements when placed against the sensor. These movements include sliding, hinging, and rotation. We describe our sensing principle and how we designed the size, shape, and layout of our sensor coils to optimize sensitivity, sensing range, recognition and tracking accuracy. Through several studies, we also demonstrated the performance of our proposed sensing technique in environments with varying levels of noise and interference conditions. We conclude by presenting demo applications on a smartwatch, as well as insights and lessons we learned from our experience.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 23519
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College & Renmin University of China"
            }
          ],
          "personId": 22862
        },
        {
          "affiliations": [
            {
              "institution": "University of Calgary"
            }
          ],
          "personId": 20520
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 11584
        },
        {
          "affiliations": [
            {
              "institution": "Dartmouth College"
            }
          ],
          "personId": 8791
        }
      ],
      "sessionIds": [
        1006
      ],
      "eventIds": []
    },
    {
      "id": 8159,
      "typeId": 11170,
      "title": "CamTrackPoint: Camera-Based Pointing Stick Using Transmitted Light through Finger",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present CamTrackPoint, a new input interface that can be controlled by finger gestures captured by front or rear cameras of a mobile device.\nCamTrackPoint mounts a 3D-printed ring on the camera's bezel, and it senses the movements of the user's finger by tracking the light passed through the finger.\nThe proposed method provides mobile devices with a new input interface that offers physical force feedback like a pointing stick.\nThe cost of our method is low as it needs only a simple ring-shaped part on the camera bezel.\nMoreover, the ring doesn't disturb the functions of the camera, unless a user uses the interface.\nWe implement a prototype for a smartphone; two CamTrackPoint rings are made for the front and rear cameras.\nWe evaluate its performance and characteristics in an experiment.\nThe proposed technique provides smooth scrolling and would give better game experience on the available smartphone.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 14139
        },
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 20414
        },
        {
          "affiliations": [
            {
              "institution": "NTT DOCOMO"
            }
          ],
          "personId": 9515
        }
      ],
      "sessionIds": [
        1006
      ],
      "eventIds": []
    },
    {
      "id": 7653,
      "typeId": 11199,
      "title": "Augmented Collaboration in Shared Space Design with Shared Attention and Manipulation",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented collaboration in a shared house design scenario has been studied widely with various approaches. However, those studies did not consider human perception. Our goal is to lower the user’s perceptual load for augmented collaboration in shared space design scenarios. Applying attention theories, we implemented shared head gaze, shared selected object, and collaborative manipulation features in our system in two different versions with HoloLens. To investigate whether user perceptions of the two different versions differ, we conducted an experiment with 18 participants (9 pairs) and conducted a survey and semi-structured interviews. The results did not show significant differences between the two versions, but produced interesting insights. Based on the findings, we provide design guidelines for collaborative AR systems.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 24299
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 18567
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 15525
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 23289
        },
        {
          "affiliations": [
            {
              "institution": "Korea Advanced Institute of Science & Technology"
            }
          ],
          "personId": 14017
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    },
    {
      "id": 7144,
      "typeId": 11170,
      "title": "Idyll: A Markup Language for Authoring and Publishing Interactive Articles on the Web",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "The web has matured as a publishing platform: news outlets regularly publish rich, interactive stories while technical writers use animation and interaction to communicate complex ideas. This style of interactive media has the potential to engage a large audience and more clearly explain concepts, but is expensive and time consuming to produce. Drawing on industry experience and interviews with domain experts, we contribute design tools to make it easier to author and publish interactive articles. We introduce Idyll, a novel \"compile-to-the-web\" language for web-based interactive narratives. Idyll implements a flexible article model, allowing authors control over document style and layout, reader-driven events (such as button clicks and scroll triggers), and a structured interface to JavaScript components. Through both examples and first-use results from undergraduate computer science students, we show how Idyll reduces the amount of effort and custom code required to create interactive articles.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 17403
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 14639
        }
      ],
      "sessionIds": [
        1623
      ],
      "eventIds": []
    },
    {
      "id": 8172,
      "typeId": 11170,
      "title": "Ubicoustics: Plug-and-Play Acoustic Activity Recognition",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Despite sound being a rich source of information, computing devices with microphones do not leverage audio to glean useful insights about their physical and social context. For example, a smart speaker sitting on a kitchen countertop cannot figure out if it is in a kitchen, let alone know what a user is doing in a kitchen – a missed opportunity. In this work, we describe a novel, real-time, sound-based activity recognition system. We start by taking an existing, state-of-the-art sound labeling model, which we then tune to classes of interest by drawing data from professional sound effect libraries traditionally used in the entertainment industry. These well-labeled and high-quality sounds are the perfect atomic unit for data augmentation, including amplitude, reverb, and mixing, allowing us to exponentially grow our tuning data in realistic ways. We quantify the performance of our approach across a range of environments and device categories and show that microphone-equipped computing devices already have the requisite capability to unlock real-time activity recognition comparable to human accuracy.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 19589
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 16566
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 19018
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 17904
        }
      ],
      "sessionIds": [
        1395
      ],
      "eventIds": []
    },
    {
      "id": 8173,
      "typeId": 11170,
      "title": "TakeToons: Script-driven Performance Animation",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Performance animation is an expressive method for animating characters through human performance. However, character motion is only one part of creating animated stories. The typical workflow also involves writing a script, coordinating actors, and editing recorded performances. In most cases, these steps are done in isolation with separate tools, which introduces friction and hinders iteration. We propose TakeToons, a script-driven approach that allows authors to annotate standard scripts with relevant animation events like character actions, camera positions, and scene backgrounds. We compile this script into a story model that persists throughout the production process and provides a consistent structure for organizing and assembling recorded performances and propagating script or timing edits to existing recordings. TakeToons enables writing, performing and editing to happen in an integrated and interleaved manner that streamlines production and facilitates iteration. Informal feedback from professional animators suggests that our approach can benefit many existing workflows supporting individual authors and production teams with many different contributors.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Michigan & Adobe Research"
            }
          ],
          "personId": 22337
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 11434
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan & Adobe Research"
            }
          ],
          "personId": 18064
        },
        {
          "affiliations": [
            {
              "institution": "Adobe Research"
            }
          ],
          "personId": 16999
        }
      ],
      "sessionIds": [
        1312
      ],
      "eventIds": []
    },
    {
      "id": 7150,
      "typeId": 11170,
      "title": "Evaluation of Interaction Techniques for a Virtual Reality Reading Room in Diagnostic Radiology",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "Today, radiologists diagnose three dimensional medical data using two dimensional displays. When designing environments with optimal conditions for such a process various aspects like contrast, screen reflection and background light have to be considered. As shown in previous research, applying virtual environments in combination with a Head-Mounted Display for diagnostic imaging provides potential benefits to reduce issues of bad posture and diagnostic mistakes. However, there is little research in exploring the usability and user experience of such beneficial environments.\n\nIn this work we designed and evaluated different means of interaction to increase radiologists' performance. Therefore we created a virtual reality radiology reading room and employed it to evaluate three different interaction techniques. These allow a direct, semi-direct and indirect manipulation for performing scrolling- and windowing- tasks which are the most important for a radiologist.\n\nA study including nine radiologists was conducted and evaluated using the User Experience Questionnaire. Results indicate that direct manipulation is the preferred interaction technique, it outscored the other two control possibilities in attractiveness and pragmatic quality.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Erlangen-Nürnberg (FAU)"
            }
          ],
          "personId": 18767
        },
        {
          "affiliations": [
            {
              "institution": "University of Erlangen-Nürnberg (FAU)"
            }
          ],
          "personId": 8578
        },
        {
          "affiliations": [
            {
              "institution": "University of Erlangen-Nürnberg (FAU)"
            }
          ],
          "personId": 20583
        },
        {
          "affiliations": [
            {
              "institution": "Siemens Healthcare GmbH"
            }
          ],
          "personId": 17735
        },
        {
          "affiliations": [
            {
              "institution": "University of Erlangen-Nürnberg (FAU)"
            }
          ],
          "personId": 13507
        }
      ],
      "sessionIds": [
        2384
      ],
      "eventIds": []
    },
    {
      "id": 8175,
      "typeId": 10484,
      "title": "Designing Inherent Interactions on Wearable Devices",
      "trackId": 10036,
      "tags": [],
      "keywords": [],
      "abstract": "Wearable devices are becoming important computing devices to personal users. They have shown promising applications in multiple domains. However, designing interactions on smartwears remains challenging as the miniature sized formfactors limit both its input and output space. My thesis research proposes a new paradigm of Inherent Interaction on smartwears, with the idea of seeking interaction opportunities from users daily activities. This is to help bridging the gap between novel smartwear interactions and real-life experiences shared among users. This report introduces the concept of Inherent Interaction with my previous and current explorations in the category.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Manitoba"
            }
          ],
          "personId": 9401
        }
      ],
      "sessionIds": [
        1992
      ],
      "eventIds": []
    },
    {
      "id": 5106,
      "typeId": 11151,
      "title": "Haptic Interface Using Tendon Electrical Stimulation",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "This demonstration corresponds to our previous paper, which deals with our finding that a proprioceptive force sensation can be presented by electrical stimulation from the skin surface to the tendon region (Tendon Electrical Stimulation: TES). We showed that TES can elicit a force sensation, and adjusting the current parameters can control the amount of the sensation. Unlike electrical muscle stimulation (EMS), which can also present force sensation by stimulating motor nerves to contract muscles, TES is thought to present a proprioceptive force sensation by stimulating receptors or sensory nerves responsible for recognizing the magnitude of the muscle contraction existing inside the tendon. In the demo, we offer the occasion for trying TES.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The University of Electro-Communications"
            }
          ],
          "personId": 21841
        },
        {
          "affiliations": [
            {
              "institution": "The University of Electro-Communications"
            }
          ],
          "personId": 16426
        },
        {
          "affiliations": [
            {
              "institution": "The University of Electro-Communications"
            }
          ],
          "personId": 9462
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 5619,
      "typeId": 11151,
      "title": "Unlimited Electric Gum: A Piezo-based Electric Taste Apparatus Activated by Chewing",
      "trackId": 10037,
      "tags": [],
      "keywords": [],
      "abstract": "Herein, we propose “unlimited electric gum,” an electric taste device that will enable users to perceive taste for as long the user is chewing the gum. We developed an in-mouth type novel electric taste-imparting apparatus using a piezoelectric element so that the piezoelectric effect is stimulated by chewing. This enabled the design of a device that does not require cables around a user's lips or batteries in their mouth. In this paper, we introduce this device and report our experimental and exhibition results.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Meiji University"
            }
          ],
          "personId": 13284
        },
        {
          "affiliations": [
            {
              "institution": "University of Tokyo"
            }
          ],
          "personId": 21165
        },
        {
          "affiliations": [
            {
              "institution": "Meiji University"
            }
          ],
          "personId": 9788
        },
        {
          "affiliations": [
            {
              "institution": "Meiji University"
            }
          ],
          "personId": 13681
        }
      ],
      "sessionIds": [
        1860
      ],
      "eventIds": []
    },
    {
      "id": 4092,
      "typeId": 11170,
      "title": "Wireless Analytics for 3D Printed Objects",
      "trackId": 10038,
      "tags": [],
      "keywords": [],
      "abstract": "We present the first wireless physical analytics  system for 3D printed objects using commonly available conductive plastic filaments. Our design can enable various data capture and wireless physical analytics capabilities for 3D printed objects, without the need for  electronics. To achieve this goal, we make three key contributions: (1) demonstrate room scale backscatter communication and sensing using conductive plastic filaments, (2)  introduce the first backscatter designs that detect a variety of bi-directional motions and support linear and rotational movements, and (3)  enable data capture and storage for later retrieval when outside the range of the wireless coverage, using a ratchet and gear system. We validate our approach by wirelessly detecting the opening and closing of a pill bottle, capturing the joint angles of a 3D printed e-NABLE prosthetic hand, and an insulin pen that can store information to track its use outside the range of a wireless receiver.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 21116
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 21164
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 17625
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 21912
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 16934
        }
      ],
      "sessionIds": [
        1903
      ],
      "eventIds": []
    },
    {
      "id": 5628,
      "typeId": 11199,
      "title": "I Know What You Want: Using Gaze Metrics to Predict Personal Interest",
      "trackId": 10039,
      "tags": [],
      "keywords": [],
      "abstract": "In daily communications, we often use interpersonal cues - telltale facial expressions and body language - to moderate responses to our conversation partners. While we are able to interpret gaze as a sign of interest or reluctance, conventional user interfaces do not yet possess this possible benefit. In our work, we evaluate to what degree fixation-based gaze metrics can be used to infer a user's personal interest in the displayed content. We report on a study (N=18) where participants were presented with a grid array of different images, whilst being recorded for gaze behavior. Our system calculated a ranking for shown images based on gaze metrics. We found that all metrics are effective indicators of the participants' interest by analyzing their agreement with regard to the system's ranking. In an evaluation in a museum, we found that this translates to in-the-wild scenarios despite environmental constraints, such as limited data accuracy.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 18651
        },
        {
          "affiliations": [
            {
              "institution": "University of Stuttgart"
            }
          ],
          "personId": 12007
        },
        {
          "affiliations": [
            {
              "institution": "University of Stuttgart"
            }
          ],
          "personId": 20105
        }
      ],
      "sessionIds": [
        2227
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 21511,
      "firstName": "Jan",
      "lastName": "Brejcha",
      "affiliations": []
    },
    {
      "id": 8199,
      "firstName": "Mike",
      "lastName": "Chen",
      "middleInitial": "Y.",
      "affiliations": []
    },
    {
      "id": 12300,
      "firstName": "Michael",
      "lastName": "Rietzler",
      "affiliations": []
    },
    {
      "id": 17422,
      "firstName": "Qin",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 11279,
      "firstName": "Johannes",
      "lastName": "Filter",
      "affiliations": []
    },
    {
      "id": 9232,
      "firstName": "Hauke",
      "lastName": "Sandhaus",
      "affiliations": []
    },
    {
      "id": 8210,
      "firstName": "DoYoung",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 17427,
      "firstName": "Geehyuk",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 15383,
      "firstName": "Vidya",
      "lastName": "Setlur",
      "affiliations": []
    },
    {
      "id": 14361,
      "firstName": "Ryo",
      "lastName": "Suzuki",
      "affiliations": []
    },
    {
      "id": 12313,
      "firstName": "Shohei",
      "lastName": "Katakura",
      "affiliations": []
    },
    {
      "id": 20520,
      "firstName": "Teddy",
      "lastName": "Seyed",
      "affiliations": []
    },
    {
      "id": 23593,
      "firstName": "Ersin",
      "lastName": "Yumer",
      "affiliations": []
    },
    {
      "id": 16426,
      "firstName": "Kenta",
      "lastName": "Tanabe",
      "affiliations": []
    },
    {
      "id": 10287,
      "firstName": "Alok",
      "lastName": "Mysore",
      "affiliations": []
    },
    {
      "id": 18479,
      "firstName": "Raimund",
      "lastName": "Dachselt",
      "affiliations": []
    },
    {
      "id": 8248,
      "firstName": "Spyros",
      "lastName": "Polychronopoulos",
      "affiliations": []
    },
    {
      "id": 11320,
      "firstName": "Jiajun",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 12347,
      "firstName": "Juho",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 10300,
      "firstName": "Alexander",
      "lastName": "Thayer",
      "affiliations": []
    },
    {
      "id": 11330,
      "firstName": "Shio",
      "lastName": "Miyafuji",
      "affiliations": []
    },
    {
      "id": 23621,
      "firstName": "Julian",
      "lastName": "Frommel",
      "affiliations": []
    },
    {
      "id": 8263,
      "firstName": "Jianzhe",
      "lastName": "Gu",
      "affiliations": []
    },
    {
      "id": 9294,
      "firstName": "Diego",
      "lastName": "Martinez Plasencia",
      "affiliations": []
    },
    {
      "id": 18510,
      "firstName": "Karthik",
      "lastName": "Ramani",
      "affiliations": []
    },
    {
      "id": 18511,
      "firstName": "Uwe",
      "lastName": "Gruenefeld",
      "affiliations": []
    },
    {
      "id": 10319,
      "firstName": "Jörg",
      "lastName": "Müller",
      "affiliations": []
    },
    {
      "id": 10325,
      "firstName": "Dae Hyun",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 23637,
      "firstName": "Steven",
      "lastName": "Feiner",
      "affiliations": []
    },
    {
      "id": 11351,
      "firstName": "Blaine",
      "lastName": "Lewis",
      "affiliations": []
    },
    {
      "id": 20568,
      "firstName": "David",
      "lastName": "Ledo",
      "affiliations": []
    },
    {
      "id": 16475,
      "firstName": "Sarah",
      "lastName": "Lim",
      "affiliations": []
    },
    {
      "id": 14435,
      "firstName": "Daniel",
      "lastName": "Vogel",
      "affiliations": []
    },
    {
      "id": 23653,
      "firstName": "Jürgen",
      "lastName": "Steimle",
      "affiliations": []
    },
    {
      "id": 10342,
      "firstName": "Hideki",
      "lastName": "Koike",
      "affiliations": []
    },
    {
      "id": 20583,
      "firstName": "Jan",
      "lastName": "Sembdner",
      "affiliations": []
    },
    {
      "id": 16493,
      "firstName": "Rastislav",
      "lastName": "Bodik",
      "affiliations": []
    },
    {
      "id": 16494,
      "firstName": "Gregorio",
      "lastName": "Palmas",
      "affiliations": []
    },
    {
      "id": 22640,
      "firstName": "Thad",
      "lastName": "Starner",
      "affiliations": []
    },
    {
      "id": 9328,
      "firstName": "Marc",
      "lastName": "Teyssier",
      "affiliations": []
    },
    {
      "id": 8305,
      "firstName": "Shih-Chieh",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 18546,
      "firstName": "Humphrey",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 9332,
      "firstName": "Jingjie",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 19572,
      "firstName": "David",
      "lastName": "Lindlbauer",
      "affiliations": []
    },
    {
      "id": 16502,
      "firstName": "Sanghwa",
      "lastName": "Hong",
      "affiliations": []
    },
    {
      "id": 22646,
      "firstName": "Yuchen",
      "lastName": "Jiao",
      "affiliations": []
    },
    {
      "id": 11382,
      "firstName": "Satoshi",
      "lastName": "Hashizume",
      "affiliations": []
    },
    {
      "id": 14458,
      "firstName": "Bukun",
      "lastName": "Son",
      "affiliations": []
    },
    {
      "id": 16508,
      "firstName": "Ryo",
      "lastName": "Kamoshida",
      "affiliations": []
    },
    {
      "id": 21630,
      "firstName": "Giuseppe",
      "lastName": "Abrami",
      "affiliations": []
    },
    {
      "id": 21631,
      "firstName": "William",
      "lastName": "Freeman",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 9343,
      "firstName": "Denys",
      "lastName": "Matthies",
      "middleInitial": "J.C.",
      "affiliations": []
    },
    {
      "id": 9346,
      "firstName": "Konstantin",
      "lastName": "Klamka",
      "affiliations": []
    },
    {
      "id": 23683,
      "firstName": "Michelle",
      "lastName": "Annett",
      "affiliations": []
    },
    {
      "id": 21636,
      "firstName": "Alex",
      "lastName": "Olwal",
      "affiliations": []
    },
    {
      "id": 19589,
      "firstName": "Gierad",
      "lastName": "Laput",
      "affiliations": []
    },
    {
      "id": 20613,
      "firstName": "Eva",
      "lastName": "Hornecker",
      "affiliations": []
    },
    {
      "id": 14470,
      "firstName": "Anke",
      "lastName": "van Oosterhout",
      "affiliations": []
    },
    {
      "id": 20614,
      "firstName": "Lining",
      "lastName": "Yao",
      "affiliations": []
    },
    {
      "id": 18567,
      "firstName": "Sungu",
      "lastName": "Nam",
      "affiliations": []
    },
    {
      "id": 13451,
      "firstName": "Sven",
      "lastName": "Mayer",
      "affiliations": []
    },
    {
      "id": 19597,
      "firstName": "Daniel",
      "lastName": "Saakes",
      "affiliations": []
    },
    {
      "id": 23696,
      "firstName": "Kai",
      "lastName": "Kunze",
      "affiliations": []
    },
    {
      "id": 8337,
      "firstName": "Han Joo",
      "lastName": "Chae",
      "affiliations": []
    },
    {
      "id": 9366,
      "firstName": "Yuanzhi",
      "lastName": "Cao",
      "affiliations": []
    },
    {
      "id": 15513,
      "firstName": "Nivedita",
      "lastName": "Arora",
      "affiliations": []
    },
    {
      "id": 21660,
      "firstName": "Velko",
      "lastName": "Vechev",
      "affiliations": []
    },
    {
      "id": 15516,
      "firstName": "Matthew",
      "lastName": "Lease",
      "affiliations": []
    },
    {
      "id": 21662,
      "firstName": "Janin",
      "lastName": "Koch",
      "affiliations": []
    },
    {
      "id": 11423,
      "firstName": "Tomoya",
      "lastName": "Sasaki",
      "affiliations": []
    },
    {
      "id": 22688,
      "firstName": "Thomas",
      "lastName": "Dreja",
      "affiliations": []
    },
    {
      "id": 16545,
      "firstName": "Masahiko",
      "lastName": "Inami",
      "affiliations": []
    },
    {
      "id": 17569,
      "firstName": "Drini",
      "lastName": "Cami",
      "affiliations": []
    },
    {
      "id": 20642,
      "firstName": "Michael",
      "lastName": "Nebeling",
      "affiliations": []
    },
    {
      "id": 21666,
      "firstName": "Haijun",
      "lastName": "Xia",
      "affiliations": []
    },
    {
      "id": 15525,
      "firstName": "Mun Yong",
      "lastName": "Yi",
      "affiliations": []
    },
    {
      "id": 12453,
      "firstName": "Henning",
      "lastName": "Pohl",
      "affiliations": []
    },
    {
      "id": 12455,
      "firstName": "Yasuyuki",
      "lastName": "Matsushita",
      "affiliations": []
    },
    {
      "id": 23720,
      "firstName": "Teresa",
      "lastName": "Hirzle",
      "affiliations": []
    },
    {
      "id": 22696,
      "firstName": "Zhili",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 15529,
      "firstName": "Amy",
      "lastName": "Zhang",
      "middleInitial": "X.",
      "affiliations": []
    },
    {
      "id": 11434,
      "firstName": "Wilmot",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 8362,
      "firstName": "Damiano",
      "lastName": "Taurino",
      "affiliations": []
    },
    {
      "id": 17579,
      "firstName": "Johan",
      "lastName": "Debattista",
      "affiliations": []
    },
    {
      "id": 11435,
      "firstName": "Nico",
      "lastName": "Ring",
      "affiliations": []
    },
    {
      "id": 23727,
      "firstName": "Bodo",
      "lastName": "Urban",
      "affiliations": []
    },
    {
      "id": 17586,
      "firstName": "William",
      "lastName": "McGrath",
      "affiliations": []
    },
    {
      "id": 13492,
      "firstName": "Theophanis",
      "lastName": "Tsandilas",
      "affiliations": []
    },
    {
      "id": 10420,
      "firstName": "Bing-Yu",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 16566,
      "firstName": "Karan",
      "lastName": "Ahuja",
      "affiliations": []
    },
    {
      "id": 20662,
      "firstName": "Chiu-Hsuan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 9398,
      "firstName": "Kristian",
      "lastName": "Gohlke",
      "affiliations": []
    },
    {
      "id": 16567,
      "firstName": "Aliaksei",
      "lastName": "Miniukovich",
      "affiliations": []
    },
    {
      "id": 12473,
      "firstName": "Lea",
      "lastName": "Verou",
      "affiliations": []
    },
    {
      "id": 9401,
      "firstName": "Teng",
      "lastName": "Han",
      "affiliations": []
    },
    {
      "id": 18618,
      "firstName": "An",
      "lastName": "Nguyen",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 13507,
      "firstName": "Bjoern",
      "lastName": "Eskofier",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 22726,
      "firstName": "Rebecca",
      "lastName": "Krosnick",
      "affiliations": []
    },
    {
      "id": 15561,
      "firstName": "Thomas",
      "lastName": "Gross",
      "middleInitial": "R.",
      "affiliations": []
    },
    {
      "id": 22729,
      "firstName": "Philipp",
      "lastName": "Kuhnz",
      "affiliations": []
    },
    {
      "id": 17611,
      "firstName": "Yuichi",
      "lastName": "Mashiba",
      "affiliations": []
    },
    {
      "id": 20684,
      "firstName": "Toshiki",
      "lastName": "Sato",
      "affiliations": []
    },
    {
      "id": 16591,
      "firstName": "Sandro",
      "lastName": "Weber",
      "affiliations": []
    },
    {
      "id": 23759,
      "firstName": "Masa",
      "lastName": "Ogata",
      "affiliations": []
    },
    {
      "id": 9424,
      "firstName": "Christian",
      "lastName": "Rendl",
      "affiliations": []
    },
    {
      "id": 14547,
      "firstName": "Gregory",
      "lastName": "Abowd",
      "affiliations": []
    },
    {
      "id": 17625,
      "firstName": "Ian",
      "lastName": "Culhane",
      "affiliations": []
    },
    {
      "id": 18651,
      "firstName": "Jakob",
      "lastName": "Karolus",
      "affiliations": []
    },
    {
      "id": 22749,
      "firstName": "Dinglu",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 8413,
      "firstName": "Sebastian",
      "lastName": "Marwecki",
      "affiliations": []
    },
    {
      "id": 21726,
      "firstName": "Timothy",
      "lastName": "Merritt",
      "affiliations": []
    },
    {
      "id": 16607,
      "firstName": "Ben",
      "lastName": "Carroll",
      "affiliations": []
    },
    {
      "id": 13536,
      "firstName": "Hiroshi",
      "lastName": "Horii",
      "affiliations": []
    },
    {
      "id": 9441,
      "firstName": "Youngbo",
      "lastName": "Shim",
      "middleInitial": "Aram",
      "affiliations": []
    },
    {
      "id": 16612,
      "firstName": "Pedro",
      "lastName": "Lopes",
      "affiliations": []
    },
    {
      "id": 17640,
      "firstName": "Chi-huan",
      "lastName": "Chiang",
      "affiliations": []
    },
    {
      "id": 9448,
      "firstName": "Martin",
      "lastName": "Raubal",
      "affiliations": []
    },
    {
      "id": 21737,
      "firstName": "Hong-Yu",
      "lastName": "Chang",
      "affiliations": []
    },
    {
      "id": 8429,
      "firstName": "Jaime",
      "lastName": "Teevan",
      "affiliations": []
    },
    {
      "id": 21743,
      "firstName": "Jürgen",
      "lastName": "Steimle",
      "affiliations": []
    },
    {
      "id": 10483,
      "firstName": "Noko",
      "lastName": "Kuratomo",
      "affiliations": []
    },
    {
      "id": 9462,
      "firstName": "Hiroyuki",
      "lastName": "Kajimoto",
      "affiliations": []
    },
    {
      "id": 18680,
      "firstName": "Matthias",
      "lastName": "Riepe",
      "middleInitial": "Wilhelm",
      "affiliations": []
    },
    {
      "id": 15610,
      "firstName": "Yonghwan",
      "lastName": "Shin",
      "affiliations": []
    },
    {
      "id": 11515,
      "firstName": "Catherine",
      "lastName": "Pelachaud",
      "affiliations": []
    },
    {
      "id": 15616,
      "firstName": "Zeyu",
      "lastName": "Yan",
      "affiliations": []
    },
    {
      "id": 20737,
      "firstName": "Michel",
      "lastName": "Beaudouin-Lafon",
      "affiliations": []
    },
    {
      "id": 11521,
      "firstName": "Shan-Yuan",
      "lastName": "Teng",
      "affiliations": []
    },
    {
      "id": 21762,
      "firstName": "Taha",
      "lastName": "Moriyama",
      "affiliations": []
    },
    {
      "id": 21763,
      "firstName": "Emma",
      "lastName": "Brunskill",
      "affiliations": []
    },
    {
      "id": 9475,
      "firstName": "Chi",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 12548,
      "firstName": "Chen-Kuo",
      "lastName": "Sun",
      "affiliations": []
    },
    {
      "id": 22789,
      "firstName": "Dan",
      "lastName": "Olsen",
      "affiliations": []
    },
    {
      "id": 10502,
      "firstName": "Stelian",
      "lastName": "Coros",
      "affiliations": []
    },
    {
      "id": 13574,
      "firstName": "Da-Yuan",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 23818,
      "firstName": "Bastian",
      "lastName": "Cornelsen",
      "affiliations": []
    },
    {
      "id": 11533,
      "firstName": "Julie",
      "lastName": "Shah",
      "affiliations": []
    },
    {
      "id": 14610,
      "firstName": "Fei-Fei",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 12565,
      "firstName": "Alanson",
      "lastName": "Sample",
      "affiliations": []
    },
    {
      "id": 22807,
      "firstName": "Andrew",
      "lastName": "Head",
      "affiliations": []
    },
    {
      "id": 13591,
      "firstName": "Fabrice",
      "lastName": "Matulic",
      "affiliations": []
    },
    {
      "id": 15642,
      "firstName": "Jo-Yu",
      "lastName": "Lo",
      "affiliations": []
    },
    {
      "id": 12572,
      "firstName": "MHD Yamen",
      "lastName": "Saraiji",
      "affiliations": []
    },
    {
      "id": 10529,
      "firstName": "Maximilian",
      "lastName": "Mackeprang",
      "affiliations": []
    },
    {
      "id": 21795,
      "firstName": "Tali",
      "lastName": "Dekel",
      "affiliations": []
    },
    {
      "id": 11556,
      "firstName": "Florian",
      "lastName": "Echtler",
      "affiliations": []
    },
    {
      "id": 14630,
      "firstName": "Yoichi",
      "lastName": "Ochiai",
      "affiliations": []
    },
    {
      "id": 14633,
      "firstName": "Stanislav",
      "lastName": "Aranovskiy",
      "affiliations": []
    },
    {
      "id": 9515,
      "firstName": "Daizo",
      "lastName": "Ikeda",
      "affiliations": []
    },
    {
      "id": 14639,
      "firstName": "Jeffrey",
      "lastName": "Heer",
      "affiliations": []
    },
    {
      "id": 12593,
      "firstName": "Bhuvaneswari",
      "lastName": "Sarupuri",
      "affiliations": []
    },
    {
      "id": 8498,
      "firstName": "Insu",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 8499,
      "firstName": "Jeremy",
      "lastName": "Warner",
      "affiliations": []
    },
    {
      "id": 19763,
      "firstName": "Stephen",
      "lastName": "DiVerdi",
      "affiliations": []
    },
    {
      "id": 10548,
      "firstName": "Oliver",
      "lastName": "Schneider",
      "affiliations": []
    },
    {
      "id": 19768,
      "firstName": "Victor",
      "lastName": "Girotto",
      "affiliations": []
    },
    {
      "id": 18747,
      "firstName": "Shamsi",
      "lastName": "Iqbal",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 8508,
      "firstName": "Kouta",
      "lastName": "Minamizawa",
      "affiliations": []
    },
    {
      "id": 15678,
      "firstName": "Nora",
      "lastName": "Willett",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 17727,
      "firstName": "Akira",
      "lastName": "Ishii",
      "affiliations": []
    },
    {
      "id": 22848,
      "firstName": "Thomas",
      "lastName": "Liu",
      "middleInitial": "F.",
      "affiliations": []
    },
    {
      "id": 12608,
      "firstName": "Karan",
      "lastName": "Goel",
      "affiliations": []
    },
    {
      "id": 11584,
      "firstName": "Josh",
      "lastName": "Davis",
      "middleInitial": "Urban",
      "affiliations": []
    },
    {
      "id": 8512,
      "firstName": "Michael",
      "lastName": "Schmitz",
      "affiliations": []
    },
    {
      "id": 17729,
      "firstName": "Sebastian",
      "lastName": "Marwecki",
      "affiliations": []
    },
    {
      "id": 21827,
      "firstName": "Brian",
      "lastName": "Vogel",
      "affiliations": []
    },
    {
      "id": 14661,
      "firstName": "Attila",
      "lastName": "Kett",
      "affiliations": []
    },
    {
      "id": 19782,
      "firstName": "Yuichi",
      "lastName": "Kurita",
      "affiliations": []
    },
    {
      "id": 15687,
      "firstName": "Kai",
      "lastName": "Kunze",
      "affiliations": []
    },
    {
      "id": 17735,
      "firstName": "Soeren",
      "lastName": "Kuhrt",
      "affiliations": []
    },
    {
      "id": 9544,
      "firstName": "Keichi",
      "lastName": "Zempo",
      "affiliations": []
    },
    {
      "id": 14668,
      "firstName": "Tianxing",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 20813,
      "firstName": "Ruchir",
      "lastName": "Patel",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 22862,
      "firstName": "Xin",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 18767,
      "firstName": "Markus",
      "lastName": "Wirth",
      "affiliations": []
    },
    {
      "id": 21841,
      "firstName": "Akifumi",
      "lastName": "Takahashi",
      "affiliations": []
    },
    {
      "id": 13649,
      "firstName": "Anne",
      "lastName": "Thompson",
      "middleInitial": "Loomis",
      "affiliations": []
    },
    {
      "id": 13651,
      "firstName": "Liwei",
      "lastName": "Chan",
      "affiliations": []
    },
    {
      "id": 15701,
      "firstName": "Daniel",
      "lastName": "Drew",
      "affiliations": []
    },
    {
      "id": 12629,
      "firstName": "Jr-Ling",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 20822,
      "firstName": "Richard",
      "lastName": "Calland",
      "middleInitial": "G.",
      "affiliations": []
    },
    {
      "id": 23895,
      "firstName": "Carmine",
      "lastName": "Elvezio",
      "affiliations": []
    },
    {
      "id": 17752,
      "firstName": "Feras",
      "lastName": "Al Taha",
      "affiliations": []
    },
    {
      "id": 19803,
      "firstName": "Ke",
      "lastName": "Sun",
      "affiliations": []
    },
    {
      "id": 17757,
      "firstName": "Masashi",
      "lastName": "Kudo",
      "affiliations": []
    },
    {
      "id": 22878,
      "firstName": "Anusha",
      "lastName": "Withana",
      "affiliations": []
    },
    {
      "id": 16737,
      "firstName": "Masaaki",
      "lastName": "Fukumoto",
      "affiliations": []
    },
    {
      "id": 15713,
      "firstName": "Bilge",
      "lastName": "Mutlu",
      "affiliations": []
    },
    {
      "id": 10593,
      "firstName": "Antoine",
      "lastName": "Weill-Duflos",
      "affiliations": []
    },
    {
      "id": 17762,
      "firstName": "Jong-Seok",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 17764,
      "firstName": "Te-Yen",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 14694,
      "firstName": "Swapna",
      "lastName": "Joshi",
      "affiliations": []
    },
    {
      "id": 9576,
      "firstName": "Tianyi",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 13681,
      "firstName": "Homei",
      "lastName": "Miyashita",
      "affiliations": []
    },
    {
      "id": 17778,
      "firstName": "Lingjia",
      "lastName": "Tang",
      "affiliations": []
    },
    {
      "id": 19827,
      "firstName": "Da-Yuan",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 17780,
      "firstName": "Maneesh",
      "lastName": "Agrawala",
      "affiliations": []
    },
    {
      "id": 20860,
      "firstName": "Byron",
      "lastName": "Wallace",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 23934,
      "firstName": "Joshua",
      "lastName": "Hibschman",
      "affiliations": []
    },
    {
      "id": 10624,
      "firstName": "Claudia",
      "lastName": "Müller-Birn",
      "affiliations": []
    },
    {
      "id": 16769,
      "firstName": "Kathrin",
      "lastName": "Probst",
      "affiliations": []
    },
    {
      "id": 15745,
      "firstName": "Joanne",
      "lastName": "Leong",
      "affiliations": []
    },
    {
      "id": 18817,
      "firstName": "Jungkook",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 17794,
      "firstName": "Sebastien",
      "lastName": "Poulmane",
      "affiliations": []
    },
    {
      "id": 8578,
      "firstName": "Stefan",
      "lastName": "Gradl",
      "affiliations": []
    },
    {
      "id": 22917,
      "firstName": "Kashyap",
      "lastName": "Todi",
      "affiliations": []
    },
    {
      "id": 19845,
      "firstName": "C.",
      "lastName": "Fraser",
      "middleInitial": "Ailie",
      "affiliations": []
    },
    {
      "id": 8581,
      "firstName": "Anton",
      "lastName": "Synytsia",
      "affiliations": []
    },
    {
      "id": 18822,
      "firstName": "James",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 23943,
      "firstName": "Amanda",
      "lastName": "Swearngin",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 18824,
      "firstName": "Kasper",
      "lastName": "Hornbæk",
      "affiliations": []
    },
    {
      "id": 9609,
      "firstName": "Herbert",
      "lastName": "Shea",
      "affiliations": []
    },
    {
      "id": 10639,
      "firstName": "Daniel",
      "lastName": "Leithinger",
      "affiliations": []
    },
    {
      "id": 15759,
      "firstName": "Elizabeth",
      "lastName": "Tate",
      "affiliations": []
    },
    {
      "id": 23952,
      "firstName": "Mark",
      "lastName": "Craft",
      "affiliations": []
    },
    {
      "id": 21912,
      "firstName": "Jennifer",
      "lastName": "Mankoff",
      "affiliations": []
    },
    {
      "id": 23961,
      "firstName": "Pin Sung",
      "lastName": "Ku",
      "affiliations": []
    },
    {
      "id": 21914,
      "firstName": "Ryuma",
      "lastName": "Niiyama",
      "affiliations": []
    },
    {
      "id": 22938,
      "firstName": "Philipp",
      "lastName": "Otto",
      "affiliations": []
    },
    {
      "id": 11676,
      "firstName": "Yasuaki",
      "lastName": "Kakehi",
      "affiliations": []
    },
    {
      "id": 18852,
      "firstName": "Chu-En",
      "lastName": "Hou",
      "affiliations": []
    },
    {
      "id": 15781,
      "firstName": "Jooyeon",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 14758,
      "firstName": "Jen-Shuo",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 14759,
      "firstName": "Aditya",
      "lastName": "Kharosekar",
      "affiliations": []
    },
    {
      "id": 12716,
      "firstName": "Gregory",
      "lastName": "Abowd",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 18865,
      "firstName": "Shane",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 20920,
      "firstName": "Takao",
      "lastName": "Onoye",
      "affiliations": []
    },
    {
      "id": 17848,
      "firstName": "Jussi",
      "lastName": "Jokinen",
      "affiliations": []
    },
    {
      "id": 15804,
      "firstName": "Andrew",
      "lastName": "Ko",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 8638,
      "firstName": "Mahzar",
      "lastName": "Eisapour",
      "affiliations": []
    },
    {
      "id": 18879,
      "firstName": "Takuto",
      "lastName": "Nakamura",
      "affiliations": []
    },
    {
      "id": 10692,
      "firstName": "Satoshi",
      "lastName": "Abe",
      "affiliations": []
    },
    {
      "id": 17860,
      "firstName": "Dong",
      "lastName": "Yun",
      "middleInitial": "Hyeok",
      "affiliations": []
    },
    {
      "id": 8645,
      "firstName": "Hideaki",
      "lastName": "Kuzuoka",
      "affiliations": []
    },
    {
      "id": 21958,
      "firstName": "Samuli",
      "lastName": "De Pascale",
      "affiliations": []
    },
    {
      "id": 18886,
      "firstName": "Keng-Ta",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 13769,
      "firstName": "Takayuki",
      "lastName": "Kawamura",
      "affiliations": []
    },
    {
      "id": 12747,
      "firstName": "Rafael",
      "lastName": "Ballagas",
      "affiliations": []
    },
    {
      "id": 17868,
      "firstName": "Hyunyoung",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 11728,
      "firstName": "Hisham Elser Bilal",
      "lastName": "Salih",
      "affiliations": []
    },
    {
      "id": 12753,
      "firstName": "Mark",
      "lastName": "Gross",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 9681,
      "firstName": "Gerold",
      "lastName": "Schneider",
      "affiliations": []
    },
    {
      "id": 13781,
      "firstName": "Teng",
      "lastName": "Han",
      "affiliations": []
    },
    {
      "id": 11733,
      "firstName": "Martin",
      "lastName": "Kaltenbrunner",
      "affiliations": []
    },
    {
      "id": 9685,
      "firstName": "Eric",
      "lastName": "Lecolinet",
      "affiliations": []
    },
    {
      "id": 16853,
      "firstName": "Winslow",
      "lastName": "Burleson",
      "affiliations": []
    },
    {
      "id": 9687,
      "firstName": "Sebastian",
      "lastName": "Herscher",
      "affiliations": []
    },
    {
      "id": 15831,
      "firstName": "Shi",
      "lastName": "Cao",
      "affiliations": []
    },
    {
      "id": 14811,
      "firstName": "Soichiro",
      "lastName": "Toyohara",
      "affiliations": []
    },
    {
      "id": 17883,
      "firstName": "David",
      "lastName": "Karger",
      "affiliations": []
    },
    {
      "id": 21979,
      "firstName": "Jeong-in",
      "lastName": "Hwang",
      "affiliations": []
    },
    {
      "id": 21980,
      "firstName": "Tianfan",
      "lastName": "Xue",
      "affiliations": []
    },
    {
      "id": 13792,
      "firstName": "Jan",
      "lastName": "Gugenheimer",
      "affiliations": []
    },
    {
      "id": 17894,
      "firstName": "Sarah",
      "lastName": "Chasins",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 15846,
      "firstName": "Yunqi",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 13798,
      "firstName": "Patrick",
      "lastName": "Baudisch",
      "affiliations": []
    },
    {
      "id": 17904,
      "firstName": "Chris",
      "lastName": "Harrison",
      "affiliations": []
    },
    {
      "id": 9714,
      "firstName": "David",
      "lastName": "Porfirio",
      "affiliations": []
    },
    {
      "id": 11762,
      "firstName": "Jeremie",
      "lastName": "Garcia",
      "affiliations": []
    },
    {
      "id": 10739,
      "firstName": "Manoj",
      "lastName": "Kristhombuge",
      "affiliations": []
    },
    {
      "id": 10743,
      "firstName": "Daniel",
      "lastName": "Weld",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 14842,
      "firstName": "Christian",
      "lastName": "Sandor",
      "affiliations": []
    },
    {
      "id": 20990,
      "firstName": "Virag",
      "lastName": "Varga",
      "affiliations": []
    },
    {
      "id": 13822,
      "firstName": "Juho",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 24063,
      "firstName": "Majed",
      "lastName": "Al-Zayer",
      "affiliations": []
    },
    {
      "id": 15872,
      "firstName": "Jasmine",
      "lastName": "Roberts",
      "affiliations": []
    },
    {
      "id": 9731,
      "firstName": "Ranjitha",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 20995,
      "firstName": "Reinhard",
      "lastName": "Schwoediauer",
      "affiliations": []
    },
    {
      "id": 24070,
      "firstName": "Ben",
      "lastName": "Lafreniere",
      "affiliations": []
    },
    {
      "id": 10759,
      "firstName": "Rong-Hao",
      "lastName": "Liang",
      "affiliations": []
    },
    {
      "id": 8715,
      "firstName": "Takuto",
      "lastName": "Nakamura",
      "affiliations": []
    },
    {
      "id": 11794,
      "firstName": "Gianluca",
      "lastName": "Memoli",
      "affiliations": []
    },
    {
      "id": 14866,
      "firstName": "Alexandra",
      "lastName": "Ion",
      "affiliations": []
    },
    {
      "id": 10771,
      "firstName": "Maria",
      "lastName": "Mueller",
      "affiliations": []
    },
    {
      "id": 24085,
      "firstName": "Jinwook",
      "lastName": "Seo",
      "affiliations": []
    },
    {
      "id": 8725,
      "firstName": "Daniel",
      "lastName": "Groeger",
      "affiliations": []
    },
    {
      "id": 16919,
      "firstName": "Yichen",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 13848,
      "firstName": "Tarfah",
      "lastName": "Alrashed",
      "affiliations": []
    },
    {
      "id": 9753,
      "firstName": "Allison",
      "lastName": "Sauppé",
      "affiliations": []
    },
    {
      "id": 21020,
      "firstName": "Xiaojun",
      "lastName": "Bi",
      "affiliations": []
    },
    {
      "id": 16925,
      "firstName": "Aws",
      "lastName": "Albarghouthi",
      "affiliations": []
    },
    {
      "id": 12830,
      "firstName": "Karolina",
      "lastName": "Sejunaite",
      "affiliations": []
    },
    {
      "id": 20000,
      "firstName": "Michael",
      "lastName": "Bernstein",
      "affiliations": []
    },
    {
      "id": 8737,
      "firstName": "Stefanie",
      "lastName": "Mueller",
      "affiliations": []
    },
    {
      "id": 16934,
      "firstName": "Shyamnath",
      "lastName": "Gollakota",
      "affiliations": []
    },
    {
      "id": 16935,
      "firstName": "Eunseok",
      "lastName": "Jeong",
      "affiliations": []
    },
    {
      "id": 18983,
      "firstName": "Aakar",
      "lastName": "Gupta",
      "affiliations": []
    },
    {
      "id": 16936,
      "firstName": "Jonathan",
      "lastName": "Bragg",
      "affiliations": []
    },
    {
      "id": 17964,
      "firstName": "Hyunchul",
      "lastName": "Lim",
      "affiliations": []
    },
    {
      "id": 17972,
      "firstName": "Ranjay",
      "lastName": "Krishna",
      "affiliations": []
    },
    {
      "id": 12853,
      "firstName": "Anhong",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 9782,
      "firstName": "Wendy",
      "lastName": "Mackay",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 16950,
      "firstName": "Christian",
      "lastName": "Holz",
      "affiliations": []
    },
    {
      "id": 17976,
      "firstName": "Gudrun",
      "lastName": "Klinker",
      "affiliations": []
    },
    {
      "id": 20026,
      "firstName": "Rosane",
      "lastName": "Ushirobira",
      "affiliations": []
    },
    {
      "id": 14907,
      "firstName": "Mathieu",
      "lastName": "Cousy",
      "affiliations": []
    },
    {
      "id": 14908,
      "firstName": "Daniel Amadeus",
      "lastName": "Gloeckner",
      "affiliations": []
    },
    {
      "id": 9788,
      "firstName": "Hiromi",
      "lastName": "Nakamura",
      "affiliations": []
    },
    {
      "id": 15932,
      "firstName": "Jun",
      "lastName": "Nishida",
      "affiliations": []
    },
    {
      "id": 20029,
      "firstName": "Jason",
      "lastName": "Mars",
      "affiliations": []
    },
    {
      "id": 23103,
      "firstName": "Adam",
      "lastName": "Finkelstein",
      "affiliations": []
    },
    {
      "id": 15939,
      "firstName": "Sriram",
      "lastName": "Subramanian",
      "affiliations": []
    },
    {
      "id": 13892,
      "firstName": "Yuki",
      "lastName": "Kon",
      "affiliations": []
    },
    {
      "id": 19018,
      "firstName": "Mayank",
      "lastName": "Goel",
      "affiliations": []
    },
    {
      "id": 23114,
      "firstName": "Enamul",
      "lastName": "Hoque",
      "affiliations": []
    },
    {
      "id": 13900,
      "firstName": "Gergely",
      "lastName": "Vakulya",
      "affiliations": []
    },
    {
      "id": 22093,
      "firstName": "Teresa",
      "lastName": "Onorati",
      "affiliations": []
    },
    {
      "id": 15949,
      "firstName": "Hirokazu",
      "lastName": "Kato",
      "affiliations": []
    },
    {
      "id": 21069,
      "firstName": "Laura Milena",
      "lastName": "Daza Parra",
      "affiliations": []
    },
    {
      "id": 13902,
      "firstName": "Keita",
      "lastName": "Watanabe",
      "affiliations": []
    },
    {
      "id": 24142,
      "firstName": "Xiaoyi",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 16979,
      "firstName": "Ana",
      "lastName": "Villanueva",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 9811,
      "firstName": "Yeong Hoon",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 22102,
      "firstName": "Diogo",
      "lastName": "Cabral",
      "affiliations": []
    },
    {
      "id": 8791,
      "firstName": "Xing-Dong",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 24154,
      "firstName": "Roshan",
      "lastName": "Peiris",
      "middleInitial": "Lalintha",
      "affiliations": []
    },
    {
      "id": 16988,
      "firstName": "Kentaro",
      "lastName": "Takemura",
      "affiliations": []
    },
    {
      "id": 15965,
      "firstName": "Christian",
      "lastName": "Spiekermann",
      "affiliations": []
    },
    {
      "id": 23134,
      "firstName": "Sharon",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 16991,
      "firstName": "Mathieu",
      "lastName": "Nancel",
      "affiliations": []
    },
    {
      "id": 14945,
      "firstName": "Pascal",
      "lastName": "Gruppe",
      "affiliations": []
    },
    {
      "id": 11874,
      "firstName": "Guanyun",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 10851,
      "firstName": "Lisa",
      "lastName": "Stähli",
      "affiliations": []
    },
    {
      "id": 20069,
      "firstName": "Susanne",
      "lastName": "Boll",
      "affiliations": []
    },
    {
      "id": 9830,
      "firstName": "Weinan",
      "lastName": "Shi",
      "affiliations": []
    },
    {
      "id": 13927,
      "firstName": "Anita",
      "lastName": "Vogl",
      "affiliations": []
    },
    {
      "id": 16999,
      "firstName": "Mira",
      "lastName": "Dontcheva",
      "affiliations": []
    },
    {
      "id": 20072,
      "firstName": "Jeff",
      "lastName": "Avery",
      "affiliations": []
    },
    {
      "id": 23147,
      "firstName": "Bongwon",
      "lastName": "Suh",
      "affiliations": []
    },
    {
      "id": 18031,
      "firstName": "Yang",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 23154,
      "firstName": "Jungmin",
      "lastName": "Chung",
      "affiliations": []
    },
    {
      "id": 8818,
      "firstName": "Markus",
      "lastName": "Funk",
      "affiliations": []
    },
    {
      "id": 10868,
      "firstName": "Xiaohui",
      "lastName": "You",
      "affiliations": []
    },
    {
      "id": 20086,
      "firstName": "Hyeungshik",
      "lastName": "Jung",
      "affiliations": []
    },
    {
      "id": 24186,
      "firstName": "Andreas",
      "lastName": "Bulling",
      "affiliations": []
    },
    {
      "id": 8826,
      "firstName": "Sarah",
      "lastName": "Schuetz",
      "affiliations": []
    },
    {
      "id": 13946,
      "firstName": "James",
      "lastName": "McCann",
      "affiliations": []
    },
    {
      "id": 12922,
      "firstName": "Abdullah",
      "lastName": "Ali",
      "middleInitial": "X.",
      "affiliations": []
    },
    {
      "id": 24187,
      "firstName": "Lydia",
      "lastName": "Chiton",
      "middleInitial": "B",
      "affiliations": []
    },
    {
      "id": 21116,
      "firstName": "Vikram",
      "lastName": "Iyer",
      "affiliations": []
    },
    {
      "id": 14972,
      "firstName": "Rubaiat Habib",
      "lastName": "Kazi",
      "affiliations": []
    },
    {
      "id": 18044,
      "firstName": "Jheng-You",
      "lastName": "Ke",
      "affiliations": []
    },
    {
      "id": 19070,
      "firstName": "Nadiya",
      "lastName": "Morenko",
      "affiliations": []
    },
    {
      "id": 10880,
      "firstName": "Suwen",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 24192,
      "firstName": "Roel",
      "lastName": "Vertegaal",
      "affiliations": []
    },
    {
      "id": 8834,
      "firstName": "Byungjoo",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 24198,
      "firstName": "Chun",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 20102,
      "firstName": "Eelke",
      "lastName": "Folmer",
      "affiliations": []
    },
    {
      "id": 20104,
      "firstName": "Jackie",
      "lastName": "Yang",
      "middleInitial": "(Junrui)",
      "affiliations": []
    },
    {
      "id": 20105,
      "firstName": "Pawel",
      "lastName": "Wozniak",
      "middleInitial": "W.",
      "affiliations": []
    },
    {
      "id": 21129,
      "firstName": "Meredith",
      "lastName": "Morris",
      "middleInitial": "Ringel",
      "affiliations": []
    },
    {
      "id": 24206,
      "firstName": "Tzu-Sheng",
      "lastName": "Kuo",
      "affiliations": []
    },
    {
      "id": 20111,
      "firstName": "Hijung Valentina",
      "lastName": "Shin",
      "affiliations": []
    },
    {
      "id": 23183,
      "firstName": "Frederica",
      "lastName": "Gonçalves",
      "affiliations": []
    },
    {
      "id": 22160,
      "firstName": "Jonas",
      "lastName": "Bounama",
      "affiliations": []
    },
    {
      "id": 18064,
      "firstName": "Eytan",
      "lastName": "Adar",
      "affiliations": []
    },
    {
      "id": 19090,
      "firstName": "Daniel",
      "lastName": "Besserer",
      "affiliations": []
    },
    {
      "id": 9877,
      "firstName": "Tovi",
      "lastName": "Grossman",
      "affiliations": []
    },
    {
      "id": 20118,
      "firstName": "Saumyaa",
      "lastName": "Krishnan",
      "affiliations": []
    },
    {
      "id": 23197,
      "firstName": "Marc",
      "lastName": "Wyss",
      "affiliations": []
    },
    {
      "id": 19102,
      "firstName": "Kazuki",
      "lastName": "Takazawa",
      "affiliations": []
    },
    {
      "id": 22175,
      "firstName": "Hsin-Yu",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 20129,
      "firstName": "Hiroshi",
      "lastName": "Ishii",
      "affiliations": []
    },
    {
      "id": 11940,
      "firstName": "Johann",
      "lastName": "Strama",
      "affiliations": []
    },
    {
      "id": 9897,
      "firstName": "Michal",
      "lastName": "Lukác",
      "affiliations": []
    },
    {
      "id": 11946,
      "firstName": "Zhihao",
      "lastName": "Yao",
      "affiliations": []
    },
    {
      "id": 13995,
      "firstName": "Radomir",
      "lastName": "Mech",
      "affiliations": []
    },
    {
      "id": 15019,
      "firstName": "Pourang",
      "lastName": "Irani",
      "affiliations": []
    },
    {
      "id": 9899,
      "firstName": "Mathieu",
      "lastName": "Poirier",
      "affiliations": []
    },
    {
      "id": 13996,
      "firstName": "Mohd Adili",
      "lastName": "Norasikin",
      "affiliations": []
    },
    {
      "id": 15020,
      "firstName": "George",
      "lastName": "Fitzmaurice",
      "affiliations": []
    },
    {
      "id": 21164,
      "firstName": "Justin",
      "lastName": "Chan",
      "affiliations": []
    },
    {
      "id": 12972,
      "firstName": "Hayato",
      "lastName": "Yamana",
      "affiliations": []
    },
    {
      "id": 17069,
      "firstName": "Géry",
      "lastName": "Casiez",
      "affiliations": []
    },
    {
      "id": 19117,
      "firstName": "Ruta",
      "lastName": "Desai",
      "affiliations": []
    },
    {
      "id": 21165,
      "firstName": "Kazuma",
      "lastName": "Aoyama",
      "affiliations": []
    },
    {
      "id": 20144,
      "firstName": "Telmo",
      "lastName": "Zarraonandia",
      "affiliations": []
    },
    {
      "id": 24240,
      "firstName": "Jiushan",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 14001,
      "firstName": "Michael",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 19125,
      "firstName": "Darle",
      "lastName": "Shinsato",
      "affiliations": []
    },
    {
      "id": 15033,
      "firstName": "Takayuki",
      "lastName": "Kameoka",
      "affiliations": []
    },
    {
      "id": 11963,
      "firstName": "Kyle",
      "lastName": "Kotowick",
      "affiliations": []
    },
    {
      "id": 22205,
      "firstName": "Jesse",
      "lastName": "Benjamin",
      "middleInitial": "Josua",
      "affiliations": []
    },
    {
      "id": 20159,
      "firstName": "Takafumi",
      "lastName": "Taketomi",
      "affiliations": []
    },
    {
      "id": 9919,
      "firstName": "Tim",
      "lastName": "Oesterreich",
      "affiliations": []
    },
    {
      "id": 17088,
      "firstName": "Xaver",
      "lastName": "Toyozaki",
      "middleInitial": "Tomihiro",
      "affiliations": []
    },
    {
      "id": 10944,
      "firstName": "Yi-Chi",
      "lastName": "Liao",
      "affiliations": []
    },
    {
      "id": 14017,
      "firstName": "Woontack",
      "lastName": "Woo",
      "affiliations": []
    },
    {
      "id": 22209,
      "firstName": "Zhengqing",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 20162,
      "firstName": "Albrecht",
      "lastName": "Schmidt",
      "affiliations": []
    },
    {
      "id": 19139,
      "firstName": "Soya",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 9923,
      "firstName": "Dan",
      "lastName": "Liebling",
      "affiliations": []
    },
    {
      "id": 14020,
      "firstName": "Youryang",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 15046,
      "firstName": "Jooyoung",
      "lastName": "Son",
      "affiliations": []
    },
    {
      "id": 16072,
      "firstName": "Mausam",
      "lastName": "Mausam",
      "affiliations": []
    },
    {
      "id": 15049,
      "firstName": "Wen-Jie",
      "lastName": "Tseng",
      "affiliations": []
    },
    {
      "id": 17099,
      "firstName": "Martin",
      "lastName": "Cadík",
      "affiliations": []
    },
    {
      "id": 9933,
      "firstName": "Kyung Yun",
      "lastName": "Choi",
      "affiliations": []
    },
    {
      "id": 15055,
      "firstName": "Suzi",
      "lastName": "Choi",
      "affiliations": []
    },
    {
      "id": 9938,
      "firstName": "Yutaka",
      "lastName": "Tokuda",
      "affiliations": []
    },
    {
      "id": 19162,
      "firstName": "Dennis",
      "lastName": "Wolf",
      "affiliations": []
    },
    {
      "id": 15070,
      "firstName": "Chia-En",
      "lastName": "Tsai",
      "affiliations": []
    },
    {
      "id": 23263,
      "firstName": "Tino",
      "lastName": "Weinkauf",
      "affiliations": []
    },
    {
      "id": 8929,
      "firstName": "Sebastian",
      "lastName": "Stickert",
      "affiliations": []
    },
    {
      "id": 24290,
      "firstName": "Kazunori",
      "lastName": "Nozawa",
      "affiliations": []
    },
    {
      "id": 22243,
      "firstName": "Savvas",
      "lastName": "Petridis",
      "affiliations": []
    },
    {
      "id": 10981,
      "firstName": "Seongkook",
      "lastName": "Heo",
      "affiliations": []
    },
    {
      "id": 12007,
      "firstName": "Patrick",
      "lastName": "Dabbert",
      "affiliations": []
    },
    {
      "id": 14056,
      "firstName": "Alexander",
      "lastName": "Plopski",
      "affiliations": []
    },
    {
      "id": 20201,
      "firstName": "Junichi",
      "lastName": "Yamaoka",
      "affiliations": []
    },
    {
      "id": 20202,
      "firstName": "Alan",
      "lastName": "Lundgard",
      "affiliations": []
    },
    {
      "id": 13035,
      "firstName": "Jaeyoung",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 24299,
      "firstName": "Yoonjeong",
      "lastName": "Cha",
      "affiliations": []
    },
    {
      "id": 17131,
      "firstName": "Greg",
      "lastName": "Priest-Dorman",
      "affiliations": []
    },
    {
      "id": 12011,
      "firstName": "Donsuk",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 18156,
      "firstName": "Scott",
      "lastName": "Bateman",
      "affiliations": []
    },
    {
      "id": 10990,
      "firstName": "Shion",
      "lastName": "Asada",
      "affiliations": []
    },
    {
      "id": 16115,
      "firstName": "Yuxi",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 15092,
      "firstName": "Nico",
      "lastName": "Boeckhoff",
      "affiliations": []
    },
    {
      "id": 12021,
      "firstName": "Xiong",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 12022,
      "firstName": "Guilhem",
      "lastName": "Buisan",
      "affiliations": []
    },
    {
      "id": 11000,
      "firstName": "Jon",
      "lastName": "Moeller",
      "affiliations": []
    },
    {
      "id": 23289,
      "firstName": "Jaeseung",
      "lastName": "Jeong",
      "affiliations": []
    },
    {
      "id": 15099,
      "firstName": "Xiuming",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 20220,
      "firstName": "Hiyoruki",
      "lastName": "Kajimoto",
      "affiliations": []
    },
    {
      "id": 23295,
      "firstName": "Leo",
      "lastName": "Hnatek",
      "affiliations": []
    },
    {
      "id": 20223,
      "firstName": "Nicolas",
      "lastName": "Saporito",
      "affiliations": []
    },
    {
      "id": 16132,
      "firstName": "Selma",
      "lastName": "Sabanovic",
      "affiliations": []
    },
    {
      "id": 21253,
      "firstName": "Ken",
      "lastName": "Perlin",
      "affiliations": []
    },
    {
      "id": 18183,
      "firstName": "Tong",
      "lastName": "Mu",
      "affiliations": []
    },
    {
      "id": 23303,
      "firstName": "Frank",
      "lastName": "Ling",
      "affiliations": []
    },
    {
      "id": 24329,
      "firstName": "Pedro",
      "lastName": "Campos",
      "middleInitial": "F.",
      "affiliations": []
    },
    {
      "id": 18185,
      "firstName": "Swagata",
      "lastName": "Das",
      "affiliations": []
    },
    {
      "id": 14092,
      "firstName": "Mitchell",
      "lastName": "Karchemsky",
      "affiliations": []
    },
    {
      "id": 13069,
      "firstName": "Kaori",
      "lastName": "Ikematsu",
      "affiliations": []
    },
    {
      "id": 23311,
      "firstName": "Tim Claudius",
      "lastName": "Stratmann",
      "affiliations": []
    },
    {
      "id": 13072,
      "firstName": "SoHyun",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 22291,
      "firstName": "James",
      "lastName": "Fogarty",
      "affiliations": []
    },
    {
      "id": 17172,
      "firstName": "Enrico",
      "lastName": "Rukzio",
      "affiliations": []
    },
    {
      "id": 22293,
      "firstName": "Liwei",
      "lastName": "Chan",
      "affiliations": []
    },
    {
      "id": 24344,
      "firstName": "Nicolas",
      "lastName": "Roussel",
      "affiliations": []
    },
    {
      "id": 8989,
      "firstName": "Aruna",
      "lastName": "Balasubramanian",
      "affiliations": []
    },
    {
      "id": 12061,
      "firstName": "Jeremy",
      "lastName": "Cooperstock",
      "affiliations": []
    },
    {
      "id": 21277,
      "firstName": "Melvin",
      "lastName": "Witte",
      "affiliations": []
    },
    {
      "id": 12063,
      "firstName": "Theophanis",
      "lastName": "Tsandilas",
      "affiliations": []
    },
    {
      "id": 17183,
      "firstName": "Yuichi",
      "lastName": "Itoh",
      "affiliations": []
    },
    {
      "id": 11040,
      "firstName": "Yoshihiro",
      "lastName": "Kawahara",
      "affiliations": []
    },
    {
      "id": 21281,
      "firstName": "Hagen",
      "lastName": "Hiller",
      "affiliations": []
    },
    {
      "id": 22307,
      "firstName": "Youngwoo",
      "lastName": "Yoon",
      "affiliations": []
    },
    {
      "id": 12069,
      "firstName": "Sunjun",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 19238,
      "firstName": "Matthew",
      "lastName": "Lakier",
      "affiliations": []
    },
    {
      "id": 16167,
      "firstName": "Takuya",
      "lastName": "Kitade",
      "affiliations": []
    },
    {
      "id": 21289,
      "firstName": "Volodymyr",
      "lastName": "Dziubak",
      "affiliations": []
    },
    {
      "id": 9002,
      "firstName": "Sean",
      "lastName": "Braley",
      "affiliations": []
    },
    {
      "id": 21290,
      "firstName": "Tatsuya",
      "lastName": "Ishibashi",
      "affiliations": []
    },
    {
      "id": 20267,
      "firstName": "Hui-Shyong",
      "lastName": "Yeo",
      "affiliations": []
    },
    {
      "id": 18220,
      "firstName": "Gilles",
      "lastName": "Bailly",
      "affiliations": []
    },
    {
      "id": 24369,
      "firstName": "David",
      "lastName": "Rudi",
      "affiliations": []
    },
    {
      "id": 22323,
      "firstName": "Meng-Ju",
      "lastName": "Hsieh",
      "affiliations": []
    },
    {
      "id": 16182,
      "firstName": "Calvin",
      "lastName": "Rubens",
      "affiliations": []
    },
    {
      "id": 22328,
      "firstName": "Linjun",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 18233,
      "firstName": "Jotaro",
      "lastName": "Shigeyama",
      "affiliations": []
    },
    {
      "id": 14139,
      "firstName": "Wataru",
      "lastName": "Yamada",
      "affiliations": []
    },
    {
      "id": 16188,
      "firstName": "Evgeny",
      "lastName": "Stemasov",
      "affiliations": []
    },
    {
      "id": 19262,
      "firstName": "Philip",
      "lastName": "Guo",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 16190,
      "firstName": "Erin",
      "lastName": "Walker",
      "affiliations": []
    },
    {
      "id": 22337,
      "firstName": "Hariharan",
      "lastName": "Subramonyam",
      "affiliations": []
    },
    {
      "id": 24388,
      "firstName": "Giuseppe",
      "lastName": "Frau",
      "affiliations": []
    },
    {
      "id": 24389,
      "firstName": "Robert",
      "lastName": "Kovacs",
      "affiliations": []
    },
    {
      "id": 16197,
      "firstName": "Haoqi",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 18247,
      "firstName": "Germán",
      "lastName": "Leiva",
      "affiliations": []
    },
    {
      "id": 12103,
      "firstName": "Jason",
      "lastName": "Situ",
      "affiliations": []
    },
    {
      "id": 20297,
      "firstName": "Shori",
      "lastName": "Ueda",
      "affiliations": []
    },
    {
      "id": 24394,
      "firstName": "Takashi",
      "lastName": "Goto",
      "affiliations": []
    },
    {
      "id": 16203,
      "firstName": "Huy Viet",
      "lastName": "Le",
      "affiliations": []
    },
    {
      "id": 23374,
      "firstName": "Changhoon",
      "lastName": "Oh",
      "affiliations": []
    },
    {
      "id": 16212,
      "firstName": "Michael",
      "lastName": "Wessely",
      "affiliations": []
    },
    {
      "id": 9045,
      "firstName": "Ken",
      "lastName": "Nakagaki",
      "affiliations": []
    },
    {
      "id": 22360,
      "firstName": "Lan",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 10073,
      "firstName": "Bjoern",
      "lastName": "Hartmann",
      "affiliations": []
    },
    {
      "id": 14170,
      "firstName": "Pascal",
      "lastName": "Fortin",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 12124,
      "firstName": "Jane",
      "lastName": "E",
      "middleInitial": "L.",
      "affiliations": []
    },
    {
      "id": 10081,
      "firstName": "Anne",
      "lastName": "Ross",
      "middleInitial": "Spencer",
      "affiliations": []
    },
    {
      "id": 12129,
      "firstName": "Elisabeth",
      "lastName": "Sulmont",
      "affiliations": []
    },
    {
      "id": 11108,
      "firstName": "Jacob",
      "lastName": "Wobbrock",
      "middleInitial": "O.",
      "affiliations": []
    },
    {
      "id": 17256,
      "firstName": "Nurcan",
      "lastName": "Gecer Ulu",
      "affiliations": []
    },
    {
      "id": 22378,
      "firstName": "Ronan",
      "lastName": "Hinchet",
      "affiliations": []
    },
    {
      "id": 10090,
      "firstName": "Siegfried",
      "lastName": "Bauer",
      "affiliations": []
    },
    {
      "id": 17258,
      "firstName": "Ke",
      "lastName": "Huo",
      "affiliations": []
    },
    {
      "id": 17259,
      "firstName": "Daniel",
      "lastName": "Wigdor",
      "affiliations": []
    },
    {
      "id": 12143,
      "firstName": "Andrew",
      "lastName": "Owens",
      "affiliations": []
    },
    {
      "id": 23408,
      "firstName": "Cristian",
      "lastName": "Felix",
      "affiliations": []
    },
    {
      "id": 14193,
      "firstName": "Alex",
      "lastName": "Olwal",
      "affiliations": []
    },
    {
      "id": 11129,
      "firstName": "Siddhesh",
      "lastName": "Krishnan",
      "affiliations": []
    },
    {
      "id": 14201,
      "firstName": "Michael",
      "lastName": "Wessely",
      "affiliations": []
    },
    {
      "id": 21370,
      "firstName": "Aritra",
      "lastName": "Dasgupta",
      "affiliations": []
    },
    {
      "id": 14204,
      "firstName": "Jeremy",
      "lastName": "Cooperstock",
      "middleInitial": "R.",
      "affiliations": []
    },
    {
      "id": 12162,
      "firstName": "Steve",
      "lastName": "Oney",
      "affiliations": []
    },
    {
      "id": 22402,
      "firstName": "Antti",
      "lastName": "Oulasvirta",
      "affiliations": []
    },
    {
      "id": 11140,
      "firstName": "Martin",
      "lastName": "Feick",
      "affiliations": []
    },
    {
      "id": 17284,
      "firstName": "Hirav",
      "lastName": "Parekh",
      "affiliations": []
    },
    {
      "id": 20359,
      "firstName": "Patrick",
      "lastName": "Parzer",
      "affiliations": []
    },
    {
      "id": 11143,
      "firstName": "Mariko",
      "lastName": "Chiba",
      "affiliations": []
    },
    {
      "id": 16264,
      "firstName": "Chloe",
      "lastName": "Eghtebas",
      "affiliations": []
    },
    {
      "id": 15247,
      "firstName": "Tony",
      "lastName": "Tang",
      "affiliations": []
    },
    {
      "id": 10127,
      "firstName": "Tico",
      "lastName": "Ballagas",
      "affiliations": []
    },
    {
      "id": 11153,
      "firstName": "Jan",
      "lastName": "Borchers",
      "affiliations": []
    },
    {
      "id": 23443,
      "firstName": "Thomas",
      "lastName": "Kosch",
      "affiliations": []
    },
    {
      "id": 14229,
      "firstName": "Kenta",
      "lastName": "Suzuki",
      "affiliations": []
    },
    {
      "id": 21399,
      "firstName": "Jian",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 19351,
      "firstName": "Ye",
      "lastName": "Tao",
      "affiliations": []
    },
    {
      "id": 23449,
      "firstName": "Walter",
      "lastName": "Lasecki",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 21401,
      "firstName": "Denis",
      "lastName": "Efimov",
      "affiliations": []
    },
    {
      "id": 10137,
      "firstName": "Ian",
      "lastName": "Oakley",
      "affiliations": []
    },
    {
      "id": 13210,
      "firstName": "Paloma",
      "lastName": "Díaz",
      "affiliations": []
    },
    {
      "id": 19354,
      "firstName": "Michael",
      "lastName": "Haller",
      "affiliations": []
    },
    {
      "id": 16283,
      "firstName": "Yusuke",
      "lastName": "Sugano",
      "affiliations": []
    },
    {
      "id": 22433,
      "firstName": "Markku",
      "lastName": "Laine",
      "affiliations": []
    },
    {
      "id": 18337,
      "firstName": "Naoki",
      "lastName": "Yanagihara",
      "affiliations": []
    },
    {
      "id": 13218,
      "firstName": "David",
      "lastName": "Karger",
      "middleInitial": "R.",
      "affiliations": []
    },
    {
      "id": 14245,
      "firstName": "Tiare",
      "lastName": "Feuchtner",
      "affiliations": []
    },
    {
      "id": 22438,
      "firstName": "Qiurui",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 9128,
      "firstName": "Tom",
      "lastName": "Yeh",
      "affiliations": []
    },
    {
      "id": 17320,
      "firstName": "Jundong",
      "lastName": "Cho",
      "affiliations": []
    },
    {
      "id": 14264,
      "firstName": "Otmar",
      "lastName": "Hilliges",
      "affiliations": []
    },
    {
      "id": 12216,
      "firstName": "Tobias",
      "lastName": "Arndt",
      "affiliations": []
    },
    {
      "id": 21433,
      "firstName": "Luis",
      "lastName": "Paredes",
      "affiliations": []
    },
    {
      "id": 9146,
      "firstName": "Xia",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 16314,
      "firstName": "Levent Burak",
      "lastName": "Kara",
      "affiliations": []
    },
    {
      "id": 20414,
      "firstName": "Hiroyuki",
      "lastName": "Manabe",
      "affiliations": []
    },
    {
      "id": 18368,
      "firstName": "Stéphane",
      "lastName": "Conversy",
      "affiliations": []
    },
    {
      "id": 10179,
      "firstName": "Xiang",
      "lastName": "Chen",
      "middleInitial": "Anthony'",
      "affiliations": []
    },
    {
      "id": 17348,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "affiliations": []
    },
    {
      "id": 11205,
      "firstName": "Eleanor",
      "lastName": "O'Rourke",
      "affiliations": []
    },
    {
      "id": 18374,
      "firstName": "Thomas",
      "lastName": "Langerak",
      "affiliations": []
    },
    {
      "id": 23495,
      "firstName": "Florian",
      "lastName": "Perteneder",
      "affiliations": []
    },
    {
      "id": 16328,
      "firstName": "Joonhwan",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 9161,
      "firstName": "Andrea",
      "lastName": "Bunt",
      "affiliations": []
    },
    {
      "id": 15309,
      "firstName": "Enrico",
      "lastName": "Bertini",
      "affiliations": []
    },
    {
      "id": 22478,
      "firstName": "Damien",
      "lastName": "Rompapas",
      "middleInitial": "Constantine",
      "affiliations": []
    },
    {
      "id": 12240,
      "firstName": "Roy",
      "lastName": "Shilkrot",
      "affiliations": []
    },
    {
      "id": 16339,
      "firstName": "Jennifer",
      "lastName": "Boger",
      "affiliations": []
    },
    {
      "id": 21463,
      "firstName": "Kenji",
      "lastName": "Suzuki",
      "affiliations": []
    },
    {
      "id": 20441,
      "firstName": "Alexander",
      "lastName": "Mehler",
      "affiliations": []
    },
    {
      "id": 22489,
      "firstName": "Gabriel",
      "lastName": "Haas",
      "affiliations": []
    },
    {
      "id": 11229,
      "firstName": "Ryo",
      "lastName": "Shirai",
      "affiliations": []
    },
    {
      "id": 23519,
      "firstName": "Jun",
      "lastName": "Gong",
      "affiliations": []
    },
    {
      "id": 19424,
      "firstName": "Keunwoo",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 17377,
      "firstName": "Chang-Hong",
      "lastName": "Hsu",
      "affiliations": []
    },
    {
      "id": 13284,
      "firstName": "Naoshi",
      "lastName": "Ooba",
      "affiliations": []
    },
    {
      "id": 9195,
      "firstName": "Walter",
      "lastName": "Talamonti",
      "affiliations": []
    },
    {
      "id": 20460,
      "firstName": "Benjamin",
      "lastName": "Meyer",
      "affiliations": []
    },
    {
      "id": 13294,
      "firstName": "Thijs",
      "lastName": "Roumen",
      "middleInitial": "Jan",
      "affiliations": []
    },
    {
      "id": 21486,
      "firstName": "Alice",
      "lastName": "Oh",
      "affiliations": []
    },
    {
      "id": 14321,
      "firstName": "Niels",
      "lastName": "Henze",
      "affiliations": []
    },
    {
      "id": 9202,
      "firstName": "Ravin",
      "lastName": "Balakrishnan",
      "affiliations": []
    },
    {
      "id": 10226,
      "firstName": "Sean",
      "lastName": "Follmer",
      "affiliations": []
    },
    {
      "id": 15347,
      "firstName": "Ignacio",
      "lastName": "Aedo",
      "affiliations": []
    },
    {
      "id": 9205,
      "firstName": "Fraser",
      "lastName": "Anderson",
      "affiliations": []
    },
    {
      "id": 17403,
      "firstName": "Matthew",
      "lastName": "Conlen",
      "affiliations": []
    },
    {
      "id": 18428,
      "firstName": "Tongda",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 12287,
      "firstName": "Florian",
      "lastName": "Geiselhart",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 16,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-02-10 13:49:48+00"
  }
}