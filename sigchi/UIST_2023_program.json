{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10103,
    "shortName": "UIST",
    "displayShortName": "",
    "year": 2023,
    "startDate": 1698537600000,
    "endDate": 1698796800000,
    "fullName": "ACM Symposium on User Interface Software and Technology",
    "url": "https://uist.acm.org/2023/",
    "location": "The Fairmont, San Francisco, California, USA",
    "timeZoneOffset": -420,
    "timeZoneName": "America/Los_Angeles",
    "logoUrl": "https://files.sigchi.org/conference/logo/10103/f7eaa983-f68a-f402-1eb3-f94fc50af93e.png",
    "accessibilityFaqUrl": "https://files.sigchi.org/conference/accessibility/10103/5050b79a-53bf-8cf6-71c6-bb2bdb222be6.html",
    "addons": {},
    "name": "UIST 2023"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 50,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2023-10-31 19:04:13+00"
  },
  "sponsors": [
    {
      "id": 10437,
      "name": "Meta Reality Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10103/logo/b4b7c2e3-35c4-c3f7-3715-ab65a2ac939a.png",
      "levelId": 10262,
      "url": "https://about.meta.com/realitylabs/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10438,
      "name": "Google",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10103/logo/0425eb7b-c0a7-ea98-e169-1fbd67644226.png",
      "levelId": 10263,
      "url": "https://www.google.com/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10439,
      "name": "Adobe",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10103/logo/260d2fdc-cb83-c40a-9234-9e345e3a771f.png",
      "levelId": 10263,
      "url": "https://www.adobe.com",
      "order": 1,
      "extraPadding": 0
    },
    {
      "id": 10440,
      "name": "Apple",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10103/logo/3ffe9ad5-0bf5-88fc-1f82-f44ed9408f4a.png",
      "levelId": 10263,
      "url": "https://www.apple.com",
      "order": 2,
      "extraPadding": 0
    },
    {
      "id": 10441,
      "name": "Autodesk",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10103/logo/1efb95a9-92e6-f06c-ab83-6dd9e91f171b.png",
      "levelId": 10263,
      "url": "https://www.autodesk.com",
      "order": 3,
      "extraPadding": 0
    },
    {
      "id": 10442,
      "name": "Toyota Research Institute",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10103/logo/cb491d82-d631-4e4a-1237-9da22b74c9ce.png",
      "levelId": 10265,
      "url": "https://www.tri.global",
      "order": 1,
      "extraPadding": 0
    },
    {
      "id": 10443,
      "name": "JPMorgan Chase & Co.",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10103/logo/2006621c-d530-ec8c-8831-70fa5a4da0fe.png",
      "levelId": 10265,
      "url": "https://www.jpmorganchase.com",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10455,
      "name": "CSIRO",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10103/logo/7ea1f140-1afc-8b68-3617-c604a458088f.png",
      "levelId": 10265,
      "url": "https://www.csiro.au/en/",
      "order": 2,
      "extraPadding": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10260,
      "name": "Sponsors",
      "rank": 5,
      "isDefault": true
    },
    {
      "id": 10262,
      "name": "Platinum Sponsors",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10263,
      "name": "Silver Sponsors",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10264,
      "name": "Gold Sponsors",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10265,
      "name": "Bronze Sponsors",
      "rank": 4,
      "isDefault": false
    }
  ],
  "floors": [
    {
      "id": 10215,
      "name": "3D floorplan",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10103/e04d9517-9a13-c270-727f-1f9524e7b145.png",
      "roomIds": [
        11393,
        11394,
        11346,
        11354,
        11347,
        11356,
        11348,
        11350,
        11357,
        11351,
        11355,
        11403,
        11353,
        11359,
        11349,
        11358,
        11373
      ]
    }
  ],
  "rooms": [
    {
      "id": 11346,
      "name": "Mezzanine Foyer",
      "setup": "NO_ROOM",
      "typeId": 13098
    },
    {
      "id": 11347,
      "name": "California Room",
      "setup": "THEATRE",
      "typeId": 13097,
      "capacity": 65,
      "note": ""
    },
    {
      "id": 11348,
      "name": "Hunt Room",
      "setup": "THEATRE",
      "typeId": 13097,
      "capacity": 45,
      "note": ""
    },
    {
      "id": 11349,
      "name": "Far East Room",
      "setup": "THEATRE",
      "typeId": 13097,
      "capacity": 20,
      "note": ""
    },
    {
      "id": 11350,
      "name": "International Room",
      "setup": "THEATRE",
      "typeId": 13097,
      "capacity": 20,
      "note": ""
    },
    {
      "id": 11351,
      "name": "State Room",
      "setup": "SPECIAL",
      "typeId": 13091,
      "capacity": 20,
      "note": ""
    },
    {
      "id": 11353,
      "name": "Crown Room",
      "setup": "SPECIAL",
      "typeId": 13092,
      "capacity": 420,
      "note": ""
    },
    {
      "id": 11354,
      "name": "Garden Room",
      "setup": "NO_ROOM",
      "typeId": 13098
    },
    {
      "id": 11355,
      "name": "Venetian Room",
      "setup": "THEATRE",
      "typeId": 13094,
      "capacity": 300,
      "note": ""
    },
    {
      "id": 11356,
      "name": "Gold Room",
      "setup": "THEATRE",
      "typeId": 13094,
      "capacity": 300,
      "note": ""
    },
    {
      "id": 11357,
      "name": "Pavilion Room",
      "setup": "SPECIAL",
      "typeId": 13095,
      "capacity": 420,
      "note": ""
    },
    {
      "id": 11358,
      "name": "Loggia & Pavilion Room",
      "setup": "SPECIAL",
      "typeId": 13095,
      "capacity": 420,
      "note": ""
    },
    {
      "id": 11359,
      "name": "Fountain Room",
      "setup": "ROUNDS",
      "typeId": 13092,
      "capacity": 50,
      "note": ""
    },
    {
      "id": 11373,
      "name": "Grand Ballroom/Lounge",
      "setup": "SPECIAL",
      "typeId": 13090,
      "capacity": 600,
      "note": ""
    },
    {
      "id": 11393,
      "name": "Nursing Room",
      "setup": "SPECIAL",
      "typeId": 13098,
      "note": "Far East Room"
    },
    {
      "id": 11394,
      "name": "Silent Room",
      "setup": "SPECIAL",
      "typeId": 13098,
      "note": "State Room"
    },
    {
      "id": 11403,
      "name": "Green Room",
      "setup": "NO_ROOM",
      "typeId": 13098
    }
  ],
  "tracks": [
    {
      "id": 12412,
      "name": "UIST 2022 Demos"
    },
    {
      "id": 12413,
      "name": "UIST 2022 Visions"
    },
    {
      "id": 12414,
      "name": "UIST 2022 Posters"
    },
    {
      "id": 12415,
      "name": "UIST 2023 Papers",
      "typeId": 13094
    },
    {
      "id": 12416,
      "name": "UIST 2022 Doctoral Symposium"
    },
    {
      "id": 12417,
      "name": "UIST 2023 Demos",
      "typeId": 13090
    },
    {
      "id": 12418,
      "name": "UIST 2023 Student Innovation Contest",
      "typeId": 13115
    },
    {
      "id": 12419,
      "name": "UIST 2023 Workshops",
      "typeId": 13097
    },
    {
      "id": 12420,
      "name": "UIST 2023 Doctoral Symposium",
      "typeId": 13091
    },
    {
      "id": 12421,
      "name": "UIST 2023 Visions",
      "typeId": 13116
    },
    {
      "id": 12422,
      "name": "UIST 2023 Posters",
      "typeId": 13095
    },
    {
      "id": 12430,
      "typeId": 13094
    },
    {
      "id": 12455,
      "typeId": 13139
    }
  ],
  "contentTypes": [
    {
      "id": 13089,
      "name": "Course",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 13090,
      "name": "Demo",
      "color": "#006d2c",
      "duration": 5,
      "displayName": "Demos"
    },
    {
      "id": 13091,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 13092,
      "name": "Event",
      "color": "#ffc034",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 13093,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 13094,
      "name": "Paper",
      "color": "#0d42cc",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 13095,
      "name": "Poster",
      "color": "#ff7a00",
      "duration": 5,
      "displayName": "Posters"
    },
    {
      "id": 13097,
      "name": "Workshop",
      "color": "#f60000",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 13098,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    },
    {
      "id": 13115,
      "name": "Student Innovation Contest",
      "color": "#969696",
      "duration": 180
    },
    {
      "id": 13116,
      "name": "Vision",
      "color": "#26e5f1",
      "duration": 30
    },
    {
      "id": 13139,
      "name": "Keynote",
      "color": "#32d923",
      "duration": 90
    }
  ],
  "timeSlots": [
    {
      "id": 13440,
      "type": "SESSION",
      "startDate": 1698663600000,
      "endDate": 1698668400000
    },
    {
      "id": 13441,
      "type": "LUNCH",
      "startDate": 1698668400000,
      "endDate": 1698675600000
    },
    {
      "id": 13442,
      "type": "SESSION",
      "startDate": 1698675600000,
      "endDate": 1698680400000
    },
    {
      "id": 13443,
      "type": "BREAK",
      "startDate": 1698680400000,
      "endDate": 1698682200000
    },
    {
      "id": 13444,
      "type": "SESSION",
      "startDate": 1698682200000,
      "endDate": 1698687000000
    },
    {
      "id": 13445,
      "type": "SESSION",
      "startDate": 1698829200000,
      "endDate": 1698834000000
    },
    {
      "id": 13446,
      "type": "BREAK",
      "startDate": 1698834000000,
      "endDate": 1698835800000
    },
    {
      "id": 13447,
      "type": "SESSION",
      "startDate": 1698835800000,
      "endDate": 1698840600000
    },
    {
      "id": 13448,
      "type": "LUNCH",
      "startDate": 1698840600000,
      "endDate": 1698847800000
    },
    {
      "id": 13449,
      "type": "SESSION",
      "startDate": 1698847800000,
      "endDate": 1698853800000
    },
    {
      "id": 13452,
      "type": "SESSION",
      "startDate": 1698742800000,
      "endDate": 1698747600000
    },
    {
      "id": 13453,
      "type": "BREAK",
      "startDate": 1698747600000,
      "endDate": 1698749400000
    },
    {
      "id": 13454,
      "type": "SESSION",
      "startDate": 1698749400000,
      "endDate": 1698754200000
    },
    {
      "id": 13455,
      "type": "LUNCH",
      "startDate": 1698754200000,
      "endDate": 1698761400000
    },
    {
      "id": 13456,
      "type": "SESSION",
      "startDate": 1698761400000,
      "endDate": 1698766200000
    },
    {
      "id": 13457,
      "type": "BREAK",
      "startDate": 1698766200000,
      "endDate": 1698768000000
    },
    {
      "id": 13458,
      "type": "SESSION",
      "startDate": 1698768000000,
      "endDate": 1698772800000
    },
    {
      "id": 13469,
      "type": "SESSION",
      "startDate": 1698656400000,
      "endDate": 1698661800000
    },
    {
      "id": 13470,
      "type": "BREAK",
      "startDate": 1698687000000,
      "endDate": 1698687600000
    },
    {
      "id": 13471,
      "type": "SESSION",
      "startDate": 1698687600000,
      "endDate": 1698689400000
    },
    {
      "id": 13472,
      "type": "BREAK",
      "startDate": 1698853800000,
      "endDate": 1698855600000
    },
    {
      "id": 13473,
      "type": "SESSION",
      "startDate": 1698855600000,
      "endDate": 1698861600000
    },
    {
      "id": 13474,
      "type": "BREAK",
      "startDate": 1698772800000,
      "endDate": 1698773700000
    },
    {
      "id": 13475,
      "type": "SESSION",
      "startDate": 1698773700000,
      "endDate": 1698775500000
    },
    {
      "id": 13476,
      "type": "BREAK",
      "startDate": 1698775500000,
      "endDate": 1698778800000
    },
    {
      "id": 13482,
      "type": "SESSION",
      "startDate": 1698570000000,
      "endDate": 1698598800000
    },
    {
      "id": 13496,
      "type": "SESSION",
      "startDate": 1698692400000,
      "endDate": 1698703200000
    },
    {
      "id": 13497,
      "type": "BREAK",
      "startDate": 1698661800000,
      "endDate": 1698663600000
    },
    {
      "id": 13498,
      "type": "SESSION",
      "startDate": 1698778800000,
      "endDate": 1698789600000
    },
    {
      "id": 13503,
      "type": "LUNCH",
      "startDate": 1698580800000,
      "endDate": 1698584400000
    },
    {
      "id": 13507,
      "type": "SESSION",
      "startDate": 1698652800000,
      "endDate": 1698688800000
    },
    {
      "id": 13508,
      "type": "SESSION",
      "startDate": 1698566400000,
      "endDate": 1698570000000
    },
    {
      "id": 13509,
      "type": "SESSION",
      "startDate": 1698825600000,
      "endDate": 1698840000000
    },
    {
      "id": 13510,
      "type": "SESSION",
      "startDate": 1698739200000,
      "endDate": 1698775200000
    },
    {
      "id": 13511,
      "type": "SESSION",
      "startDate": 1698598800000,
      "endDate": 1698609600000
    },
    {
      "id": 13542,
      "type": "SESSION",
      "startDate": 1698825600000,
      "endDate": 1698861600000
    }
  ],
  "sessions": [
    {
      "id": 126889,
      "name": "Beyond Words: Text and Large Language Models",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/Wv1oDukDLUI?si=F9GzkTx4BefZsype"
        }
      },
      "isParallelPresentation": false,
      "importedId": "4317e153-8a46-485b-b8ec-aebf2f0d68f5",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        129936
      ],
      "contentIds": [
        126651,
        126779,
        126786,
        126721,
        126734,
        126685
      ],
      "source": "SYS",
      "timeSlotId": 13440
    },
    {
      "id": 127748,
      "name": "Workshop - Architecting Novel Interactions with Generative AI Models",
      "isParallelPresentation": false,
      "importedId": "abac2285-8b5c-4dbf-9159-8c5c37d3bcb9",
      "typeId": 13097,
      "roomId": 11347,
      "chairIds": [],
      "contentIds": [
        127195
      ],
      "source": "SYS",
      "timeSlotId": 13482
    },
    {
      "id": 127749,
      "name": "Workshop - Electro-actuated Materials for Future Haptic Interfaces",
      "isParallelPresentation": false,
      "importedId": "41d60331-658f-4677-bf19-d43376323b19",
      "typeId": 13097,
      "roomId": 11349,
      "chairIds": [],
      "contentIds": [
        127204
      ],
      "source": "SYS",
      "timeSlotId": 13482
    },
    {
      "id": 127750,
      "name": "Workshop - Future Paradigms for Sustainable Making",
      "isParallelPresentation": false,
      "importedId": "085054e2-63c9-49d2-ad25-09e705e08f56",
      "typeId": 13097,
      "roomId": 11350,
      "chairIds": [],
      "contentIds": [
        127203
      ],
      "source": "SYS",
      "timeSlotId": 13482
    },
    {
      "id": 127751,
      "name": "Workshop - XR and AI: AI-Enabled Virtual, Augmented, and Mixed Reality",
      "isParallelPresentation": false,
      "importedId": "2bbaf60a-4bc0-4bb3-aac0-58b0d65494f2",
      "typeId": 13097,
      "roomId": 11348,
      "chairIds": [],
      "contentIds": [
        127198
      ],
      "source": "SYS",
      "timeSlotId": 13482
    },
    {
      "id": 127752,
      "name": "Haptic Hype: Haptics in AR and VR",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/8Pzk_2D0Bxo?si=MB_M_W8VqRlEF8wW"
        }
      },
      "isParallelPresentation": false,
      "importedId": "abcf21b4-288a-4774-98c2-2230df672822",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        125866
      ],
      "contentIds": [
        126646,
        126754,
        126645,
        126674,
        126792,
        126731
      ],
      "source": "SYS",
      "timeSlotId": 13440
    },
    {
      "id": 127753,
      "name": "Masterful Media: Audio and Video Authoring Tools",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/8Pzk_2D0Bxo?si=MB_M_W8VqRlEF8wW"
        }
      },
      "isParallelPresentation": false,
      "importedId": "16975246-e74d-4413-beed-7bd648a477ae",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        126549
      ],
      "contentIds": [
        126752,
        126791,
        126881,
        126801,
        126856,
        126808
      ],
      "source": "SYS",
      "timeSlotId": 13442
    },
    {
      "id": 127754,
      "name": "Creative Makers: Textiles, Craft and Computation",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/Wv1oDukDLUI?si=F9GzkTx4BefZsype"
        }
      },
      "isParallelPresentation": false,
      "importedId": "4f3f9fd8-0418-4854-898d-6ae75b814cbe",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        126238
      ],
      "contentIds": [
        126759,
        126728,
        126737,
        126703,
        126652,
        126684
      ],
      "source": "SYS",
      "timeSlotId": 13442
    },
    {
      "id": 127755,
      "name": "Digital Dexterity: Touching and Typing Techniques",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/Wv1oDukDLUI?si=F9GzkTx4BefZsype"
        }
      },
      "isParallelPresentation": false,
      "importedId": "0f253613-0e9d-46a5-8d54-2f0f20380924",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        129931
      ],
      "contentIds": [
        126788,
        126841,
        126877,
        126720,
        126700,
        127828
      ],
      "source": "SYS",
      "timeSlotId": 13444
    },
    {
      "id": 127756,
      "name": "Green Machine: Sustainability in Hardware and Fabrication",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/Wv1oDukDLUI?si=F9GzkTx4BefZsype"
        }
      },
      "isParallelPresentation": false,
      "importedId": "495502f8-6b05-45f2-b09a-da018447635e",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        125767
      ],
      "contentIds": [
        126746,
        126730,
        126809,
        126650,
        126722,
        126681
      ],
      "source": "SYS",
      "timeSlotId": 13444
    },
    {
      "id": 127757,
      "name": "Inclusive Interactions: Accessibility Techniques and Systems",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/YzCC3NcGVrM?si=GJBhdIeh-Eqv0HJD"
        }
      },
      "isParallelPresentation": false,
      "importedId": "d6afcde4-cff9-4bfb-a9d3-771c1abca610",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        126094
      ],
      "contentIds": [
        126859,
        126756,
        126670,
        126662,
        126776,
        126669
      ],
      "source": "SYS",
      "timeSlotId": 13452
    },
    {
      "id": 127758,
      "name": "Write Right: Reading and Writing Tools",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/I4s6q2RYQjI?si=y0ysHSi1W0NMkd0C"
        }
      },
      "isParallelPresentation": false,
      "importedId": "0d854309-9ec5-4545-98e3-3fcc250e37ac",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        126954
      ],
      "contentIds": [
        126665,
        126787,
        126844,
        127843,
        126692,
        126736
      ],
      "source": "SYS",
      "timeSlotId": 13452
    },
    {
      "id": 127759,
      "name": "Creative Visions: Creativity Support Tools",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/I4s6q2RYQjI?si=y0ysHSi1W0NMkd0C"
        }
      },
      "isParallelPresentation": false,
      "importedId": "bb4892e6-9bf0-48d5-ad10-5dbad53e3db2",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        126292
      ],
      "contentIds": [
        126742,
        126857,
        126773,
        126667,
        126818,
        128173
      ],
      "source": "SYS",
      "timeSlotId": 13454
    },
    {
      "id": 127760,
      "name": "Fab Magic: 3D Fabrication Techniques",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/YzCC3NcGVrM?si=GJBhdIeh-Eqv0HJD"
        }
      },
      "isParallelPresentation": false,
      "importedId": "f07b7e42-77a6-4e34-8e67-6e2b19446624",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        129935
      ],
      "contentIds": [
        126878,
        126664,
        126704,
        126830,
        126845,
        126774
      ],
      "source": "SYS",
      "timeSlotId": 13454
    },
    {
      "id": 127761,
      "name": "Teamwork Triumphs: Collaborative Experiences",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/I4s6q2RYQjI?si=y0ysHSi1W0NMkd0C"
        }
      },
      "isParallelPresentation": false,
      "importedId": "70360c48-4240-416a-bca4-9ca53c2ae697",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        126414
      ],
      "contentIds": [
        126813,
        126858,
        126817,
        126745,
        126668,
        126761
      ],
      "source": "SYS",
      "timeSlotId": 13456
    },
    {
      "id": 127762,
      "name": "Feel the Future: Toolkits for Haptics",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/YzCC3NcGVrM?si=GJBhdIeh-Eqv0HJD"
        }
      },
      "isParallelPresentation": false,
      "importedId": "a061859c-a176-4b12-8730-7b6ba9043afe",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        127082
      ],
      "contentIds": [
        126816,
        126672,
        126831,
        126706,
        126847,
        126807
      ],
      "source": "SYS",
      "timeSlotId": 13456
    },
    {
      "id": 127763,
      "name": "Code Craftsmanship: Programming Support Tools",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/YzCC3NcGVrM?si=GJBhdIeh-Eqv0HJD"
        }
      },
      "isParallelPresentation": false,
      "importedId": "68f67a5f-f3ce-482f-85d0-6e87555f227e",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        125867
      ],
      "contentIds": [
        126686,
        126701,
        126789,
        126735,
        126723,
        126806,
        126688
      ],
      "source": "SYS",
      "timeSlotId": 13458
    },
    {
      "id": 127764,
      "name": "Sensing Sorcery: Novel Sensing Techniques and Systems",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/I4s6q2RYQjI?si=y0ysHSi1W0NMkd0C"
        }
      },
      "isParallelPresentation": false,
      "importedId": "eb432222-aabd-49d1-a4e8-2923f948ec67",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        129933
      ],
      "contentIds": [
        126846,
        126671,
        126777,
        126880,
        126687,
        126805
      ],
      "source": "SYS",
      "timeSlotId": 13458
    },
    {
      "id": 127765,
      "name": "Sensory Shenanigans: Immersion and Illusions in Mixed Reality",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/IiKX6HRSiSk?si=Mzhv2ahA4Ut9cOdL"
        }
      },
      "isParallelPresentation": false,
      "importedId": "41914d7c-13a9-46fb-9041-28e5ca1bd144",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        126522
      ],
      "contentIds": [
        126755,
        126689,
        126719,
        126666,
        126819,
        126683
      ],
      "source": "SYS",
      "timeSlotId": 13445
    },
    {
      "id": 127766,
      "name": "Data Dreamers: Math, Stats and Visualization",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/PkXHuPkatpk?si=HDNdRpsAfvbN9pgN"
        }
      },
      "isParallelPresentation": false,
      "importedId": "79f12235-347c-49cb-bf32-93d7e7678c0b",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        126631
      ],
      "contentIds": [
        126647,
        126781,
        126778,
        126854,
        126760,
        126758
      ],
      "source": "SYS",
      "timeSlotId": 13445
    },
    {
      "id": 127767,
      "name": "Words and Visuals: Authoring Tools for Text and Images",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/PkXHuPkatpk?si=HDNdRpsAfvbN9pgN"
        }
      },
      "isParallelPresentation": false,
      "importedId": "9695948f-262a-48ba-87d7-6aa913f2a426",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        126033
      ],
      "contentIds": [
        126717,
        126653,
        126814,
        126743,
        126780,
        126771
      ],
      "source": "SYS",
      "timeSlotId": 13447
    },
    {
      "id": 127768,
      "name": "Touching the Future: Haptics and Gestures",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/IiKX6HRSiSk?si=Mzhv2ahA4Ut9cOdL"
        }
      },
      "isParallelPresentation": false,
      "importedId": "ca76e357-b4d5-44f3-b957-654fb5167909",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        125957
      ],
      "contentIds": [
        126718,
        126829,
        126753,
        126775,
        126673,
        126733
      ],
      "source": "SYS",
      "timeSlotId": 13447
    },
    {
      "id": 127769,
      "name": "Interface Evolution: Learning, Adaptation, Customisation",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/PkXHuPkatpk?si=HDNdRpsAfvbN9pgN"
        }
      },
      "isParallelPresentation": false,
      "importedId": "9115f8d5-e556-44f2-a450-50ccd9aee10a",
      "typeId": 13094,
      "roomId": 11356,
      "chairIds": [
        126602
      ],
      "contentIds": [
        126702,
        126832,
        126729,
        126724,
        126879,
        126732,
        126872
      ],
      "source": "SYS",
      "timeSlotId": 13449
    },
    {
      "id": 127770,
      "name": "Reality Refined: Augmented Reality Techniques",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/IiKX6HRSiSk?si=Mzhv2ahA4Ut9cOdL"
        }
      },
      "isParallelPresentation": false,
      "importedId": "539118f6-22e7-44b7-83ee-0f5ad6546c79",
      "typeId": 13094,
      "roomId": 11355,
      "chairIds": [
        129932
      ],
      "contentIds": [
        126691,
        126709,
        126682,
        126790,
        126744,
        126710,
        126708
      ],
      "source": "SYS",
      "timeSlotId": 13449
    },
    {
      "id": 127845,
      "name": "Vision Talk - The Ultimate Interface",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/CK-EiEB-pxo?si=r-kDha7pJ922SFBS"
        }
      },
      "isParallelPresentation": false,
      "importedId": "63f4e3a4-1f55-411a-b711-6cdf6575f19e",
      "typeId": 13116,
      "roomId": 11355,
      "chairIds": [],
      "contentIds": [
        127382
      ],
      "source": "SYS",
      "timeSlotId": 13471
    },
    {
      "id": 127846,
      "name": "Vision Talk - AGI is coming",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/CK-EiEB-pxo?si=r-kDha7pJ922SFBS"
        }
      },
      "isParallelPresentation": false,
      "importedId": "1bf8b32e-1df8-4a64-90cb-171f5ddfd25f",
      "typeId": 13116,
      "roomId": 11355,
      "chairIds": [],
      "contentIds": [
        127399
      ],
      "source": "SYS",
      "timeSlotId": 13475
    },
    {
      "id": 127849,
      "name": "Doctoral Consortium (Not Public)",
      "isParallelPresentation": false,
      "importedId": "b7727711-959c-4fb5-8d2f-75b07ee32eaa",
      "typeId": 13091,
      "roomId": 11351,
      "chairIds": [],
      "contentIds": [
        127188,
        127200,
        127187,
        127202,
        127199,
        127192,
        127201,
        127193
      ],
      "source": "SYS",
      "timeSlotId": 13482
    },
    {
      "id": 127850,
      "name": "Opening Remarks & Keynote (David Holz)",
      "isParallelPresentation": false,
      "importedId": "374723fc-a47a-4e06-a44f-5b571a83ade9",
      "typeId": 13139,
      "roomId": 11355,
      "chairIds": [
        126123,
        130029
      ],
      "contentIds": [
        130026
      ],
      "source": "SYS",
      "timeSlotId": 13469
    },
    {
      "id": 127851,
      "name": "Closing Keynote & Remarks (Judy Fan)",
      "addons": {
        "liveLink": {
          "type": "broadcastLink",
          "url": "https://www.youtube.com/live/cG0jp13-uyU?si=KR2a1yqCGWRFbcft"
        }
      },
      "isParallelPresentation": false,
      "importedId": "6f2cd17f-7392-4b7f-a770-1dedf325c127",
      "typeId": 13139,
      "roomId": 11355,
      "chairIds": [
        126123,
        130029
      ],
      "contentIds": [
        130027
      ],
      "source": "SYS",
      "timeSlotId": 13473
    },
    {
      "id": 127865,
      "name": "Coffee Break & Poster Session A",
      "isParallelPresentation": true,
      "importedId": "73070a9a-cfe7-443c-b26b-889e9db87bc5",
      "typeId": 13095,
      "roomId": 11358,
      "chairIds": [],
      "contentIds": [
        127395,
        127387,
        127370,
        127363,
        127372,
        127358,
        127402,
        127364,
        127383,
        127371,
        127400,
        127376,
        127369,
        127394,
        127389,
        127405,
        127393,
        127377,
        127403,
        127379,
        127392,
        127390,
        127398,
        127385,
        127367
      ],
      "source": "SYS",
      "timeSlotId": 13443
    },
    {
      "id": 127867,
      "name": "Coffee Break & Poster Session B",
      "isParallelPresentation": true,
      "importedId": "276522bd-9620-437c-9c78-05913d791096",
      "typeId": 13095,
      "roomId": 11358,
      "chairIds": [],
      "contentIds": [
        127408,
        127397,
        127360,
        127388,
        127407,
        127381,
        127365,
        127366,
        127384,
        127386,
        127362,
        127380,
        127396,
        127361,
        127401,
        127357,
        127359,
        127391,
        127406,
        127375,
        127374,
        127404,
        127373,
        127378,
        127368
      ],
      "source": "SYS",
      "timeSlotId": 13457
    },
    {
      "id": 128180,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "671947e2-3173-447d-a212-32ae94808404",
      "roomId": 11354,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13507
    },
    {
      "id": 128181,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "e1da89ed-89a5-416d-a204-e6201a8012d0",
      "roomId": 11354,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13510
    },
    {
      "id": 128182,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "6926c7af-dd7a-4cc2-9c61-150c3b8010ca",
      "roomId": 11354,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13509
    },
    {
      "id": 128183,
      "name": "Workshop Registration",
      "isParallelPresentation": false,
      "importedId": "5c375797-92c7-46f6-97a3-734fe4058794",
      "roomId": 11346,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13508
    },
    {
      "id": 128184,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "572cddfa-b411-456c-ab24-f5216ac1186e",
      "roomId": 11354,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13511
    },
    {
      "id": 128188,
      "name": "Demo/SIC/Reception",
      "isParallelPresentation": true,
      "importedId": "c9a0efbf-3369-40ed-be82-6ca75fb3dccd",
      "typeId": 13090,
      "roomId": 11373,
      "chairIds": [],
      "contentIds": [
        127091,
        127135,
        127132,
        127130,
        127098,
        127118,
        127115,
        127155,
        127125,
        127142,
        127131,
        127126,
        127151,
        127111,
        127136,
        127089,
        127127,
        127097,
        127150,
        127107,
        127121,
        127094,
        127133,
        127128,
        127138,
        127103,
        127105,
        127104,
        127112,
        127143,
        127122,
        127153,
        127144,
        127110,
        127101,
        127148,
        127119,
        127149,
        127145,
        127120,
        127095,
        127102,
        127123,
        127146,
        127147,
        127100,
        127141,
        127114,
        127116,
        127088,
        127109,
        127092,
        127117,
        127152,
        127129,
        127124,
        127096,
        127158,
        127140,
        127108,
        127157,
        127090,
        127113,
        127106,
        127134,
        127137,
        127156,
        127093,
        127139,
        127154,
        127099,
        127189,
        127196,
        127197,
        127205,
        127186,
        127191,
        127190,
        127194,
        127206,
        127185
      ],
      "source": "SYS",
      "timeSlotId": 13496
    },
    {
      "id": 129720,
      "name": "Coffee Break & Poster Session B",
      "isParallelPresentation": true,
      "importedId": "65a792a4-3603-4dba-8013-8d5ca0cbaf17",
      "typeId": 13095,
      "roomId": 11358,
      "chairIds": [],
      "contentIds": [
        127408,
        127397,
        127360,
        127388,
        127407,
        127381,
        127365,
        127366,
        127384,
        127386,
        127362,
        127380,
        127396,
        127361,
        127401,
        127357,
        127359,
        127391,
        127406,
        127375,
        127374,
        127404,
        127373,
        127378,
        127368
      ],
      "source": "SYS",
      "timeSlotId": 13446
    },
    {
      "id": 129721,
      "name": "Coffee Break & Poster Session A",
      "isParallelPresentation": true,
      "importedId": "deb95fe3-2aa9-4da9-b968-8cf4e0dbba27",
      "typeId": 13095,
      "roomId": 11358,
      "chairIds": [],
      "contentIds": [
        127395,
        127387,
        127370,
        127363,
        127372,
        127358,
        127402,
        127364,
        127383,
        127371,
        127400,
        127376,
        127369,
        127394,
        127389,
        127405,
        127393,
        127377,
        127403,
        127379,
        127392,
        127390,
        127398,
        127385,
        127367
      ],
      "source": "SYS",
      "timeSlotId": 13453
    },
    {
      "id": 130126,
      "name": "Lactation Room",
      "isParallelPresentation": false,
      "importedId": "a166b1ce-b38c-4073-99d3-cc5a4c4c36b1",
      "roomId": 11349,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13507
    },
    {
      "id": 130127,
      "name": "Lactation Room",
      "isParallelPresentation": false,
      "importedId": "908ded98-5b70-47ac-abcb-a409873c1dc2",
      "roomId": 11349,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13510
    },
    {
      "id": 130128,
      "name": "Lactation Room",
      "isParallelPresentation": false,
      "importedId": "62ba15a8-f2bc-45a7-93b2-9e90827e2a81",
      "roomId": 11349,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13542
    },
    {
      "id": 130129,
      "name": "Silent Room",
      "isParallelPresentation": false,
      "importedId": "4ddb4200-7b3f-4843-ac24-1268f19c2cf9",
      "roomId": 11351,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13507
    },
    {
      "id": 130130,
      "name": "Silent Room",
      "isParallelPresentation": false,
      "importedId": "6f06f6dd-2f9a-4b4f-93ba-85dfbbd978fb",
      "roomId": 11351,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13510
    },
    {
      "id": 130131,
      "name": "Silent Room",
      "isParallelPresentation": false,
      "importedId": "86a5e624-e9c7-45e3-82b2-c8c2d2df2784",
      "roomId": 11351,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13542
    },
    {
      "id": 130132,
      "name": "Speaker Prep",
      "isParallelPresentation": false,
      "importedId": "853e08eb-735b-4a9d-916c-3d958188c5b5",
      "roomId": 11403,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13507
    },
    {
      "id": 130133,
      "name": "Speaker Prep",
      "isParallelPresentation": false,
      "importedId": "7de0b926-0eb0-4ae6-87fe-4d36fcb25809",
      "roomId": 11403,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13542
    },
    {
      "id": 130134,
      "name": "Speaker Prep",
      "isParallelPresentation": false,
      "importedId": "47616062-0592-4b2c-97e9-17fc7219759b",
      "roomId": 11403,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13510
    }
  ],
  "events": [
    {
      "id": 126893,
      "name": "Women's Luncheon (By Application)",
      "isParallelPresentation": false,
      "importedId": "fb95d0f0-8481-4ccd-b969-01dcca829dda",
      "typeId": 13092,
      "roomId": 11353,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1698754200000,
      "endDate": 1698761400000,
      "link": {
        "href": "https://forms.gle/juUpLjVU5TecWE2k7",
        "label": "Event details"
      },
      "description": "Application details: https://forms.gle/juUpLjVU5TecWE2k7",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 127772,
      "name": "Welcome Reception",
      "isParallelPresentation": false,
      "importedId": "d86508e2-f430-4065-b9e8-f6ea0b1aea52",
      "typeId": 13092,
      "roomId": 11353,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1698598800000,
      "endDate": 1698609600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 127779,
      "name": "Banquet",
      "isParallelPresentation": false,
      "importedId": "a46a4661-d462-4639-9593-ca61326519fa",
      "typeId": 13092,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1698778800000,
      "endDate": 1698789600000,
      "location": "Exploratorium",
      "coordinates": "37.80170550267322, -122.39735324920659",
      "link": {
        "href": "https://www.exploratorium.edu/",
        "label": "See More"
      },
      "description": "Exploratorium",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 128174,
      "name": "Workshop and DC Coffee Break",
      "isParallelPresentation": false,
      "importedId": "e67fbcf6-44be-47c7-88cd-8a209124d909",
      "typeId": 13092,
      "roomId": 11346,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1698574500000,
      "endDate": 1698576300000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 128177,
      "name": "Workshop and DC Coffee Break",
      "isParallelPresentation": false,
      "importedId": "3445bd85-894e-44a4-918e-faa0615e6f47",
      "typeId": 13092,
      "roomId": 11346,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1698589800000,
      "endDate": 1698591600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 128178,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "108de4b1-70f1-46f4-b070-e316e7957a37",
      "typeId": 13092,
      "roomId": 11357,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1698661800000,
      "endDate": 1698663600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 128179,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "5b9cc6d1-fd40-4e80-a01c-454f327834d5",
      "typeId": 13092,
      "roomId": 11357,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1698853800000,
      "endDate": 1698855600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 129678,
      "name": "Transport to and from Exploratorium",
      "isParallelPresentation": false,
      "importedId": "5f86ef3e-d6a9-4bdc-939e-9e88a12e7ea4",
      "typeId": 13092,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1698777000000,
      "endDate": 1698789600000,
      "location": "Exploratorium",
      "coordinates": "37.80170550267322, -122.39735324920659",
      "link": {
        "href": "https://maps.app.goo.gl/hCPFtTnYqpstnVHG8",
        "label": "Google Map Location"
      },
      "description": "Transport to and from Exploratorium",
      "presenterIds": [],
      "source": "SYS"
    }
  ],
  "contents": [
    {
      "id": 126645,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Double-Sided Tactile Interactions for Grasping in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606798"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8177",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127752
      ],
      "eventIds": [],
      "abstract": "For grasping, tactile stimuli to multiple fingertips are crucial for realistic shape rendering and precise manipulation. Pinching is particularly important in virtual reality since it is frequently used to grasp virtual objects. However, the interaction space of tactile feedback around pinching is underexplored due to a lack of means to provide co-located but different stimulation to finger pads. We propose a double-sided electrotactile device with a thin and flexible form factor to fit within pinched fingerpads, comprising two overlapping 3 √ó 3 electrode arrays. Using this new tactile interface, we define a new concept of double-sided tactile interactions with three feedback modes: (1) single-sided stimulation, (2) simultaneous double-sided stimulation, and (3) spatiotemporal double-sided stimulation. Through two user studies, we (1) demonstrate that participants can accurately discriminate between single-sided and double-sided stimulation and find a qualitative difference in tactile sensation; and (2) confirm the occurrence of apparent tactile motion between fingers and present optimal parameters for continuous or discrete movements. Based on these findings, we demonstrate five VR applications to exemplify how double-sided tactile interactions can produce spatiotemporal movement of a virtual object between fingers and enrich touch feedback for UI operation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University",
              "dsl": ""
            }
          ],
          "personId": 126006
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 125866
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 125896
        }
      ]
    },
    {
      "id": 126646,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "AirCharge: Amplifying Ungrounded Impact Force by Accumulating Air Propulsion Momentum",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606768"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1890",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127752
      ],
      "eventIds": [],
      "abstract": "Impact events, which generate directional forces with extremely short impulse durations and large force magnitudes, are prevalent in both virtual reality (VR) games and real-world experiences. However, despite recent advancement in ungrounded force feedback technologies, such as air jet propulsion and propellers, these technologies remain 5-100x weaker and 10-500x slower compared to real-world impact events. For instance, they can only achieve 4ùëÅ with a minimal duration of 50-500ùëöùë† compared to the 20-400ùëÅ forces generated within 1-5ùëöùë† for baseball, ping-pong, drumming, and tennis. To overcome these limitations, we present AirCharge, a novel haptic device that accumulates air propulsion momentum to generate instantaneous, directional impact forces. By mounting compressed air jets on rotating swingarms, AirCharge can amplify impact force magnitude by more than 10x while matching real-world impulse duration of 3ùëöùë†. To support high-frequency impacts, we explored and evaluated a series of device designs, culminating in a novel reciprocating dual-swingarm design that leverages a reversing bevel gearbox to eliminate gyro effects and to achieve impact feedback of up to 10ùêªùëß. User experience evaluation (n = 16) showed that AirCharge significantly enhanced realism and is preferred by participants compared to air jets without the charging mechanism.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 125758
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 126402
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 126272
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 126034
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 126218
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 125771
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 125868
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 125881
        }
      ]
    },
    {
      "id": 126647,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "FFL: A Language and Live Runtime for Styling and Labeling Typeset Math Formulas",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606731"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5576",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127766
      ],
      "eventIds": [],
      "abstract": "As interest grows in learning math concepts in fields like data science and machine learning, it is becoming more important to help broad audiences engage with math notation. In this paper, we explore how authoring tools can help authors better style and label formulas to support their readability. We introduce a markup language for augmenting formulas called FFL, or \"Formula Formatting Language,\" which aims to lower the threshold to stylize and diagram formulas. The language is designed to be concise, writable, readable, and integrable into web-based document authoring environments. It was developed with an accompanying runtime that supports live application of augmentations to formulas. Our lab study shows that FFL improves the speed and ease of editing augmentation markup, and the readability of augmentation markup compared to baseline LaTeX tools. These results clarify the role tooling can play in supporting the explanation of math notation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Department of Computer and Information Science"
            }
          ],
          "personId": 125759
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Department of Computer and Information Science"
            }
          ],
          "personId": 125814
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Department of Computer and Information Science"
            }
          ],
          "personId": 126616
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Department of Computer and Information Science"
            }
          ],
          "personId": 126286
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Department of Computer and Information Science"
            }
          ],
          "personId": 126477
        }
      ]
    },
    {
      "id": 126650,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Skinergy: Machine-Embroidered Silicone-Textile Composites as On-Skin Self-Powered Input Sensors",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606729"
        },
        "Preview": {
          "title": "Skinergy: Machine-Embroidered Silicone-Textile Composites as On-Skin Self-Powered Input Sensors",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=OAc6P487-H0"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4374",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127756
      ],
      "eventIds": [],
      "abstract": "We propose Skinergy for self-powered on-skin input sensing, a step towards prolonged on-skin device usage. In contrast to prior on-skin gesture interaction sensors, Skinergy's sensor operation does not require external power. Enabled by the triboelectric nanogenerator (TENG) phenomenon, the machine-embroidered silicone-textile composite sensor converts mechanical energy from the input interaction into electrical energy. Our proof-of-concept untethered sensing system measures the voltages of generated electrical signals which are then processed for a diverse set of sensing tasks: discrete touch detection, multi-contact detection, contact localization, and gesture recognition. Skinergy is fabricated with off-the-shelf materials. The aesthetic and functional designs can be easily customized and digitally fabricated. We characterize Skinergy and conduct a 10-participant user study to (1) evaluate its gesture recognition performance and (2) probe user perceptions and potential applications. Skinergy achieves 92.8% accuracy for an 11-class gesture recognition task. Our findings reveal that human factors (e.g., individual differences in skin properties, and aesthetic preferences) are key considerations in designing self-powered on-skin sensors for human inputs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Hybrid Body Lab"
            }
          ],
          "personId": 125938
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Department of Human Centered Design"
            }
          ],
          "personId": 126076
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Hybrid Body Lab"
            }
          ],
          "personId": 126208
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Cornell University"
            }
          ],
          "personId": 126431
        }
      ]
    },
    {
      "id": 126651,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606756"
        },
        "Preview": {
          "title": "Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qfMpUX3MqJ0"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7877",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        126889
      ],
      "eventIds": [],
      "abstract": "People are increasingly turning to large language models (LLMs) for complex information tasks like academic research or planning a move to another city. However, while they often require working in a nonlinear manner --- e.g., to arrange information spatially to organize and make sense of it, current interfaces for interacting with LLMs are generally linear to support conversational interaction. To address this limitation and explore how we can support LLM-powered exploration and sensemaking, we developed Sensecape, an interactive system designed to support complex information tasks with an LLM by enabling users to (1) manage the complexity of information through multilevel abstraction and (2) switch seamlessly between foraging and sensemaking. Our within-subject user study reveals that Sensecape empowers users to explore more topics and structure their knowledge hierarchically, thanks to the externalization of levels of abstraction. We contribute implications for LLM-based workflows and interfaces for information tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126040
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 126519
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California",
              "dsl": "Design Lab"
            }
          ],
          "personId": 126638
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126359
        }
      ]
    },
    {
      "id": 126652,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Dynamic Toolchains: Software Infrastructure for Digital Fabrication Workflows",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606802"
        },
        "Preview": {
          "title": "Dynamic Toolchains: Software Infrastructure for Digital Fabrication Workflows",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=sG0b-GUexWc"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5580",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127754
      ],
      "eventIds": [],
      "abstract": "New digital fabrication workflows require both software development and digital/physical material exploration. To support digital fabrication workflow development, we contribute infrastructure that prioritizes extensibility and iteration. Dynamic Toolchains are dataflow programs with event-driven feedback between interactive, stateful modules. We contribute a browser-based dataflow environment for running Dynamic Toolchains, a library of fabrication-oriented front- and back-end modules for design and machine control, and a development framework for building custom modules. Furthermore, we show how our infrastructure supports unconventional fabrication workflows with demonstrations that include interactive watercolor painting, map plotting, machine knitting, audio embroidery, textured 3d printing, and computer-controlled milling. These demonstrations show how our infrastructure supports multiple kinds of engagement including reuse, remix, and extension. Finally, we discuss how this work contributes to broader conversations in HCI on creativity across the digital/physical divide.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Human Centered Design and Engineering"
            }
          ],
          "personId": 126170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 126615
        }
      ]
    },
    {
      "id": 126653,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "PoseVEC: Authoring Adaptive Pose-aware Effects Using Visual Programming and Demonstrations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606788"
        },
        "Preview": {
          "title": "PoseVEC: Authoring Adaptive Pose-aware Effects Using Visual Programming and Demonstrations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=6_miO0YlRVM"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6308",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127767
      ],
      "eventIds": [],
      "abstract": "Pose-aware visual effects where graphics assets and animations are rendered reactively to the human pose have become increasingly popular, appearing on mobile devices, the web, or even head-mounted displays like AR glasses. Yet, creating such effects still remains difficult for novices. In a traditional video editing workflow, a creator could utilize keyframes to create expressive but non-adaptive results which cannot be reused for other videos. Alternatively, programming-based approaches allow users to develop interactive effects, but are cumbersome for users to quickly express their creative intents. In this work, we propose a lightweight visual programming workflow for authoring adaptive and expressive pose effects. By combining a programming by demonstration paradigm with visual programming, we simplify three key tasks in the authoring process: creating pose triggers, designing animation parameters, and rendering. We evaluated our system with a qualitative user study and a replicated example study, finding that all participants can create effects efficiently. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 125755
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 126028
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 125891
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126561
        }
      ]
    },
    {
      "id": 126662,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Front Row: Automatically Generating Immersive Audio Representations of Tennis Broadcasts for Blind Viewers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606830"
        },
        "Preview": {
          "title": "Front Row: Automatically Generating Immersive Audio Representations of Tennis Broadcasts for Blind Viewers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_Y5jS78t6Lg"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3829",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127757
      ],
      "eventIds": [],
      "abstract": "Blind and low-vision (BLV) people face challenges watching sports due to the lack of accessibility of sports broadcasts. Currently, BLV people rely on descriptions from TV commentators, radio announcers, or their friends to understand the game. These descriptions, however, do not allow BLV viewers to visualize the action by themselves. We present Front Row, a system that automatically generates an immersive audio representation of sports broadcasts, specifically tennis, allowing BLV viewers to more directly perceive what is happening in the game. Front Row first recognizes gameplay from the video feed using computer vision, then renders players‚Äô positions and shots via spatialized (3D) audio cues. User evaluations with 12 BLV participants show that Front Row gives BLV viewers a more accurate understanding of the game compared to TV and radio, enabling viewers to form their own opinions on players' moods and strategies. We discuss future implications of Front Row and illustrate several applications, including a Front Row plug-in for video streaming platforms to enable BLV people to visualize the action in sports videos across the Web.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 126264
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University ",
              "dsl": "Computer Science - School of Engineering and Applied Sciences - Computer-Enabled Abilities Laboratory"
            }
          ],
          "personId": 126348
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Hunter College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126160
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Claremont",
              "institution": "Pomona College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126569
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 125950
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Albany",
              "institution": "SUNY At Albany",
              "dsl": ""
            }
          ],
          "personId": 126119
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 126559
        }
      ]
    },
    {
      "id": 126664,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "3D Printing Magnetophoretic Displays",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606804"
        },
        "Preview": {
          "title": "3D Printing Magnetophoretic Displays",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cpRst51JOWE"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2732",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127760
      ],
      "eventIds": [],
      "abstract": "We present a pipeline for printing interactive and always-on magnetophoretic displays using affordable FDM 3D printers. Using our pipeline, an end-user can convert the surface of a 3D shape into a matrix of voxels. The generated model can be sent to an FDM 3D printer equipped with an additional syringe-based injector. During the printing process, an oil and iron powder-based liquid mixture is injected into each voxel cell, allowing the appearance of the once-printed object to be editable with external magnetic sources. To achieve this, we conducted modifications to the 3D printer hardware and the firmware. We also implemented a 3D editor to prepare printable models. We demonstrate our pipeline with a variety of examples, including a printed Stanford bunny with customizable appearances, a small espresso mug that can be used as a post-it note surface, a board game figurine with a computationally updated display, and a collection of flexible wearable accessories with editable visuals.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126059
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 125803
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Graphics Technology"
            }
          ],
          "personId": 126290
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126094
        }
      ]
    },
    {
      "id": 126665,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Living Papers: A Language Toolkit for Augmented Scholarly Communication",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606791"
        },
        "Preview": {
          "title": "Living Papers: A Language Toolkit for Augmented Scholarly Communication",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YoCDW5FZePM"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3943",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127758
      ],
      "eventIds": [],
      "abstract": "Computing technology has deeply shaped how academic articles are written and produced, yet article formats and affordances have changed little over centuries. The status quo consists of digital files optimized for printed paper‚Äîill-suited to interactive reading aids, accessibility, dynamic figures, or easy information extraction and reuse. Guided by formative discussions with scholarly communication researchers and publishing tool developers, we present Living Papers, a language toolkit for producing augmented academic articles that span print, interactive, and computational media. Living Papers articles may include formatted text, references, executable code, and interactive components. Articles are parsed into a standardized document format from which a variety of outputs are generated, including static PDFs, dynamic web pages, and extraction APIs for paper content and metadata. We describe Living Papers' architecture, document model, and reactive runtime, and detail key aspects such as citation processing and conversion of interactive components to static content. We demonstrate the use and extension of Living Papers through examples spanning traditional research papers, explorable explanations, information extraction, and reading aids such as enhanced citations, cross-references, and equations. Living Papers is available as an extensible, open source platform intended to support both article authors and researchers of augmented reading and writing experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125992
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 126422
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126463
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126147
        }
      ]
    },
    {
      "id": 126666,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "RadarFoot: Fine-grain Ground Surface Context Awareness for Smart Shoes",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606738"
        },
        "Preview": {
          "title": "RadarFoot: Fine-grain Ground Surface Context Awareness for Smart Shoes",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YH_vFjdXE8Q"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9375",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127765
      ],
      "eventIds": [],
      "abstract": "Everyday, billions of people use footwear for walking, running, or exercise. Of emerging interest are ``smart footwear'', which help users track gait, count steps or even analyse performance. However, such nascent footwear lack fine-grain ground surface context awareness, which could allow them to adapt to the conditions and create usable functions and experiences. Hence, this research aims to recognize the walking surface using a radar sensor embedded in a shoe, enabling ground context-awareness. Using data collected from 23 participants from an in-the-wild setting, we developed several classification models. We show that our model can detect five common terrain types with an accuracy of 80.0\\% and further ten terrain types with an accuracy of 66.3\\%, while moving. Importantly, it can detect the gait motion types such as `walking', `stepping up', `stepping down', `still', with an accuracy of 90\\%. Finally, we present potential use cases and insights for future work based on such ground-aware smart shoes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Department of Human Centred Computing"
            }
          ],
          "personId": 125948
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "Computer Science & Engineering/UNSW/HCI BoDi Lab"
            }
          ],
          "personId": 126225
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": ""
            }
          ],
          "personId": 125799
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO‚Äôs Data61 ",
              "dsl": "Science Director and Deputy Director"
            }
          ],
          "personId": 125982
        }
      ]
    },
    {
      "id": 126667,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "CurveCrafter: A System for Animated Curve Manipulation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606792"
        },
        "Preview": {
          "title": "CurveCrafter: A System for Animated Curve Manipulation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=f4kg2mxLHGU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4357",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127759
      ],
      "eventIds": [],
      "abstract": "Linework on 3D animated characters is an important aspect of stylized looks for films. We present CurveCrafter, a system allowing animators to create new lines on 3D models and to edit the shape and opacity of silhouette curves. Our tools allow users to draw, redraw, erase, edit and retime user created curves. Silhouette curves can have their shape edited or reverted, and their opacity erased or revealed. Our algorithm for propagating edits over tracked silhouette curves ensures temporal consistency even as curves expand and merge. Five professional animators used our system to animate lines on three shots with different characters. Additionally, the effects lead from the short film \"Pete\" used our system to more easily recreate edits on a film shot. CurveCrafter was able to successfully enhance the resulting animations with additional linework.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Emeryville",
              "institution": "Pixar Animation Studios",
              "dsl": ""
            }
          ],
          "personId": 126553
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Emeryville",
              "institution": "Pixar Animation Studios",
              "dsl": ""
            }
          ],
          "personId": 126162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Emeryville",
              "institution": "Pixar Animation Studios",
              "dsl": ""
            }
          ],
          "personId": 125749
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": ""
            }
          ],
          "personId": 125752
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Emeryville",
              "institution": "Pixar Animation Studios",
              "dsl": ""
            }
          ],
          "personId": 125879
        }
      ]
    },
    {
      "id": 126668,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "WorldSmith: A Multi-Modal Image Synthesis Tool for Fictional World Building",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606772"
        },
        "Preview": {
          "title": "WorldSmith: A Multi-Modal Image Synthesis Tool for Fictional World Building",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=U1iF6GVbHL4"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7079",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127761
      ],
      "eventIds": [],
      "abstract": "Crafting a rich and unique environment is crucial for fictional world-building, but can be difficult to achieve since illustrating a world from scratch requires time and significant skill. We investigate the use of recent multi-modal image generation systems to enable users iteratively visualize and modify elements of their fictional world using a combination of text input, sketching, and region-based filling. WorldSmith enables novice world builders to quickly visualize a fictional world with layered edits and hierarchical compositions. Through a formative study (4 participants) and first-use study (13 participants) we demonstrate that WorldSmith offers more expressive interactions with prompt-based models. With this work, we explore how creatives can be empowered to leverage prompt-based generative AI as a tool in their creative process, beyond current \"click-once\" prompting UI paradigms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "HCI+AI"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 126140
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 126549
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 126520
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 126545
        }
      ]
    },
    {
      "id": 126669,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Starrypia: An AR Gamified Music Adjuvant Treatment Application for Children with Autism Based on Combined Therapy",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606755"
        },
        "Preview": {
          "title": "Starrypia: An AR Gamified Music Adjuvant Treatment Application for Children with Autism Based on Combined Therapy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xY7oq3a1H-8"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5445",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127757
      ],
      "eventIds": [],
      "abstract": "In this paper, we present Starrypia, a lightweight gamified music adjuvant treatment application to improve the symptoms of mild autistic children, eliminating the geographical and time constraints faced by traditional treatment. Adopting ABA (Applied Behavior Analysis) behavioral theory as the principle, Starrypia follows the stimulus-response-reinforcement-pause process and incorporates music therapy and sensory integration. Based on AR, Starrypia provides multi-sensory intervention through music generated by BiLSTM deep model, 3D visual scenes, touch interaction to keep children focused and calm. We conducted a controlled experiment on 20 children to test Starrypia‚Äôs effectiveness and attraction. Children‚Äôs pre-test and post-test scores on two autism rating scales and performance during the test were applied to measure their abilities and engagement. Experimental results indicated that children showed great interest in Starrypia and presented evident symptom remission and advance in overall abilities after 4 weeks of use. In conclusion, Starrypia is practicable in both therapeutic effect and user experience, and conspicuously instrumental in promoting sensory ability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiaotong University",
              "dsl": "School of Design"
            }
          ],
          "personId": 126109
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiaotong University",
              "dsl": "School of Design"
            }
          ],
          "personId": 126279
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "East China University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 126501
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "School of Design"
            }
          ],
          "personId": 126037
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "School of Design"
            }
          ],
          "personId": 126122
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 126375
        }
      ]
    },
    {
      "id": 126670,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "GenAssist: Making Image Generation Accessible",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606735"
        },
        "Preview": {
          "title": "GenAssist: Making Image Generation Accessible",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PFh2C4sFrvM"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4355",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127757
      ],
      "eventIds": [],
      "abstract": "Blind and low vision (BLV) creators use images to communicate with sighted audiences. However, creating or retrieving images is challenging for BLV creators as it is difficult to use authoring tools or assess image search results. Thus, creators limit the types of images they create or recruit sighted collaborators. \r\nWhile text-to-image generation models let creators generate high-fidelity images based on a text description (i.e. prompt), it is difficult to assess the content and quality of generated images. We present GenAssist, a system to make text-to-image generation accessible. Using our interface, creators can verify whether generated image candidates followed the prompt, access additional details in the image not specified in the prompt, and skim a summary of similarities and differences between image candidates. \r\nTo power the interface, GenAssist uses a large language model to generate visual questions, vision-language models to extract answers, and a large language model to summarize the results. \r\nOur study with 12 BLV creators demonstrated that GenAssist enables and simplifies the process of image selection and generation, making visual authoring more accessible to all.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas, Austin",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 125987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126328
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas, Austin",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126414
        }
      ]
    },
    {
      "id": 126671,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "SmartPoser: Arm Pose Estimation With a Smartphone and Smartwatch Using UWB and IMU Data",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606821"
        },
        "Preview": {
          "title": "SmartPoser: Arm Pose Estimation With a Smartphone and Smartwatch Using UWB and IMU Data",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GCcSgbvk_7s"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5564",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127764
      ],
      "eventIds": [],
      "abstract": "The ability to track a user's arm pose could be valuable in a wide range of applications, including fitness, rehabilitation, augmented reality input, life logging, and context-aware assistants. Unfortunately, this capability is not readily available to consumers. Systems either require cameras, which carry privacy issues, or utilize multiple worn IMUs or markers. In this work, we describe how an off-the-shelf smartphone and smartwatch can work together to accurately estimate arm pose. Moving beyond prior work, we take advantage of more recent ultra-wideband (UWB) functionality on these devices to capture absolute distance between the two devices. This measurement is the perfect complement to inertial data, which is relative and suffers from drift. We quantify the performance of our software-only approach using off-the-shelf devices, showing it can estimate the wrist and elbow joints with a median positional error of 11.0~cm, without the user having to provide training data. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126171
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126353
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126334
        }
      ]
    },
    {
      "id": 126672,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "MagKnitic: Machine-knitted Passive and Interactive Haptics Textiles with Integrated Binary Sensing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606765"
        },
        "Preview": {
          "title": "MagKnitic: Machine-knitted Passive and Interactive Haptics Textiles with Integrated Binary Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qe95Ix-v8WU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7500",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127762
      ],
      "eventIds": [],
      "abstract": "In this paper, we introduce \\textit{MagKnitic}, a novel approach to integrate passive force feedback and binary sensing into fabrics via digital machine knitting. Our approach utilizes digital fabrication technology to enable haptic interfaces that are soft, flexible, lightweight, and conform to the user's body shape. Despite these characteristics, our interfaces provide diverse, interactive, and responsive force feedback, expanding the design space for haptic experiences. \\textit{MagKnitic} provides scalable and customizable passive haptic sensations by utilizing the attractive force between ferromagnetic yarns and permanent magnets, both of which are seamlessly integrated into knitted fabrics. Moreover, we present a binary sensing capability based on the resistance drop resulting from the activated electrical path between the integrated magnets and ferromagnetic yarn upon direct contact. We offer parametric design templates for users to customize \\textit{MagKnitic} layouts and patterns. With various design layouts and combinations, \\textit{MagKnitic} supports passive haptics interactions of linear, polar, angular, planar, radial, and user-defined motions. We perform a technical evaluation of the passive force feedback and the binary sensing capabilities with different machine knitting layouts and patterns, embedded magnet sizes, and interaction distances. In addition, we conduct two user studies to validate the effectiveness of \\textit{MagKnitic}. Finally, we demonstrate various application scenarios, including wearable input interfaces, game controllers, passive VR/AR wearables, and interactive furniture coverings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology (MIT)",
              "dsl": "Computer Science and Artificial Intelligence Laboratory (CSAIL)"
            }
          ],
          "personId": 125787
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126128
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "los angeles",
              "institution": "Tencent",
              "dsl": "Lightspeed & Quantum studios"
            }
          ],
          "personId": 126248
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 126630
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126305
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 126072
        }
      ]
    },
    {
      "id": 126673,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Taste Retargeting via Chemical Taste Modulators",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606818"
        },
        "Preview": {
          "title": "Taste Retargeting via Chemical Taste Modulators",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=J29c6QmVaGQ"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3270",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127768
      ],
      "eventIds": [],
      "abstract": "Prior research has explored modifying taste through electrical stimulation. While promising, such interfaces often only elicit taste changes while in contact with the user‚Äôs tongue (e.g., cutlery with electrodes), making them incompatible with eating and swallowing real foods. Moreover, most interfaces cannot selectively alter basic tastes, but only the entire flavor profile (e.g., cannot selectively alter bitterness). To tackle this, we propose taste retargeting, a method of altering taste perception by delivering chemical modulators to the mouth before eating. These modulators temporarily change the response of taste receptors to foods, selectively suppressing or altering basic tastes. Our first study identified six accessible taste modulators that suppress salty, umami, sweet, or bitter and transform sour into sweet. Using these findings, we demonstrated an interactive application of this technique with the example of virtual reality, which we validated in our second study. We found that taste retargeting reduced the flavor mismatch between a food prop and other virtual foods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125782
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        }
      ]
    },
    {
      "id": 126674,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "FeetThrough: Electrotactile Foot Interface that Preserves Real-World Sensations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606808"
        },
        "Preview": {
          "title": "FeetThrough: Electrotactile Foot Interface that Preserves Real-World Sensations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=rKwKWXOPFbs"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2181",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127752
      ],
      "eventIds": [],
      "abstract": "Haptic interfaces have been extended to the feet to enhance foot-based activities, such as guidance while walking or stepping on virtual textures. Most feet haptics use mechanical actuators, namely vibration motors. However, we argue that vibration motors are not the ideal actuators for all feet haptics. Instead, we demonstrate that electrotactile stimulation provides qualities that make it a powerful feet-haptic interface: (1) Users wearing electrotactile can not only feel the stimulation but can also better feel the terrain under their feet‚Äîthis is critical as our feet are also responsible for the balance on uneven terrains and stairs‚Äîelectrotactile achieves this improved ‚Äúfeel-through‚Äù effect because it is thinner than vibrotactile actuators, at 0.1 mm in our prototype; (2) While a single vibrotactile actuator will also vibrate surrounding skin areas, we found improved two-point discrimination thresholds for electrotactile; (3) Electrotactile can be applied directly to soles, insoles or socks, enabling new applications such as barefoot interactive experiences or without requiring users to have custom-shoes with built-in vibration motors. Finally, we demonstrate applications in which electrotactile feet interfaces allow users to feel not only virtual information but also the real terrain under their shoes, such as a VR experience where users walk on ground props and a tactile navigation system that augments the ground with virtual tactile paving to assist pedestrians in low-vision situations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126309
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        }
      ]
    },
    {
      "id": 126681,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "BioWeave: Weaving Thread-Based Sweat-Sensing On-Skin Interfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606769"
        },
        "Preview": {
          "title": "BioWeave: Weaving Thread-Based Sweat-Sensing On-Skin Interfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cY1qj6iThak"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1517",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127756
      ],
      "eventIds": [],
      "abstract": "There has been a growing interest in developing and fabricating wearable sweat sensors in recent years, as sweat contains various analytes that can provide non-invasive indications of various conditions in the body. Although recent HCI research has been looking into wearable sensors for understanding health conditions, textile-based wearable sweat sensors remain underexplored. We present BioWeave, a woven thread-based sweat-sensing on-skin interface. Through weaving single-layer and multi-layer structures, we combine sweat-sensing threads with versatile fiber materials. We identified a design space consisting of colorimetric and electrochemical sensing approaches, targeting biomarkers including pH, glucose, and electrolytes. We explored 2D and 3D weaving structures for underexplored body locations to seamlessly integrate sweat-sensing thread into soft wearable interfaces. We developed five example applications to demonstrate the design capability offered. The BioWeave sensing interface can provide seamless integration into everyday textile-based wearables and offers the unobtrusive analysis of health conditions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 126017
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 125980
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 126515
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 126381
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 126386
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Cornell University"
            }
          ],
          "personId": 126431
        }
      ]
    },
    {
      "id": 126682,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Reframe: An Augmented Reality Storyboarding Tool for Character-Driven Analysis of Security & Privacy Concerns",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606750"
        },
        "Preview": {
          "title": "Reframe: An Augmented Reality Storyboarding Tool for Character-Driven Analysis of Security & Privacy Concerns",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=TFdgx44l6FE"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1758",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127770
      ],
      "eventIds": [],
      "abstract": "While current augmented reality (AR) authoring tools lower the technical barrier for novice AR designers, they lack explicit guidance to consider potentially harmful aspects of AR with respect to security & privacy (S&P).\r\nTo address potential threats in the earliest stages of AR design, we developed Reframe, a digital storyboarding tool for designers with no formal training to analyze S&P threats.\r\nWe accomplish this through a frame-based authoring approach, which captures and enhances storyboard elements that are relevant for threat modeling, and character-driven analysis tools, which personify S&P threats from an underlying threat model to provide simple abstractions for novice designers.\r\nBased on evaluations with novice AR designers and S&P experts, we find that Reframe enables designers to analyze threats and propose mitigation techniques that experts consider good quality.\r\nWe discuss how Reframe can facilitate collaboration between designers and S\\&P professionals and propose extensions to Reframe to incorporate additional threat models. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 126404
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126408
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 126026
        }
      ]
    },
    {
      "id": 126683,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606779"
        },
        "Preview": {
          "title": "Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ZotZpB9KQS0"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1999",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127765
      ],
      "eventIds": [],
      "abstract": "Imagine being able to listen to the birds chirping in a park without hearing the chatter from other hikers, or being able to block out traffic noise on a busy street while still being able to hear emergency sirens and car honks. We introduce semantic hearing, a novel capability for hearable devices that enables them to, in real-time, focus on, or ignore, specific sounds from real-world environments, while also preserving the spatial cues. To achieve this, we make two technical contributions: 1)  we present the  first neural network that can achieve binaural target sound extraction in the presence of interfering sounds and background noise, and 2) we  design a training methodology that allows our system to generalize to real-world use. Results show that our system can operate with 20 sound classes and that our transformer-based network has a runtime of 6.56 ms on a connected smartphone. In-the-wild evaluation with participants in previously unseen indoor and outdoor scenarios shows  that our proof-of-concept system can extract the target  sounds and  generalize  to preserve the spatial cues in its binaural output. \r\n\r\nProject page with code: https://semantichearing.cs.washington.edu\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "SEATTLE",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen Center for Computer Science & Engineering"
            }
          ],
          "personId": 126584
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen Center for Computer Science and Engineering"
            }
          ],
          "personId": 126634
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 126556
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": "Audio and AI"
            }
          ],
          "personId": 126540
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "university of Washington",
              "dsl": ""
            }
          ],
          "personId": 125999
        }
      ]
    },
    {
      "id": 126684,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "An Adaptable Workflow for Manual-Computational Ceramic Surface Ornamentation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606726"
        },
        "Preview": {
          "title": "An Adaptable Workflow for Manual-Computational Ceramic Surface Ornamentation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=X9TcO9LeSu8"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8151",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127754
      ],
      "eventIds": [],
      "abstract": "Surface ornamentation is a rich component of ceramic manufacture wherein craftspeople use multiple methods to create intricate patterns on vessels. Computational fabrication can extend manual ceramic ornamentation through procedural pattern generation and automated fabrication; however, to be effective in traditional ceramics, computational fabrication systems must remain compatible with existing processes and materials. We contribute an interactive design workflow, CeramWrap, in which craftspeople can procedurally design and fabricate decorative patterned stencils tailored to radially symmetrical vessels. Our approach extends manual techniques through a workflow where craftspeople design and edit repetitive motifs directly on a 3D digital model of a vessel and then interactively adjust the unrolling of the 3D design to a 2D format suitable for digitally fabricating stencils and templates. Through a series of example artifacts, we demonstrate how our workflow generalizes across multiple vessel geometries, supports manual and digital clay fabrication, and is adaptable to different surface ornamentation methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 125777
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 126511
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Albuquerque",
              "institution": "University of New Mexico",
              "dsl": "Hand and Machine Lab"
            }
          ],
          "personId": 125910
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 125867
        }
      ]
    },
    {
      "id": 126685,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "PromptPaint: Steering Text-to-Image Generation Through Paint Medium-like Interactions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606777"
        },
        "Preview": {
          "title": "PromptPaint: Steering Text-to-Image Generation Through Paint Medium-like Interactions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=3SPRsm213_I"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9483",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        126889
      ],
      "eventIds": [],
      "abstract": "While diffusion-based text-to-image (T2I) models provide a simple and powerful way to generate images, guiding this generation remains a challenge. For concepts that are difficult to describe through language, users may struggle to create prompts. Moreover, many of these models are built as end-to-end systems, lacking support for iterative shaping of the image. In response, we introduce PromptPaint, which combines T2I generation with interactions that model how we use colored paints. PromptPaint allows users to go beyond language and mix prompts to express challenging concepts. Just as we iteratively tune colors by the layered placement of paint on a physical canvas, PromptPaint similarly allows users to apply different prompts to different parts of the generative process and canvas areas. Through a set of studies, we characterize different approaches for mixing prompts, design trade-offs, and technical and socio-technical challenges to the use of generative models. With PromptPaint we provide insight into future steerable generative tools. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "SpaceCraft Inc.",
              "dsl": ""
            }
          ],
          "personId": 125942
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 125900
        }
      ]
    },
    {
      "id": 126686,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Lorgnette: Creating Malleable Code Projections",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606817"
        },
        "Preview": {
          "title": "Lorgnette: Creating Malleable Code Projections",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VGNiF2bqTqw"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4227",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127763
      ],
      "eventIds": [],
      "abstract": "Projections of computer languages are tools that help users interact with representations that better fit their needs than plain text. We collected 62 projections from the literature and from a design workshop and found that 60% of them can be implemented using a table, a graph or a form. However, projections are often hardcoded for specific languages and situations, and in most cases only the developers of a code editor can create or adapt projections, leaving no room for appropriation by their users. We introduce Lorgnette, a new framework for letting programmers augment their code editor with projections. We demonstrate five examples that use Lorgnette to create projections that can be reused in new contexts. We discuss how this approach could help democratise projections and conclude with future work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Universit√© Paris-Saclay, CNRS, Inria",
              "dsl": "LISN"
            }
          ],
          "personId": 126261
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Universit√© Paris-Saclay, CNRS, Inria",
              "dsl": "LISN"
            }
          ],
          "personId": 126502
        }
      ]
    },
    {
      "id": 126687,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Joie: a Joy-based BCI",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606761"
        },
        "Preview": {
          "title": "Joie: a Joy-based BCI",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VmiD2eC7L2o"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6890",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127764
      ],
      "eventIds": [],
      "abstract": "The size and cost of electroencephalography (EEG) headsets have been decreasing at a steadfast pace. Cortical frontal activity is a promising input method that is also important for affect regulation. We created Joie, a joy-based EEG brain-computer interface (BCI) which uses prefrontal asymmetries associated with joyful thoughts as input to an endless runner video game. The more prefrontal asymmetries are activated, the more the character collects coins in response. In a lab study (20 participants, 15 training sessions per participant, up to two weeks of training), we found that our experiment group instructed to imagine positive music, winning awards, and similar strategies, demonstrated significantly greater ability in activating asymmetries compared to our placebo and control groups. In our analysis, Joie demonstrates the ability for frontal asymmetries to be used as input to an affective BCI and builds upon prior work in this area. In the future, training these asymmetries can teach mental strategies that have applications in mental health.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 125776
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 126197
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 126610
        }
      ]
    },
    {
      "id": 126688,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Odyssey: An Interactive Workbench for Expert-Driven Floating-Point Expression Rewriting",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606819"
        },
        "Preview": {
          "title": "Odyssey: An Interactive Workbench for Expert-Driven Floating-Point Expression Rewriting",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2uCkUodZ22A"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8951",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127763
      ],
      "eventIds": [],
      "abstract": "In recent years, researchers have proposed a number of automated tools to identify and improve floating-point rounding error in math- ematical expressions. However, users struggle to effectively apply these tools. In this paper, we work with novices, experts, and tool developers to investigate user needs during the expression rewriting process. We find that users follow an iterative design process. They want to compare expressions on multiple input ranges, integrate and guide various rewriting tools, and understand where errors come from. We organize this investigation‚Äôs results into a three- stage workflow and implement that workflow in a new, extensible workbench dubbed Odyssey. Odyssey enables users to: (1) diagnose problems in an expression, (2) generate solutions automatically or by hand, and (3) tune their results. Odyssey tracks a working set of expressions and turns a state-of-the-art automated tool ‚Äúinside out,‚Äù giving the user access to internal heuristics, algorithms, and functionality. In a user study, Odyssey enabled five expert numerical analysts to solve challenging rewriting problems where state-of- the-art automated tools fail. In particular, the experts unanimously praised Odyssey‚Äôs novel support for interactive range modification and local error visualization.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125898
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125872
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126606
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 125889
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125827
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Salt Lake City",
              "institution": "University of Utah",
              "dsl": "Kahlert School of Computing"
            }
          ],
          "personId": 126532
        }
      ]
    },
    {
      "id": 126689,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Exploring Locomotion Methods with Upright Redirected Views for VR Users in Reclining & Lying Positions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606714"
        },
        "Preview": {
          "title": "Exploring Locomotion Methods with Upright Redirected Views for VR Users in Reclining & Lying Positions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=64Yg25x9_Qs"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1078",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127765
      ],
      "eventIds": [],
      "abstract": "Using VR in reclining & lying positions is getting common for users, but upward views caused by posture have to be redirected to be parallel to the ground as when users are standing. This affects users' locomotion performances in VR due to potential physical restrictions, and the visual-vestibular-proprioceptive conflict. This paper is among the first to investigate the suited locomotion methods and how reclining & lying positions and redirection affect them in such conditions. A user-elicitation study was carried out to construct a set of locomotion methods based on users' preferences when they were in different reclining & lying positions. A second study developed user-preferred 'tapping' and 'chair rotating' gestures, by evaluating their performances at various body reclining angles, we measured the general impacts of posture and redirection. The results showed that these methods worked effectively, but exposed some shortcomings, and users performed worst at 45-degree reclining angles. Finally, four upgraded methods were designed and verified to improve the locomotion performances.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software",
              "dsl": "Chinese Academy of Sciences"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "College of Computer Science and Technology",
              "dsl": "University of Chinese Academy of Sciences"
            }
          ],
          "personId": 125784
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing University of Technology",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 126295
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "School of Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 126447
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Normal University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126276
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "College of Artificial Intelligence",
              "dsl": "Nanjing University of Information Science and Technology"
            }
          ],
          "personId": 125885
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 125813
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 126244
        }
      ]
    },
    {
      "id": 126691,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606716"
        },
        "Preview": {
          "title": "RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ACuuK1Pil2U"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4595",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127770
      ],
      "eventIds": [],
      "abstract": "We introduce RealityCanvas, a mobile AR sketching tool that can easily augment real-world physical motion with responsive hand-drawn animation. Recent research in AR sketching tools has enabled users to not only embed static drawings into the real world but also dynamically animate them with physical motion. However, existing tools often lack the flexibility and expressiveness of possible animations, as they primarily support simple line-based geometry. To address this limitation, we explore both expressive and improvisational AR sketched animation by introducing a set of responsive scribble animation techniques that can be directly embedded through sketching interactions: 1) object binding, 2) flip-book animation, 3) action trigger, 4) particle effects, 5) motion trajectory, and 6) contour highlight. These six animation effects were derived from the analysis of 172 existing video-edited scribble animations. We showcase these techniques through various applications, such as video creation, augmented education, storytelling, and AR prototyping. The results of our user study and expert interviews confirm that our tool can lower the barrier to creating AR-based sketched animation, while allowing creative, expressive, and improvisational AR sketching experiences. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 125962
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Delhi",
              "city": "New Delhi",
              "institution": "IIIT-Delhi",
              "dsl": "Weave Lab"
            },
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 125873
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 126379
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 125846
        }
      ]
    },
    {
      "id": 126692,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Use of an AI-powered Rewriting Support Software in Context with Other Tools: A Study of Non-Native English Speakers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606810"
        },
        "Preview": {
          "title": "Use of an AI-powered Rewriting Support Software in Context with Other Tools: A Study of Non-Native English Speakers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=XanS91KzXmI"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2175",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127758
      ],
      "eventIds": [],
      "abstract": "Academic writing in English can be challenging for non-native English speakers (NNESs). AI-powered rewriting tools can potentially improve NNESs' writing outcomes at a low cost. However, whether and how NNESs make valid assessments of the revisions provided by these algorithmic recommendations remains unclear. We report a study where NNESs leverage an AI-powered rewriting tool, Langsmith, to polish their drafted academic essays. We examined the participants' interactions with the tool via user studies and interviews. Our data reveal that most participants used Langsmith in combination with other tools, such as machine translation (MT), and those who used MT had different ways of understanding and evaluating Langsmith's suggestions than those who did not. Based on these findings, we assert that NNESs' quality assessment in AI-powered rewriting tools is influenced by the simultaneous use of multiple tools, offering valuable insights into the design of future rewriting tools for NNESs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Langsmith inc.",
              "dsl": ""
            }
          ],
          "personId": 126498
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Keihanna",
              "institution": "NTT",
              "dsl": ""
            }
          ],
          "personId": 126077
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Langsmith inc.",
              "dsl": ""
            }
          ],
          "personId": 126372
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Edge Intelligence Systems Inc.",
              "dsl": ""
            }
          ],
          "personId": 126343
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN",
              "dsl": ""
            }
          ],
          "personId": 126346
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "College of Information Studies"
            }
          ],
          "personId": 125833
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Keihanna",
              "institution": "NTT",
              "dsl": ""
            }
          ],
          "personId": 126534
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Graduate School of Information Sciences"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN",
              "dsl": "AIP Center"
            }
          ],
          "personId": 125979
        }
      ]
    },
    {
      "id": 126700,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "TouchType-GAN: Modeling Touch Typing with Generative Adversarial Network",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606760"
        },
        "Preview": {
          "title": "TouchType-GAN: Modeling Touch Typing with Generative Adversarial Network",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=niIwOzStabU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8380",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127755
      ],
      "eventIds": [],
      "abstract": "Models that can generate touch typing tasks are important to the\r\ndevelopment of touch typing keyboards. We propose TouchType-\r\nGAN, a Conditional Generative Adversarial Network that can sim-\r\nulate locations and time stamps of touch points in touch typing.\r\nTouchType-GAN takes arbitrary text as input to generate realistic\r\ntouch typing both spatially (i.e., (ùë•, ùë¶) coordinates of touch points)\r\nand temporally (i.e., timestamps of touch points). TouchType-GAN in-\r\ntroduces a variational generator that estimates Gaussian Distribu-\r\ntions for every target letter to prevent mode collapse. Our experi-\r\nments on a dataset with 3k typed sentences show that TouchType-\r\nGAN outperforms existing touch typing models, including the Ro-\r\ntational Dual Gaussian model for simulating the distribution\r\nof touch points, and the Finger-Fitts Euclidean Model for sim-\r\nulating typing time. Overall, our research demonstrates that the\r\nproposed GAN structure can learn the distribution of user typed\r\ntouch points, and the resulting TouchType-GAN can also estimate\r\ntyping movements. TouchType-GAN can serve as a valuable tool\r\nfor designing and evaluating touch typing input systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126331
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 126112
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126211
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 125775
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126142
        }
      ]
    },
    {
      "id": 126701,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Live, Rich, and Composable Programming with Engraft",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606733"
        },
        "Preview": {
          "title": "Live, Rich, and Composable Programming with Engraft",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=C5U1FXhDz9I"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2832",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127763
      ],
      "eventIds": [],
      "abstract": "Live & rich tools can support a diversity of domain-specific programming tasks, from visualization authoring to data wrangling. Real-world programming, however, requires performing multiple tasks in concert, calling for the use of multiple tools alongside conventional code. Programmers lack environments capable of composing live & rich tools to support these situations. To enable this composition, we contribute Engraft, a component-based API that allows live & rich tools to be embedded within larger environments like computational notebooks. Through recursive embedding of components, Engraft enables several new forms of composition: not only embedding tools inside environments, but also embedding environments within each other and embedding tools and environments in the outside world, including conventional codebases. We demonstrate Engraft with examples from diverse domains, including web-application development, command-line scripting, and physics education. By providing composability, Engraft can help cultivate a cycle of use and innovation in live & rich programming.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126147
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125992
        }
      ]
    },
    {
      "id": 126702,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Towards Flexible and Robust User Interface Adaptations With Multiple Objectives",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606799"
        },
        "Preview": {
          "title": "Towards Flexible and Robust User Interface Adaptations With Multiple Objectives",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YRFkphV1730"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8267",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127769
      ],
      "eventIds": [],
      "abstract": "This paper proposes a new approach for online UI adaptation that aims to overcome the limitations of the most commonly used UI optimization method involving multiple objectives: weighted sum optimization. Weighted sums are highly sensitive to objective formulation, limiting the effectiveness of UI adaptations. We propose ParetoAdapt, an adaptation approach that uses online multi-objective optimization with a posteriori articulated preferences---that is, articulation of preferences after the optimization has concluded to make UI adaptation robust to incomplete and inaccurate objective formulations. It offers users a flexible way to control adaptations by selecting from a set of Pareto optimal adaptation proposals and adjusting them to fit their needs. We showcase the feasibility and flexibility of ParetoAdapt by implementing an online layout adaptation system in a state-of-the-art 3D UI adaptation framework. We further evaluate its robustness and run-time in simulation-based experiments that allow us to systematically change the accuracy of the estimated user preferences. We conclude by discussing how our approach may impact the usability and practicality of online UI adaptations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126090
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126368
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": "Computational Interaction Lab"
            }
          ],
          "personId": 126125
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126600
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 125899
        }
      ]
    },
    {
      "id": 126703,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606723"
        },
        "Preview": {
          "title": "Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=rB6dJUELx8E"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9356",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127754
      ],
      "eventIds": [],
      "abstract": "With recent advances in Generative AI, it is becoming easier to automatically manipulate 3D models. However, current methods tend to apply edits to models globally, which risks compromising the intended functionality of the 3D model when fabricated in the physical world. For example, modifying functional segments in 3D models, such as the base of a vase, could break the original functionality of the model, thus causing the vase to fall over. We introduce a method for automatically segmenting 3D models into functional and aesthetic elements. This method allows users to selectively modify aesthetic segments of 3D models, without affecting the functional segments. To develop this method we first create a taxonomy of functionality in 3D models by qualitatively analyzing 1000 models sourced from a popular 3D printing repository, Thingiverse. With this taxonomy, we develop a semi-automatic classification method to decompose 3D models into functional and aesthetic elements. We propose a system called Style2Fab that allows users to selectively stylize 3D models without compromising their functionality. We evaluate the effectiveness of our classification method compared to human-annotated data, and demonstrate the utility of Style2Fab with a user study to show that functionality-aware segmentation helps preserve model functionality.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126527
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge ",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126146
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126215
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT ",
              "dsl": "Center for Bits and Atoms"
            }
          ],
          "personId": 126550
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126023
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 125802
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 125923
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Khoury College of Computer Science"
            }
          ],
          "personId": 125788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126305
        }
      ]
    },
    {
      "id": 126704,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "BrightMarker: 3D Printed Fluorescent Markers for Object Tracking",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606758"
        },
        "Preview": {
          "title": "BrightMarker: 3D Printed Fluorescent Markers for Object Tracking",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YfHQmuIcTS4"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2035",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127760
      ],
      "eventIds": [],
      "abstract": "Existing invisible object tagging methods are prone to low resolution, which impedes tracking performance. We present BrightMarker, a fabrication method that uses fluorescent filaments to embed easily trackable markers in 3D printed color objects. By using an infrared-fluorescent filament that \"shifts\" the wavelength of the incident light, our optical detection setup filters out all the noise to only have the markers present in the infrared camera image. The high contrast of the markers allows us to track them robustly regardless of the moving objects‚Äô surface color.\r\n\r\nWe built a software interface for automatically embedding these markers for the input object geometry, and hardware modules that can be attached to existing mobile devices and AR/VR headsets. Our image processing pipeline robustly localizes the markers in real-time from the captured images.\r\n\r\nBrightMarker can be used in a variety of applications, such as custom fabricated wearables for motion capture, tangible interfaces for AR/VR, rapid product tracking, and privacy-preserving night vision. BrightMarker exceeds the detection rate of state-of-the-art invisible marking, and even small markers (1\"x1\") can be tracked at distances exceeding 2m.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 125841
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            },
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Leganes",
              "institution": "University Carlos III of Madrid",
              "dsl": ""
            }
          ],
          "personId": 126570
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126097
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126474
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 125921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126130
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Leganes",
              "institution": "Universidad Carlos III de Madrid",
              "dsl": "University Group for Identification Technologies"
            }
          ],
          "personId": 126489
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126305
        }
      ]
    },
    {
      "id": 126706,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "WRLKit: Computational Design of Personalized Wearable Robotic Limbs",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606748"
        },
        "Preview": {
          "title": "WRLKit: Computational Design of Personalized Wearable Robotic Limbs",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=aJs2fq2ZBRo"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2163",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127762
      ],
      "eventIds": [],
      "abstract": "Wearable robotic limbs (WRLs) augment human capabilities through robotic structures that attach to the user‚Äôs body. \r\nWhile WRLs are intensely researched and various device designs have been presented, it remains difficult for non-roboticists to engage with this exciting field. \r\nWe aim to empower interaction designers and application domain experts to explore novel designs and applications by rapidly prototyping personalized WRLs that are customized for different tasks, different body locations, or different users. \r\nIn this paper, we present WRLKit, an interactive computational design approach that enables designers to rapidly prototype a personalized WRL without requiring extensive robotics and ergonomics expertise. \r\nThe body-aware optimization approach starts by capturing the user‚Äôs body dimensions and dynamic body poses. \r\nThen, an optimized fabricable structure of the WRL is generated for a desired mounting location and workspace of the WRL, to fit the user‚Äôs body and intended task.\r\nThe results of a user study and several implemented prototypes demonstrate the practical feasibility and versatility of WRLKit.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 126224
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 125926
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 126058
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 126004
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Max Planck Institute for Informatics",
              "dsl": "Visual Computing and Artificial Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 126639
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Max Planck Institute for Informatics",
              "dsl": "Visual Computing and Artificial Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 126268
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 125963
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 125931
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Max Planck Institute for Informatics",
              "dsl": "Visual Computing and Artificial Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 126199
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 125896
        }
      ]
    },
    {
      "id": 126708,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "AR-Enhanced Workouts: Exploring Visual Cues for At-Home Workout Videos in AR Environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606796"
        },
        "Preview": {
          "title": "AR-Enhanced Workouts: Exploring Visual Cues for At-Home Workout Videos in AR Environment",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GxT6C_I15Ng"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6995",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127770
      ],
      "eventIds": [],
      "abstract": "In recent years, with growing health consciousness, at-home workout has become increasingly popular for its convenience and safety. Most people choose to follow video guidance during exercising. However, our preliminary study revealed that fitness-minded people face challenges when watching exercise videos on handheld devices or fixed monitors, such as limited movement comprehension due to static camera angles and insufficient feedback. To address these issues, we reviewed popular workout videos, identified user requirements, and came up with an augmented reality (AR) solution. Following a user-centered iterative design process, we proposed a design space of AR visual cues for workouts and implemented an AR-based application. Specifically, we captured users‚Äô exercise performance with pose-tracking technology and provided feedback via AR visual cues. Two user experiments showed that incorporating AR visual cues could improve movement comprehension and enable users to adjust their movements based on real-time feedback. Finally, we presented several suggestions to inspire future design and apply AR visual cues to sports training.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "State Key Lab of CAD&CG"
            }
          ],
          "personId": 126460
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": "Department of Computing"
            }
          ],
          "personId": 126245
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "ZheJiang University",
              "dsl": ""
            }
          ],
          "personId": 126467
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "State Key Lab of CAD&CG"
            }
          ],
          "personId": 126129
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "State Key Lab of CAD&CG"
            }
          ],
          "personId": 125812
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "Department of Sports Science"
            }
          ],
          "personId": 126013
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang Province",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "Department of Sports Science"
            }
          ],
          "personId": 126387
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "State Key Lab of CAD&CG"
            }
          ],
          "personId": 126062
        }
      ]
    },
    {
      "id": 126709,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "STAR: Smartphone-analogous Typing in Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606803"
        },
        "Preview": {
          "title": "STAR: Smartphone-analogous Typing in Augmented Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=aB0OnlVDSUE"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6515",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127770
      ],
      "eventIds": [],
      "abstract": "While text entry is an essential and frequent task in Augmented Reality (AR) applications, devising an efficient and easy-to-use text entry method for AR remains an open challenge. This research presents STAR, a smartphone-analogous AR text entry technique that leverages a user's familiarity with smartphone two-thumb typing. With STAR, a user performs thumb typing on a virtual QWERTY keyboard that is overlain on the skin of their hands. During an evaluation study of STAR, participants achieved a mean typing speed of 21.9 WPM (i.e., 56% of their smartphone typing speed), and a mean error rate of 0.3% after 30 minutes of practice. We further analyze the major factors implicated in the performance gap between STAR and smartphone typing, and discuss ways this gap could be narrowed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 126024
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 126135
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 126033
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126361
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126054
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            },
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125957
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta Reality Labs Research",
              "dsl": ""
            }
          ],
          "personId": 126195
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 126400
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 125832
        }
      ]
    },
    {
      "id": 126710,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "SwarmFidget: Exploring Programmable Actuated Fidgeting with Swarm Robots",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606746"
        },
        "Preview": {
          "title": "SwarmFidget: Exploring Programmable Actuated Fidgeting with Swarm Robots",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=6ZXp4dDyIIk"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7606",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127770
      ],
      "eventIds": [],
      "abstract": "We introduce the concept of programmable actuated fidgeting, a type of fidgeting that involves devices integrated with actuators, sensors, and computing to enable a customizable interactive fidgeting experience. In particular, we explore the potential of a swarm of tabletop robots as an instance of programmable actuated fidgeting as robots are becoming increasingly available. Through ideation sessions among researchers and feedback from the participants, we formulate the design space for SwarmFidget, where swarm robots are used to facilitate programmable actuated fidgeting. To gather user impressions, we conducted an exploratory study where we introduced the concept of SwarmFidget to twelve participants and had them experience and provide feedback on six example fidgeting interactions. Our study demonstrates the potential of SwarmFidget for facilitating fidgeting interaction and provides insights and guidelines for designing effective and engaging fidgeting interactions with swarm robots. We believe our work can inspire future research in the area of programmable actuated fidgeting and open up new opportunities for designing novel swarm robot-based fidgeting systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 126628
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Center for Design Research"
            }
          ],
          "personId": 126117
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 126388
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 126022
        }
      ]
    },
    {
      "id": 126717,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606725"
        },
        "Preview": {
          "title": "Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=5nqVHCGRos8"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7280",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127767
      ],
      "eventIds": [],
      "abstract": "Text-to-image generative models have demonstrated remarkable capabilities in generating high-quality images based on textual prompts. However, crafting prompts that accurately capture the user's creative intent remains challenging. It often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user's intention. To address these challenges, we present Promptify, an interactive system that supports prompt exploration and refinement for text-to-image generative models. Promptify utilizes a suggestion engine powered by large language models to help users quickly explore and craft diverse prompts. Our interface allows users to organize the generated images flexibly, and based on their preferences, Promptify suggests potential changes to the original prompt. This feedback loop enables users to iteratively refine their prompts and enhance desired features while avoiding unwanted ones. Our user study shows that Promptify effectively facilitates the text-to-image workflow, allowing users to create visually appealing images on their first attempt while requiring significantly less cognitive load than a widely-used baseline tool.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125765
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126302
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science "
            }
          ],
          "personId": 126185
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 126051
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126361
        }
      ]
    },
    {
      "id": 126718,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "GestureCanvas: A Programming by Demonstration System for Prototyping Compound Freehand Interaction in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606736"
        },
        "Preview": {
          "title": "GestureCanvas: A Programming by Demonstration System for Prototyping Compound Freehand Interaction in VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0sQ7LYXqUb0"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2944",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127768
      ],
      "eventIds": [],
      "abstract": "As the use of hand gestures becomes increasingly prevalent in virtual reality (VR) applications, prototyping Compound Freehand Interactions (CFIs) effectively and efficiently has become a critical need in the design process. Compound Freehand Interaction (CFI) is a sequence of freehand interactions where each sub-interaction in the sequence conditions the next. Despite the need for interactive prototypes of CFI in the early design stage, creating them is effortful and remains a challenge for designers since it requires a highly technical workflow that involves programming the recognizers, system responses and conditionals for each sub-interaction. To bridge this gap, we present GestureCanvas, a freehand interaction-based immersive prototyping system that enables a rapid, end-to-end, and code-free workflow for designing, testing, refining, and subsequently deploying CFI by leveraging the three pillars of interaction models: event-driven state machine, trigger-action authoring, and programming by demonstration. The design of GestureCanvas includes three novel design elements ‚Äî (i) appropriating the multimodal recording of freehand interaction into a CFI authoring workspace called Design Canvas, (ii) semi-automatic identification of the input trigger logic from demonstration to reduce the manual effort of setting up triggers for each sub-interaction, (iii) on the fly testing for independently validating the input conditionals in-situ. We validate the workflow enabled by GestureCanvas through an interview study with professional designers and evaluate its usability through a user study with non-experts. Our work lays the foundation for advancing research on immersive prototyping systems allowing even highly complex gestures to be easily prototyped and tested within VR environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125940
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125961
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 126028
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126311
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126377
        }
      ]
    },
    {
      "id": 126719,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "RadarVR: Exploring Spatiotemporal Visual Guidance in Cinematic VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606734"
        },
        "Preview": {
          "title": "RadarVR: Exploring Spatiotemporal Visual Guidance in Cinematic VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=fR-x8xBF8qg"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7166",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127765
      ],
      "eventIds": [],
      "abstract": "In cinematic VR, viewers can only see a limited portion of the scene at any time. As a result, they may miss important events outside their field of view. While there are many techniques which offer spatial guidance (where to look), there has been little work on temporal guidance (when to look). Temporal guidance offers viewers a look-ahead time and allows viewers to plan their head motion for important events. This paper introduces spatiotemporal visual guidance and presents a new widget, RadarVR, which shows both spatial and temporal information of regions of interest (ROIs) in a video. Using RadarVR, we conducted a study to investigate the impact of temporal guidance and explore trade-offs between spatiotemporal and spatial-only visual guidance. Results show spatiotemporal feedback allows users to see a greater percentage of ROIs, with 81% more seen from their initial onset. We discuss design implications for future work in this space.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Stanford University"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Stanford University"
            }
          ],
          "personId": 126622
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 126342
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126361
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 126400
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 126571
        }
      ]
    },
    {
      "id": 126720,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Stereoscopic Viewing and Monoscopic Touching: Selecting Distant Objects in VR Through a Mobile Device",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606809"
        },
        "Preview": {
          "title": "Stereoscopic Viewing and Monoscopic Touching: Selecting Distant Objects in VR Through a Mobile Device",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=bfNDt8mflIs"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8376",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127755
      ],
      "eventIds": [],
      "abstract": "In this study, we explore a new way to complementarily utilize the immersive visual output of VR and the physical haptic input of a smartphone. In particular, we focus on interacting with distant virtual objects using a smartphone in a through-plane manner and present a novel selection technique that overcomes the binocular parallax that occurs in such an arrangement. In our proposed technique, when a user in the stereoscopic viewing mode needs to perform a distant selection, the user brings the fingertip near the screen of the mobile device, triggering a smoothly animated transition to the monoscopic touching mode. Using a novel proof-of-concept implementation that utilizes a transparent acrylic panel, we conducted a user study and found that the proposed technique is significantly quicker, more precise, more direct, and more intuitive compared to the ray casting baseline. Subsequently, we created VR applications that explore the rich and interesting use cases of the proposed technique.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126596
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 125916
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126603
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126134
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126151
        }
      ]
    },
    {
      "id": 126721,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Cells, Generators, and Lenses: Design Framework for Object-Oriented Interaction with Large Language Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606833"
        },
        "Preview": {
          "title": "Cells, Generators, and Lenses: Design Framework for Object-Oriented Interaction with Large Language Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Q9L2mOWHRhM"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1178",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        126889
      ],
      "eventIds": [],
      "abstract": "Large Language Models (LLMs) have become the backbone of numerous writing interfaces with the goal of supporting end-users across diverse writing tasks. While LLMs reduce the effort of manual writing, end-users may need to experiment and iterate with various generation configurations (e.g., inputs and model parameters) until results meet their goals. However, these interfaces are not designed for experimentation and iteration, and can restrict how end-users track, compare, and combine configurations. In this work, we present \"cells, generators, and lenses\", a framework to designing interfaces that support interactive objects that embody configuration components (i.e., input, model, output). Interface designers can apply our framework to produce interfaces that enable end-users to create variations of these objects, combine and recombine them into new configurations, and compare them in parallel to efficiently iterate and experiment with LLMs. To showcase how our framework generalizes to diverse writing tasks, we redesigned three different interfaces‚Äîstory writing, copywriting, and email composing‚Äîand, to demonstrate its effectiveness in supporting end-users, we conducted a comparative study (N=18) where participants used our interactive objects to generate and experiment more. Finally, we investigate the usability of the framework through a workshop with designers (N=3) where we observed that our framework served as both bootstrapping and inspiration in the design process.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 126070
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 126011
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 125858
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 126439
        }
      ]
    },
    {
      "id": 126722,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Substiports: User-Inserted Ad Hoc Objects as Reusable Structural Support Replacements for Unmodified FDM 3D Printers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606718"
        },
        "Preview": {
          "title": "Substiports: User-Inserted Ad Hoc Objects as Reusable Structural Support Replacements for Unmodified FDM 3D Printers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YGQjhCvmy4g"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5660",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127756
      ],
      "eventIds": [],
      "abstract": "We contribute a technical solution to reduce print time and material with unmodified fused deposition modelling printers. The approach uses ad hoc objects inserted by a user during printing as a replacement for printed support of overhanging structures. Examples of objects include household items like books, toy bricks, and custom mechanisms like a screw jack. A software-only system is integrated into existing slicing software to analyze generated support print paths, search a library of objects to find suitable replacements, optimize combinations of replacement objects, and make necessary adjustments to impacted printing layers and paths. During printing, the user is prompted to insert objects with the help of lightweight printed holders to guide placement and prevent movement. Instructions printed on the build-plate help identify and position objects. A technical evaluation measures performance and benefits with different sets of ad hoc objects and different levels of user involvement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 126466
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            }
          ],
          "personId": 125829
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 126356
        }
      ]
    },
    {
      "id": 126723,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "MIWA: Mixed-Initiative Web Automation for Better User Control and Confidence",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606720"
        },
        "Preview": {
          "title": "MIWA: Mixed-Initiative Web Automation for Better User Control and Confidence",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=roWPRq6Unm8"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3361",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127763
      ],
      "eventIds": [],
      "abstract": "In the era of Big Data, web automation is frequently used by data scientists, domain experts, and programmers to complete time-consuming data collection tasks. However, developing web automation scripts requires familiarity with a programming language and HTML, which remains a key learning barrier for non-expert users. We provide MIWA, a mixed-initiative web automation system that enables users to create web automation scripts by demonstrating what content they want from the targeted websites. Compared to existing web automation tools, MIWA helps users better understand a generated script and build trust in it by (1) providing a step-by-step explanation of the script's behavior with visual correspondence to the target website, (2) supporting greater autonomy and control over web automation via step-through debugging and fine-grained demonstration refinement, and (3) automatically detecting potential corner cases that are handled improperly by the generated script. We conducted a within-subjects user study with 24 participants and compared MIWA with Rousillon, a state-of-the-art web automation tool. Results showed that, compared to Rousillon, MIWA reduced the task completion time by half while helping participants gain more confidence in the generated script.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126538
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126156
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan, Ann Arbor",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126369
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126449
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 125870
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 126349
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126194
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126166
        }
      ]
    },
    {
      "id": 126724,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Learning Custom Experience Ontologies via Embedding-based Feedback Loops",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606715"
        },
        "Preview": {
          "title": "Learning Custom Experience Ontologies via Embedding-based Feedback Loops",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8B7b5hOun4Q"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6867",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127769
      ],
      "eventIds": [],
      "abstract": "Companies and organizations rely on behavioral analytics tools like Google Analytics to monitor their digital experiences. Making sense of the data these tools capture, however, requires manual event tagging and filtering---often a tedious process. Prior research approaches have trained machine learning models to automatically tag interaction data, but they draw from fixed digital experience vocabularies which cannot be easily augmented or customized. This paper introduces a novel machine learning feedback loop that generates customized tag predictions for organizations. The approach uses a general experience vocabulary to bootstrap initial tag predictions on interactive Sankey diagrams representing user navigation paths on a digital asset. By interacting with the path visualization, organizations can manually revise predictions. The system leverages this feedback to refine an organization's experience ontology, computing custom word embeddings for each of its terms via vector space refinement algorithms. The updates made to the custom experience ontology and its associated word embeddings result in better event tag predictions for that organization in the future. We conducted a needfinding interview with web analytics professionals to ground our design choices, and present a real-world deployment that demonstrates how, even with just a few training examples, custom tags can be predicted over new data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Inc.",
              "dsl": "UserTesting"
            }
          ],
          "personId": 126572
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "UserTesting, Inc.",
              "dsl": ""
            }
          ],
          "personId": 126187
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francsico",
              "institution": "Inc.",
              "dsl": "UserTesting"
            }
          ],
          "personId": 126329
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "UserTesting, Inc.",
              "dsl": ""
            }
          ],
          "personId": 126475
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Franciscoe",
              "institution": "UserTesting, Inc",
              "dsl": ""
            }
          ],
          "personId": 125989
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "User Testing, Inc.",
              "dsl": ""
            }
          ],
          "personId": 126478
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "UserTesting, Inc.",
              "dsl": ""
            }
          ],
          "personId": 126594
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "UserTesting, Inc.",
              "dsl": ""
            }
          ],
          "personId": 126383
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "UserTesting",
              "dsl": ""
            }
          ],
          "personId": 126314
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "UserTesting, Inc.",
              "dsl": ""
            }
          ],
          "personId": 126564
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 126231
        }
      ]
    },
    {
      "id": 126728,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "EmTex: Prototyping Textile-Based Interfaces through An Embroidered Construction Kit",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606815"
        },
        "Preview": {
          "title": "EmTex: Prototyping Textile-Based Interfaces through An Embroidered Construction Kit",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Gklg_CrhRLY"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5768",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127754
      ],
      "eventIds": [],
      "abstract": "As electronic textiles have become more advanced in sensing, ac- tuating, and manufacturing, incorporating smartness into fabrics has become of special interest to ubiquitous computing and interac- tion researchers and designers. However, innovating smart textile interfaces for numerous input and output modalities usually re- quires expert-level knowledge of specific materials, fabrication, and protocols. This paper presents EmTex, a construction kit based on embroidered textiles, patterned with dedicated sensing, actuating, and connecting components to facilitate the design and prototyp- ing of smart textile interfaces. With machine embroidery, EmTex is compatible with a wide range of threads and underlay fabrics, proficient in various stitches to control the electric parameters, and capable of integrating versatile and reliable interaction functionali- ties with aesthetic patterns and precise designs. EmTex consists of 28 textile-based sensors, actuators, connectors, and displays, pre- sented with standardized visual and tactile effects. Along with a visual programming tool, EmTex enables the prototyping of every- day textile interfaces for diverse life-living scenarios, that embody their touch input, and visual and haptic output properties. With EmTex, we conducted a workshop and invited 25 designers and makers to create freeform textile interfaces. Our findings revealed that EmTex helped the participants explore novel interaction oppor- tunities with various smart textile prototypes. We also identified challenges EmTex shall face for practical use in promoting the design innovation of smart textiles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 126265
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation "
            }
          ],
          "personId": 126500
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 126624
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 126019
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 126303
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 126642
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 125813
        }
      ]
    },
    {
      "id": 126729,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "From Gap to Synergy: Enhancing Contextual Understanding through Human-Machine Collaboration in Personalized Systems",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606741"
        },
        "Preview": {
          "title": "From Gap to Synergy: Enhancing Contextual Understanding through Human-Machine Collaboration in Personalized Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WpVol8AO4hM"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9574",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127769
      ],
      "eventIds": [],
      "abstract": "This paper presents LangAware, a collaborative approach for constructing personalized context for context-aware applications. The need for personalization arises due to significant variations in context between individuals based on scenarios, devices, and preferences. However, there is often a notable gap between humans and machines in the understanding of how contexts are constructed, as observed in trigger-action programming studies such as IFTTT. LangAware enables end-users to participate in establishing contextual rules in-situ using natural language. The system leverages large language models (LLMs) to semantically connect low-level sensor detectors to high-level contexts and provide understandable natural language feedback for effective user involvement. We conducted a user study with 16 participants in real-life settings, which revealed an average success rate of 87.50% for defining contextual rules in a variety of 12 campus scenarios, typically accomplished within just two modifications. Furthermore, users reported a better understanding of the machine's capabilities by interacting with LangAware.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 125955
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 126236
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 126503
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 126479
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 126016
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 126396
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 125766
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 125878
        }
      ]
    },
    {
      "id": 126730,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Biohybrid Devices: Prototyping Interactive Devices with Growable Materials",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606774"
        },
        "Preview": {
          "title": "Biohybrid Devices: Prototyping Interactive Devices with Growable Materials",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=LTsU04hZ1mU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4798",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127756
      ],
      "eventIds": [],
      "abstract": "Living bio-materials are increasingly used in HCI for fabricating objects by growing. However, how to integrate electronics to make these objects interactive still needs to be clarified. This paper presents an exploration of the fabrication design space of Biohybrid Interactive Devices, a class of interactive devices fabricated by merging electronic components and living organisms. From the exploration of this space using bacterial cellulose, we outline a fabrication framework centered on the biomaterials‚Äò life cycle phases. We introduce a set of novel fabrication techniques for embedding conductive elements, sensors, and output components through biological (e.g. bio-fabrication and bio-assembling) and digital processes. We demonstrate the combinatory aspect of the framework by realizing three tangible, wearable, and shape-changing interfaces. Finally, we discuss the sustainability of our approach, its limitations, and the implications for bio-hybrid systems in HCI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris La D√©fense",
              "institution": "L√©onard de Vinci P√¥le Universitaire, Research Center",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126345
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris La D√©fense",
              "institution": "L√©onard de Vinci P√¥le Universitaire, Research Center",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "N√Æmes",
              "institution": "Universit√© de Nimes",
              "dsl": "Projekt and Chrome"
            }
          ],
          "personId": 125773
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 126640
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "ile de France",
              "city": "Palaiseau",
              "institution": "T√©l√©com Paris, Institut Polytechnique de Paris",
              "dsl": "Dpt. SES, CNRS i3"
            }
          ],
          "personId": 126285
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 125896
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris La D√©fense",
              "institution": "L√©onard de Vinici P√¥le Universitaire",
              "dsl": "Research Center"
            }
          ],
          "personId": 125983
        }
      ]
    },
    {
      "id": 126731,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Ubi-TOUCH: Ubiquitous Tangible Object Utilization through Consistent Hand-object interaction in Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606793"
        },
        "Preview": {
          "title": "Ubi-TOUCH: Ubiquitous Tangible Object Utilization through Consistent Hand-object interaction in Augmented Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=O-sdyL94_Fs"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3468",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127752
      ],
      "eventIds": [],
      "abstract": "Utilizing everyday objects as tangible proxies for Augmented Reality (AR) provides users with haptic feedback while interacting with virtual objects. Yet, existing methods focus on the attributes of the objects, constraining the possible proxies and yielding inconsistency in user experience. Therefore, we propose Ubi-TOUCH, an AR system that assists users in seeking a wider range of tangible proxies for AR applications based on the hand-object interaction (HOI) they desire. Given the target interaction with a virtual object, the system scans the users' vicinity and recommends object proxies with similar interactions. Upon user selection, the system simultaneously tracks and maps users' physical HOI to the virtual HOI, adaptively optimizing object 6 DoF and the hand gesture to provide consistency between the interactions. We showcase promising use cases of Ubi-TOUCH, such as remote tutorials, AR gaming, and Smart Home control. Finally, we evaluate the performance and usability of Ubi-TOUCH with a user study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Department of Electrical and Computer Engineering "
            }
          ],
          "personId": 125756
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Elmore Family School of Electrical and Computer Engineering"
            }
          ],
          "personId": 125768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Mechanical Engineering,C Design Lab"
            }
          ],
          "personId": 126267
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 126446
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 126344
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 126209
        }
      ]
    },
    {
      "id": 126732,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Never-ending Learning of User Interfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606824"
        },
        "Preview": {
          "title": "Never-ending Learning of User Interfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=IZ6F3dyE3n8"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2134",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127769
      ],
      "eventIds": [],
      "abstract": "Machine learning models have been trained to predict semantic information about user interfaces (UIs) to make apps more accessible, easier to test, and to automate. Currently, most models rely on datasets that are collected and labeled by human crowd-workers, a process that is costly and surprisingly error-prone for certain tasks. For example, it is possible to guess if a UI element is ‚Äútappable‚Äù from a screenshot (i.e., based on visual signifiers) or from potentially unreliable metadata (e.g., a view hierarchy), but one way to know for certain is to programmatically tap the UI element and observe the effects. We built the Never-ending UI Learner, an app crawler that automatically installs real apps from a mobile app store and crawls them to discover new and challenging training examples to learn from. The Never-ending UI Learner has crawled for more than 5,000 device-hours, performing over half a million actions on 6,000 apps to train three computer vision models for i) tappability prediction, ii) draggability prediction, and iii) screen similarity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126399
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 126118
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 126602
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 126631
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "Apple Inc",
              "dsl": ""
            }
          ],
          "personId": 126528
        }
      ]
    },
    {
      "id": 126733,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Haptic Rendering of Neural Radiance Fields",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606811"
        },
        "Preview": {
          "title": "Haptic Rendering of Neural Radiance Fields",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vFr84Fl7PhE"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6066",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127768
      ],
      "eventIds": [],
      "abstract": "The neural radiance field (NeRF) is attracting increasing attentions from researchers in various fields. While NeRF has produced visu- ally plausible results and found its potential applications in virtual reality, users are only allowed to rotate the camera to observe the scene represented as NeRF. We study the haptic interaction with NeRF models in this paper to enable the experience of touching ob- jects reconstructed by NeRF. Existing haptic rendering algorithms do not work well for NeRF-represented models because NeRF is of- ten noisy. We propose a stochastic haptic rendering method to deal with the collision response between the haptic proxy and NeRF. We validate our method with complex NeRF models and experimental results show the efficacy of our proposed algorithm.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": ""
            }
          ],
          "personId": 125837
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": ""
            }
          ],
          "personId": 126430
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": ""
            }
          ],
          "personId": 126618
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": ""
            }
          ],
          "personId": 126403
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Instrument Science and Engineering"
            }
          ],
          "personId": 125855
        }
      ]
    },
    {
      "id": 126734,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606800"
        },
        "Preview": {
          "title": "VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qr50PXMl0PU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7158",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        126889
      ],
      "eventIds": [],
      "abstract": "In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confirmed the usability and effectiveness of VISAR in facilitating the argumentative writing planning process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126389
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore University of Technology and Design",
              "dsl": "Information Systems Technology and Design"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126542
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of English"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of English"
            }
          ],
          "personId": 126148
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 125993
        }
      ]
    },
    {
      "id": 126735,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "DiLogics: Creating Web Automation Programs with Diverse Logics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606822"
        },
        "Preview": {
          "title": "DiLogics: Creating Web Automation Programs with Diverse Logics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=dFua0GjUI44"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8238",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127763
      ],
      "eventIds": [],
      "abstract": "Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provides an efficient, intuitive, and expressive method for developing web automation programs satisfying diverse specifications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126131
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 126270
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 126036
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 126590
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 126349
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126194
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126428
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126361
        }
      ]
    },
    {
      "id": 126736,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Storyfier: Exploring Vocabulary Learning Support with Text Generation Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606786"
        },
        "Preview": {
          "title": "Storyfier: Exploring Vocabulary Learning Support with Text Generation Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=oG-Ep02t56o"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5893",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127758
      ],
      "eventIds": [],
      "abstract": "Vocabulary learning support tools have widely exploited existing materials, e.g., stories or video clips, as contexts to help users memorize each target word. However, these tools could not provide a coherent context for any target words of learners‚Äô interests, and they seldom help practice word usage. In this paper, we work with teachers and students to iteratively develop Storyfier, which lever- ages text generation models to enable learners to read a generated story that covers any target words, conduct a story cloze test, and use these words to write a new story with adaptive AI assistance. Our within-subjects study (N=28) shows that learners generally favor the generated stories for connecting target words and writ- ing assistance for easing their learning workload. However, in the read-cloze-write learning sessions, participants using Storyfier per- form worse in recalling and using target words than learning with a baseline tool without our AI features. We discuss insights into supporting learning tasks with generative models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            },
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126075
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126288
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126525
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Guangdong Polytechnic of Industry & Commerce",
              "dsl": ""
            }
          ],
          "personId": 126027
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 126222
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126516
        }
      ]
    },
    {
      "id": 126737,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "KnitScript: A Domain-Specific Scripting Language for Advanced Machine Knitting",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606789"
        },
        "Preview": {
          "title": "KnitScript: A Domain-Specific Scripting Language for Advanced Machine Knitting",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=emV3xNVlg-0"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6731",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127754
      ],
      "eventIds": [],
      "abstract": "Knitting machines can fabricate complex fabric structures using robust industrial fabrication machines. However, machine knitting's full capabilities are only available through low-level programming languages that operate on individual machine operations. We present KnitScript, a domain-specific machine knitting scripting language that supports computationally driven knitting designs. KnitScript provides a comprehensive virtual model of knitting machines, giving access to machine-level capabilities as they are needed while automating a variety of tedious and error-prone details. Programmers can extend KnitScript with Python programs to create more complex programs and user interfaces. We evaluate the expressivity of KnitScript through a user study where nine machine knitters used KnitScript code to modify knitting patterns. We demonstrate the capabilities of KnitScript through three demonstrations where we create: a program for generating knitted figures of randomized trees, a parameterized hat template that can be modified with accessibility features, and a pattern for a parametric mixed-material lampshade. KnitScript advances the state of machine-knitting research by providing a platform to develop and share complex knitting algorithms, design tools, and patterns. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Khoury College of Computer Sciences"
            }
          ],
          "personId": 125788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126079
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 125754
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 125871
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126585
        }
      ]
    },
    {
      "id": 126742,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Beyond the Artifact: Power as a Lens for Creativity Support Tools",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606831"
        },
        "Preview": {
          "title": "Beyond the Artifact: Power as a Lens for Creativity Support Tools",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9tWltjgL9H4"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2803",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127759
      ],
      "eventIds": [],
      "abstract": "Researchers who build creativity support tools (CSTs) define abstractions and software representations that align with user needs to give users the power to accomplish tasks. However, these specifications also structure and limit how users can and should think, act, and express themselves. Thus, tool designers unavoidably exert power over their users by enacting a ``normative ground'' through their tools. Drawing on interviews with 11 creative practitioners, tool designers, and CST researchers, we offer a definition of empowerment in the context of creative practice, build a preliminary theory of how power relationships manifest in CSTs, and explain why researchers have had trouble addressing these concepts in the past. We re-examine CST literature through a lens of power and argue that mitigating power imbalances at the level of technical design requires enabling users in both vertical movement along levels of abstraction as well as horizontal movement between tools through interoperable representations. A lens of power is one possible orientation that lets us recognize the methodological shifts required towards building ``artistic support tools.'' ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 126488
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Electrical Engineering and Computer Science"
            }
          ],
          "personId": 126604
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 126069
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 126137
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 126123
        }
      ]
    },
    {
      "id": 126743,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Usable and Fast Interactive Mental Face Reconstruction",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606795"
        },
        "Preview": {
          "title": "Usable and Fast Interactive Mental Face Reconstruction",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qP-ncu7r4T4"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9440",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127767
      ],
      "eventIds": [],
      "abstract": "We introduce an end-to-end interactive system for mental face reconstruction - the challenging task of visually reconstructing a face image a person only has in their mind.\r\nIn contrast to existing methods that suffer from low usability and high mental load, our approach only requires the user to rank images over multiple iterations according to the perceived similarity with their mental image.\r\nBased on these rankings, our mental face reconstruction system extracts image features in each iteration, combines them into a joint feature vector, and then uses a generative model to visually reconstruct the mental image.\r\nTo avoid the need for collecting large amounts of human training data, we further propose a computational user model that can simulate human ranking behaviour using data from an online crowd-sourcing study (N=215).\r\nResults from a 12-participant user study show that our method can reconstruct mental images that are visually similar to existing approaches but has significantly higher usability, lower perceived workload, and is 40% faster. \r\nIn addition, results from a third 22-participant lineup study in which we validated our reconstructions on a face ranking task show a identification rate of 55.3%, which is in line with prior work.\r\nThese results represent an important step towards new interactive intelligent systems that can robustly and effortlessly reconstruct a user's mental image. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "Institute for Visualisation and Interactive Systems"
            }
          ],
          "personId": 126259
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "Institute for Visualisation and Interactive Systems"
            }
          ],
          "personId": 125764
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": ""
            }
          ],
          "personId": 126312
        }
      ]
    },
    {
      "id": 126744,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606727"
        },
        "Preview": {
          "title": "HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PMBIuiYoHKI"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8347",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127770
      ],
      "eventIds": [],
      "abstract": "This paper introduces HoloBots, a mixed reality remote collaboration system that augments holographic telepresence with synchronized mobile robots. \r\nBeyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also \\textit{physically} engage with local users and their environment. HoloBots allows the users to touch, grasp, manipulate, and interact with the remote physical environment as if they were co-located in the same shared space. \r\nWe achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio).\r\nBeyond the existing physical telepresence, HoloBots contributes to an exploration of broader design space, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. \r\nWe evaluate our system with twelve participants by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results confirm that our system significantly enhances the level of co-presence and shared experience, compared to the other conditions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 125865
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 126434
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 126574
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 126340
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 125846
        }
      ]
    },
    {
      "id": 126745,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "The View from MARS: Empowering Game Stream Viewers with Metadata Augmented Real-time Streaming",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606753"
        },
        "Preview": {
          "title": "The View from MARS: Empowering Game Stream Viewers with Metadata Augmented Real-time Streaming",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=F0Eaam3QpBc"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8226",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127761
      ],
      "eventIds": [],
      "abstract": "We present MARS (Metadata Augmented Real-time Streaming), a system that enables game-aware streaming interfaces for Twitch. Current streaming interfaces provide a video stream of gameplay and a chat channel for conversation, but do not allow viewers to interact with game content independently from the steamer or other viewers. With MARS, a Unity game‚Äôs metadata is rendered in real-time onto a Twitch viewer‚Äôs interface. The metadata can then power viewer-side interfaces that are aware of the streamer‚Äôs game activity and provide new capacities for viewers. Use cases include providing contextual information (e.g. clicking on a unit to learn more), improving accessibility (e.g. slowing down text presentation speed), and supporting novel stream-based game designs (e.g. asymmetric designs where the viewers know more than the streamer). We share the details of MARS‚Äô architecture and capabilities in this paper, and showcase a working prototype for each of our three proposed use cases.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126635
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126039
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126433
        }
      ]
    },
    {
      "id": 126746,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "ecoEDA: Recycling E-waste During Electronics Design",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606745"
        },
        "Preview": {
          "title": "ecoEDA: Recycling E-waste During Electronics Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4tMZ-wUFiEQ"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8904",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127756
      ],
      "eventIds": [],
      "abstract": "The amount of e-waste generated by discarding devices is enormous but options for recycling remain limited. However, inside a discarded device (from consumer devices to one‚Äôs own prototypes), an electronics designer could find dozens to thousands of reusable components, including microcontrollers, sensors, voltage regulators, etc. Despite this, existing electronic design tools assume users will buy all components anew. To tackle this, we propose ecoEDA, an interactive tool that enables electronics designers to explore recycling electronic components during the design process. We accomplish this via (1) creating suggestions to assist users in identifying and designing with recycled components; and (2) maintaining a library of useful data relevant to reuse (e.g., allowing users to find which devices contain which components). Through example use-cases, we demonstrate how our tool can enable various pathways to recycling e-waste. To evaluate it, we conducted a user study where participants used our tool to create an electronic schematic with components from torn-down e-waste devices. We found that participants‚Äô designs made with ecoEDA featured an average of 66% of recycled components. Last, we reflect on challenges and opportunities for building software that promotes e-waste reuse.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126523
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago ",
              "dsl": ""
            }
          ],
          "personId": 126465
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126064
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126201
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "University of Chicago"
            }
          ],
          "personId": 126241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        }
      ]
    },
    {
      "id": 126752,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Automated Conversion of Music Videos into Lyric Videos",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606757"
        },
        "Preview": {
          "title": "Automated Conversion of Music Videos into Lyric Videos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nZ9kOwvWgIU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8580",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127753
      ],
      "eventIds": [],
      "abstract": "Musicians and fans often produce lyric videos, a form of music videos that showcase the song's lyrics, for their favorite songs. However, making such videos can be challenging and time-consuming as the lyrics need to be added in synchrony and visual harmony with the video. Informed by prior work and close examination of existing lyric videos, we propose a set of design guidelines to help creators make such videos. Our guidelines ensure the readability of the lyric text while maintaining a unified focus of attention. We instantiate these guidelines in a fully automated pipeline that converts an input music video into a lyric video. We demonstrate the robustness of our pipeline by generating lyric videos from a diverse range of input sources. A user study shows that lyric videos generated by our pipeline are effective in maintaining text readability and unifying the focus of attention.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 126121
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126250
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 125864
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 125891
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 125972
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 126168
        }
      ]
    },
    {
      "id": 126753,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "VoxelHap: A Toolkit for Constructing Proxies Providing Tactile and Kinesthetic Haptic Feedback in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606722"
        },
        "Preview": {
          "title": "VoxelHap: A Toolkit for Constructing Proxies Providing Tactile and Kinesthetic Haptic Feedback in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=O0IrTgkqQy8"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4417",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127768
      ],
      "eventIds": [],
      "abstract": "Experiencing virtual environments is often limited to abstract interactions with objects. Physical proxies allow users to feel virtual objects, but are often inaccessible. We present the VoxelHap toolkit which enables users to construct highly functional proxy objects using Voxels and Plates. Voxels are blocks with special functionalities that form the core of each physical proxy. Plates increase a proxy‚Äôs haptic resolution, such as its shape, texture or weight. Beyond pro- viding physical capabilities to realize haptic sensations, VoxelHap utilizes VR illusion techniques to expand its haptic resolution. We evaluated the capabilities of the VoxelHap toolkit through the construction of a range of fully functional proxies across a variety of use cases and applications. In two experiments with 24 participants, we investigate a subset of the constructed proxies, studying how they compare to a traditional VR controller. First, we investigated VoxelHap‚Äôs combined haptic feedback and second, the trade-offs of using ShapePlates. Our findings show that VoxelHap‚Äôs proxies outperform traditional controllers and were favored by participants.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126102
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126251
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "DFKI"
            }
          ],
          "personId": 125825
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus",
              "dsl": "DFKI"
            }
          ],
          "personId": 126485
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore Management University",
              "dsl": ""
            }
          ],
          "personId": 126297
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126473
        }
      ]
    },
    {
      "id": 126754,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Fluid Reality: Electroosmotic Pump Arrays for Fine-Grained AR/VR Haptics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606771"
        },
        "Preview": {
          "title": "Fluid Reality: Electroosmotic Pump Arrays for Fine-Grained AR/VR Haptics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=5AfYxFkRues"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2116",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127752
      ],
      "eventIds": [],
      "abstract": "Virtual and augmented reality headsets are making significant progress in audio-visual immersion and consumer adoption. However, their haptic immersion remains low, due in part to the limitations of vibrotactile actuators which dominate the AR/VR market. In this work, we present a new approach to create high-resolution shape-changing fingerpad arrays with 20 haptic pixels/cm\\textsuperscript{2}. Unlike prior pneumatic approaches, our actuators are low-profile (5mm thick), low-power (approximately 10mW/pixel), and entirely self-contained, with no tubing or wires running to external infrastructure. We show how multiple actuator arrays can be built into a five-finger, 160-actuator haptic glove that is untethered, lightweight (207g, including all drive electronics and battery), and has the potential to reach consumer price points at volume production. We describe the results from a technical performance evaluation and a suite of eight user studies, quantifying the diverse capabilities of our system. This includes recognition of object properties such as complex contact geometry, texture, and compliance, as well as expressive spatiotemporal effects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 126089
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126247
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126334
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126443
        }
      ]
    },
    {
      "id": 126755,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Scene Responsiveness for Visuotactile Illusions in Mixed Reality",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606825"
        },
        "Preview": {
          "title": "Scene Responsiveness for Visuotactile Illusions in Mixed Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=m5skWGWcXFU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7374",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127765
      ],
      "eventIds": [],
      "abstract": "Manipulating their environment is one of the fundamental actions that humans, and actors more generally, perform. \r\nYet, today's mixed reality systems enable us to situate virtual content in the physical scene but fall short of expanding the visual illusion to believable environment manipulations. \r\nIn this paper, we present the concept and system of Scene Responsiveness, the visual illusion that virtual actions affect the physical scene. \r\nUsing co-aligned digital twins for coherence-preserving just-in-time virtualization of physical objects in the environment, Scene Responsiveness allows actors to seemingly manipulate physical objects as if they were virtual. \r\nBased on Scene Responsiveness, we propose two general types of end to-end illusionary experiences that ensure visuotactile consistency through the presented techniques of object elusiveness and object rephysicalization. \r\nWe demonstrate how our Daydreaming illusion enables virtual characters to enter the scene through a physically closed door and vandalize the physical scene, or users to enchant and summon far-away physical objects. \r\nIn a user evaluation of our Copperfield illusion, we found that Scene Responsiveness can be rendered so convincingly that it lends itself to magic tricks. \r\nWe present our system architecture and conclude by discussing the implications of scene-responsive mixed reality for gaming and telepresence.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "Chair for Integrated Information Systems"
            }
          ],
          "personId": 126487
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Essen",
              "institution": "Institute for Computer Science and Business Information Systems",
              "dsl": "University Duisburg-Essen"
            }
          ],
          "personId": 126491
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Menlo Park",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 126493
        }
      ]
    },
    {
      "id": 126756,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "TacNote: Tactile and Audio Note-Taking for Non-Visual Access",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606784"
        },
        "Preview": {
          "title": "TacNote: Tactile and Audio Note-Taking for Non-Visual Access",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=V4V6Qj2CoqI"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7376",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127757
      ],
      "eventIds": [],
      "abstract": "Blind and visually impaired (BVI) people primarily rely on non-visual senses to interact with a physical environment. Doing so requires a high cognitive load to perceive and memorize the presence of a large set of objects, such as at home or in a learning setting. In this work, we explored opportunities to enable object-centric note-taking by using a 3D printing pen for interactive, personalized tactile annotations. We first identified the benefits and challenges of self-created tactile graphics in a formative diary study. Then, we developed TacNote, a system that enables BVI users to annotate, explore, and memorize critical information associated with everyday objects. Using TacNote, the users create tactile graphics with a 3D printing pen and attach them to the target objects. They capture and organize the physical labels by using TacNote‚Äôs camera-based mobile app. In addition, they can specify locations, ordering, and hierarchy via finger-pointing interaction and receive audio feedback. Our user study with ten BVI participants showed that TacNote effectively alleviated the memory burden, offering a promising solution for enhancing users‚Äô access to information.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei ",
              "institution": "National Taiwan University ",
              "dsl": "Information Management"
            }
          ],
          "personId": 125925
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Graduate Institute of Networking and Multimedia"
            }
          ],
          "personId": 126043
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University , Taipei, Taiwan",
              "dsl": "Computer Science & Information Engineering"
            }
          ],
          "personId": 126420
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Department of Computer Science and Information Engineering"
            }
          ],
          "personId": 126384
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 126444
        }
      ]
    },
    {
      "id": 126758,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Olio: A Semantic Search Interface for Data Repositories",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606806"
        },
        "Preview": {
          "title": "Olio: A Semantic Search Interface for Data Repositories",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=N_H7So8gKBo"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7006",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127766
      ],
      "eventIds": [],
      "abstract": "Search and information retrieval systems are becoming more expressive in interpreting user queries beyond the traditional weighted bag-of-words model of document retrieval. For example, searching for a flight status or a game score returns a dynamically generated response along with supporting, pre-authored documents contextually relevant to the query. In this paper, we extend this hybrid search paradigm to data repositories that contain curated data sources and visualization content. We introduce a semantic search interface, OLIO, that provides a hybrid set of results comprising both auto-generated visualization responses and pre-authored charts to blend analytical question-answering with content discovery search goals. We specifically explore three search scenarios - question-and-answering, exploratory search, and design search over data repositories. The interface also provides faceted search support for users to refine and filter the conventional best-first search results based on parameters such as author name, time, and chart type. A preliminary user evaluation of the system demonstrates that OLIO's interface and the hybrid search paradigm collectively afford greater expressivity in how users discover insights and visualization content in data repositories.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 125835
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Tableau",
              "dsl": ""
            }
          ],
          "personId": 126202
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 126557
        }
      ]
    },
    {
      "id": 126759,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "FibeRobo: Fabricating 4D Fiber Interfaces by Continuous Drawing of Temperature Tunable Liquid Crystal Elastomers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606732"
        },
        "Preview": {
          "title": "FibeRobo: Fabricating 4D Fiber Interfaces by Continuous Drawing of Temperature Tunable Liquid Crystal Elastomers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Q3d0PIKimtI"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9428",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127754
      ],
      "eventIds": [],
      "abstract": "We present FibeRobo, a thermally-actuated liquid crystal elastomer (LCE) fiber that can be embedded or structured into textiles and enable silent and responsive interactions with shape-changing, fiber-based interfaces. Three definitive properties distinguish FibeRobo from other actuating fibers explored in HCI. First, they exhibit rapid thermal self-reversing actuation with large displacements (~40%) without twisting. Second, we present a reproducible UV fiber drawing setup that produces hundreds of meters of fiber with a sub-millimeter diameter. Third, FibeRobo is fully compatible with existing textile manufacturing machinery such as weaving looms, embroidery, and industrial knitting machines. This paper contributes to developing temperature-responsive LCE fibers, a facile and scalable fabrication pipeline with optional heating element integration for digital control, mechanical characterization, and the establishment of higher hierarchical textile structures and design space. Finally, we introduce a set of demonstrations that illustrate the design space FibeRobo enables.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Center for Bits and Atoms"
            }
          ],
          "personId": 125760
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 126339
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 126219
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 126082
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 126599
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Khoury College of Computer Science"
            }
          ],
          "personId": 125788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Aeronautics and Astronautics"
            }
          ],
          "personId": 126045
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "MIT Media Lab",
              "dsl": "Tangible Media"
            }
          ],
          "personId": 126021
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 126630
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 126210
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Center for Bits and Atoms"
            }
          ],
          "personId": 125907
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 125974
        }
      ]
    },
    {
      "id": 126760,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "VegaProf: Profiling Vega Visualizations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606790"
        },
        "Preview": {
          "title": "VegaProf: Profiling Vega Visualizations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ZiadStwJK4k"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6952",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127766
      ],
      "eventIds": [],
      "abstract": "Domain-specific languages (DSLs) for visualization aim to facilitate visualization creation by providing abstractions that offload implementation and execution details from users to the system layer.\r\nTherefore, DSLs often execute user-defined specifications by transforming them into intermediate representations (IRs) in successive lowering operations. \r\nHowever, DSL-specified visualizations can be difficult to profile and, hence, optimize due to the layered abstractions.\r\nTo better understand visualization profiling workflows and challenges, we conduct formative interviews with visualization engineers who use Vega in production.\r\nVega is a popular visualization DSL that transforms specifications into dataflow graphs, which are then executed to render visualization primitives.\r\nOur formative interviews reveal that current developer tools are ill-suited for visualization profiling since they are disconnected from the semantics of Vega's specification and its IRs at runtime.\r\nTo address this gap, we introduce VegaProf, the first performance profiler for Vega visualizations.\r\nVegaProf instruments the Vega library by associating a declarative specification with its compilation and execution.\r\nIntegrated into a Vega code playground, VegaProf coordinates visual performance inspection at three abstraction levels: function, dataflow graph, and visualization specification.\r\nWe evaluate VegaProf through use cases and feedback from visualization engineers as well as original developers of the Vega library.\r\nOur results suggest that VegaProf makes visualization profiling more tractable and actionable by enabling users to interactively probe time performance across layered abstractions of Vega.\r\nFurthermore, we distill recommendations from our findings and advocate for co-designing visualization DSLs together with their introspection tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126177
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Sigma Computing",
              "dsl": ""
            }
          ],
          "personId": 126407
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Sigma Computing",
              "dsl": ""
            }
          ],
          "personId": 126333
        }
      ]
    },
    {
      "id": 126761,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "WavoID:  Robust and Secure Multi-modal User Identification via mmWave-voice Mechanism",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606775"
        },
        "Preview": {
          "title": "WavoID:  Robust and Secure Multi-modal User Identification via mmWave-voice Mechanism",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ZTByV3Qdn-s"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9427",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127761
      ],
      "eventIds": [],
      "abstract": "With the increasing deployment of voice-controlled devices in homes and enterprises, there is an urgent demand for voice identification to prevent unauthorized access to sensitive information and property loss. However, due to the broadcast nature of sound wave, a voice only system is vulnerable to adverse conditions and malicious attacks. We observe that the cooperation of millimeter waves (mmWave) and voice signals can significantly improve the effectiveness and security of user identification. Based on the properties, we propose a multi-modal user identification system (named WavoID) by fusing the uniqueness of mmWave sensed vocal vibration and mic-recorded voice of users. To estimate fine-grained waveforms, WavoID splits signals and adaptively combines useful decomposed signals according to correlative contents in both mmWave and voice. An elaborated anti-spoofing module in WavoID comprising biometric bimodal information defend against attacks. WavoID produces and fuses the response maps of mmWave and voice to improve the representation power of fused features, benefiting accurate identification, even facing adverse circumstances. We evaluate WavoID using commercial sensors on extensive experiments. WavoID has significant performance on user identification with over 98% accuracy on 100 user datasets.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 125838
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126243
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 126116
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at buffalo",
              "dsl": "CSE"
            }
          ],
          "personId": 125929
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "Computer Science and Engineering",
              "dsl": "State University of New York at Buffalo"
            }
          ],
          "personId": 126144
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Colorado Denver",
              "dsl": ""
            }
          ],
          "personId": 125811
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at buffalo",
              "dsl": ""
            }
          ],
          "personId": 126583
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Kunshan",
              "institution": "Duke Kunshan University",
              "dsl": "Data and Computational Science"
            }
          ],
          "personId": 126175
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Cyber Science and Technology"
            }
          ],
          "personId": 125994
        }
      ]
    },
    {
      "id": 126771,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Color Field: Developing Professional Vision by Visualizing the Effects of Color Filters",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606828"
        },
        "Preview": {
          "title": "Color Field: Developing Professional Vision by Visualizing the Effects of Color Filters",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=p08FlV1utFQ"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5619",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127767
      ],
      "eventIds": [],
      "abstract": "Color filters are ubiquitous across visual digital media due to their transformative effect. However, it can be difficult to understand how a color filter will affect an image, especially for novices. In order to become experts, we argue that novices need to develop Goodwin‚Äôs notion of Professional Vision. Then, they can \"see\" and interpret their work in terms of their domain knowledge like experts. Using the theory of Professional Vision, we present two design objectives for systems that aim to help users develop expertise. These goals were used to develop Color Field, an interactive visualization of color filters as a vector field over the Hue-Saturation-Lightness color space. We conducted an exploratory user study in which five color grading novices and four experts were asked to analyze color filters. We found that Color Field enabled multiple strategies to make sense of filters (e.g. reviewing the overall shape of the vector field) and discuss them (e.g. using spatial language). We conclude with other applications of Color Field and future work to leverages Professional Vision in HCI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Creativity Lab"
            }
          ],
          "personId": 126614
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "UCSD",
              "dsl": ""
            }
          ],
          "personId": 126560
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126359
        }
      ]
    },
    {
      "id": 126773,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Interactive Flexible Style Transfer for Vector Graphics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606751"
        },
        "Preview": {
          "title": "Interactive Flexible Style Transfer for Vector Graphics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=brHab1gPl7w"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3797",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127759
      ],
      "eventIds": [],
      "abstract": "Vector graphics are an industry-standard way to represent and share visual designs. Designers frequently source and incorporate styles from existing designs into their work. Unfortunately, popular design tools are not well suited for this task. We present VST, Vector Style Transfer, a novel design tool for flexibly transferring visual styles between vector graphics. The core of VST lies in leveraging automation while respecting designers' tastes and the subjectivity inherent to style transfer. In VST, designers tune a cross-design element correspondence and customize which style attributes to change. We report results from a user study in which designers used VST to control style transfer between several designs, including designs participants created with external tools beforehand. VST shows that enabling design correspondence tuning and customization is one way to support interactive, flexible style transfer.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 126435
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 126281
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 125988
        }
      ]
    },
    {
      "id": 126774,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "ThermalRouter: Enabling Users to Design Thermally-Sound Devices ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606747"
        },
        "Preview": {
          "title": "ThermalRouter: Enabling Users to Design Thermally-Sound Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=P7z_fIlmj6o"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9300",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127760
      ],
      "eventIds": [],
      "abstract": "Users often 3D model enclosures that interact with significant heat sources, such as electronics or appliances that generate heat (e.g., CPU, motor, lamps, etc.). While parts made by users might function well aesthetically or structurally, they are rarely thermally-sound. This happens because heat transfer is non-intuitive; thus, engineering thermal solutions is not straightforward. To tackle this, we developed ThermalRouter, a CAD plugin that assists with improving the thermal performance of their models. ThermalRouter automatically converts regions of the model to be made from thermally-conductive materials (such as nylon or metallic-silicone). These regions act as heat channels, branching away from hotspots to dissipate heat. The key is that ThermalRouter automatically simulates the thermal performance of many possible heat channel configurations and presents the user with the most thermally-sound design (e.g., lowest temperature). Furthermore, it allows users to customize by balancing costs, indicating non-modifiable geometry, etc. Most importantly, ThermalRouter achieves this without requiring manual labor to set up or parse the results of complex thermal simulations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126355
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126629
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125903
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125836
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "University of Chicago"
            }
          ],
          "personId": 126241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125909
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        }
      ]
    },
    {
      "id": 126775,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "TactTongue: Prototyping ElectroTactile Stimulations on the Tongue",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606829"
        },
        "Preview": {
          "title": "TactTongue: Prototyping ElectroTactile Stimulations on the Tongue",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hEy1KsCuE1g"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3310",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127768
      ],
      "eventIds": [],
      "abstract": "The tongue is a remarkable human organ with a high concentration of taste receptors and an exceptional ability to sense touch. This work uses electro-tactile stimulation to explore the intricate interplay between tactile perception and taste rendering on the tongue.\r\nTo facilitate this exploration, we utilized a flexible, high-resolution electro-tactile prototyping platform that can be administered in the mouth. We have created a design tool that abstracts users from the low-level stimulation parameters, enabling them to focus on higher-level design objectives. Through this platform, we present the results of three studies.\r\nOur first study evaluates the design tool's qualitative and formative aspects. In contrast, the second study measures the qualitative attributes of the sensations produced by our device, including tactile sensations and taste. In the third study, we demonstrate the ability of our device to sense touch input through the tongue when placed on the hard palate region in the mouth.\r\nFinally, we present a range of application demonstrators that span diverse domains, including accessibility, medical surgeries, and extended reality. These demonstrators showcase the versatility and potential of our platform, highlighting its ability to enable researchers and practitioners to explore new ways of leveraging the tongue's unique capabilities. Overall, this work presents new opportunities to deploy tongue interfaces and has broad implications for designing interfaces that incorporate the tongue as a sensory organ.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 126240
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maine",
              "city": "Orono",
              "institution": "University of Maine",
              "dsl": "School of Computing and Information Science"
            }
          ],
          "personId": 125843
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126080
        }
      ]
    },
    {
      "id": 126776,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "V-DAT (Virtual Reality Data Analysis Tool): Supporting Self-Awareness for Autistic People from Multimodal VR Sensor Data",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606797"
        },
        "Preview": {
          "title": "V-DAT (Virtual Reality Data Analysis Tool): Supporting Self-Awareness for Autistic People from Multimodal VR Sensor Data",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1coUG4R5lqU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8576",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127757
      ],
      "eventIds": [],
      "abstract": "Virtual reality (VR) has become a valuable tool for social and educational purposes for autistic people, as it provides flexible environmental support to create a variety of experiences. A growing body of recent research has examined the behaviors of autistic people using sensor-based data to better understand autistic people and investigate the effectiveness of VR. Comprehensive analysis of the various signals that can be easily collected in the VR environment can promote understanding of autistic people. While this quantitative evidence has the potential to help both autistic people and others (e.g., autism experts) to understand behaviors of autistic people, existing studies have focused on single signal analysis and have not determined the acceptability of signal analysis results from the autistic person's point of view. To facilitate the use of multiple sensor signals in VR for autistic people and experts, we introduce V-DAT (Virtual Reality Data Analysis Tool), designed to support a VR sensor data handling pipeline. V-DAT takes into account four sensor modalities - head position and rotation, eye movement, audio, and physiological signals - that are actively used in current VR research for autistic people. We explain the characteristics and processing methods of the data for each modality as well as the analysis with comprehensive visualizations of V-DAT. We also conduct a case study to investigate the feasibility of V-DAT as a way of broadening understanding of autistic people from the perspectives of both autistic people and autism experts. Finally, we discuss issues with the process of V-DAT development and complementary measures for the applicability and scalability of a sensor data management system for autistic people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "School of Intelligence Computing"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "School of Intelligence Computing"
            }
          ],
          "personId": 126189
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 126455
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 126203
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Deajeon",
              "institution": "KAIST",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Deajeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126120
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Data Science"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Data Science"
            }
          ],
          "personId": 126287
        }
      ]
    },
    {
      "id": 126777,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "PressurePick: Muscle Tension Estimation for Guitar Players Using Unobtrusive Pressure Sensing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606742"
        },
        "Preview": {
          "title": "PressurePick: Muscle Tension Estimation for Guitar Players Using Unobtrusive Pressure Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=V1acwTM-DZk"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8328",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127764
      ],
      "eventIds": [],
      "abstract": "When learning to play an instrument, it is crucial for the learner's muscles to be in a relaxed state when practicing. Identifying, which parts of a song lead to increased muscle tension requires self-awareness during an already cognitively demanding task. In this work, we investigate unobtrusive pressure sensing for estimating muscle tension while practicing songs with the guitar. First, we collected data from twelve guitarists. Our apparatus consisted of three pressure sensors (one on each side of the guitar pick and one on the guitar neck) to determine the sensor that is most suitable for automatically estimating muscle tension. Second, we extracted features from the pressure time series that are indicative of muscle tension. Third, we present the hardware and software design of our PressurePick prototype, which is directly informed by the data collection and subsequent analysis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126292
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126180
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126074
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126318
        }
      ]
    },
    {
      "id": 126778,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606827"
        },
        "Preview": {
          "title": "Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=d_sW9TuocqE"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9659",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127766
      ],
      "eventIds": [],
      "abstract": "We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125927
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126256
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Programmable Reality Lab"
            }
          ],
          "personId": 126091
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 125846
        }
      ]
    },
    {
      "id": 126779,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Generative Agents: Interactive Simulacra of Human Behavior",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606763"
        },
        "Preview": {
          "title": "Generative Agents: Interactive Simulacra of Human Behavior",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ZdoU9vI2yCg"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9419",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        126889
      ],
      "eventIds": [],
      "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture‚Äîobservation, planning, and reflection‚Äîeach contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 126609
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 126341
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 125875
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 126393
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 125876
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126088
        }
      ]
    },
    {
      "id": 126780,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Spellburst: a node-based interface for exploratory creative coding with natural language prompts  ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606719"
        },
        "Preview": {
          "title": "Spellburst: a node-based interface for exploratory creative coding with natural language prompts",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KyTZ4J38auU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4092",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127767
      ],
      "eventIds": [],
      "abstract": "Creative coding tasks are often exploratory in nature. When producing digital artwork, artists usually begin with a high-level semantic construct such as a ‚Äústained glass filter‚Äù and programmatically implement it by varying code parameters such as shape, color, lines, and opacity to produce visually appealing results. Based on interviews with artists, it can be effortful to translate semantic constructs to program syntax, and current programming tools don‚Äôt lend well to rapid creative exploration. To address these challenges, we introduce Spellburst, a large language model (LLM) powered creative-coding environment. Spellburst provides (1) a node-based interface that allows artists to create generative art and explore variations through branching and merging operations, (2) expressive prompt-based interactions to engage in semantic programming, and (3) dynamic prompt-driven interfaces and direct code editing to seamlessly switch between semantic and syntactic exploration. Our evaluation with artists demonstrates Spellburst‚Äôs potential to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Replit, Inc.",
              "dsl": ""
            }
          ],
          "personId": 126060
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126271
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Graduate School of Education"
            }
          ],
          "personId": 125852
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126427
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 125750
        }
      ]
    },
    {
      "id": 126781,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Statslator: Interactive Translation of NHST and Estimation Statistics Reporting Styles in Scientific Documents",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606762"
        },
        "Preview": {
          "title": "Statslator: Interactive Translation of NHST and Estimation Statistics Reporting Styles in Scientific Documents",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=-zfuEcaL18M"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7917",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127766
      ],
      "eventIds": [],
      "abstract": "Inferential statistics are typically reported using p-values (NHST) or confidence intervals on effect sizes (estimation). This is done using a range of styles, but some readers have preferences about how statistics should be presented and others have limited familiarity with alternatives. We propose a system to interactively translate statistical reporting styles in existing documents, allowing readers to switch between interval estimates, p-values, and standardized effect sizes, all using textual and graphical reports that are dynamic and user customizable. Forty years of CHI papers are examined. Using only the information reported in scientific documents, equations are derived and validated on simulated datasets to show that conversions between p-values and confidence intervals are accurate. The system helps readers interpret statistics in a familiar style, compare reports that use different styles, and even validate the correctness of reports. Code and data: https://osf.io/x4ue7",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 125956
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL",
              "dsl": ""
            }
          ],
          "personId": 126145
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL",
              "dsl": ""
            }
          ],
          "personId": 125863
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 126356
        }
      ]
    },
    {
      "id": 126786,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Graphologue: Exploring Large Language Model Responses with Interactive Diagrams",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606737"
        },
        "Preview": {
          "title": "Graphologue: Exploring Large Language Model Responses with Interactive Diagrams",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YcQ53-yhI5E"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6261",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        126889
      ],
      "eventIds": [],
      "abstract": "Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented ability to synthesize text responses to diverse user questions. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can interact with the diagrams to flexibly adjust the graphical presentation and to submit context-specific prompts to obtain more information. Utilizing diagrams, Graphologue enables graphical, non-linear dialogues between humans and LLMs, facilitating information exploration, organization, and comprehension.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126182
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126067
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126531
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": ""
            }
          ],
          "personId": 126359
        }
      ]
    },
    {
      "id": 126787,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Synergi: A Mixed-Initiative System for Scholarly Synthesis and Sensemaking",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606759"
        },
        "Preview": {
          "title": "Synergi: A Mixed-Initiative System for Scholarly Synthesis and Sensemaking",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=o1IdX65aU9U"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4756",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127758
      ],
      "eventIds": [],
      "abstract": "Efficiently reviewing scholarly literature and synthesizing prior art are crucial for scientific progress.\r\nYet, the growing scale of publications and the burden of knowledge make synthesis of research threads more challenging than ever.\r\nWhile significant research has been devoted to helping scholars interact with individual papers, building research threads scattered across multiple papers remains a challenge.\r\nMost top-down synthesis (and LLMs) make it difficult to personalize and iterate on the output, while bottom-up synthesis is costly in time and effort.\r\nHere, we explore a new design space of mixed-initiative workflows.\r\nIn doing so we develop a novel computational pipeline, Synergi, that ties together user input of relevant seed threads with citation graphs and LLMs, to expand and structure them, respectively.\r\nSynergi allows scholars to start with an entire threads-and-subthreads structure generated from papers relevant to their interests, and to iterate and customize on it as they wish.\r\nIn our evaluation, we find that Synergi helps scholars efficiently make sense of relevant threads, broaden their perspectives, and increases their curiosity.\r\nWe discuss future design implications for thread-based, mixed-initiative scholarly synthesis support tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 125991
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126046
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for AI",
              "dsl": "AI2"
            }
          ],
          "personId": 126453
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 126207
        }
      ]
    },
    {
      "id": 126788,
      "typeId": 13094,
      "durationOverride": 13,
      "title": " Robust Finger Interactions with COTS Smartwatches via Unsupervised Siamese Adaptation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606794"
        },
        "Preview": {
          "title": "Robust Finger Interactions with COTS Smartwatches via Unsupervised Siamese Adaptation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=TW3D19p6rqs"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3544",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127755
      ],
      "eventIds": [],
      "abstract": "Wearable devices like smartwatches and smart wristbands have gained substantial popularity in recent years. However, their small interfaces create inconvenience and limit computing functionality. To fill this gap, we propose ViWatch, which enables robust finger interactions under deployment variations, and relies on a single IMU sensor that is ubiquitous in COTS smartwatches.\r\nTo this end, we design an unsupervised Siamese adversarial learning method. We built a real-time system on commodity smartwatches and tested it with over one hundred volunteers. Results show that the system accuracy is about 97% over a week. In addition, it is resistant to deployment variations such as different hand shapes, finger activity strengths, and smartwatch positions on the wrist. We also developed a number of mobile applications using our interactive system and conducted a user study where all participants preferred our unsupervised approach to supervised calibration. The demonstration of ViWatch is shown at https://youtu.be/N5-ggvy2qfI",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology (MIT)",
              "dsl": "Computer Science & Artificial Intelligence Laboratory (CSAIL)"
            }
          ],
          "personId": 125978
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 125808
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": ""
            }
          ],
          "personId": 126293
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Shenzhen University",
              "dsl": ""
            }
          ],
          "personId": 126001
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "VibInt Limited",
              "dsl": ""
            }
          ],
          "personId": 126454
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "ECE Department"
            }
          ],
          "personId": 126482
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 126072
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125762
        }
      ]
    },
    {
      "id": 126789,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Hypothesizer: A Hypothesis-Based Debugger to Find and Test Debugging Hypotheses",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606781"
        },
        "Preview": {
          "title": "Hypothesizer: A Hypothesis-Based Debugger to Find and Test Debugging Hypotheses",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=baBPNDTkOgc"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6143",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127763
      ],
      "eventIds": [],
      "abstract": "When software defects occur, developers begin the debugging process by formulating hypotheses to explain the cause. These hypotheses guide the investigation process, determining which evidence developers gather to accept or reject the hypothesis, such as parts of the code and program state developers examine. However, existing debugging techniques do not offer support in finding relevant hypotheses, leading to wasted time testing hypotheses and examining code that ultimately does not lead to a fix. To address this issue, we introduce a new type of debugging tool, the hypothesis-based debugger, and an implementation of this tool in Hypothesizer. Hypothesis-based debuggers support developers from the beginning of the debugging process by finding relevant hypotheses until the defect is fixed. To debug using Hypothesizer, developers first demonstrate the defect, generating a recording of the program behavior with code execution, user interface events, network communications, and user interface changes. Based on this information and the developer's descriptions of the symptoms, Hypothesizer finds relevant hypotheses, analyzes the code to identify relevant evidence to test the hypothesis, and generates an investigation plan through a timeline view. This summarizes all evidence items related to the hypothesis, indicates whether the hypothesis is likely to be true by showing which evidence items were confirmed in the recording, and enables the developer to quickly check evidence in the recording by viewing code snippets for each evidence item. A randomized controlled experiment with 16 professional developers found that, compared to traditional debugging tools and techniques such as breakpoint debuggers and Stack Overflow, Hypothesizer dramatically improved the success rate of fixing defects by a factor of five and decreased the time to debug by a factor of three.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 126278
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126597
        }
      ]
    },
    {
      "id": 126790,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "PaperToPlace: Transforming Instruction Documents into Spatialized and Context-Aware Mixed Reality Experiences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606832"
        },
        "Preview": {
          "title": "PaperToPlace: Transforming Instruction Documents into Spatialized and Context-Aware Mixed Reality Experiences",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=AnMee1j98vo"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4510",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127770
      ],
      "eventIds": [],
      "abstract": "While paper instructions are one of the mainstream medium for sharing knowledge, consuming such instructions and translating them into activities are inefficient due to the lack of connectivity with physical environment. We present PaperToPlace, a novel workflow comprising an authoring pipeline, which allows the authors to rapidly transform and spatialize existing paper instructions into MR experience, and a consumption pipeline, which computationally place each instruction step at an optimal location that is easy to read and do not occlude key interaction areas. Our evaluations of the authoring pipeline with 12 participants demonstrated the usability of our workflow and the effectiveness of using a machine learning based approach to help extracting the spatial locations associated with each steps. A second within-subject study with another 12 participants demonstrates the merits of our consumption pipeline by reducing efforts of context switching, delivering the segmented instruction steps and offering the hands-free affordances.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "The Design Lab"
            }
          ],
          "personId": 126139
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 126028
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": " Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 126031
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 125939
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 126101
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "The Design Lab"
            }
          ],
          "personId": 126506
        }
      ]
    },
    {
      "id": 126791,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Mirrorverse: Live Tailoring of Video Conferencing Interfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606767"
        },
        "Preview": {
          "title": "Mirrorverse: Live Tailoring of Video Conferencing Interfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jBiG48rW6Ck"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9773",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127753
      ],
      "eventIds": [],
      "abstract": "How can we let users adapt video-based meetings as easily as they rearrange furniture in a physical meeting room? \r\nWe describe a design space for video conferencing systems that includes a five-step ``ladder of tailorability,'' from minor adjustments to live reprogramming of the interface. We then present Mirrorverse and show how it applies the principles of computational media to support live tailoring of video conferencing interfaces to accommodate highly diverse meeting situations. We present multiple use scenarios, including a virtual workshop, an online yoga class, and a stand-up team meeting to evaluate the approach and demonstrate its potential for new, remote meetings with fluid transitions across activities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126186
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 125936
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Computer Science, Aarhus University",
              "dsl": ""
            }
          ],
          "personId": 126445
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Inria",
              "dsl": "ExSitu"
            }
          ],
          "personId": 125930
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Universit√© Paris-Saclay",
              "dsl": ""
            }
          ],
          "personId": 126502
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126600
        }
      ]
    },
    {
      "id": 126792,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Turn-It-Up: Rendering Resistance for Knobs in Virtual Reality through Undetectable Pseudo-Haptics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606787"
        },
        "Preview": {
          "title": "Turn-It-Up: Rendering Resistance for Knobs in Virtual Reality through Undetectable Pseudo-Haptics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=p9WoxkGJ_10"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6017",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127752
      ],
      "eventIds": [],
      "abstract": "Rendering haptic feedback for interactions with virtual objects is an essential part of effective virtual reality experiences. In this work, we explore providing haptic feedback for rotational manipulations, e.g., through knobs. We propose the use of a Pseudo-Haptic technique alongside a physical proxy knob to simulate various physical resistances. In a psychophysical experiment with 20 participants, we found that designers can introduce unnoticeable offsets between real and virtual rotations of the knob, and we report the corresponding detection thresholds. Based on these, we present the Pseudo-Haptic Resistance technique to convey physical resistance while applying only unnoticeable pseudo-haptic manipulation. Additionally, we provide a first model of how C/D gains correspond to physical resistance perceived during object rotation, and outline how our results can be translated to other rotational manipulations. Finally, we present two example use cases that demonstrate the versatility and power of our approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126102
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126049
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universit√§t Hamburg",
              "dsl": ""
            }
          ],
          "personId": 126167
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore Management University",
              "dsl": ""
            }
          ],
          "personId": 126297
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126251
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126473
        }
      ]
    },
    {
      "id": 126801,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Video2Action: Reducing Human Interactions in Action Annotation of App Tutorial Videos",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606778"
        },
        "Preview": {
          "title": "Video2Action: Reducing Human Interactions in Action Annotation of App Tutorial Videos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=EGD4BgABlbw"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1910",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127753
      ],
      "eventIds": [],
      "abstract": "Tutorial videos of mobile apps have become a popular and compelling way for users to learn unfamiliar app features. To make the video accessible to the users, video creators always need to annotate the actions in the video, including what actions are performed and where to tap. However, this process can be time-consuming and labor-intensive. In this paper, we introduce a lightweight approach Video2Action, to automatically generate the action scenes and predict the action locations from the video by using image-processing and deep-learning methods. The automated experiments demonstrate the good performance of Video2Action in acquiring actions from the videos, and a user study shows the usefulness of our generated action cues in assisting video creators with action annotation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 125976
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 126306
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "ACT",
              "city": "ACTON",
              "institution": "CSIRO's Data61 adn Australian National University",
              "dsl": ""
            }
          ],
          "personId": 125856
        }
      ]
    },
    {
      "id": 126805,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Pant≈ìnna: Mouth Pose Estimation for AR/VR Headsets Using Low-Profile Antenna and Impedance Characteristic Sensing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606805"
        },
        "Preview": {
          "title": "Pant≈ìnna: Mouth Pose Estimation for AR/VR Headsets Using Low-Profile Antenna and Impedance Characteristic Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=NUlNB56vzNM"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1114",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127764
      ],
      "eventIds": [],
      "abstract": "Methods for faithfully capturing a user's holistic pose have immediate uses in AR/VR, ranging from multimodal input to expressive avatars. Although body-tracking has received the most attention, the mouth is also of particular importance, given that it is the channel for both speech and facial expression. In this work, we describe a new RF-based approach for capturing mouth pose using an antenna integrated into the underside of a VR/AR headset. Our approach side-steps privacy issues inherent in camera-based methods, while simultaneously supporting silent facial expressions that audio-based methods cannot. Further, compared to bio-sensing methods such as EMG and EIT, our method requires no contact with the wearer's body and can be fully self-contained in the headset, offering a high degree of physical robustness and user practicality. We detail our implementation along with results from two user studies, which show a mean 3D error of 2.6 mm for 11 mouth keypoints across worn sessions without re-calibration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 125973
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126334
        }
      ]
    },
    {
      "id": 126806,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Riffle: Reactive Relational State for Local-First Applications",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606801"
        },
        "Preview": {
          "title": "Riffle: Reactive Relational State for Local-First Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=EhOgP3ScW-s"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9089",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127763
      ],
      "eventIds": [],
      "abstract": "The reactive paradigm for developing user interfaces promises both simplicity and scalability, but existing frameworks usually compromise one for the other. We present Riffle, a reactive state management system that achieves both simplicity and scalability by managing the entire state of a web application in a client-side persistent relational database. Data transformations over the application state are defined in a graph of reactive relational queries, providing developers with a simple spreadsheet-like reactivity model. Domain state and UI state are unified within the same system, and efficient incremental query maintenance ensures the UI remains responsive. We present a formative case study of using Riffle to build a music management application with complex data and stringent performance requirements.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 125801
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Anthropic",
              "dsl": ""
            }
          ],
          "personId": 125970
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "N/A",
              "dsl": ""
            }
          ],
          "personId": 126588
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 126601
        }
      ]
    },
    {
      "id": 126807,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "SleeveIO: Modular and Reconfigurable Platform for Multimodal Wearable Haptic Feedback Interactions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606739"
        },
        "Preview": {
          "title": "SleeveIO: Modular and Reconfigurable Platform for Multimodal Wearable Haptic Feedback Interactions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Ckn11DinEQY"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1231",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127762
      ],
      "eventIds": [],
      "abstract": "SleeveIO is a modular and reconfigurable hardware platform for rapid prototyping of multimodal wearable haptic feedback interactions. SleeveIO features engineered machine-knitted sleeve and band substrates, and five categories of haptic feedback actuator modules including vibrotactors, bellows, muscles, suction/puffing cups, and quad-chamber actuators. A universal magnetic attachment mechanism unifies the different types of actuators, enabling countless multimodal haptic experiences involving combinations of different actuator types in different configurations. SleeveIO is compatible with a variety of hardware/software control platforms, such as FlowIO [42], which enables individual control of each haptic actuator and makes the system battery-powered and untethered. This paper presents the SleeveIO platform in detail along with replication resources, a novel generalized approach to making different types of haptic actuators modular and interoperable, new application possibilities enabled by SleeveIO, and a pilot assessment of the viability of the platform as a whole and each module individually.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 126149
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 126098
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 125779
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta",
              "dsl": ""
            }
          ],
          "personId": 126457
        }
      ]
    },
    {
      "id": 126808,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Soundify: Matching Sound Effects to Video",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606823"
        },
        "Preview": {
          "title": "Soundify: Matching Sound Effects to Video",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_tHG5rrtyY4"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7223",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127753
      ],
      "eventIds": [],
      "abstract": "In the art of video editing, sound helps add character to an object and immerse the viewer within a space. Through formative interviews with professional editors (N=10), we found that the task of adding sounds to video can be challenging. This paper presents Soundify, a system that assists editors in matching sounds to video. Given a video, Soundify identifies matching sounds, synchronizes the sounds to the video, and dynamically adjusts panning and volume to create spatial audio. In a human evaluation study (N=889), we show that Soundify is capable of matching sounds to video out-of-the-box for a diverse range of audio categories. In a within-subjects expert study (N=12), we demonstrate the usefulness of Soundify in helping video editors match sounds to video with lighter workload, reduced task completion time, and improved usability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126483
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Runway",
              "dsl": ""
            }
          ],
          "personId": 126415
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Runway",
              "dsl": ""
            }
          ],
          "personId": 126092
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Runway",
              "dsl": ""
            }
          ],
          "personId": 126486
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 125751
        }
      ]
    },
    {
      "id": 126809,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Sustainflatable: Harvesting, Storing and Utilizing Ambient Energy for Pneumatic Morphing Interfaces",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606721"
        },
        "Preview": {
          "title": "Sustainflatable: Harvesting, Storing and Utilizing Ambient Energy for Pneumatic Morphing Interfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Vha0ggtwdpA"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3780",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127756
      ],
      "eventIds": [],
      "abstract": "While the majority of pneumatic interfaces are powered and controlled by traditional electric pumps and valves, alternative sustainable energy-harnessing technology has been attracting attention. This paper presents a novel solution to this challenge with the development of the Sustainflatable system, a self-sustaining pneumatic system that can harvest renewable energy sources such as wind, water flow, moisture, and sunlight, convert the energy into compressed air, and store it for later use in a programmable and intelligent way. The system is completely electronic-free, incorporating customized energy harvesting pumps, storage units with variable volume-pressure characteristics, and tailored valves that operate autonomously. Additionally, the paper provides a design tool to guide the development of the system and includes several environmental applications to showcase its capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University ",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126509
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Lab"
            }
          ],
          "personId": 126470
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126042
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-computer Interaction Institute"
            }
          ],
          "personId": 126537
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 126452
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126193
        }
      ]
    },
    {
      "id": 126813,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "VRoxy: Wide-Area Collaboration From an Office Using a VR-Driven Robotic Proxy",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606743"
        },
        "Preview": {
          "title": "VRoxy: Wide-Area Collaboration From an Office Using a VR-Driven Robotic Proxy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8AEBnX7lB-A"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1109",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127761
      ],
      "eventIds": [],
      "abstract": "Recent research in robotic proxies has demonstrated that one can automatically reproduce many non-verbal cues important in co-located collaboration. However, they often require a symmetrical hardware setup in each location. We present the VRoxy system, designed to enable access to remote spaces through a robotic embodiment, using a VR headset in a much smaller space, such as a personal office. VRoxy maps small movements in VR space to larger movements in the physical space of the robot, allowing the user to navigate large physical spaces easily. Using VRoxy, the VR user can quickly explore and navigate in a low-fidelity rendering of the remote space. Upon the robot's arrival, the system uses the feed of a 360 camera to support real-time interactions. The system also facilitates various interaction modalities by rendering the micro-mobility around shared spaces, head and facial animations, and pointing gestures on the proxy. We demonstrate how our system can accommodate mapping multiple physical locations onto a unified virtual space. In a formative study, users could complete a design decision task where they navigated and collaborated in a complex 7.5m x 5m layout using a 3m x 2m VR space. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 126216
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 126605
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence ",
              "institution": "Brown University",
              "dsl": "Computer Science/Brown University/Human-Computer Interaction Laboratory"
            }
          ],
          "personId": 126437
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 126562
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 125893
        }
      ]
    },
    {
      "id": 126814,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Wakey-Wakey: Animate Text by Mimicking Characters in a GIF",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606813"
        },
        "Preview": {
          "title": "Wakey-Wakey: Animate Text by Mimicking Characters in a GIF",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0n-a_2AFHbA"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2679",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127767
      ],
      "eventIds": [],
      "abstract": "With appealing visual effects, kinetic typography (animated text) has prevailed in movies, advertisements, and social media. However, it remains challenging and time-consuming to craft its animation scheme. We propose an automatic framework to transfer the animation scheme of a rigid body on a given meme GIF to text in vector format. First, the trajectories of key points on the GIF anchor are extracted and mapped to the text's control points based on local affine transformation. Then the temporal positions of the control points are optimized to maintain the text topology. We also develop an authoring tool that allows intuitive human control in the generation process. A questionnaire study provides evidence that the output results are aesthetically pleasing and well preserve the animation patterns in the original GIF, where participants were impressed by a similar emotional semantics of the original GIF. In addition, we evaluate the utility and effectiveness of our approach through a workshop with general users and designers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Fudan University",
              "dsl": "School of Data Science"
            }
          ],
          "personId": 126138
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126398
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Fudan University",
              "dsl": ""
            }
          ],
          "personId": 125769
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Microsoft Research Asia",
              "dsl": ""
            }
          ],
          "personId": 126421
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126516
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Fudan University",
              "dsl": "School of Data Science"
            }
          ],
          "personId": 126165
        }
      ]
    },
    {
      "id": 126816,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Parametric Haptics: Versatile Geometry-based Tactile Feedback Devices",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606766"
        },
        "Preview": {
          "title": "Parametric Haptics: Versatile Geometry-based Tactile Feedback Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uhP1v3ekZLM"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9510",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127762
      ],
      "eventIds": [],
      "abstract": "Haptic feedback is important for immersive, assistive, or multimodal interfaces, but engineering devices that generalize across applications is notoriously difficult. To address the issue of versatility, we propose Parametric Haptics, geometry-based tactile feedback devices that are customizable to render a variety of tactile sensations. To achieve this, we integrate the actuation mechanism with the tactor geometry into passive 3D printable patches, which are then connected to a generic wearable actuation interface consisting of micro gear motors. The key benefit of our approach is that the 3D-printed patches are modular, can consist of varying numbers and shapes of tactors, and that the tactors can be grouped and moved by our actuation geometry over large areas of the skin. The patches are soft, thin, conformable, and easy to customize to different use cases, thus potentially enabling a large design space of diverse tactile sensations. \r\nIn our user study, we investigate the mapping between geometry parameters of our haptic patches and users‚Äô tactile perceptions. Results indicate a good agreement between our parameters and the reported sensations, showing initial evidence that our haptic patches can produce a wide range of sensations for diverse use scenarios. We demonstrate the utility of our approach with wearable prototypes in immersive Virtual Reality (VR) scenarios, embedded into wearable objects such as glasses, and as wearable navigation and notification interfaces. We support designing such patches with a design tool in Rhino.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 125946
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 125860
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126235
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126086
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126254
        }
      ]
    },
    {
      "id": 126817,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Going Incognito in the Metaverse: Achieving Theoretically Optimal Privacy-Usability Tradeoffs in VR",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606754"
        },
        "Preview": {
          "title": "Going Incognito in the Metaverse: Achieving Theoretically Optimal Privacy-Usability Tradeoffs in VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=gZl1zpuyfqA"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4971",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127761
      ],
      "eventIds": [],
      "abstract": "Virtual reality (VR) telepresence applications and the so-called \"metaverse\" promise to be the next major medium of human-computer interaction. However, with recent studies demonstrating the ease at which VR users can be profiled and deanonymized, metaverse platforms carry many of the privacy risks of the conventional internet (and more) while at present offering few of the defensive utilities that users are accustomed to having access to. To remedy this, we present the first known method of implementing an \"incognito mode\" for VR. Our technique leverages local Œµ-differential privacy to quantifiably obscure sensitive user data attributes, with a focus on intelligently adding noise when and where it is needed most to maximize privacy while minimizing usability impact. Our system is capable of flexibly adapting to the unique needs of each VR application to further optimize this trade-off. We implement our solution as a universal Unity (C#) plugin that we then evaluate using several popular VR applications. Upon faithfully replicating the most well-known VR privacy attack studies, we show a significant degradation of attacker capabilities when using our solution.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Department of Electrical Engineering and Computer Sciences"
            }
          ],
          "personId": 126385
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126114
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Department of Electrical Engineering and Computer Sciences"
            }
          ],
          "personId": 126226
        }
      ]
    },
    {
      "id": 126818,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "PColorizor: Re-coloring Ancient Chinese Paintings with Ideorealm-congruent Poems",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606814"
        },
        "Preview": {
          "title": "PColorizor: Re-coloring Ancient Chinese Paintings with Ideorealm-congruent Poems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=iXh3DNaxh8M"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7334",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127759
      ],
      "eventIds": [],
      "abstract": "Color restoration of ancient Chinese paintings plays a significant role in Chinese culture protection and inheritance. However, traditional color restoration is challenging and time-consuming because it requires professional restorers to conduct detailed literature reviews on numerous paintings for reference colors. After that, they have to fill in the inferred colors on the painting manually. In this paper, we present PColorizor, an interactive system that integrates advanced deep-learning models and novel visualizations to ease the difficulties of color restoration. PColorizor is established on the principle of poem-painting congruence. Given a color-fading painting, we employ both explicit and implicit color guidance implied by ideorealm-congruent poems to associate reference paintings. To enable quick navigation of color schemes extracted from the reference paintings, we introduce a novel visualization based on a mountain metaphor that shows color distribution overtime at the ideorealm and imagery levels. Moreover, we demonstrate the ideorealm understood by deep learning models through intuitive visualizations to bridge the communication gap between human restorers and deep learning models. We also adopt intelligent color-filling techniques to accelerate manual color restoration further. To evaluate PColorizor, we collaborate with domain experts to conduct two case studies to collect their feedback. The results suggest that PColorizor could be beneficial in enabling the effective restoration of color-fading paintings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Art and Archaeology"
            }
          ],
          "personId": 125884
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "State Key Lab of CAD&CG"
            }
          ],
          "personId": 126141
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Art and Archeaology"
            }
          ],
          "personId": 125810
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Art and Archaeology"
            }
          ],
          "personId": 125851
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Art and Archaeology"
            }
          ],
          "personId": 125981
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "State Key Lab of CAD&CG"
            }
          ],
          "personId": 126062
        }
      ]
    },
    {
      "id": 126819,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "HRTF Estimation in the Wild",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606782"
        },
        "Preview": {
          "title": "HRTF Estimation in the Wild",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Bg4-XVReudQ"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9866",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127765
      ],
      "eventIds": [],
      "abstract": "Head Related Transfer Functions (HRTFs) play a crucial role in\r\ncreating immersive spatial audio experiences. However, HRTFs dif-\r\nfer significantly from person to person, and traditional methods\r\nfor estimating personalized HRTFs are expensive, time-consuming,\r\nand require specialized equipment. We imagine a world where your\r\npersonalized HRTF can be determined by capturing data through\r\nearbuds in everyday environments. In this paper, we propose a\r\nnovel approach for deriving personalized HRTFs that only relies\r\non in-the-wild binaural recordings and head tracking data. By ana-\r\nlyzing how sounds change as the user rotates their head through\r\ndifferent environments with different noise sources, we can accu-\r\nrately estimate their personalized HRTF. Our results show that our\r\npredicted HRTFs closely match ground-truth HRTFs measured in\r\nan anechoic chamber. Furthermore, listening studies demonstrate\r\nthat our personalized HRTFs significantly improve sound local-\r\nization and reduce front-back confusion in virtual environments.\r\nOur approach offers an efficient and accessible method for deriving\r\npersonalized HRTFs and has the potential to greatly improve spatial\r\naudio experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 125897
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle ",
              "institution": "University of Washington ",
              "dsl": ""
            }
          ],
          "personId": 126283
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 126298
        }
      ]
    },
    {
      "id": 126829,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Transferable Microgestures Across Hand Posture and Location Constraints: Leveraging the Middle, Ring, and Pinky Fingers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606713"
        },
        "Preview": {
          "title": "Transferable Microgestures Across Hand Posture and Location Constraints: Leveraging the Middle, Ring, and Pinky Fingers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hbBDdf72m7E"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4965",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127768
      ],
      "eventIds": [],
      "abstract": "Microgestures can enable auxiliary input when the hands are occupied. Although prior work has evaluated the comfort of microgestures performed by the index finger and thumb, these gestures cannot be performed while the fingers are constrained by specific hand locations or postures. As the hand can be freely positioned with no primary posture, partially constrained while forming a pose, or highly constrained while grasping an object at a specific location, we leverage the middle, ring, and pinky fingers to provide additional opportunities for auxiliary input across varying levels of hand constraints. A design space and applications demonstrate how such microgestures can transfer across hand location and posture constraints. An online study evaluated their comfort and effort and a lab study evaluated their use for task-specific microinteractions. The results revealed that many middle finger microgestures were comfortable, and microgestures performed while forming a pose were preferred over baseline techniques.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 126351
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125957
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Menlo Park",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 126493
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 125947
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs"
            }
          ],
          "personId": 125874
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 126195
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 126356
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 126400
        }
      ]
    },
    {
      "id": 126830,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Reprogrammable Digital Metamaterials for Interactive Devices",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606752"
        },
        "Preview": {
          "title": "Reprogrammable Digital Metamaterials for Interactive Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=64adwqDYd_w"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9067",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127760
      ],
      "eventIds": [],
      "abstract": "We present digital mechanical metamaterials that enable multiple computation loops and reprogrammable logic functions, making a significant step towards passive yet interactive devices. Our materials consist of many cells that transmit signals using an embedded bistable spring. When triggered, the bistable spring displaces and triggers the next cell. We integrate a recharging mechanism to recharge the bistable springs, enabling multiple computation rounds. Between the iterations, we enable reprogramming the logic functions after fabrication. We demonstrate that such materials can trigger a simple controlled actuation anywhere in the material to change the local shape, texture, stiffness, and display. This enables large-scale interactive and functional materials with no or a small number of external actuators. We showcase the capabilities of our system with various examples: a haptic floor with tunable stiffness for different VR scenarios, a display with easy-to-reconfigure messages after fabrication, or a tactile notification integrated into users‚Äô desktops.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Computer Science and Technology"
            }
          ],
          "personId": 126035
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 125757
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 125945
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 125878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126254
        }
      ]
    },
    {
      "id": 126831,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Telextiles: End-to-end Remote Transmission of Fabric Tactile Sensation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606764"
        },
        "Preview": {
          "title": "Telextiles: End-to-end Remote Transmission of Fabric Tactile Sensation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0y74v7WMPbE"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6111",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127762
      ],
      "eventIds": [],
      "abstract": "The tactile sensation of textiles is critical in determining the comfort of clothing. For remote use, such as online shopping, users cannot physically touch the textile of clothes, making it difficult to evaluate its tactile sensation. Tactile sensing and actuation devices are required to transmit the tactile sensation of textiles. The sensing device needs to recognize different garments, even with hand-held sensors. In addition, the existing actuation device can only present a limited number of known patterns and cannot transmit unknown tactile sensations of textiles.\r\nTo address these issues, we propose Telextiles, an interface that can remotely transmit tactile sensations of textiles by creating a latent space that reflects the proximity of textiles through contrastive self-supervised learning. We confirm that textiles with similar tactile features are located close to each other in the latent space through a two-dimensional plot.\r\nWe then compress the latent features for known textile samples into the 1D distance and apply the 16 textile samples to the rollers in the order of the distance. The roller is rotated to select the textile with the closest feature if an unknown textile is detected.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Interdisciplinary Information Studies/ Rekimoto Lab (Human Augmentation Lab)"
            }
          ],
          "personId": 125805
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 126625
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Rekimoto Lab"
            }
          ],
          "personId": 125928
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 126078
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 125780
        }
      ]
    },
    {
      "id": 126832,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "InteractionAdapt: Interaction-driven Workspace Adaptation in Situated Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606717"
        },
        "Preview": {
          "title": "InteractionAdapt: Interaction-driven Workspace Adaptation in Situated Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2kKQCIHIz6s"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-6107",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127769
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) has the potential to transform how we work: it enables flexible and personalized workspaces beyond what is possible in the physical world. However, while most VR applications are designed to operate in a single empty physical space, work environments are often populated with real-world objects and increasingly diverse due to the growing amount of work in mobile scenarios. In this paper, we present InteractionAdapt, an optimization-based method for adapting VR workspaces for situated use in varying everyday physical environments, allowing VR users to transition between real-world settings while retaining most of their personalized VR environment for efficient interaction to ensure temporal consistency and visibility. InteractionAdapt leverages physical affordances in the real world to optimize UI elements for the respectively most suitable input technique, including on-surface touch, mid-air touch and pinch, and cursor control. Our optimization term thereby models the trade-off across these interaction techniques based on experimental findings of 3D interaction in situated physical environments. Our two evaluations of InteractionAdapt in a selection task and a travel planning task established its capability of supporting efficient interaction, during which it produced adapted layouts that participants preferred to several baselines. We further showcase the versatility of our approach through applications that cover a wide range of use cases.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Z√ºrich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126081
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126003
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126318
        }
      ]
    },
    {
      "id": 126841,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Structured Light Speckle: Joint egocentric depth estimation and low-latency contact detection via remote vibrometry",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606749"
        },
        "Preview": {
          "title": "Structured Light Speckle: Joint egocentric depth estimation and low-latency contact detection via remote vibrometry",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=yg1CnV8K0Iw"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4838",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127755
      ],
      "eventIds": [],
      "abstract": "Despite advancements in egocentric hand tracking using head-mounted cameras, contact detection with real-world objects remains challenging, particularly for the quick motions often performed during interaction in Mixed Reality. In this paper, we introduce a novel method for detecting touch on discovered physical surfaces purely from an egocentric perspective using optical sensing. We leverage structured laser light to detect real-world surfaces from the disparity of reflections in real-time and, at the same time, extract a time series of remote vibrometry sensations from laser speckle motions. The pattern caused by structured laser light reflections enables us to simultaneously sample the mechanical vibrations that propagate through the user's hand and the surface upon touch.\r\n\r\nWe integrated Structured Light Speckle into TapLight, a prototype system that is a simple add-on to Mixed Reality headsets. In our evaluation with a Quest 2, TapLight---while moving---reliably detected horizontal and vertical surfaces across a range of surface materials. TapLight also reliably detected rapid touch contact and robustly discarded other hand motions to prevent triggering spurious input events. Despite the remote sensing principle of Structured Light Speckle, our method achieved a latency for event detection in realistic settings that matches body-worn inertial sensing without needing such additional instrumentation. We conclude with a series of VR demonstrations for situated interaction that leverage the quick touch interaction TapLight supports.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126504
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 125809
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126274
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Z√ºrich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126318
        }
      ]
    },
    {
      "id": 126844,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "CriTrainer: An Adaptive Training Tool for Critical Paper Reading",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606816"
        },
        "Preview": {
          "title": "CriTrainer: An Adaptive Training Tool for Critical Paper Reading",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=m3MKh457m8A"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4038",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127758
      ],
      "eventIds": [],
      "abstract": "Learning to read scientific papers critically, which requires first grasping their main ideas and then raising critical thoughts, is important yet challenging for novice researchers. The traditional ways to develop critical paper reading (CPR) skills, e.g., checking general tutorials or taking reading courses, often can not provide individuals with adaptive and accessible support. In this paper, we first derive user requirements of a CPR training tool based on literature and a survey study (N=52). Then, we develop CriTrainer, an interactive tool for CPR training. It leverages text summarization techniques to train readers‚Äô skills in grasping the paper‚Äôs main ideas. It further utilizes template-based generated questions to help them learn how to raise critical thoughts. A mixed-design study (N=24) shows that compared to a baseline tool with general CPR guidance, students trained by CriTrainer perform better in independently raising critical thinking questions on a new paper. We conclude with design considerations for CPR training tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 125965
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126518
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126093
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126075
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 126322
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 126222
        }
      ]
    },
    {
      "id": 126845,
      "typeId": 13094,
      "durationOverride": 13,
      "title": " AirTied: Automatic Personal Fabrication of Truss Structures",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606820"
        },
        "Preview": {
          "title": "AirTied: Automatic Personal Fabrication of Truss Structures",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=old54MtTius"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8520",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127760
      ],
      "eventIds": [],
      "abstract": "We present AirTied, a device that fabricates truss structures in a fully automatic fashion. AirTied achieves this by un-rolling a 20cm-wide inflatable plastic tube and tying nodes into it. AirTied creates nodes by holding onto a segment of tube, stacking additional tube segments on top of it, tying them up, and releasing the result. The resulting structures are material efficient and light as well as sturdy, as we demonstrate by creating a 6m-tower. Unlike the prior art, AirTied requires no scaffolding and no building blocks, bringing automated truss construction into the reach of personal fabrication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 126418
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 126234
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126406
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126423
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso-Plattner-Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 126484
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 125922
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126217
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126238
        }
      ]
    },
    {
      "id": 126846,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "CubeSense++: Smart Environment Sensing with Interaction-Powered Corner Reflector Mechanisms",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606744"
        },
        "Preview": {
          "title": "CubeSense++: Smart Environment Sensing with Interaction-Powered Corner Reflector Mechanisms",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JQbdwJAUriI"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4158",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127764
      ],
      "eventIds": [],
      "abstract": "Smart environment sensing provides valuable contextual informa- tion by detecting occurrences of events such as human activities and changes of object status, enabling computers to collect per- sonal and environmental informatics to perform timely responses to user‚Äôs needs. Conventional approaches either rely on tags that re- quire batteries and frequent maintenance, or have limited detection capabilities bounded by only a few coarsely predefined activities. In response, this paper explores corner reflector mechanisms that encode user interactions with everyday objects into structured responses to millimeter wave radar, which has the potential for integration into smart environment entities such as speakers, light bulbs, thermostats, and autonomous vehicles. We presented the design space of 3D printed reflectors and gear mechanisms, which are low-cost, durable, battery-free, and can retrofit to a wide array of objects. These mechanisms convert the kinetic energy from user interactions into rotational motions of corner reflectors which we computationally designed with a genetic algorithm. We built an end-to-end radar detection pipeline to recognize fine-grained activity information such as state, direction, rate, count, and usage based on the characteristics of radar responses. We conducted stud- ies for multiple instrumented objects in both indoor and outdoor environments, with promising results demonstrating the feasibility of the proposed approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 126055
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Mechanical and Aerospace Engineering"
            }
          ],
          "personId": 126163
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 125915
        }
      ]
    },
    {
      "id": 126847,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Interactive Benefits from Switching Electrical to Magnetic Muscle Stimulation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606812"
        },
        "Preview": {
          "title": "Interactive Benefits from Switching Electrical to Magnetic Muscle Stimulation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8h3GCUuOhAs"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4950",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127762
      ],
      "eventIds": [],
      "abstract": "Electrical muscle stimulation (EMS) became a popular method for force-feedback without mechanical-actuators. While much has been written about the advantages of EMS, not much work has investigated circumventing its key limitations: (1) as impulses traverse the skin, they cause an uncomfortable ‚Äútingling‚Äù; (2) impulses are delivered via gelled-electrodes, which not only require direct skin contact (must be worn under clothes); but, also (3) dry up after a few hours. To tackle these, we explore switching from electrical to magnetic muscle stimulation (MMS), via electromagnetic fields generated by coils. The first advantage is that MMS coils do not require direct skin contact and can actuate up to 5 cm away (Study#1)‚Äîthis enables applications not possible with EMS, such as stimulation over the clothes and without ever replacing electrodes. Second, and more important, MMS results in ~50 % less discomfort caused by tingling than EMS (Study#2). We found that reducing this tingling discomfort has two downstream effects for interactive systems: (1) participants rated MMS force-feedback as more realistic than that of EMS (Study#3); and (2) participants could more accurately perceive the pose actuated by the interactive system (Study#4). Finally, we demonstrated applications where our proposed switch from EMS to MMS improves user experience, including for VR feedback, gaming, and pose-control.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126143
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126002
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        }
      ]
    },
    {
      "id": 126854,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "SPEERLoom: An Open-Source Loom Kit for Interdisciplinary Engagement in Math, Engineering, and Textiles",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606724"
        },
        "Preview": {
          "title": "SPEERLoom: An Open-Source Loom Kit for Interdisciplinary Engagement in Math, Engineering, and Textiles",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jEAPo4e6N9E"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-8075",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127766
      ],
      "eventIds": [],
      "abstract": "Weaving is a fabrication process that is grounded in mathematics and engineering: from the binary, matrix-like nature of the pattern drafts weavers have used for centuries, to the punch card programming of the first Jacquard looms. This intersection of disciplines provides an opportunity to ground abstract mathematical concepts in a concrete and embodied art, viewing this textile art through the lens of engineering. Currently, available looms are not optimized to take advantage of this opportunity to increase mathematics learning by providing hands-on interdisciplinary learning in collegiate classrooms. In this work, we present SPEERLoom: an open-source, robotic Jacquard loom kit designed to be a tool for interweaving cloth fabrication, mathematics, and engineering to support interdisciplinary learning in the classroom. We discuss the design requirements and subsequent design of SPEERLoom. We also present the results of a pilot study in a post-secondary class finding that SPEERLoom supports hands-on, interdisciplinary learning of math, engineering, and textiles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 126176
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126496
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine ",
              "dsl": "School of Information and Computer Sciences"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California Irvine ",
              "dsl": "Connected Learning Lab "
            }
          ],
          "personId": 125990
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "UCI",
              "dsl": ""
            }
          ],
          "personId": 126378
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies Institute"
            }
          ],
          "personId": 126370
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "School of Education / Creativity Labs"
            }
          ],
          "personId": 126325
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 126620
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon",
              "dsl": ""
            }
          ],
          "personId": 126577
        }
      ]
    },
    {
      "id": 126856,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606776"
        },
        "Preview": {
          "title": "PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PpPaxbxV5BI"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3851",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127753
      ],
      "eventIds": [],
      "abstract": "Audio-visual learning seeks to enhance the computer‚Äôs multi-modal perception leveraging the correlation between the auditory and visual modalities. Despite their many useful downstream tasks, such as video retrieval, AR/VR, and accessibility, the performance and adoption of existing audio-visual models have been impeded by the availability of high quality datasets. Annotating audio-visual datasets is laborious, expensive, and time consuming. To address this challenge, we designed and developed an efficient audio visual annotation tool called Peanut. Peanut‚Äôs human-AI collaborative pipeline separates the multi-modal task into two single-modal tasks, and utilizes state-of-the-art object detection and sound-tagging models to reduce the annotators‚Äô effort to process each frame and the number of manually-annotated frames needed. A within-subject user study with 20 participants found that Peanut can significantly accelerate the audio-visual data annotation process while maintaining high annotation accuracy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126389
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126113
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 125828
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126424
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 125993
        }
      ]
    },
    {
      "id": 126857,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "XCreation: A Graph-Based Crossmodal Generative Creativity Support Tool",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606826"
        },
        "Preview": {
          "title": "XCreation: A Graph-Based Crossmodal Generative Creativity Support Tool",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vfQvrATPjt0"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-3730",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127759
      ],
      "eventIds": [],
      "abstract": "Creativity Support Tools (CSTs) aid in the efficient and effective composition of creative content, such as picture books. However, many existing CSTs only allow for mono-modal creation, whereas previous studies have become theoretically and technically mature to support multi-modal innovative creations. To overcome this limitation, we introduce XCreation, a novel CST that leverages generative AI to support cross-modal storybook creation. Nevertheless, directly deploying AI models to CSTs can still be problematic as they are mostly black-box architectures that are not comprehensible to human users. Therefore, we integrate an interpretable entity-relation graph to intuitively represent picture elements and their relations, improving the usability of the underlying generative structures. Our between-subject user study demonstrates that XCreation supports continuous plot creation with increased creativity, controllability, usability, and interpretability. XCreation is applicable to various scenarios, including interactive storytelling and picture book creation, thanks to its multimodal nature.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 126284
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 126213
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "School of Computing"
            }
          ],
          "personId": 126255
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 125849
        }
      ]
    },
    {
      "id": 126858,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606773"
        },
        "Preview": {
          "title": "CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8I1yXNRcm54"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7421",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127761
      ],
      "eventIds": [],
      "abstract": "Despite the advances and ubiquity of digital communication media such as videoconferencing and virtual reality, they remain oblivious to the rich intentions expressed by users. Beyond transmitting audio, videos, and messages, we envision digital communication media as proactive facilitators that can provide unobtrusive assistance to enhance communication and collaboration. Informed by the results of a formative study, we propose three key design concepts to explore the systematic integration of intelligence into communication and collaboration, including the panel substrate, language-based intent recognition, and lightweight interaction techniques. We developed CrossTalk, a videoconferencing system that instantiates these concepts, which was found to enable a more fluid and flexible communication and collaboration experience. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126359
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 126546
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Manipal",
              "institution": "Manipal Institute of Technology",
              "dsl": "Department of Media Technology"
            }
          ],
          "personId": 125953
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126182
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 126150
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": ""
            }
          ],
          "personId": 126179
        }
      ]
    },
    {
      "id": 126859,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "BrushLens: Hardware Interaction Proxies for Accessible Touchscreen Interface Actuation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606730"
        },
        "Preview": {
          "title": "BrushLens: Hardware Interaction Proxies for Accessible Touchscreen Interface Actuation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VQ7FKdFF3PI"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4392",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127757
      ],
      "eventIds": [],
      "abstract": "Touchscreen devices, designed with an assumed range of user abilities and interaction patterns, often present challenges for individuals with diverse abilities to operate independently. Prior efforts to improve accessibility through tools or algorithms necessitated alterations to touchscreen hardware or software, making them inapplicable for the large number of existing legacy devices. In this paper, we introduce BrushLens, a hardware interaction proxy that performs physical interactions on behalf of users while allowing them to continue utilizing accessible interfaces, such as screenreaders and assistive touch on smartphones, for interface exploration and command input. BrushLens maintains an interface model for accurate target localization and utilizes exchangeable actuators for physical actuation across a variety of device types, effectively reducing user workload and minimizing the risk of mistouch. Our evaluations reveal that BrushLens lowers the mistouch rate and empowers visually and motor impaired users to interact with otherwise inaccessible physical touchscreens more effectively.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126617
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Electrical Engineering and Computer Science"
            }
          ],
          "personId": 126315
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "The University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 126589
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan - Ann Arbor",
              "dsl": "Robotics"
            }
          ],
          "personId": 126558
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 126041
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126260
        }
      ]
    },
    {
      "id": 126872,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Unveiling the Tricks: Automated Detection of Dark Patterns in Mobile Applications",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606783"
        },
        "Preview": {
          "title": "Unveiling the Tricks: Automated Detection of Dark Patterns in Mobile Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=19krNfZugtU"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-5905",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127769
      ],
      "eventIds": [],
      "abstract": "Mobile apps bring us many conveniences, such as online shopping and communication, but some use malicious designs called dark patterns to trick users into doing things that are not in their best interest. Many works have been done to summarize the taxonomy of these patterns and some have tried to mitigate the problems through various techniques. However, these techniques are either time-consuming, not generalisable or limited to specific patterns. To address these issues, we propose UIGuard, a knowledge-driven system that utilizes computer vision and natural language pattern matching to automatically detect a wide range of dark patterns in mobile UIs. Our system relieves the need for manually creating rules for each new UI/app and covers more types with superior performance. In detail, we integrated existing taxonomies into a consistent one, conducted a characteristic analysis and distilled knowledge from real-world examples and the  taxonomy. Our UIGuard consists of two components, Property Extraction and Knowledge-Driven Dark Pattern Checker. We collected the first dark pattern dataset, which contains 4,999 benign UIs and 1,353 malicious UIs of 1,660 instances spanning 1,023 mobile apps. Our system achieves a superior performance in detecting dark patterns (micro averages: 0.82 in precision, 0.77 in recall, 0.79 in F1 score). A user study involving 58 participants further showed that UIGuard significantly increases users' knowledge of dark patterns. We demonstrated potential use cases of our work, which can benefit different stakeholders, and serve as a training tool for raising awareness of dark patterns",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "CSIRO's Data61",
              "dsl": ""
            }
          ],
          "personId": 125794
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW(AUS)",
              "city": "Sydney",
              "institution": "CSIRO's Data61",
              "dsl": ""
            }
          ],
          "personId": 126084
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 125976
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "ACT",
              "city": "ACTON",
              "institution": "CSIRO's Data61 adn Australian National University",
              "dsl": ""
            }
          ],
          "personId": 125856
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO",
              "dsl": "Data61"
            }
          ],
          "personId": 126066
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Eveleigh",
              "institution": "CSIRO",
              "dsl": "Data61"
            }
          ],
          "personId": 125753
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 126306
        }
      ]
    },
    {
      "id": 126877,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "ShadowTouch: Enabling Free-Form Touch-Based Hand-to-Surface Interaction with Wrist-Mounted Illuminant by Shadow Projection",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606785"
        },
        "Preview": {
          "title": "ShadowTouch: Enabling Free-Form Touch-Based Hand-to-Surface Interaction with Wrist-Mounted Illuminant by Shadow Projection",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=NVEWOuXsopM"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-1543",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127755
      ],
      "eventIds": [],
      "abstract": "We present ShadowTouch, a novel sensing method to recognize the subtle hand-to-surface touch state for independent fingers based on optical auxiliary. ShadowTouch mounts a forward-facing light source on the user's wrist to construct shadows on the surface in front of the fingers when the corresponding fingers are close to the surface. With such an optical design, the subtle vertical movements of near-surface fingers are magnified and turned to shadow features cast on the surface, which are recognizable for computer vision algorithms. To efficiently recognize the touch state of each finger, we devised a two-stage CNN-based algorithm that first extracted all the fingertip regions from each frame and then classified the touch state of each region from the cropped consecutive frames. Evaluations showed our touch state detection algorithm achieved a recognition accuracy of 99.1% and an F-1 score of 96.8% in the leave-one-out cross-user evaluation setting. We further outlined the hand-to-surface interaction space enabled by ShadowTouch's sensing capability from the aspects of touch-based interaction, stroke-based interaction, and out-of-surface information and developed four application prototypes to showcase ShadowTouch's interaction potential. The usability evaluation study showed the advantages of ShadowTouch over threshold-based techniques in aspects of lower mental demand, lower effort, lower frustration, more willing to use, easier to use, better integrity, and higher confidence.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            },
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 126085
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 125933
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong SAR",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "IIP (Computational Media and Arts)"
            },
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong SAR",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "IIP (Computational Media and Arts)"
            }
          ],
          "personId": 126505
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 126530
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": "Computational Media and Arts Thrust"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong SAR",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 125845
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 126236
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 125878
        }
      ]
    },
    {
      "id": 126878,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Constraint-Driven Robotic Surfaces, at Human-Scale",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606740"
        },
        "Preview": {
          "title": "Constraint-Driven Robotic Surfaces, at Human-Scale",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uWH9dYtBiAA"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-2750",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127760
      ],
      "eventIds": [],
      "abstract": "Robotic surfaces, whose form and function are under computational control, offer exciting new possibilities for environments that can be customized to fit user-specific needs. When these surfaces can be reprogrammed, a once-static structure can be repurposed to serve multiple different roles over time. In this paper, we introduce such a system. This is an architectural-scale robotic surface, which is able to begin in a neutral state, assume a desired functional shape, and later return to its neutral (flat) position. The surface can then assume a completely different functional shape, all under program control. \r\n\r\nThough designed for large-scale applications, our surface uses small, power-efficient constraints to reconfigure itself dynamically. The driving actuation force, instead of being positioned at each \"joint\" of the structure, is relocated to outer edges of the surface. Within the work presented here, we illustrate the design and implementation of such a surface, showcase a number of human-scale example functional forms that can be achieved (such as dynamic furniture), and present technical evaluations of the results.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126632
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction "
            }
          ],
          "personId": 125902
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126220
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 125797
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126254
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126585
        }
      ]
    },
    {
      "id": 126879,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Neighbor-Environment Observer: An Intelligent Agent for Immersive Working Companionship",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606728"
        },
        "Preview": {
          "title": "Neighbor-Environment Observer: An Intelligent Agent for Immersive Working Companionship",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=O6P9mvgJjFY"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-4379",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127769
      ],
      "eventIds": [],
      "abstract": "Human-computer symbiosis is a crucial direction for the development of artificial intelligence. \r\nAs intelligent systems become increasingly prevalent in our work and personal lives, it is important to develop strategies to support users across physical and virtual environments.\r\nWhile technological advances in personal digital devices, such as personal computers and virtual reality devices, can provide immersive experiences, they can also disrupt users' awareness of their surroundings and enhance the frustration caused by disturbances.\r\nIn this paper, we propose a joint observation strategy for artificial agents to support users across virtual and physical environments.\r\nWe introduce a prototype system, neighbor-environment observer (NEO), that utilizes non-invasive sensors to assist users in dealing with disruptions to their immersive experience.\r\nSystem experiments evaluate NEO from different perspectives and demonstrate the effectiveness of the joint observation strategy.\r\nA user study is conducted to evaluate its usability.\r\nThe results show that NEO could lessen users' workload with the learned user preference.\r\nWe suggest that the proposed strategy can be applied to various smart home scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute for General Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 125844
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute for General Artificial Intellgence",
              "dsl": ""
            }
          ],
          "personId": 126419
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute for General Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 126327
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute for General Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 125882
        }
      ]
    },
    {
      "id": 126880,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "SUPREYES: SUPer Resolutin for EYES Using Implicit Neural Representation Learning",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606780"
        },
        "Preview": {
          "title": "SUPREYES: SUPer Resolutin for EYES Using Implicit Neural Representation Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=fJY3BPvBiKI"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7530",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127764
      ],
      "eventIds": [],
      "abstract": "We introduce SUPREYES ‚Äì a novel self-supervised method to increase the spatio-temporal resolution of gaze data recorded using low(er)-resolution eye trackers. Despite continuing advances in eye tracking technology, the vast majority of current eye trackers ‚Äì particularly mobile ones and those integrated into mobile devices ‚Äì suffer from low-resolution gaze data, thus fundamentally limiting their practical usefulness. SUPREYES learns a continuous implicit neural representation from low-resolution gaze data to up-sample the gaze data to arbitrary resolutions. We compare our method with commonly used interpolation methods on arbitrary scale super-resolution and demonstrate that SUPREYES outperforms these baselines by a significant margin. We also test on the sample downstream task of gaze-based user identification and show that our method improves the performance of original low-resolution gaze data and outperforms other baselines. These results are promising as they open up a new direction for increasing eye tracking fidelity as well as enabling new gaze-based applications without the need for new eye tracking equipment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "Institute for Visualisation and Interactive Systems"
            }
          ],
          "personId": 126607
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "Institute for Visualisation and Interactive Systems"
            }
          ],
          "personId": 125785
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "Institute for Visualisation and Interactive Systems"
            }
          ],
          "personId": 125764
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "Institute for Visualisation and Interactive Systems"
            }
          ],
          "personId": 126312
        }
      ]
    },
    {
      "id": 126881,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Papeos: Augmenting Research Papers with Talk Videos",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606770"
        },
        "Preview": {
          "title": "Papeos: Augmenting Research Papers with Talk Videos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Y_IdUW1lrF0"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-7764",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127753
      ],
      "eventIds": [],
      "abstract": "Research consumption has been traditionally limited to the reading of academic papers‚Äîa static, dense, and formally written format. Alternatively, pre-recorded conference presentation videos, which are more dynamic, concise, and colloquial, have recently become more widely available but potentially under-utilized. In this work, we explore the design space and benefits for combining academic papers and talk videos to leverage their complementary nature to provide a rich and fluid research consumption experience. Based on formative and co-design studies, we present Papeos, a novel reading and authoring interface that allow authors to augment their papers by segmenting and localizing talk videos alongside relevant paper passages with automatically generated suggestions. With Papeos, readers can visually skim a paper through clip thumbnails, and fluidly switch between consuming dense text in the paper or visual summaries in the video. In a comparative lab study (n=16), Papeos reduced mental load, scaffolded navigation, and facilitated more comprehensive reading of papers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 126070
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for AI",
              "dsl": ""
            }
          ],
          "personId": 126627
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 126451
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "CSE"
            }
          ],
          "personId": 126536
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for AI",
              "dsl": "AI2"
            }
          ],
          "personId": 126453
        }
      ]
    },
    {
      "id": 127088,
      "typeId": 13090,
      "durationOverride": 180,
      "title": " Odyssey: An Interactive Workbench for Expert-Driven Floating-Point Expression Rewriting",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606798"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1048",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "In recent years, researchers have proposed a number of automated tools to identify and improve floating-point rounding error in mathematical expressions. However, users find these tools difficult to use, frequently mis-specifying their problem, misunderstanding the tools‚Äô guarantees, or misapplying their results. In this paper, we describe an interactive design process, working with novices, experts, and tool developers to better understand users‚Äô mental models and how those lead to issues when interacting with automated tools. We find that tools make a number of incorrect assumptions about users‚Äô workflows, assuming that users have specific input ranges in mind, wish to work with a specific tool, and are interested in the accuracy of specific programs. Instead, we propose an ideal workflow for floating-point error improvement consisting of three steps‚Äîdiagnosis, solution generation, and tuning‚Äîand identify interactive feedback cycles between these steps.Building upon this ideal workflow, we construct Odyssey, an interactive tool that puts actions for diagnosis, solution generation, and tuning close at hand, and supports an interactive use case. We find that Odyssey can reuse internal heuristics, algorithms, and functionality of existing automated tools, and is able to better support both novices and experts in floating-point error improvement. In a user study, five expert numerical analysts found Odyssey‚Äôs support for interactive range modification and local error visualization particularly helpful.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125898
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125872
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 127035
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": ""
            }
          ],
          "personId": 125889
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 125827
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Salt Lake City",
              "institution": "University of Utah",
              "dsl": "Kahlert School of Computing"
            }
          ],
          "personId": 126532
        }
      ]
    },
    {
      "id": 127089,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating Dynamic Toolchains: Software Infrastructure for Digital Fabrication Workflows",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1047",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Digital Fabrication",
        "Computer-Controlled",
        "CAD/CAM",
        "Workflow",
        "Dataflow",
        "Creativity Support Tools"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "In this demonstration, we present Dynamic Toolchains, software infrastructure for digital fabrication workflows. Dynamic Toolchains are dataflow programs with event-driven feedback between interactive, stateful modules. Toolchains are built and run in our browser-based dataflow environment. Our live demonstration will show how a toolchain can be used to control a machine, namely a plotter, for interactive watercolor painting. We will also present a collection of artifacts which were fabricated using other toolchains built with our infrastructure. These artifacts show how Dynamic Toolchains can be used to support fabrication workflows that integrate a variety of machines, techniques, and materials, including map plotting, machine knitting, audio embroidery, textured 3d printing, and computer-controlled milling.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Human Centered Design and Engineering"
            }
          ],
          "personId": 126170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 126615
        }
      ]
    },
    {
      "id": 127090,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Sparkybot:An Embodied AI Agent-Powered Robot with Customizable Characters andInteraction Behavior for Children",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615804"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1009",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "robotic platform",
        "children-robot interaction",
        "embodied agent",
        "contextual behaviors"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Interpersonal conflicts are often more difficult to mediate when communicating remotely. The lack of social cues and external mediation makes it difficult for positive conflict behaviors to occur. To this end, robots have been shown to have the potential as mediators. In this paper, we attempt to discuss how to design appropriate bodily contact interactions for the different roles of a robot mediator so as to facilitate the effectiveness of its mediation. We first conduct a pilot interview to probe the potential roles and design elements of robot contact in this study. Then, we explore the relationship between these roles and design elements through a 16-participant design workshop. Finally, we analyze these findings and propose design suggestions for future robot mediator design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            }
          ],
          "personId": 127049
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "Univeristy of Tsukuba",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "Univeristy of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 127074
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Academy of Arts and Design"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Academy of Arts and Design"
            }
          ],
          "personId": 127079
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua Universuty",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua Universuty",
              "dsl": ""
            }
          ],
          "personId": 126966
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Normal University ",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Normal University ",
              "dsl": ""
            }
          ],
          "personId": 126960
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 127072
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 127078
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 127054
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 126939
        }
      ]
    },
    {
      "id": 127091,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Bringing Context-Aware Completion Suggestions to Arbitrary Text Entry Interfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615825"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1090",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "large language models",
        "Web accessibility",
        "intelligent user interfaces"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Large language models (LLMs) can predict ‚Äúobvious‚Äù next steps that users will take in text entry fields, especially the tedious components of tasks like software engineering or email composition. These models are not only useful in large, unbroken text fields, however. We present OmniFill, a browser extension that detects text entry fields and offers ‚Äúautofill‚Äù-style suggestions based on context from the browsing session. The system constructs an LLM prompt that includes three main components: (a) a description of the active tab's text fields and their current values, (b) information from the user's recent web browsing context, and (c) a history, if available, of the user's prior submissions to the web form (alongside those submissions' associated browsing context). Suggestions from the LLM's response are offered to the user to be automatically typed into each corresponding text field. We offer a motivating example of a time-saving interaction and discuss the broader utility of interface-agnostic LLM integrations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Electrical Engineering and Computer Sciences"
            }
          ],
          "personId": 127037
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Electrical Engineering and Computer Sciences"
            }
          ],
          "personId": 127065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 125988
        }
      ]
    },
    {
      "id": 127092,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Parametric Haptics: Versatile Geometry-based Tactile Feedback Devices",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1093",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Haptic feedback is important for immersive, assistive, or multi- modal interfaces, but engineering devices that generalize across applications is notoriously difficult. To address the issue of versatility, we propose Parametric Haptics, geometry-based tactile feedback devices that are customizable to render a variety of tactile sensations. To achieve this, we integrate the actuation mechanism with the tactor geometry into passive 3D printable patches, which are then connected to a generic wearable actuation interface consisting of micro gear motors. The key benefit of our approach is that the 3D-printed patches are modular, can consist of varying numbers and shapes of tactors, and that the tactors can be grouped and moved by our actuation geometry over large areas of the skin. The patches are soft, thin, conformable, and easy to customize to different use cases, thus potentially enabling a large design space of diverse tactile sensations.\r\nIn our user study, we investigate the mapping between geometry parameters of our haptic patches and users‚Äô tactile perceptions. Results indicate a good agreement between our parameters and the reported sensations, showing initial evidence that our haptic patches can produce a wide range of sensations for diverse use scenarios. We demonstrate the utility of our approach with wearable prototypes in immersive Virtual Reality (VR) scenarios, embedded into wearable objects such as glasses, and as wearable navigation and notification interfaces. We support designing such patches with a design tool in Rhino.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 125946
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 127067
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126235
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126086
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126254
        }
      ]
    },
    {
      "id": 127093,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Towards Image Design Space Exploration in Spreadsheets with LLM Formulae",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615790"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1092",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "text-to-image models",
        "spreadsheets",
        "LLMs",
        "design space exploration"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Users of Text-to-Image (TTI) models like DALL‚Ä¢E and Stable Diffusion typically engage in a lot of iteration, exploring a design space with two main inputs: (1) prompt text spanning image content and style; and (2) stochastic (e.g., random seeds) and other opaque (e.g., classifier-free guidance) variables. Here, we demo an early prototype interface using a spreadsheet metaphor to enable exploration and display of multiple input changes simultaneously, and affording prompt-crafting using spreadsheet formula construction. New LLM-based functions aid rapid exploration of the prompt text input space, by generating new variations on existing prompts and context-relevant lists of prompt keyword options.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127055
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 126281
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California: Berkeley",
              "dsl": "Berkeley Institute of Design Lab"
            }
          ],
          "personId": 126955
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 125988
        }
      ]
    },
    {
      "id": 127094,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating ThermalRouter: Enabling Users to Design Thermally-Sound Devices ",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1095",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Users often 3D model enclosures that interact with significant heat sources, such as electronics or appliances that generate heat (e.g., CPU, motor, lamps, etc.). While parts made by users might function well aesthetically or structurally, they are rarely thermally-sound. This happens because heat transfer is non-intuitive; thus, engineering thermal solutions is not straightforward. To tackle this, we developed ThermalRouter, a CAD plugin that assists with improving the thermal performance of their models. ThermalRouter automatically converts regions of the model to be made from thermally-conductive materials (such as nylon or metallic-silicone). These regions act as heat channels, branching away from hotspots to dissipate heat. The key is that ThermalRouter automatically simulates the thermal performance of many possible heat channel configurations and presents the user with the most thermally-sound design (e.g., lowest temperature). Furthermore, it allows users to customize their solutions (e.g., balancing costs, non-modifiable geometry, etc.). Most importantly, ThermalRouter achieves this without requiring manual labor to set up or parse the results of complex thermal simulations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126355
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126629
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125903
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125836
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "University of Chicago"
            }
          ],
          "personId": 126241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125909
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        }
      ]
    },
    {
      "id": 127095,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "LayerShift: Reconfigurable Layer Expression Using Robotic Transparent Displays",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615812"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1051",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "User Interface",
        "Human-Computer Interaction"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "In this paper, we propose a new robotic display equipped with a transparent display, whose position and posture can be controlled, and a design space that uses it. Robotic displays have been researched in which the display is mounted on a movable robotic arm to extend the expression through motion expression in addition to visual expression. In this paper, we propose combining a transparent display with a robotic display to extend the design space of the robotic display through the characteristics of transparent display. We implement a prototype and propose a design space for a single display and multiple cooperating displays.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa-ken",
              "city": "Yokosuka-shi",
              "institution": "NTT Coropration",
              "dsl": "NTT Human Infomatics Laboratories"
            }
          ],
          "personId": 127060
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka-shi",
              "institution": "NTT Corp.",
              "dsl": "NTT Human Infomatics Laboratories"
            }
          ],
          "personId": 127073
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokosuka",
              "institution": "NTT Corp.",
              "dsl": "NTT Human Infomatics Laboratories"
            }
          ],
          "personId": 127052
        }
      ]
    },
    {
      "id": 127096,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1050",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "People are increasingly turning to large language models (LLMs) for complex information tasks like academic research or planning a move to another city. However, while they often require working in a nonlinear manner --- e.g., to arrange information spatially to organize and make sense of it, current interfaces for interacting with LLMs are generally linear to support conversational interaction. To address this limitation and explore how we can support LLM-powered exploration and sensemaking, we developed Sensecape, an interactive system designed to support complex information tasks with an LLM by enabling users to (1) manage the complexity of information through multilevel abstraction and (2) seamlessly switch between foraging and sensemaking. Our within-subject user study reveals that Sensecape empowers users to explore more topics and structure their knowledge hierarchically. We contribute implications for LLM-based workflows and interfaces for information tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126040
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 126519
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California",
              "dsl": "Design Lab"
            }
          ],
          "personId": 126638
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126359
        }
      ]
    },
    {
      "id": 127097,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating SuperMagneShape: Interactive Usage of a Passive Pin-Based Shape-Changing Display",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615768"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1053",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Shape-changing display, shape display, magnet"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We propose SuperMagneShape, a method for creating interactive applications using MagneShape. MagneShape is a magnetically-actuated pin-based shape-changing display. It utilizes the forces produced by its magnetic components to control the levitation heights of passive magnetic pins, eliminating the need for an electric actuator for each pin. Although the MagneShape pin array is simple and inexpensive, rapid change of the magnetic pattern that drives the device is challenging. Proper display of shapes and characters requires appropriate magnetic patterns generated by a dedicated pattern generator and a time-consuming magnetization process. To minimize the complexity of the process, as well as the time taken between input and output, we designed a high-density pin array and a magnetic belt conveyor system. When the user handwrites a magnetic pattern in the shape of the letter \"A\" on a section of the magnetic belt, the imprinted magnetic pattern is conveyed under the high-density pin array and causes the pin array to display an \"A\" shape moving along it. We have also implemented games where the hand-drawn shape presents physical action as it is conveyed under the pin array. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Atsugi",
              "institution": "Nippon Telegraph and Telephone Corporation",
              "dsl": "NTT Communication Science Laboratories"
            }
          ],
          "personId": 126551
        }
      ]
    },
    {
      "id": 127098,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Color Field: Developing Professional Vision by Visualizing the Effects of Color Filters",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1052",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "visualization",
        "creativity support tool",
        "professional vision",
        "support novice",
        "color filter",
        "color grading"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Color filters are ubiquitous across visual digital media due to their transformative effect. However, it can be difficult to understand how a color filter will affect an image, especially for novices. In order to become experts, we argue that novices need to develop Goodwin‚Äôs notion of Professional Vision. Then, they can \"see\" and interpret their work in terms of their domain knowledge like experts. Using the theory of Professional Vision, we present two design objectives for systems that aim to help users develop expertise. These goals were used to develop Color Field, an interactive visualization of color filters as a vector field over the Hue-Saturation-Lightness color space. We conducted an exploratory user study in which five color grading novices and four experts were asked to analyze color filters. We found that Color Field enabled multiple strategies to make sense of filters (e.g. reviewing the overall shape of the vector field) and discuss them (e.g. using spatial language). We conclude with other applications of Color Field and future work to leverages Professional Vision in HCI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Creativity Lab"
            }
          ],
          "personId": 126614
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": ""
            }
          ],
          "personId": 126560
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126359
        }
      ]
    },
    {
      "id": 127099,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Virtual Rolling Temple: Expanding the Vertical Input Space of a Smart Glasses Touchpad",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615813"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1096",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Smart Glasses",
        "Pointing Device",
        "Two-dimensional Input"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Smart glasses have not favored two-dimensional (2D) GUI. Such a trend may have originated from the limitations of smart glasses in display and input devices. While the display restriction is rapidly being resolved nowadays, 1D GUI is still the majority, indicating that the touch input device is the possible bottleneck. To tackle this issue by expanding the vertical input space of the temple touchpad, we propose the Virtual Rolling Temple (VRT). The concept is to perform 2D gestures by moving the hand in any direction while keep touching the prototype as if the temple rotates. The VRT touchpad is as thin as the spectacles' temples, but it provides the users with input space approximately equivalent to an 80 √ó 80 mm square touchpad. This is 8 and 13.9 times larger than Google Glass and VUZIX M400, respectively. To validate the concept of the VRT, we constructed three demo scenarios: 2D Pointing, 2D Menu, and 2D Gesture, to cover different types of general 2D input for smart glasses.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "HCI Lab, School of Computing"
            }
          ],
          "personId": 127012
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 127038
        }
      ]
    },
    {
      "id": 127100,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Marvista: Exploring the Design of a Human-AI Collaborative News Reading Tool",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1132",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We explore the design of Marvista‚Äîa human-AI collaborative tool that employs a suite of natural language processing models to provide end-to-end support for reading online news articles. Before reading an article, Marvista helps a user plan what to read by filtering text based on how much time one can spend and what questions one is interested to find out from the article. During reading, Marvista helps the user reflect on their understanding of each paragraph with AI-generated questions. After reading, Marvista generates an explainable human-AI summary that combines both AI‚Äôs processing of the text, the user‚Äôs reading behavior, and user-generated data in the reading process. In contrast to prior work that offered (content-independent) interaction techniques or devices for reading, Marvista takes a human-AI collaborative approach that contributes text-specific guidance (content-aware) to support the entire reading process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 125849
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Salesforce AI",
              "dsl": ""
            }
          ],
          "personId": 126962
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Salesforce Research",
              "dsl": ""
            }
          ],
          "personId": 126982
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Salesforce Research",
              "dsl": ""
            }
          ],
          "personId": 127059
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Salesforce Research",
              "dsl": ""
            }
          ],
          "personId": 127021
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Salesforce",
              "dsl": ""
            }
          ],
          "personId": 126964
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Salesforce",
              "dsl": "Salesforce Research"
            }
          ],
          "personId": 127018
        }
      ]
    },
    {
      "id": 127101,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "FocusFlow: Leveraging Focal Depth for Gaze Interaction in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615818"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1055",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Current gaze input methods for VR headsets predominantly utilize the gaze ray as a pointing cursor, often neglecting depth information in it. This study introduces FocusFlow, a novel gaze interaction technique that integrates focal depth into gaze input dimensions, facilitating users to actively shift their focus along the depth dimension for interaction. A detection algorithm to identify the user's focal depth is developed. Based on this, a layer-based UI is proposed, which uses focal depth changes to enable layer switch operations, offering an intuitive hands-free selection method. We also designed visual cues to guide users to adjust focal depth accurately and get familiar with the interaction process. Preliminary evaluations demonstrate the system's usability, and several potential applications are discussed. Through FocusFlow, we aim to enrich the input dimensions of gaze interaction, achieving more intuitive and efficient human-computer interactions on headset devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126978
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 127007
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois Urbana Champaign",
              "dsl": "Thamas Siebel Dept of Computer Science"
            }
          ],
          "personId": 127026
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126994
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois urbana Champaign",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126951
        }
      ]
    },
    {
      "id": 127102,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Laser-Powered Vibrotactile Rendering",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615795"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1098",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Haptics",
        "Energy Harvesting",
        "Virtual Reality"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We investigate the feasibility of a vibrotactile device that is both battery-free and electronic-free. Our approach leverages lasers as a wireless power transfer and haptic control mechanism, which can drive small actuators commonly used in AR/VR and mobile applications with DC or AC signals. To validate the feasibility of our method, we developed a proof-of-concept prototype that includes low-cost eccentric rotating mass (ERM) motors and linear resonant actuators (LRAs) connected to photovoltaic (PV) cells. This prototype enabled us to capture laser energy from any distance across a room. Through different vibration patterns rendered using either a single motor or two motors, we demonstrate the effectiveness of our approach in generating vibration patterns.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": ""
            }
          ],
          "personId": 126944
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": ""
            }
          ],
          "personId": 126990
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 126273
        }
      ]
    },
    {
      "id": 127103,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstration of ChromoCloth: Re-Programmable Multi-Color Textures through Flexible and Portable Light Source",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615811"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1057",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Digital Fabrication",
        "Programmable Textures",
        "Photochromic Dyes"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "In this demo, we present ChromoCloth, a flexible and portable light source for reprogrammable multi-color texture on photochromic objects, whose color can be reprogrammed with external light sources. While prior work used external projectors to trigger the color change, ChromoCloth initiates the color change by covering the object. ChromoCloth consists of a textile substrate, 3D printed diffusive housing glued on top of the substrate and a flexible LED strip that is weaved through the housings. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126547
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 126630
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 126459
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126128
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126953
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 127086
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 127006
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 127031
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": ""
            }
          ],
          "personId": 125767
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126305
        }
      ]
    },
    {
      "id": 127104,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstration of Masonview: Content-Driven Viewport Management",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615827"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1012",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Window Management",
        "UI Mashups",
        "End-user Customization",
        "Web"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "The web is now rich with information and functionality, allowing users to only use portions of webpages to complete their tasks. However, current desktop interfaces only allow us to manage entire windows, leading to inefficient use of screen space and suboptimal workflows. We present Masonview, a content-driven viewport management system that provides mechanisms to detach desired elements from their webpages as viewports and compose these views into UI mashups with viewplates. We demonstrate how these mechanisms enable more free-form organization and management of content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 126519
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Creativity Lab"
            }
          ],
          "personId": 126614
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126040
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126359
        }
      ]
    },
    {
      "id": 127105,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstration of Joie: A Joy-based Brain-Computer Interface (BCI) with Wearable Skin Conformal Polymer Electrodes",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615803"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1056",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Brain-computer interfaces (BCI)",
        "Wearable technology",
        "Affect"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We designed Joie, a joy-based electroencephalography (EEG) brain-computer interface (BCI). Users interact with Joie by imagining joyous thoughts and images that alter their  prefrontal EEG asymmetries. These asymmetries control their character's movement in an endless runner video game, where joyous thoughts cause left prefrontal asymmetry that leads to receiving a reward. In this demonstration, we assemble a prototype for a wearable, skin conformal polymer electrode EEG headband and evaluate this prototype with the Joie BCI. We conducted a pilot evaluation (11 participants, 3 training sessions per participant) to assess trainability and workload. We observed that our participants were able to perform relative left activation significantly greater than right activation and create single-session improvements in resting baseline asymmetry. We also report on perceived user demand, effort and performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 125776
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology ",
              "dsl": "Media Lab"
            }
          ],
          "personId": 127046
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 126197
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 126999
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 126610
        }
      ]
    },
    {
      "id": 127106,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "SwarmFidget: Exploring Programmable Actuated Fidgeting with Swarm Robots",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615806"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1058",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "fidgeting",
        "swarm robots",
        "tangible user interface",
        "programmable actuated fidgeting"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Fidgeting is a common behavior that one tends to engage in during periods of inattention or mind wandering. Although attempts were undertaken to enhance fidget devices with advanced technology, such as sensors and displays, no works exist that explored fidgeting with actuated devices. To fill this gap, we introduce the concept of programmable actuated fidgeting and the design space for SwarmFidget. Programmable actuated fidgeting is a type of fidgeting that involves devices integrated with actuators, sensors, and computing to enable a dynamic and customizable interactive fidgeting experience. SwarmFidget is an instance of a platform where tabletop swarm robots are used to facilitate programmable actuated fidgeting. To engage with actuated fidgets, users can input commands through various modalities such as touch or gesture, and the actuators in the fidgeting device will respond in a programmable manner to provide haptic, visual, or audio feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 126969
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 127083
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 126968
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Center for Design Research"
            }
          ],
          "personId": 126117
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 126388
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 126022
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 126628
        }
      ]
    },
    {
      "id": 127107,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating Swarm Robots Capable of Cooperative Transitioning between Table and Wall",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615763"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1019",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "swarm user interfaces",
        "human-robot interaction"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Swarm User Interfaces (Swarm UIs) enable manipulation of user environment through the dynamic arrangement of small robots. However, the operation range of Swarm UIs are limited to a single plane due to their locomotion constraints, which are typically two-wheel-propelled. Here, We present a proof-of-concept design for swarm robots, which enables them to cooperatively transition between horizontal and vertical surfaces. Notably, this design requires only passive mechanical structure and does not rely on any powered electrical components. We demonstrate several application examples to showcase the feasibility of the robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 126337
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 126998
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 126350
        }
      ]
    },
    {
      "id": 127108,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Snap'N'Go: An Extendable Framework for Evaluating Mechanisms in Spatial Crowdsourcing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615794"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1060",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "spatial crowdsourcing",
        "incentives",
        "evaluation",
        "piggyback",
        "prototyping"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Given the challenges of evaluating the effect of task allocation mechanisms and their associated incentives on the behavior of participants in spatial crowdsourcing, there is a need for an evaluation framework that simplifies the process of benchmarking such mechanisms with real crowds. In this demo, we present Snap'N'Go, a spatial crowdsourcing application that is modeled as a scavenger-hunt game, which is integrated within a larger modular framework that can be easily extended to evaluate various task allocation mechanisms and incentive models, without affecting the user-experience. Moreover, we present the details of the initial launch of the framework, in which we benchmark multiple mechanisms, and discuss how it can be easily extended for various experimental settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College '24",
              "dsl": ""
            }
          ],
          "personId": 127075
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College '24",
              "dsl": ""
            }
          ],
          "personId": 126956
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 127081
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 127051
        }
      ]
    },
    {
      "id": 127109,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "PURRtentio: Implementing a Smart Litter Box for Feline Urinalysis with Electrochemical Biosensors",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615820"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1062",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "biosensor",
        "electrochemistry",
        "potentiostat",
        "urinalysis"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Traditional collection and analysis of feline urine samples for health monitoring are invasive, expensive, and infrequent. This paper introduces PURRtentio, a novel litter box system utilizing an electrochemical biosensor to monitor analytes in feline urine. The system comprises a DIY biosensor, potentiostat, microcontroller, distance sensor, and mobile application. Performance validation compared PURRtentio with an industry-grade potentiostat. PURRtentio presents an innovative and non-invasive approach for consistent monitoring of chemistry elements in feline urine, enabling early detection and management of cat's health conditions. This technology has the potential to revolutionize feline health monitoring, providing a solution for veterinarians and pet owners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": ""
            }
          ],
          "personId": 127005
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "lima",
              "institution": "Universidad Peruana de Ciencias",
              "dsl": ""
            }
          ],
          "personId": 127032
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 126986
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": "School of Veterinary Medicine"
            }
          ],
          "personId": 126989
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Department of Design"
            }
          ],
          "personId": 127015
        }
      ]
    },
    {
      "id": 127110,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "FluxTangible: Simple and Dynamic Haptic Tangible with Bumps and Vibrations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615762"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1064",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Tangible interactions",
        "tactile",
        "haptic",
        "magnet",
        "touchscreen"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "To add haptic feedback and tactile cues of a physical interface to interaction with mobile touch displays, several passive systems have been proposed as Tangible User Interfaces (TUIs). The haptic feedback from such passive objects can be customized by changing their shape and structure but cannot be changed dynamically. Therefore, we propose a simple TUI that can provide dynamic haptic feedback on a touch device. We use an electromagnet embedded in the tangible object and a magnetic sheet fixed on the back of a touch device. An electric current flowing through the electromagnet and the customizable magnetic pattern printed on the magnetic sheet together produce magnetic interference. The magnetic interference caused by these magnetic objects is actively generated during interaction with the touch device to provide dynamic haptic feedback of both bumps and vibrations. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Atsugi",
              "institution": "Nippon Telegraph and Telephone Corporation",
              "dsl": "NTT Communication Science Laboratories"
            }
          ],
          "personId": 127066
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Atsugi",
              "institution": "Nippon Telegraph and Telephone Corporation",
              "dsl": "NTT Communication Science Laboratories"
            }
          ],
          "personId": 126551
        }
      ]
    },
    {
      "id": 127111,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating AirTied: Automatic Personal Fabrication of Truss Structures",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1063",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We present AirTied, a device that fabricates truss structures in a fully automatic fashion. AirTied achieves this by unrolling a 20cm-wide inflatable plastic tube and tying nodes into it. AirTied creates nodes by holding onto a segment of tube, stacking additional tube segments on top of it, tying them up, and releasing the result. The resulting structures are material-efficient and light as well as sturdy, as we demonstrate by creating a 6m-tower. Unlike the prior art, AirTied requires no scaffolding and no building blocks, bringing automated truss construction into the reach of personal fabrication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 126418
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 126234
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126406
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126423
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso-Plattner-Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 126484
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 125922
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126217
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126158
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 126238
        }
      ]
    },
    {
      "id": 127112,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstration of Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615769"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1065",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "personal fabrication",
        "digital fabrication",
        "3d printing",
        "generative AI"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "With recent advances in Generative AI, it is becoming easier to automatically manipulate 3D models. However, current methods tend to apply edits to models globally, which risks compromising the intended functionality of the 3D model when fabricated in the physical world. For example, modifying functional segments in 3D models, such as the base of a vase, could break the original functionality of the model, thus causing the vase to fall over. We introduce Style2Fab, a system for automatically segmenting 3D models into functional and aesthetic elements, and selectively modifying the aesthetic segments, without affecting the functional segments. Style2Fab uses a semi-automatic classification method to decompose 3D models into functional and aesthetic elements, and differentiable rendering to selectively stylize the functional segments. We demonstrate the functionality of this tool with six application examples across domains of Home Interior Design, Medical Applications, and Personal Accessories.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126527
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge ",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126146
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126215
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT ",
              "dsl": "Center for Bits and Atoms"
            }
          ],
          "personId": 126550
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126023
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 125802
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 125923
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Khoury College of Computer Sciences"
            }
          ],
          "personId": 125788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126305
        }
      ]
    },
    {
      "id": 127113,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Storyfier: Exploring Vocabulary Learning Support with Text Generation Models",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1068",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "vocabulary learning",
        "story generation",
        "language models"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Vocabulary learning support tools have widely exploited existing materials, e.g., stories or video clips, as contexts to help users memorize each target word. However, these tools could not provide a coherent context for any target words of learners‚Äô interests, and they seldom help practice word usage. In this paper, we work with teachers and students to iteratively develop Storyfier, which leverages text generation models to enable learners to read a generated story that covers any target words, conduct a story cloze test, and use these words to write a new story with adaptive AI assistance. Our within-subjects study (N=28) shows that learners generally favor the generated stories for connecting target words and writing assistance for easing their learning workload. However, in the read-cloze-write learning sessions, participants using Storyfier perform worse in recalling and using target words than learning with a baseline tool without our AI features. We discuss insights into supporting learning tasks with generative models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126075
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "NYC",
              "institution": "Cornell University",
              "dsl": "Weill Cornell Medicine"
            }
          ],
          "personId": 126288
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126525
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Guangdong Polytechnic of Industry & Commerce",
              "dsl": ""
            }
          ],
          "personId": 126027
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 126222
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126516
        }
      ]
    },
    {
      "id": 127114,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615796"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1101",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Human-AI Interaction",
        "Large Language Models",
        "Chatbots"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "The recent advent of large language models (LLM) has resulted in high-performing conversational agents such as ChatGPT. These agents must remember key information from an ongoing conversation to provide responses that are contextually relevant to the user. However, these agents have limited memory and can be distracted by irrelevant parts of the conversation. While many strategies exist to manage conversational memory, users currently lack affordances for viewing and controlling what the agent remembers, resulting in a poor mental model and conversational breakdowns. In this paper, we present Memory Sandbox, an interactive system and design probe that allows users to manage the conversational memory of LLM-powered agents. By treating memories as data objects that can be viewed, manipulated, recorded, summarized, and shared across conversations, Memory Sandbox provides interaction affordances for users to manage how the agent should `see' the conversation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 127071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Temple University",
              "dsl": "Computer and Information Sciences"
            }
          ],
          "personId": 126947
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Temple University",
              "dsl": ""
            }
          ],
          "personId": 126997
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Temple University",
              "dsl": "Computer and Information Sciences"
            }
          ],
          "personId": 125770
        }
      ]
    },
    {
      "id": 127115,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Constraint-Driven Robotic Surfaces, at Human-Scale",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1023",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Robotic surfaces, which can change form and function under program control, offer interesting new possibilities for customizable objects and adaptive environments. In this demonstration, we introduce such a surface (a \"robotic wall\"), which is able to begin in a neutral state, assume a desired functional shape, and later return to its neutral state. The surface can then assume a completely different functional shape, all under program control.\r\n\r\nOur surface is designed for large-scale applications, but still uses small, power-efficient actuators to reconfigure itself dynamically. Instead of driving the shape change at many points (i.e. at each hinge of our structure), we use lightweight motors to constrain its shape, and relocate the transformative actuation force to the outer edges of our surface.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126632
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 125797
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126220
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction "
            }
          ],
          "personId": 125902
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126254
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126585
        }
      ]
    },
    {
      "id": 127116,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Mo2Hap: Rendering VR Performance Motion Flow to Upper-body Vibrotactile Haptic Feedback",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615775"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1100",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Haptic Rendering",
        "Haptic Device",
        "Human centered computing"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We introduce a unique haptic rendering framework that transforms the performer's actions into wearable vibrotactile feedback for an immersive virtual reality~(VR) performance experience. To capture essential movements from the virtual performer, we propose a method called Motion Salient Triangle. Motion Salient Triangle is a real-time 3D polygon that computes haptic characteristics~(intensity, location) based on motion skeletal data. Here, we employ an entire upper-body haptic system that provides vibrotactile feedback on the torso, back, and shoulders. This haptic rendering pipeline enable audiences to experience immersive VR performance by accommodating the performer's motions on top of motion-to-haptic feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST ",
              "dsl": "Graduate School of Culture Technology"
            }
          ],
          "personId": 127008
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Graduate School of Culture Technology"
            }
          ],
          "personId": 127039
        }
      ]
    },
    {
      "id": 127117,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Printed Circuit Board (PCB) Probe Tester (PCBPT) - a Compact Desktop System that Helps with Automatic PCB Debugging",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615800"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1067",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "compact desktop system",
        "PCB",
        "automatic debugging",
        "in-circuit debugging"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "PCB debugging can be tricky. For example, if we want to use an oscilloscope to measure signals of interest in the PCB, we need to locate them in the schematic and select the appropriate pad for each signal on the PCB layout on which to put the oscilloscope probes. This process hence requires frequent switching between the schematics and the PCB layouts. Moreover, our hands may not be precise and stable enough to accurately place the probes on the pads without causing short circuits with adjacent pins, which can lead to further issues. Additionally, if multiple signals need to be tested, two hands will not be enough. Probe hook clips can be used, but this often necessitates the use of extension wires that must be soldered onto the targeted pads. To streamline the debugging process, we introduce the PCBPT (PCB Probing Tester). This innovative solution seamlessly bridges from schematic to test equipment by using a robotic probe and actuated board holder. By selecting signals of interest directly from a GUI, users can instantly monitor the output on an oscilloscope, significantly improving the effectiveness of the debugging process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab, Responsive Environments group"
            }
          ],
          "personId": 126946
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments"
            }
          ],
          "personId": 126959
        }
      ]
    },
    {
      "id": 127118,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Conductive, Ferromagnetic and Bendable 3D Printed Hair for Designing Interactive Objects",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615823"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1103",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "digital fabrication",
        "3D printing",
        "tactile presentation",
        "hair structure"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "The hair structure, a characteristic object that can be formed using 3D printers, is used to enrich the expressivity and haptic sensation of a printed object. However, in conventional 3D printing techniques, the hair structure is printed using a uniform and general plastic such as Poly-Lactic Acid (PLA). In this study, we attempt to print the hair structure using conductive and magnetic iron filaments, commonly used to allow an Fused Deposition Modeling (FDM) printer to create a functional object, to extend the possibility of the 3D printed hair technique. Furthermore, we planted the printed hair in the soft resin used for resin crafts. We provide detailed material information and validate the printability of each filament. With these methods, we demonstrate applications, such as hairy devices that can detect when a human touches hair, brushes attracted by magnet arrays, and flexible attachment of hair structures to the human body.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hokkaido",
              "city": "Hakodate",
              "institution": "Future University Hakodate",
              "dsl": "Information Architecture / Tsukada Lab"
            },
            {
              "country": "Japan",
              "state": "Hokkaido",
              "city": "Hakodate",
              "institution": "Future University Hakodate",
              "dsl": "Information Architecture / Tsukada Lab"
            }
          ],
          "personId": 127069
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Shiga",
              "city": "Kusatsu-shi",
              "institution": "Ritsumeikan University",
              "dsl": "College of Information Science and Engineering"
            },
            {
              "country": "Japan",
              "state": "Shiga",
              "city": "Kusatsu-shi",
              "institution": "Ritsumeikan University",
              "dsl": "College of Information Science and Engineering"
            }
          ],
          "personId": 125823
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hakodate",
              "institution": "Information Architecture",
              "dsl": "Future University Hakodate"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Hakodate",
              "institution": "Information Architecture",
              "dsl": "Future University Hakodate"
            }
          ],
          "personId": 126983
        }
      ]
    },
    {
      "id": 127119,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Graphologue: Exploring Large Language Model Responses with Interactive Diagrams",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1025",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Large Language Model",
        "Natural Language Interface",
        "Visualization"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented ability to synthesize text responses to diverse user questions. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can interact with the diagrams to flexibly adjust the graphical presentation and to submit context-specific prompts to obtain more information. Utilizing diagrams, Graphologue enables graphical, non-linear dialogues between humans and LLMs, facilitating information exploration, organization, and comprehension.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126182
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126067
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": "Dept of Cognitive Science"
            }
          ],
          "personId": 126531
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126359
        }
      ]
    },
    {
      "id": 127120,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Knowledge Compass: A Question Answering System Guiding Students with Follow-Up Question Recommendations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615785"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1069",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Online Learning, E-learning, Question Answering, Follow-up Question, Question Recommendation"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Pedagogical question-answering (QA) systems have been utilized for providing individual support in online learning courses. However, existing systems often neglect the education practice of guiding and encouraging students to think of relevant questions for deeper and more comprehensive learning. To address this gap, we introduce Knowledge Compass, an interactive QA system. The system can recommend follow-up questions that provide potential further explorations of the topics students ask about. Additionally, the system applies a course outline visualization and a set of interactive features for students to track the relationship between their questions and the course content. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126942
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126979
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 127019
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126975
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hubei",
              "city": "Wuhan",
              "institution": "School of Computer Science",
              "dsl": "Wuhan University"
            },
            {
              "country": "China",
              "state": "Hubei",
              "city": "Wuhan",
              "institution": "School of Computer Science",
              "dsl": "Wuhan University"
            }
          ],
          "personId": 127030
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "The University of North Carolina at Chapel Hill",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "The University of North Carolina at Chapel Hill",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127047
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina",
              "dsl": ""
            }
          ],
          "personId": 127023
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 126516
        }
      ]
    },
    {
      "id": 127121,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating Taste Retargeting via Chemical Taste Modulators",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1105",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Prior research has explored modifying taste through electrical stimulation. While promising, such interfaces often only elicit taste changes while in contact with the user‚Äôs tongue (e.g., cutlery with electrodes), making them incompatible with eating and swallowing real foods. Moreover, most interfaces cannot selectively alter basic tastes, but only the entire flavor profile (e.g., cannot selectively alter bitterness). To tackle this, we propose taste retargeting, a method of altering taste perception by delivering chemical modulators to the mouth before eating. These modulators temporarily change the response of taste receptors to foods, selectively suppressing or altering basic tastes. Our first study identified six accessible taste modulators that suppress salty, umami, sweet, or bitter and transform sour into sweet. Using these findings, we demonstrated an interactive application of this technique with the example of virtual reality, which we validated in our second study. We found that taste retargeting reduced the flavor mismatch between a food prop and other virtual foods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125782
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        }
      ]
    },
    {
      "id": 127122,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Experiencing Visual Captions: Augmented Communication with Real-time Visuals using Large Language Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615978"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1104",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "augmented communication",
        "large language models",
        "video-mediated communication",
        "online meeting",
        "collaborative work",
        "dataset",
        "text-to-visual",
        "AI agent",
        "augmented reality"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We demonstrate Visual Captions, a real-time system that integrates with a video conferencing platform to enrich verbal communication. Visual Captions leverages a fine-tuned large language model to proactively suggest visuals that are relevant to the context of the ongoing conversation. We implemented Visual Captions as a user-customizable Chrome plugin with three levels of AI proactivity: Auto-display (AI autonomously adds visuals), Auto-suggest (AI proactively recommends visuals), and On-demand-suggest (AI suggests visuals when prompted). We showcase the usage of Visual Captions in open-vocabulary settings, and how the addition of visuals based on the context of conversations could improve comprehension of complex or unfamiliar concepts. In addition, we demonstrate three approaches people can interact with the system with different levels of AI proactivity. Visual Captions is open-sourced at https://github.com/google/archat.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 126977
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127076
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127020
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 126384
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Inc.",
              "dsl": ""
            }
          ],
          "personId": 127011
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 125849
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126954
        }
      ]
    },
    {
      "id": 127123,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "LearnThatDance: Augmenting TikTok Dance Challenge Videos with an Interactive Practice Support System Powered by Automatically Generated Lesson Plans",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615801"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1107",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "TikTok",
        "Dance Learning",
        "Dance Education",
        "Part Learning",
        "Motor Learning",
        "Guidance Hypothesis"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "In this demo we showcase an interactive application to support the learning of \"TikTok dance challenge'' short dance choreographies. Our system utilizes dance challenge videos as the information source, performing music analysis and pose estimation to segment the dance into learnable chunks and generate a practice plan that implements motor learning techniques such as incremental part-learning and fading guidance. These plans are presented in a web app that implements video demonstration, augmented webcam mirroring, practice recording/review functionality, and both concurrent and terminal feedback. By operating on a ubiquitous information source, generating the  lessons automatically, and requiring only a web browser and webcam in the user interface, our system is a step towards significantly expanding the reach of dance choreography learning and a platform for further research into dance HCI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": ""
            }
          ],
          "personId": 127084
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Psychological and Brain Sciences"
            }
          ],
          "personId": 126948
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Psychological and Brain Sciences"
            }
          ],
          "personId": 127085
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127013
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University ",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127027
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Education"
            }
          ],
          "personId": 126995
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126981
        }
      ]
    },
    {
      "id": 127124,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "SculptAR: Direct Manipulations of Machine Toolpaths in Augmented Reality for 3D Clay Printing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615822"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1109",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Digital Fabrication",
        "Direct Manipulation",
        "Augmented Reality",
        "Computer-Aided Machining",
        "Clay 3D printing"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Specifying designs for additive manufacturing using machine toolpaths unlocks design attributes such as surface textures and shapes determined by material and machine constrains compared to higher level representations like 3D geometries. Current methodologies for authoring these designs necessitate a high level of programming or geometric understanding, posing a significant barrier to entry and limited control. Additionally, the confinement of these workflows within computer screens obscures the comprehension of material and dimensional constraints. To bridge this gap, we demonstrate the  direct manipulation of machine toolpaths in Augmented Reality for clay 3D printing. Our application relies on hand interactions to edit path control points. We also provide a set of options that allow the user to control how their changes to one control points are broadcast to others to determine surface shapes and textures.  By leveraging AR interactions in a physical context, our proposal aims to leverage existing physical workflows and enable practitioners to apply their understanding of material properties and visual understanding of physical 3D objects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Goleta",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 127034
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 125867
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126965
        }
      ]
    },
    {
      "id": 127125,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "DeckFlow: A Card Game Interface for Exploring Generative Model Flows",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615821"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1108",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Generative Model",
        "Multimodal Interaction",
        "Text-To-Image Generation"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Recent Generative AI models have been shown to be substantially useful in different fields, often bridging modal gaps, such as text-prompted image or human motion generation. However, their accompanying interfaces do not sufficiently support iteration and interaction between models, and due to the computational intensity of generative technology, can be unforgiving to user errors and missteps. We propose DeckFlow, a no-code interface for multimodal generative workflows which encourages rapid iteration and experimentation between disparate models. \\system emphasizes the persistence of output, the maintenance of generation settings and dependencies, and continual steering through user-defined concept groups. Taking design cues from Card Games and Affinity Diagrams, DeckFlow is aimed to lower the barrier for non-experts to explore and interact with generative AI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 127064
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "SpaceCraft Inc.",
              "dsl": ""
            }
          ],
          "personId": 125942
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 126940
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126996
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126949
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126260
        }
      ]
    },
    {
      "id": 127126,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demo of Z-Ring: Context-Aware Subtle Input Using Single-Point Bio-Impedance Sensing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615809"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1071",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Wearable Computing",
        "Context Aware",
        "RF Sensing",
        "Human-Computer Interaction"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "This paper presents Z-Ring, a novel wearable device that uses radio frequency (RF) based sensing to offer unique capabilities for human-computer interaction, including subtle input, object recognition, user identification, and passive surface interaction. With only a single sensing modality, Z-Ring achieves diverse and concurrent interactions that can enhance the user experience. We illustrate the potential of Z-Ring to enable seamless context-aware interactions via a custom music player application. In the future, we plan to expand Z-Ring's functionality with user customization and explore usage for additional applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School for Computer Science & Engineering"
            }
          ],
          "personId": 126958
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 127029
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 127048
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 126957
        }
      ]
    },
    {
      "id": 127127,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating HUGO, a High-Resolution Tactile Emulator for Complex Surfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615772"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1073",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Haptics",
        "High-Resolution Haptics",
        "Haptic Textures",
        "Human Computer Interface",
        "User Study"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We demonstrate HUGO, a novel device developed to deliver enhanced tactile feedback and facilitate interaction with real-world surfaces. The device aims to overcome the limitations of existing cutaneous feedback devices, which often provide a restricted range of sensations and are primarily tested on simple synthetic surfaces. HUGO was meticulously designed through a human-centered process to enable users to experience realistic touch sensations encountered in various real-world scenarios. HUGO utilizes a parallel manipulator and a pin-array mechanism that operate concurrently at a frequency of up to 200Hz to simulate both coarse and fine geometrical features. By employing a high operation frequency and decomposing the tactile feedback into distinct features, HUGO enables a more accurate replication of tactile experiences associated with different surfaces. The demonstration will showcase HUGO‚Äôs capabilities in providing authentic haptic feedback. This includes facilitating social interactions, enhancing e-commerce experiences, and improving gaming interactions through realistic haptic engagement with real-world surfaces. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "Technion - Israel Institute of Technology",
              "dsl": "Mechanical engineering"
            }
          ],
          "personId": 126980
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "Technion - Israel Institute of Technology",
              "dsl": "Mechanical engineering"
            }
          ],
          "personId": 127041
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "Technion - Israel Institute of Technology",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 127028
        }
      ]
    },
    {
      "id": 127128,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstration of 3D Printed Magnetophoretic Displays",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615771"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1072",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "\"Magnetophoretic",
        "3D Printing Display",
        "Low Power Display",
        "Liquid Injection",
        "3D printing",
        "3D Printer Modification\""
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "In this demonstration, we showcase a suite of 3D printed magnetophoretic displays. These examples include a printed Stanford bunny with customizable appearances, a small espresso mug that can double as a post-it note surface, a board game figurine with a computationally updated display, and a collection of flexible wearable accessories with editable visuals. We will demonstrate the 3D printer design as well as the primary process of printing these magnetophoretic displays.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126059
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 125803
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Graphics Technology"
            }
          ],
          "personId": 126290
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126094
        }
      ]
    },
    {
      "id": 127129,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Reprogrammable Digital Metamaterials for Interactive Devices",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1075",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We present digital mechanical metamaterials that enable multiple computation loops and reprogrammable logic functions, making a significant step towards passive yet interactive devices. Our materials consist of many cells that transmit signals using an embedded bistable spring. When triggered, the bistable spring displaces and triggers the next cell. We integrate a recharging mechanism to recharge the bistable springs, enabling multiple computation rounds. Between the iterations, we enable reprogramming the logic functions after fabrication. We demonstrate that such materials can trigger a simple controlled actuation anywhere in the material to change the local shape, texture, stiffness, and display. This enables large-scale interactive and functional materials with no or a small number of external actuators. We showcase the capabilities of our system with various examples: a haptic floor with tunable stiffness for different VR scenarios, a display with easy-to-reconfigure messages after fabrication, or a tactile notification integrated into users‚Äô desktops.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Computer Science and Technology"
            }
          ],
          "personId": 126035
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 125757
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 125945
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 125878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126254
        }
      ]
    },
    {
      "id": 127130,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "ChromaNails: Re-Programmable Multi-Colored High-Resolution On-Body Interfaces using Photochromic Nail Polish",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615824"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1031",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "fabrication",
        "photochromic",
        "fingernail",
        "tangible",
        "smart material",
        "on-body interface",
        "color-change"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We demonstrate ChromaNails, a physical nail reprogramming device that enables high-resolution multi-color textures on fingernails using photochromic nail polish for on-body interaction. Our ChromaNails reprogrammer uses a miniature RGB projector and a UV light source to project different wavelengths of light onto our photochromic nail polish. We create this nail polish by mixing cyan, magenta, and yellow (CMY) photochromic dye into a base substrate polish. This enables us to control the saturation and desaturation of the CMY particles inside our nail polish to various colors inside the CMY color space. Our integrated user interface enables laypeople to select their preferred color texture and adapts to various nail shapes. We demonstrate the usefulness of ChromaNails for on-body interaction through four application examples on reprogrammable fingernail QR codes, on-body calendars, security, and fashion.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": ""
            }
          ],
          "personId": 127014
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": ""
            }
          ],
          "personId": 127040
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": ""
            }
          ],
          "personId": 126987
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": ""
            }
          ],
          "personId": 125767
        }
      ]
    },
    {
      "id": 127131,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demo of Double-Sided Tactile Interactions for Grasping in Virtual Reality",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1074",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Haptics",
        "tactile display",
        "electrotactile stimulation",
        "double-sided",
        "virtual reality",
        "skin."
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "For grasping, tactile stimuli to multiple fingertips are crucial for realistic shape rendering and precise manipulation. Pinching is particularly important in virtual reality since it is frequently used to grasp virtual objects. However, the interaction space of tactile feedback around pinching is underexplored due to a lack of means to provide co-located but different stimulation to finger pads. We propose a double-sided electrotactile device with a thin and flexible form factor to fit within pinched fingerpads. The device comprises two overlapping 3 √ó 3 electrode arrays to enable single-sided, simultaneous double-sided, and spatiotemporal double-sided electrotactile stimulation. Through two user studies, we (1) demonstrate that participants can accurately discriminate between single-sided and double-sided stimulation and find a qualitative difference in tactile sensation; and (2) confirm the occurrence of apparent tactile motion between fingers and present optimal parameters for continuous or discrete movements. In five VR applications, we demonstrate how double-sided tactile interactions can produce spatiotemporal movement of a virtual object between fingers and enrich touch feedback for UI operation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbruecken",
              "institution": "Saarland University",
              "dsl": ""
            }
          ],
          "personId": 126006
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 125866
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 125896
        }
      ]
    },
    {
      "id": 127132,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Cells, Generators, and Lenses: Design Framework for Object-Oriented Interaction with Large Language Models",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1030",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Machine Learning",
        "Text/Speech/Language",
        "Generative Models",
        "Large Language Models",
        "Writing-Support Tools"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Recently, numerous writing interfaces have been designed that connect end-users to large language models (LLMs) like GPT-4 to help them achieve their writing goals with minimal manual writing. However, as these generative models are non-deterministic and black box, end-users are faced with a new task: configuring generation pipelines (e.g., modifying inputs and parameters) until results are tailored to their needs. In this work, we present \"cells, generators, and lenses\", a framework for designing interfaces that support interactive objects that embody the components of generation pipelines (i.e., input, model, output). By applying our framework, designers can create interfaces that allow end-users to create multiple versions of these objects, compose them into pipelines, and mix-and-match pipelines to efficiently iterate and experiment on diverse configurations. Applying our framework, we redesigned three different writing systems‚Äîstory writing, copywriting, and email composing‚Äîand conducted a user study (N=18) to showcase how cells, generators, and lenses encouraged users to test more, generate more, and make more use of generated outputs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 126070
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 126011
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 125858
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 126439
        }
      ]
    },
    {
      "id": 127133,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating ecoEDA: Recycling E-waste During Electronics Design",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1077",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "sustainability",
        "recycling",
        "reuse",
        "electronic design automation (EDA)"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "The amount of e-waste generated by discarding devices is enormous but options for recycling remain limited. However, inside a discarded device (from consumer devices to one‚Äôs own prototypes), an electronics designer could find dozens to thousands of reusable components, including microcontrollers, sensors, voltage regulators, etc. Despite this, existing electronic design tools assume users will buy all components anew. To tackle this, we propose ecoEDA, an interactive tool that enables electronics designers to explore recycling electronic components during the design process. We accomplish this via (1) creating suggestions to assist users in identifying and designing with recycled components; and (2) maintaining a library of useful data relevant to reuse (e.g., allowing users to find which devices contain which components). Through example use-cases, we demonstrate how our tool can enable various pathways to recycling e-waste. To evaluate it, we conducted a user study where participants used our tool to create an electronic schematic with components from torn-down e-waste devices. We found that participants‚Äô designs made with ecoEDA featured an average of 66% of recycled components. Last, we reflect on challenges and opportunities for building software that promotes e-waste reuse.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126523
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago ",
              "dsl": ""
            }
          ],
          "personId": 126465
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126064
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126201
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "University of Chicago"
            }
          ],
          "personId": 126241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        }
      ]
    },
    {
      "id": 127134,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Telextiles: End-to-end Remote Transmission of Fabric Tactile Sensation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615770"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1033",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Texture recognition",
        "Self supervised learning",
        "Haptic feedback"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "The tactile sensation of textiles is critical in determining the comfort of clothing. In remote scenarios such as online shopping, sensors need to distinguish different garments even with hand-held sensors, and current actuation devices can only present a limited number of known patterns and cannot transmit unknown tactile sensations.\r\nWe propose Telextiles, an interface for remotely transmitting textile tactile sensations, which uses contrastive self-supervised learning to create a latent space that reflects the relative proximity of textiles. We convert the latent features into a scalar for the one-dimensional structure of the roller. We then select 16 equidistant samples from this line to represent different regions of the latent space. The roller rotates to select the textile with the closest feature. We also show visually the relationship between a textile touched by a remote user and a set of textiles previously registered in our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Interdisciplinary Information Studies/ Rekimoto Lab (Human Augmentation Lab)"
            }
          ],
          "personId": 125805
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Cluster Inc.",
              "dsl": "Metaverse Lab."
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 126625
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Rekimoto Lab"
            }
          ],
          "personId": 125928
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 126078
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 125780
        }
      ]
    },
    {
      "id": 127135,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "BrushLens: Hardware Interaction Proxies for Accessible Touchscreen Interface Actuation",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1076",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Touchscreen appliances",
        "Accessibility",
        "Interaction proxy",
        "Computer vision",
        "Touch actuation"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Touchscreen devices, designed with an assumed range of user abilities and interaction patterns, often present challenges for individuals with diverse abilities to operate independently. Prior efforts to improve accessibility through tools or algorithms necessitated alterations to touchscreen hardware or software, making them inapplicable for the large number of existing legacy devices. In this paper, we introduce BrushLens, a hardware interaction proxy that performs physical interactions on behalf of users while allowing them to continue utilizing accessible interfaces, such as screenreaders and assistive touch on smartphones, for interface exploration and command input. BrushLens maintains an interface model for accurate target localization and utilizes exchangeable actuators for physical actuation across a variety of device types, effectively reducing user workload and minimizing the risk of mistouch. Our evaluations reveal that BrushLens lowers the mistouch rate and empowers visually and motor impaired users to interact with otherwise inaccessible physical touchscreens more effectively.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126617
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Electrical Engineering and Computer Science"
            }
          ],
          "personId": 126315
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "The University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 126589
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan - Ann Arbor",
              "dsl": "Robotics"
            }
          ],
          "personId": 126558
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 126041
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126260
        }
      ]
    },
    {
      "id": 127136,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating BrightMarkers: Fluorescent Tracking Markers Embedded in 3D Printed Objects",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615977"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1032",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "3D printing",
        "digital fabrication",
        "fluorescence",
        "infrared imaging",
        "marker tracking",
        "invisible markers",
        "object tracking"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "In this demonstration, we showcase BrightMarker, a fabrication method that uses fluorescent filaments to embed easily trackable markers in 3D printed color objects. By employing an infrared-fluorescent filament that emits light at a wavelength higher than the incident light, our optical detection setup filters out all the noise to only have the markers present in the infrared camera image. The high contrast of the markers allows us to robustly track them when objects are in motion.\r\n\r\nWe also demonstrate a software interface for automatically embedding these markers for the input object geometry, and a hardware module with a high-speed camera that can be attached to existing AR/VR headsets. We developed an image processing pipeline to robustly localize the markers in real time from the captured images. We illustrate applications for fabricating wearables for motion capture, tangible interfaces for AR/VR, and enabling rapid product tracking, as well as privacy-preserving night vision.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 125841
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126570
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126097
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126474
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Leganes",
              "institution": "Universidad Carlos III de Madrid",
              "dsl": "University Group for Identification Technologies"
            }
          ],
          "personId": 126489
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 126305
        }
      ]
    },
    {
      "id": 127137,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Towards Designing a Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation: A Demonstration of GazePointAR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615819"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1112",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "augmented reality",
        "multimodal input",
        "voice assistants",
        "gaze tracking",
        "pointing gesture recognition",
        "LLM"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Voice assistants (VAs) like Siri and Alexa have transformed how humans interact with technology; however, their inability to consider a user's spatiotemporal context, such as surrounding objects, drammatically limits natural dialogue. In this demo paper, we introduce GazePointAR, a wearable augmented reality (AR) system that resolves ambiguity in speech queries using eye gaze, pointing gesture, and conversation history. With GazePointAR, a user can ask \"what‚Äôs over there?\" or \"how do I solve this math problem?\" simply by looking and/or pointing. We describe GazePointAR's design and highlight supported use cases.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126992
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 127087
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computational Linguistics"
            }
          ],
          "personId": 127058
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Applied and Computational Mathematical Sciences"
            }
          ],
          "personId": 127004
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126967
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 127022
        }
      ]
    },
    {
      "id": 127138,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstration of A Figma Plugin to Simulate A Large-Scale Network for Prototyping Social Systems",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615780"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1078",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "social computing, prototyping, network simulation, generative AI"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Prototyping novel social computing systems is a challenge in the field of Social Computing. Rapid experimentation with novel social network sites can offer valuable insights into their pro-social benefits before their public release to a large audience. In this demo, we present SocialSketch, a Figma Plugin to simulate a crowd in a social network. This demo introduces a plugin for Figma, a no-code interactive prototyping tool, enabling the creation of profile frames and prototype links for large crowds based on realistic network models. Privacy-protective profile content is generated using AI. The plugin aids UX designers, researchers, and students in prototyping social apps and exploring social system design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Waltham",
              "institution": "Wellesley College",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 126941
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College",
              "dsl": ""
            }
          ],
          "personId": 126945
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College",
              "dsl": ""
            }
          ],
          "personId": 127075
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College",
              "dsl": ""
            }
          ],
          "personId": 127009
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127081
        }
      ]
    },
    {
      "id": 127139,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Towards Real-time Computer Vision and Augmented Reality to Support Low Vision Sports: A Demonstration of ARTennis",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615815"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1111",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "augmented reality",
        "accessibility",
        "sports",
        "computer vision"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Individuals with low vision (LV) can experience vision-related challenges when participating in sports, especially those with fast-moving objects. We introduce ARTennis, a prototype for wearable augmented reality (AR) that utilizes real-time computer vision (CV) to enhance the visual saliency of tennis balls. Preliminary findings indicate that while ARTennis is helpful, combining both visual and auditory cues may be more effective. As AR and CV technologies continue to improve, we expect head-worn AR to broaden the inclusivity of sports.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126992
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127002
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 126963
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": ""
            }
          ],
          "personId": 127010
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 127087
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 127001
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 127022
        }
      ]
    },
    {
      "id": 127140,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "SmartPoser: Arm Pose Estimation With a Smartphone and Smartwatch Using UWB and IMU Data",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1113",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Pose Tracking",
        "Input Devices",
        "Wearables"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "The ability to track a user‚Äôs arm pose could be valuable in a wide range of applications, including fitness, rehabilitation, augmented reality input, life logging, and context-aware assistants. Unfortu- nately, this capability is not readily available to consumers. Systems either require cameras, which carry privacy issues, or utilize mul- tiple worn IMUs or markers. In this work, we describe how an off-the-shelf smartphone and smartwatch can work together to accurately estimate arm pose. Moving beyond prior work, we take advantage of more recent ultra-wideband (UWB) functionality on these devices to capture absolute distance between the two devices. This measurement is the perfect complement to inertial data, which is relative and suffers from drift. We quantify the performance of our software-only approach using off-the-shelf devices, showing it can estimate the wrist and elbow joints with a median positional error of 11.0 cm, without the user having to provide training data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126171
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126353
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126334
        }
      ]
    },
    {
      "id": 127141,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Melody Slot Machine on iPhone: Dial-type Interface for Morphed Melody",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615764"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1039",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Generative Ttheory of Tonal Music (GTTM)",
        "Melody Morphing, Slot Machine, Dial Type Interface"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We present Melody Slot Machine on iPhone, an iPhone application using the melodic morphing method on the basis of the Generative Theory of Tonal Music (GTTM). We previously developed a demonstration system called Melody Slot Machine to introduce the melodic morphing method and presented it at international conferences and exhibitions. Since in-person demonstrations were reduced due to the Covid-19 Pandemic, we implemented an application with the same functionality and made it available for downloading to experience it. Our Melody Slot Machine on iPhone currently has two contents available for download, and we plan to add more contents in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN",
              "dsl": ""
            }
          ],
          "personId": 126214
        }
      ]
    },
    {
      "id": 127142,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demo of Biohybrid Devices: Prototyping Interactive Devices with Growable Materials ",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1038",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Living bio-materials are increasingly used in HCI for fabricating objects by growing. However, how to integrate electronics to make these objects interactive still needs to be clarified. This paper presents an exploration of the fabrication design space of Biohybrid Interactive Devices, a class of interactive devices fabricated by merging electronic components and living organisms. From the exploration of this space using bacterial cellulose, we outline a fabrication framework centered on the biomaterials‚Äò life cycle phases. We introduce a set of novel fabrication techniques for embedding conductive elements, sensors, and output components through biological (e.g. bio-fabrication and bio-assembling) and digital processes. We demonstrate the combinatory aspect of the framework by realizing three tangible, wearable, and shape-changing interfaces. Finally, we discuss the sustainability of our approach, its limitations, and the implications for bio-hybrid systems in HCI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris La D√©fense",
              "institution": "L√©onard de Vinci P√¥le Universitaire, Research Center",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126345
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris La D√©fense",
              "institution": "L√©onard de Vinci P√¥le Universitaire, Research Center",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "N√Æmes",
              "institution": "Universit√© de Nimes",
              "dsl": "Projekt and Chrome"
            }
          ],
          "personId": 125773
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 126640
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "ile de France",
              "city": "Palaiseau",
              "institution": "T√©l√©com Paris, Institut Polytechnique de Paris",
              "dsl": "Dpt. SES, CNRS i3"
            }
          ],
          "personId": 126285
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 125896
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris La D√©fense",
              "institution": "L√©onard de Vinici P√¥le Universitaire",
              "dsl": "Research Center"
            }
          ],
          "personId": 125983
        }
      ]
    },
    {
      "id": 127143,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Experiencing Visual Blocks for ML: Visual Prototyping of AI Pipelines",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615817"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1115",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Visual Programming",
        "Large Language Models",
        "Visual Prototyping",
        "Multi-Modal Models",
        "Node-Graph Editor",
        "Deep Neural Networks",
        "Data Augmentation",
        "Deep Learning",
        "Visual Analytics"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We demonstrate Visual Blocks for ML, a visual programming platform that facilitates rapid prototyping of ML-based multimedia applications. As the public version of Rapsai , we further integrated large language models and custom APIs into the platform. In this demonstration, we will showcase how to build interactive AI pipelines in a few drag-and-drops, how to perform interactive data augmentation, and how to integrate pipelines into Colabs. In addition, we demonstrate a wide range of community-contributed pipelines in Visual Blocks for ML, covering various aspects including interactive graphics, chains of large language models, computer vision, and multi-modal applications. Finally, we encourage students, designers, and ML practitioners to contribute ML pipelines through https://github.com/google/visualblocks/tree/main/pipelines to inspire creative use cases. Visual Blocks for ML is available at http://visualblocks.withgoogle.com.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127053
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127045
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127033
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127020
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126961
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126974
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126971
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127056
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126952
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127068
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126005
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127080
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126970
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127043
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Inc.",
              "dsl": ""
            }
          ],
          "personId": 127011
        }
      ]
    },
    {
      "id": 127144,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Fluid Reality: High-Resolution, Untethered Haptic Gloves using Electroosmotic Pump Arrays",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1080",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Haptic Gloves",
        "Tactile Feedback",
        "Shape-changing Interfaces",
        "EEOPs",
        "Virtual Reality",
        "Augmented Reality",
        "XR"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Virtual and augmented reality headsets are making significant progress in audio-visual immersion and consumer adoption. However, their haptic immersion remains low, due in part to the limitations of vibrotactile actuators which dominate the AR/VR market. In this work, we present a new approach to create high-resolution shape-changing fingerpad arrays with 20 haptic pixels/cm^2. Unlike prior pneumatic approaches, our actuators are low-profile (5mm thick), low-power (approximately 10mW/pixel), and entirely self-contained, with no tubing or wires running to external infrastructure. We show how multiple actuator arrays can be built into a five-finger, 160-actuator haptic glove that is untethered, lightweight (207g, including all drive electronics and battery), and has the potential to reach consumer price points at volume production. We describe the results from a technical performance evaluation and a suite of eight user studies, quantifying the diverse capabilities of our system. This includes recognition of object properties such as complex contact geometry, texture, and compliance, as well as expressive spatiotemporal effects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 126089
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Fluid Reality Inc",
              "dsl": ""
            }
          ],
          "personId": 126247
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126334
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            },
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Fluid Reality Inc",
              "dsl": ""
            }
          ],
          "personId": 126443
        }
      ]
    },
    {
      "id": 127145,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "InkBrush: A Flexible and Controllable Authoring Tool for 3D Ink Painting",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615782"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1082",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "modeling",
        "sketching",
        "ink painting",
        "graphic"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "As a stylized representation of traditional painting art, ink painting has been widely used in most of Asian countries. Art creations with ink painting strokes in 3D space have shown potential in animation and games.We propose a sketch-based authoring tool for drawing ink painting style strokes in 3D space. With automatic strip-based modeling and procedural texture generation, users can easily create ink painting style artworks by drawing sketches. Our system generates ink effects in real time and provides a simple non-linear workflow.User evaluations show that novice users can easily use the system and quickly create intriguing 3D ink paintings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 127078
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 127072
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Academy of Arts & DesignÔºåTsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Academy of Arts & DesignÔºåTsinghua University",
              "dsl": ""
            }
          ],
          "personId": 126988
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 127016
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Academy of Arts and Design, the Future Lab"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Academy of Arts and Design, the Future Lab"
            }
          ],
          "personId": 126950
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 126452
        }
      ]
    },
    {
      "id": 127146,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "LensTouch: Touch Input on Lens Surfaces of Smart Glasses",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615792"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1081",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Smart Glasses",
        "Augmented Reality",
        "Touch Input"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Smart glasses are promising devices that allow the user to experience augmented reality (AR) and watch movies as on a big screen.As their design is primarily focused on their function as output devices, their input functionality is limited.We propose LensTouch, which enhances the input vocabulary by using touches on the lens of the smart glasses as inputs.The user can place his/her finger on the lens while viewing both the finger and the image displayed.An experiment shows the user can select the target quickly and/or accurately depending on the setting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127042
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 125861
        }
      ]
    },
    {
      "id": 127147,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "LiveLocalizer: Augmenting Mobile Text-to-Speech with Microphone Arrays, Optimized Localization and Beamforming",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615789"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1084",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Speech-to-text, ASR, STT, audio, speech, microphone array, beamforming, accessibility"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Speech-to-text capabilities on mobile devices have proven helpful for language translation, note-taking, hearing and speech accessibility, and meeting transcripts. However, their usefulness is constrained by being unable to distinguish between multiple speakers, track which direction speech is coming from, and provide acceptable performance in noisy environments. \r\n\r\nThis work introduces efficient real-time audio localization and adaptive beamforming algorithms on custom sound perception hardware running on a low-power microcontroller and four integrated microphones. A prototype is implemented in a  phone case form factor and is plug-and-play with modern smartphones. \r\n    \r\nWe characterize the performance in technical evaluations of localization, beamforming, and diarization. We demonstrate how the phone case extends existing smartphones with speaker diarization in a speech-to-text app, sound direction visualization, and sound enhancement through beamforming. In the future, we hope our approach will inspire the widespread adoption of advanced microphone arrays that natively unlock the potential of spatial sound processing and perception in mobile and wearable devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 127082
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 126972
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127062
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 127003
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127044
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Inc.",
              "dsl": ""
            }
          ],
          "personId": 127011
        }
      ]
    },
    {
      "id": 127148,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "GenAssist: Making Image Generation Accessible",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1083",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Blind and low vision (BLV) creators use images to communicate with sighted audiences. However, creating or retrieving images is challenging for BLV creators as it is difficult to use authoring tools or assess image search results. Thus, creators limit the types of images they create or recruit sighted collaborators. While text-to-image generation models let creators generate high-fidelity images based on a text description (i.e. prompt), it is difficult to assess the content and quality of generated images. We present GenAssist, a system to make text-to-image generation accessible. Using our interface, creators can verify whether generated image candidates followed the prompt, access additional details in the image not specified in the prompt, and skim a summary of similarities and differences between image candidates. To power the interface, GenAssist uses a large language model to generate visual questions, vision-language models to extract answers, and a large language model to summarize the results. Our study with 12 BLV creators demonstrated that GenAssist enables and simplifies the process of image selection and generation, making visual authoring more accessible to all.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas, Austin",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 125987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126328
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas, Austin",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126414
        }
      ]
    },
    {
      "id": 127149,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1086",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "This paper introduces HoloBots, a mixed reality interface that augments holographic telepresence with a synchronized actuated tangible user interface. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also enables them to \\textit{physically} engage with the local users and their environment, allowing them to touch, grasp, manipulate, and interact with remote physical objects as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio), which enables scalable, deployable, and generalizable tangible remote collaboration. HoloBots allows various interactions, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results with 12 participants confirm that our system significantly enhances the level of co-presence and shared experience of mixed reality remote collaboration, compared to the other conditions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 125865
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 126434
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 126574
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 126340
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 125846
        }
      ]
    },
    {
      "id": 127150,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating Sustainflatable: Harvesting, Storing and Utilizing Ambient Energy for Pneumatic Morphing Interfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615765"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1042",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "pneumatic interface",
        "energy harvesting",
        "shape-changing interface"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "While the majority of pneumatic interfaces are powered and controlled by traditional electric pumps and valves, alternative sustainable energy harnessing technology has been attracting attention. This paper presents a novel solution to this challenge with the development of the Sustainflatable system, a self-sustaining pneumatic system that can harvest renewable energy sources such as wind, waterflow, moisture, and sunlight, convert the energy into compressed air, and store it for later use in a programmable and intelligent way. The system is completely electronic-free, incorporating customized energy harvesting pumps, storage units with variable volume-pressure characteristics, and tailored valves that operate autonomously. Additionally, the paper provides a design tool to guide the development of the system and includes several environmental applications to showcase its capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University ",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126509
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 126042
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Lab"
            }
          ],
          "personId": 126470
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Future Interfaces Group"
            }
          ],
          "personId": 126537
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 126452
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126193
        }
      ]
    },
    {
      "id": 127151,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Demonstrating 1D-Touch: NLP-Assisted Coarse Text Selection via a Semi-Direct Gesture",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615797"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1085",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Text selection",
        "Natural Language Processing",
        "Touch interface"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Existing text selection techniques on touchscreen focus on improving the control for moving the carets. Coarse-grained text selection on word- and phrase- levels have not received much support beyond word-snapping and entity recognition. We introduce 1D-Touch, a novel text selection method that complements the carets-based sub-word selection by facilitating the selection of words and larger semantic units. This method employs a simple vertical slide gesture to expand and contract a selection area from a word. The expansion can be by words or by semantic chunks ranging from sub-phrases to sentences, as implemented in two variants of our technique named WordTouch and ChunkTouch. This approach shifts the concept of text selection, away from defining a range by locating the first and last characters, towards a dynamic process of expanding and contracting a textual entity. While the full paper (expected to appear at the ACM ISS 2023) details the evaluation, this demonstration showcases 1D-Touch with a few applications of coarse-grained text selection, to engage the audience in discussions about its effectiveness and applications, as well as its integration with existing character-level selection techniques.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126182
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": "Computational Media and Arts Thrust"
            }
          ],
          "personId": 126993
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 127077
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 126359
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 127036
        }
      ]
    },
    {
      "id": 127152,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Projectoroid: A Mobile Robot-Based SAR Display Approach",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615793"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1041",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Spatial Augmented Reality",
        "Robotics",
        "Projection Mapping",
        "Mobile"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "This paper introduces \"Projectoroid,\" a mobile robot equipped with a projector that facilitates mobile spatial augmented reality (SAR) displays, and examines the potential of SAR deployment via a mobile robot by comparing and evaluating content display methods. Projectoroid converts its real-world position and orientation calculated by its internal sensors into the position and orientation of a camera in virtual space. Consequently, it can project the image captured within the virtual space onto the real-world floor. Thus, Projectoroid is capable of revealing a segment of an expansive virtual space into the real world based on its position and orientation. In this paper, we refer to the degree to which people recognize the correspondence between the virtual space and the real world from the projected image as the \"sense of reveal.\"We believe that an increase in the sense of reveal would heighten people's interest in the content from the virtual world. The results of our user study show that the sense of reveal is amplified as the outline of the projected image becomes more circular.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Fujisawa",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 126973
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Fujisawa",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 126190
        }
      ]
    },
    {
      "id": 127153,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "EyeClick: A Robust Two-Step Eye-Hand Interaction for Text Entry in Augmented Reality Glasses",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615814"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1121",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "text entry, text input, augmented reality, gaze detection, eye-tracking"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Efficient text entry is a crucial aspect of the user experience for augmented reality (AR) head-mounted displays (HMD). Eye-tracking for virtual keyboard interaction is a popular choice for AR text entry, as it is intuitive, privacy-preserving, and socially acceptable. However, as AR HMDs move toward more consumer-friendly form factors and price points, technical constraints necessitate trade-offs that result in a limited field of view (FoV) and reduced eye-tracking accuracy.\r\n\r\nTo address these challenges, we develop EyeClick, a novel two-step eye-hand interaction method utilizing a modified QWERTY keyboard for AR HMDs. To type, users first employ gaze to select a large area containing multiple characters, then press a button on a handheld controller to choose a single character from the selected area. We demonstrated competitive speed and reduced corrected error rate (CER) with two variants of the EyeClick design: ColType (7.30 WPM) and RowType (9.41 WPM). \r\n\r\nWe also integrated ChatGPT with the EyeClick keyboard to build an interactive information retrieval AR application. We demonstrate that the EyeClick keyboard can help users efficiently extract information about surroundings by typing to ChatGPT in mixed reality.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": ""
            }
          ],
          "personId": 126943
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": ""
            }
          ],
          "personId": 126984
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": "HMI"
            }
          ],
          "personId": 127050
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Robert Bosch LLC",
              "dsl": ""
            }
          ],
          "personId": 127025
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": ""
            }
          ],
          "personId": 127061
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research and Technology Center, North America",
              "dsl": ""
            }
          ],
          "personId": 127063
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Robert Bosch Research",
              "dsl": ""
            }
          ],
          "personId": 127024
        }
      ]
    },
    {
      "id": 127154,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Tyche: In Situ Exploration of Random Testing Effectiveness",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615788"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1088",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "random testing",
        "property-based testing",
        "data visualization"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Automated testing tools have adapted to increasing program\r\ncomplexity by reducing the\r\nuser's role in the testing process.\r\nApproaches like property-based testing supplement traditional\r\nunit-testing with a mode declarative approach: rather than write traditional\r\ninput-output examples, the user writes executable specifications of their\r\nprograms. The testing framework then exercises those specifications with\r\nrandomly generated values.\r\n\r\nHowever, more automated approaches to testing risk hiding\r\ntoo much from the\r\nuser. Current property-based testing frameworks give insufficient feedback\r\nabout the specific values that were used to test a given program and\r\nabout the distributional trends in those values. In the worst case,\r\nthis lack of visibility process may give users false\r\nconfidence, encouraging them to believe their testing was thorough when,\r\nin fact, it had critical gaps.\r\n\r\nWe demonstrate Tyche, an editor extension that recovers\r\nvisibility into the property-based testing process. Tyche provides an\r\ninteractive interface for understanding testing effectiveness, surfacing both\r\n\"pre-testing\" information about test inputs and their distributions\r\nand \"post-testing\"\r\ninformation like code coverage. The extension is designed to work\r\nout of the box with tests written in Python's popular Hypothesis framework, so\r\nusers can immediately start using it to improve their testing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Computer and Information Science"
            }
          ],
          "personId": 126991
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Computer & Information Science"
            }
          ],
          "personId": 127000
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Department of Computer and Information Science"
            }
          ],
          "personId": 126477
        }
      ]
    },
    {
      "id": 127155,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "CriTrainer: An Adaptive Training Tool for Critical Paper Reading",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1043",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Human-centered computing",
        "Paper reading",
        "Critical thinking"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Learning to read scientific papers critically, which requires first grasping their main ideas and then raising critical thoughts, is important yet challenging for novice researchers. The traditional ways to develop critical paper reading (CPR) skills, e.g., checking general tutorials or taking reading courses, often can not provide individuals with adaptive and accessible support. In this paper, we first derive user requirements of a CPR training tool based on literature and a survey study (N=52). Then, we develop \\name{}, an interactive tool for CPR training. It leverages text summarization techniques to train readers‚Äô skills in grasping the paper‚Äôs main ideas. It further utilizes template-based generated questions to help them learn how to raise critical thoughts. A mixed-design study (N=24) shows that compared to a baseline tool with general CPR guidance, students trained by \\name{} perform better in independently raising critical thinking questions on a new paper. We conclude with design considerations for CPR training tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 125965
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126518
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126093
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 126075
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 126322
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 126222
        }
      ]
    },
    {
      "id": 127156,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "Towards Generating UI Design Feedback with LLMs",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615810"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1120",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "Large Language Models, Computational UI Design, Design Feedback"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Feedback on user interface (UI) mockups is crucial for the design process, and designers often seek and leverage feedback to improve their UIs. However, human feedback is not always readily available. Given the recent emergence of LLMs, which have been shown to be proficient in rule-based reasoning, we explore the potential of LLMs to provide feedback automatically. In particular, we investigate automating heuristic evaluation, which currently entails a human expert assessing how well a UI adheres to a given set of design guidelines. We build an LLM-based heuristic  evaluation plugin for Figma, which designers can use to evaluate their UI mockups. The plugin queries the LLM with the guidelines and a JSON representation of the UI mockup and then renders the identified guideline violations as constructive suggestions for design improvements. Future work is needed to study what types of usability problems can be successfully identified by LLM-driven heuristic evaluation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 126985
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 126435
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 125988
        }
      ]
    },
    {
      "id": 127157,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "SoundBlender: Manipulating Sounds for Accessible Mixed-Reality Awareness",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615787"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1087",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "VR/AR",
        "extended reality",
        "mixed reality",
        "sound",
        "interaction design",
        "accessibility"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Sounds are everywhere, from real-world content to virtual audio presented by hearing devices, which create a mixed-reality soundscape that entails rich but intricate information. However, sounds often overlap and conflict in priorities, which makes them hard to perceive and differentiate. This is exacerbated in mixed-reality settings, where real-world and virtual sounds can conflict with each other. This may exacerbate the awareness of mixed reality for blind people who heavily rely on audio information in their everyday life. To address this, we present a sound rendering framework SoundBlender, consisting of six sound manipulators for users to better organize and manipulate real and virtual sounds across time and space: Ambience Builder, Feature Shifter, Earcon Generator, Prioritizer, Spatializer, and Stylizer. We demonstrate how the sound manipulators can increase mixed-reality awareness through a simulated working environment, and a meeting application.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126565
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126976
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 127017
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126260
        }
      ]
    },
    {
      "id": 127158,
      "typeId": 13090,
      "durationOverride": 180,
      "title": "SketchSearch: Fine-tuning Reference Maps to Create Exercises In Support of Video-based Learning for Surgeons",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3615816"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23e-1122",
      "source": "PCS",
      "trackId": 12417,
      "tags": [],
      "keywords": [
        "video-based coaching",
        "video navigation",
        "keyframe extraction",
        "surgical training",
        "minimally invasive surgery"
      ],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Video-based surgical coaching involves mentors reviewing surgery footage with trainees. Although effective, its use is sporadic due to time constraints. We propose AI-augmented coaching through SketchSearch, allowing experts to create exercises with automated feedback in surgery videos for self-learning. Surgeons often seek specific scenes for teaching, relying on visual cues. SketchSearch simplifies this through a three-step process: key frame extraction, template reference maps creation via image segmentation, and fine-tuning for frame retrieval.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 127057
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 126949
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Medical School "
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 127070
        }
      ]
    },
    {
      "id": 127185,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "ZINify: Transforming Research Papers into Engaging Zines with Large Language Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625118"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-3915",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Research papers are a vital building block for scientific discussion. While these papers follow effective structures for the relevant community, they are unable to cater to novice readers and express otherwise creative ideas in creative mediums. To this end, we propose ZINify, the first approach to automatically transform research papers into engaging zines using large language models (LLM) and text-to-image generators. Following zine's long history of supporting independent, creative expression, we propose a technique that can work with authors to build more engaging, marketable, and unconventional content that is based on their research. We believe that our work will help make research more engaging and accessible to all while helping papers stand out in crowded online venues.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": ""
            }
          ],
          "personId": 127170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127184
        }
      ]
    },
    {
      "id": 127186,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "Integrating a LLM into an Automatic Dance Practice Support System: Breathing Life Into The Virtual Coach",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625119"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-9482",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We propose using a LLM to breathe life into our existing dance learning app in the form of an AI dance coach persona. The current app guides the user through dance practice plans, using the webcam and pose estimation to give feedback. Using the LLM, voice recognition, speech synthesis, and affect recognition, we plan to transform the interface from a mechanical click-on-screen experience to a hands-free speak-with-the-coach interaction. In particular, we'll use the LLM to announce coaching guidance, provide encouragement, communicate feedback, and interpret the user's spoken intent.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": ""
            }
          ],
          "personId": 127084
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": ""
            }
          ],
          "personId": 127183
        }
      ]
    },
    {
      "id": 127187,
      "typeId": 13091,
      "title": "Democratizing Content Creation and Consumption through Human-AI Copilot Systems",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23f-1014",
      "source": "PCS",
      "trackId": 12420,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127849
      ],
      "eventIds": [],
      "abstract": "Content creation and consumption play vital roles in our lives. However, creating high-quality content can be challenging for beginners, while navigating through and consuming vast amounts of media content can be overwhelming and cumbersome. My Ph.D. research focuses on democratizing content creation and improving content consumption experiences for everyday users. I achieve this by designing and evaluating interactive AI systems that serve as copilots, assisting users with tedious tasks. I explore various media modalities, such as video, audio, text, and images, and investigate how their interplay can address problems in individual modalities. This paper offers a comprehensive overview of my research agenda, including recent contributions, on-going progress, and future directions.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126302
        }
      ]
    },
    {
      "id": 127188,
      "typeId": 13091,
      "title": "Chemical Interfaces: New Methods for Interfacing with the Human Senses",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23f-1022",
      "source": "PCS",
      "trackId": 12420,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127849
      ],
      "eventIds": [],
      "abstract": "Since the advent of computer interfaces, significant strides have been made toward providing high-fidelity visual, auditory, and haptic experiences. However, stimulating the senses of smell, taste, and temperature has sorely stagnated over the last century, primarily due to the field‚Äôs dependence on adapted methods from mechanical and robotic engineering. I posit that novel interfacing techniques are necessary to engage the human senses fully. My research explores the integration of these underutilized yet crucial senses by investigating a new class of devices termed ‚Äúchemical interfaces.‚Äù These devices manipulate the human senses by interfacing with our perceptions‚Äô biochemical cascades through carefully selected chemicals. Unlike conventional approaches originating from robotics or mechanical engineering, chemical interfaces provide distinctive affordances. My research illustrates several such affordances, including reduced power consumption for thermal feedback, miniaturized versatile mechanisms for haptics, and new interactions for taste via chemical selectivity. My approach lays a promising foundation for incorporating these rich senses into our digital interactions and, maybe in the future, influencing how we engage with our food and the air we breathe in our everyday life.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 125782
        }
      ]
    },
    {
      "id": 127189,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "4-Frame Manga Drawing Support System",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625218"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-2588",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "This project proposes a 4-frame manga drawing support system that assists users in creating drawings. The proposed system recognizes each frame of an unfinished manga drawn by a user and proposes successive frames, considering the content recognized till then, e.g., the storyline, frame composition, and punchline. The system updates the proposal as the manga drawing proceeds. The proposed system comprises four modules for 1) recognizing the user‚Äôs drawings, 2) generating four sentences that describe the storyline, 3) generating images, each of which corresponds to a manga frame from the above sentences, and 4) choosing the user-preferred manga candidate from a number of potential choices. The proposed system does not require AI to generate the 4-frame manga; instead, the user draws the 4-frame manga with the help of the system. In other words, the user decides whether to accept or reject the AI‚Äôs proposal.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sanda",
              "institution": "Kwansei Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 127169
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sanda",
              "institution": "Kwansei Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 127171
        }
      ]
    },
    {
      "id": 127190,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "Narratron: Co-writing and Co-performing Children Story With Large Language Model through Shadow Play",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625120"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-4657",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Shadow puppetry or shadow play, allows bodily participation into the process of linguistic storytelling, while the potential of multi-modal interaction through shadow plays in existing large-language-model-based creative tools has not been fully discovered. We propose Narratron, a generative story-making tool that co-creates and co-performs children stories from shadow using Claude 2 model. To achieve Narratron, our system is designed to recognize hand gestural inputs as main character and to develop story plot in accordance with character change. Through our system, we seek to stimulate creativity in shadow play storytelling and to facilitate a multi-modal human-AI collaboration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "HCDE"
            }
          ],
          "personId": 127162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "Graduate School of Design"
            }
          ],
          "personId": 127161
        }
      ]
    },
    {
      "id": 127191,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "LingoLand: An AI-Assisted Immersive Game for Language Learning",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625117"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-5541",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Immersion with a foreign language is key to increased motivation, satisfaction, and learning success. However, this can be inhibited by anxiety and lack of access to immersive settings. LingoLand will address this by allowing people to immerse in foreign language learning environments designed to mimic real scenarios. Using generative machine learning, LingoLand presents players with customized missions where they can freely interact with game characters, all of whom endlessly patient and supportive of new language learners. Players receive instant feedback through a natural, voice-based interaction. Through LingoLand, players gain an understanding of different cultures while building practical language skills in a fun way.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": ""
            }
          ],
          "personId": 125954
        }
      ]
    },
    {
      "id": 127192,
      "typeId": 13091,
      "title": "Physical-Digital Programming",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23f-1019",
      "source": "PCS",
      "trackId": 12420,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127849
      ],
      "eventIds": [],
      "abstract": "In exploratory digital fabrication, artists, scientists, and engineers use machines and code to prototype object forms and ways of making that do not currently exist. However, prior research has yet to provide expressive agency for makers doing this work, which includes rapid experimentation, safe adjustments and detailed, accurate control of machines and data. To address this, I frame exploratory fabrication as physical-digital programming: handling physical contingencies within core features of a programming language. I posit that physical-digital programming will let exploratory makers build digital fabrication workflows that are otherwise prohibitively difficult to construct, share, and improve. To test this hypothesis, I built three systems that prototype components of physical-digital programming: a common, multimodal programming environment; a formal grammar of machine functionality; and a method of generating task-specific visualizations of machine behavior. In addition, I am working on a fourth system intended to synchronize code assumptions with physical-world states through user-defined axioms and augmented reality. These tools will help both novice and experienced makers use programming to solve emerging problems that require physical automation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 126137
        }
      ]
    },
    {
      "id": 127193,
      "typeId": 13091,
      "title": "User Interface Constraints to Influence User Behaviour when Reading and Writing",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23f-1008",
      "source": "PCS",
      "trackId": 12420,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127849
      ],
      "eventIds": [],
      "abstract": "Constraints are fundamental to human-centered design. Although by definition, constraints \"limit\" or \"restrict\" the capability of software, when designed correctly, they can have enabling characteristics as well. In my dissertation, I seek to understand how user interface constraints can influence user behaviour when reading and writing text. First, I discuss a document reader with auto-scrolling to facilitate time-bounded reading for increased focus. Second, I contribute the idea of limiting how much text can be highlighted in a document to encourage readers to think more about what is truly important in the document. Lastly, I discuss how constraining an AI writing assistant through prompts with varying levels of detail may improve a writer's feelings of ownership. Through these three projects, my dissertation will contribute novel constraints-based interaction techniques that can be integrated into new or existing systems, which is of interest to the UIST community and the HCI community more broadly.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 126351
        }
      ]
    },
    {
      "id": 127194,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "ProactiveAgent: Personalized Context-Aware Reminder System",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625115"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-5311",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We present ProactiveAgent, a proactive reminder and suggestion application that offers personalized recommendations by integrating real-time contextual information, user history, and memory. Leveraging the capabilities of large language models and personal agents, our system comprehends user intent by analyzing real-world context, past interactions, and speech. Through a synthesis of user behavior insights and experiences, the application provides tailored suggestions that help guide the user in their daily lives. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 127172
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 127163
        }
      ]
    },
    {
      "id": 127195,
      "typeId": 13097,
      "durationOverride": 480,
      "title": "Architecting Novel Interactions with Generative AI Models",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23h-9482",
      "source": "PCS",
      "trackId": 12419,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127748
      ],
      "eventIds": [],
      "abstract": "The new generation of generative AI models offers interactive opportunities that may fulfill long-standing aspirations in human-computer interaction and open doors to new forms of interaction that we have yet to imagine. The UIST community has a unique vantage point that can lead to critical contributions in envisioning a future of interactive computing that appropriately leverages the power of these new generative AI models. However, we are only just beginning to understand the research area that exists at the intersection of interaction and generative AI. By bringing together members of the UIST community interested in this intersection, we seek to initiate discussions on the potential of generative AI in architecting new forms of interactions. Key topics of interest include the exploration of novel categories of interactions made possible by generative AI, the development of methods for enabling more powerful and direct user control of generative AI, and the identification of model and architecture requirements for generative AI in interaction literature. The workshop will foster community building and produce concrete deliverables, including a research agenda, model/architecture requirements, and a simulated debate generated by a generative agent architecture.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126088
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 126609
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google DeepMind",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 126393
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research AI",
              "dsl": ""
            }
          ],
          "personId": 127180
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 127182
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Electrical Engineering and Computer Science"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 127165
        }
      ]
    },
    {
      "id": 127196,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "Aisen - Web-Based Gaze-Tracking Assistive Communication Interface with Word Cards Generated by LLMs",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625116"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-5671",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Aisen is an innovative web-based communication tool that integrates the WebGazer.js library with advanced large language models (LLMs). Designed as an affordable communication solution for those with communication challenges, Aisen facilitates expression through a unique word-selection interface. Rather than using a traditional keyboard, Aisen introduces a two-tiered \"word card\" system. This system includes a static set of cards tailored to the patient's specific needs and a \"dynamic\" set where cards are intelligently generated by LLMs based on user input and preferences. Our research delineated three specific user personas, emphasizing Aisen's applicability for elderly patients. The platform integrates an eye-tracking mechanism, a gaze-responsive interface, and a word card repository enriched with LLMs. This endeavor highlights the transformative potential of web-enabled eye-tracking and LLMs in enhancing communication for individuals with impairments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "Institute of Information Security",
              "dsl": ""
            }
          ],
          "personId": 127174
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "Institute of Service Science",
              "dsl": ""
            }
          ],
          "personId": 127167
        }
      ]
    },
    {
      "id": 127197,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "AudiLens: Configurable LLM-Generated Audiences for Public Speech Practice",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625114"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-9817",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "AudiLens is a large-language model (LLM)-based audience simulator for public speech practice that allows speakers to generate and configure a group of generated audiences, and use them to receive feedback on their speech during and after the practice in multiple aspects. AudiLens leverages the capability of LLMs in being able to generate a diverse set of personas and being able to simulate human behavior, and provide flexibility to the speaker in terms of practicing their speech with multiple sets of audience groups in multiple speech formats. We demonstrate the use of AudiLens in two scenarios‚Äîgiving a tutorial and debating.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 127177
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 127176
        }
      ]
    },
    {
      "id": 127198,
      "typeId": 13097,
      "durationOverride": 480,
      "title": "XR and AI: AI-Enabled Virtual, Augmented, and Mixed Reality",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23h-9048",
      "source": "PCS",
      "trackId": 12419,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127751
      ],
      "eventIds": [],
      "abstract": "This workshop aims to unite experts and practitioners in XR and AI to envision the future of AI-enabled virtual, augmented, and mixed reality experiences. Our expansive discussion includes a variety of key topics: Generative XR, Large Language Models (LLMs) for XR, Adaptive and Context-Aware XR, Explainable AI for XR, and harnessing AI to enhance and prototype XR experiences. We aim to identify the opportunities and challenges of how recent advances of AI could bring new XR experiences, which cannot be done before, with a keen focus on the seamless blending of our digital and physical worlds.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 125846
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127178
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 126262
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126086
        }
      ]
    },
    {
      "id": 127199,
      "typeId": 13091,
      "title": "Intelligent Textiles for Physical Human-Environment Interactions",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23f-1007",
      "source": "PCS",
      "trackId": 12420,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127849
      ],
      "eventIds": [],
      "abstract": "Physical human-environment interaction is a fundamental aspect of our daily lives, involving the constant use of our sensory and motor systems to extract, process, and communicate information. Capturing, modeling, and augmenting these physical interactions are crucial for enhancing human well-being and promoting intelligent system designs. However, the pervasive and diverse nature of these interactions poses challenges that require scalable and adaptable systems. To address these challenges, I adopt an integrated approach that combines digital fabrication and machine learning techniques. The approach involves developing a digital design and fabrication pipeline to integrate sensing and actuation capabilities into textile-based platforms, and capturing diverse datasets on human-environment interactions to enable intelligent and adaptive applications. The dissertation showcases past and ongoing works on intelligent textile-based sensing and actuating platforms that embody this approach, including tactile sensing garments, an intelligent carpet for human pose estimation, programmable textile-based actuators for assistive wearables, and smart gloves for adaptive tactile interaction transfer. Moving forward, I aim to explore applications of the developed systems in healthcare, robotics, and human behaviors intervention, and expand to diverse sensing and actuation modalities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology (MIT)",
              "dsl": "Computer Science and Artificial Intelligence Laboratory (CSAIL)"
            }
          ],
          "personId": 125787
        }
      ]
    },
    {
      "id": 127200,
      "typeId": 13091,
      "title": "Decomposable Interactive Systems",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23f-1026",
      "source": "PCS",
      "trackId": 12420,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127849
      ],
      "eventIds": [],
      "abstract": "As sustainability becomes an increasingly pressing concern across disciplines, the design and fabrication communities within HCI are rapidly discovering and sharing a wealth of novel materials, tools, and workflows, allowing us to make physical artifacts that are more eco-friendly than ever before. Still, sustainability and functionality are often at odds with one another when it comes to the design of interactive systems, with most systems still relying on conventional electronic components that must be extracted and individually handled at end of life. My work offers approaches for designing decomposable interactive systems that are made with materials that are widely available, safe, and even edible, empowering the ``everyday designer'' to make sustainable systems for applications that do not demand long operation times or high power. Enabled by the growing ecosystem of decomposable materials and systems, I also propose new opportunities for designing for unmaking, a counterpart to making that opens the opaque, industrial processes of recycling and composting as rich design spaces to encourage further engagement and critical reflection around themes of sustainability, materiality, and consumption.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127166
        }
      ]
    },
    {
      "id": 127201,
      "typeId": 13091,
      "title": "Supporting Independence of Autistic Adults through Mobile and Virtual Reality Technologies",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23f-1027",
      "source": "PCS",
      "trackId": 12420,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127849
      ],
      "eventIds": [],
      "abstract": "Pervasive symptoms in autistic individuals, such as facing more frequent challenges in social situations, pose significant obstacles in their pursuit of an independent life in adulthood. Although much research has proposed computer-assisted programs (e.g., smartphone apps and VR-based systems), there is a significant lack of systems designed for autistic adults and their independence, and the preferences or characteristics of autistic individuals are not carefully reflected in the design process. Thus, in my dissertation, I focus on two requirements of autistic adults for supporting their independence: (1) an independent and healthy lifestyle and (2) positive social skills practice. These requirements were externalized in two gamified mobile apps (PuzzleWalk and RoutineAid) and two VR-based systems (VISTA and V-DAT). My research aims to design and develop mobile/VR systems and derive design guidelines to support the independence of autistic adults.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "School of Intelligence Computing"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "School of Intelligence Computing"
            }
          ],
          "personId": 126189
        }
      ]
    },
    {
      "id": 127202,
      "typeId": 13091,
      "title": "Developing Action-Oriented Systems for Manual-Computational Craft Workflows",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23f-1016",
      "source": "PCS",
      "trackId": 12420,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127849
      ],
      "eventIds": [],
      "abstract": "Manual crafts typically involve action-oriented workflows, wherein the craftsperson performs a series of repeated actions to create beautiful intricate artifacts. Despite working with similar materials, digital fabrication workflows often implement methods that differ fundamentally from manual ones. In my dissertation work, I aim to develop action-oriented digital fabrication systems that implement abstractions derived from craft domain-expert knowledge. I theorize that action-oriented systems have the potential to (1) support the integration of computational tools with manual practices, (2) enable complex design tasks that would be challenging to achieve through other means, and  (3) leverage the unique properties of materials through fine control over machine toolpath. To investigate this theory, I developed three digital fabrication systems, which I evaluated by producing various artifacts. Additionally, I propose analyzing my work and HCI system research in general through the lenses of three theoretical frameworks that reflect on materiality, i.e. new materialism, indigenous epistemologies, and post-colonial theory on skill mastery. I believe this analysis can offer alternative perspectives that promote a more inclusive understanding of the relationship between humans, materials, and technology in the context of digital fabrication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 126511
        }
      ]
    },
    {
      "id": 127203,
      "typeId": 13097,
      "durationOverride": 480,
      "title": "Future Paradigms for Sustainable Making",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23h-7870",
      "source": "PCS",
      "trackId": 12419,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127750
      ],
      "eventIds": [],
      "abstract": "This workshop provides the first opportunity for the UIST community to discuss sustainability challenges and opportunities in rapid prototyping. We will discuss key issues such as waste generation from intermediate prototypes, strategies for sustainable materials, circular prototyping (e.g., promoting re-use of components), knowledge sharing infrastructures (e.g., open-sourcing hardware), and so forth. The goal is to identify potential HCI research directions that can foster a more sustainable \"making\" environment inside of labs and beyond.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126059
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Interactive Computing",
              "dsl": "Georgia Institute of Technology"
            }
          ],
          "personId": 126592
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126523
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 126335
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126094
        }
      ]
    },
    {
      "id": 127204,
      "typeId": 13097,
      "durationOverride": 480,
      "title": "Electro-actuated Materials for Future Haptic Interfaces",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23h-3244",
      "source": "PCS",
      "trackId": 12419,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127749
      ],
      "eventIds": [],
      "abstract": "Electro-actuated materials (EAMs) have received wide attention within material science and soft robotics for their ability to dynamically change physical properties, such as shape and stiffness, in response to electrical stimuli. While researchers have begun exploring the haptic characteristics of EAMs, their integration into Human-Computer Interaction (HCI) shows challenges, including limited commercial availability and a lack of interdisciplinary knowledge exchange. This workshop specifically focuses on electrostatic (ES), soft electrohydraulic (SEH), and electroosmotic (EO) actuators. By bringing together researchers in the field, we aim to facilitate the exchange of findings, techniques, fabrication practices, and tacit knowledge within the HCI community. The workshop combines interactive demos, focused discussions, and hands-on ideation, providing a platform to explore the haptic potential of EAMs, identify key challenges and opportunities, and envision how these programmable materials can unlock new haptic interactions and interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado, Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 127164
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado, Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 127160
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "Artimus Robotics Inc",
              "dsl": ""
            }
          ],
          "personId": 127173
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "SHAPE Lab, Dept. of Mechanical Engineering"
            }
          ],
          "personId": 127159
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 125813
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Fluid Reality",
              "dsl": ""
            }
          ],
          "personId": 126443
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Fluid Reality",
              "dsl": ""
            }
          ],
          "personId": 126247
        }
      ]
    },
    {
      "id": 127205,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "Docent: Digital Operation-Centric Elicitation of Novice-friendly Tutorials",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625121"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-8283",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "Nowadays, novice users often turn to digital tutorials for guidance in software. However, searching and utilizing the tutorial remains a challenge due to the request for proper problem articulation, extensive searches and mind-intensive follow-through. \r\nWe introduce \"Docent\", a system designed to bridge this knowledge-seeking gap. Powered by Large Language Models (LLMs), Docent takes vague user input and recent digital operation contexts to reason, seek, and present the most relevant tutorials in-situ. We assume that Docent smooths the user experience and facilitates learning of the software.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 127179
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "China",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Academy of Art and Design"
            }
          ],
          "personId": 127168
        }
      ]
    },
    {
      "id": 127206,
      "typeId": 13115,
      "durationOverride": 180,
      "title": "Smart-Pikachu: Extending Interactivity of Stuffed Animals with Large Language Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3625219"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23d-6392",
      "source": "PCS",
      "trackId": 12418,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        128188
      ],
      "eventIds": [],
      "abstract": "We propose Smart-Pikachu, a stuffed animal equipped with sensing and actuation to explore the use of large language models (LLM's) with sensor data inputs. The augmentation of pressure sensing will allow for the LLM to interpret various interactions such as hugs and handshakes with the user. Furthermore, the actuation capabilities will extend our system's interactivity by providing physical feedback to the user. We will also incorporate text-to-speech output from the LLM to add another mode of interaction between the system and user. In this Student Innovation Challenge, we intend to explore applications at the intersection of sensing and interaction through LLM's and demonstrate an extension of LLMs' multimodal capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 127175
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 127181
        }
      ]
    },
    {
      "id": 127357,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Palette-PrintAR: an Augmented Reality Fluidic Design Tool for Multicolor Resin 3D Printing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616684"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-4657",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "While 3D printing affords designers unprecedented geometrical complexity, fewer design tools for multi-material platforms exist, and those that have been proposed for resin printing lack interactivity. Here, we report our ongoing work leveraging our novel multi- material 3D printing method, which prints channels concurently with the object itself, to empower designers to, with minimal hardware changes, paint 3D objects in real time. Our framework allows users to print a fluidic network to distribute different colors spatioselectively into the vat and control this fabrication process with an augmented reality (AR) design and simulation tool, which we term Palette-PrintAR. Below, we describe the mode of user interaction with our tool, describe our underlying simulation approach, and report initial experimental validation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 127255
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 127348
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Chemical Engineering"
            }
          ],
          "personId": 127213
        }
      ]
    },
    {
      "id": 127358,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "FlavourFrame: Visualizing Tasting Experiences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616640"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-5108",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "We present FlavourFrame, a canvas-based app that helps tasters capture and visualize their perceptions during mindful tasting experiences. Taste perceptions are difficult to document because they are subjective, multisensory, and ephemeral; and everyday people have limited dedicated vocabulary to describe such experiences. Our customizable tool is designed to help novice and experienced tasters structure and record tasting experiences. FlavourFrame superimposes visual and text layers to personalize visual and word-based expression of flavor experience. Through autoethnographic reflections, we generated sample data and identified strengths and limitations of the prototype. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Interactions Lab"
            }
          ],
          "personId": 127330
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 127296
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 127274
        }
      ]
    },
    {
      "id": 127359,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "ScentCarving: Fabricating Thin, Multi-layered and Paper-Based Scent Release through Laser Printing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616673"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-5626",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Differentiating from commonly-seen Olfactory Displays (ODs) that utilize rigid mechanisms within HCI, we propose ScentCarving, a controllable, lightweight, and flexible odor-release mechanism through a thin, multi-layered structure. ScentCarving consists of four layers, including the (1) paper-based substrate, (2) odor layer containing the ink-based aroma, (3) odor-sealing layer by introducing a thermoplastic material, and (4) a heating module using conductive cooper to soften the sealing layer and release the scent. ScentCarving also involves an easy-to-access Odor Printing technique capable of engraving fragrances onto thin and flexible substrates. We conducted a small-scale user study to test the scent-releasing behavior in terms of smell distance and responsive time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Donghua University",
              "dsl": "College of Fashion and Design"
            }
          ],
          "personId": 127301
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Donghua University",
              "dsl": ""
            }
          ],
          "personId": 127228
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai ",
              "institution": "Donghua University",
              "dsl": "College of  Fashion and  Design"
            }
          ],
          "personId": 127232
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 126265
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji university",
              "dsl": ""
            }
          ],
          "personId": 127320
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 127317
        }
      ]
    },
    {
      "id": 127360,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "AwakenFlora: Exploring Proactive Smell Experience in Virtual Reality through Mid-Air Gestures",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616667"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-5824",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "We explored mid-air gestural interactions for proactive smell experiences in Virtual Reality (VR). With a wearable scent-delivery device, a set of gestures that interact with virtual objects are mapped to corresponding olfactory tasks, i.e., scent release, scent intensity adjustment, and scent switch. We conducted a user study for preliminary validation, revealing significant advantages of gestural interactions over the traditional handle controller for scent release. Our findings demonstrate the potential of gestural interactions in enhancing proactive smell experiences in VR and contribute insights into how proactive gestural input can benefit engaging olfactory experiences in virtual environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 127318
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Donghua University",
              "dsl": "College of  Fashion and  Design"
            }
          ],
          "personId": 127270
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 127285
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 127346
        }
      ]
    },
    {
      "id": 127361,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "ModBand: Design of a Modular Headband for Multimodal Data Collection and Inference",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616682"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-5665",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Collecting multimodal user data during physical tasks such as cooking, maintenance, or physical rehab is crucial to enable the design of better AI models, interfaces, and applications. However, this is a challenging task with external cameras and sensors due to user movement, self-occlusions and diversity of data streams during task performance. In this work, we present ModBand, a wearable sensor headband with an accompanying software pipeline to collect and visualize data such as facial images, pupillometry, egocentric video, and heart rate during physical task performance. ModBand can be modified, extended, and used both as a standalone device or integrated with existing head-mounted AR devices for AI-based task guidance. Our modular design incorporates cost-effective fabrication methods, such as 3D printing, and enables convenient integration or exclusion of sensors to support custom data collection needs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 127307
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": "Media Arts and Technology"
            }
          ],
          "personId": 127279
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125820
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 126262
        }
      ]
    },
    {
      "id": 127362,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "iKnowde: Interactive Learning Path Generation System Based on Knowledge Dependency Graphs",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616628"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-9625",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "This paper presents a system that addresses the challenges faced by novice learners by identifying and tracking their learning topics and current knowledge status, and providing a suitable, dynamically updated learning path. The system represents the dependencies between learning objects using a directed graph, and utilizes a binary questionnaire interface to continuously determine and update the user's learning topics and knowledge status. This system enables users to clearly understand where they are in their learning process and what they should learn next, thereby improving learning efficiency, motivation, and self-efficiency. We conducted an experiment involving 9 participants, and our results implied that the proposed system is beneficial for beginners, particularly in reducing learners' cognitive load and enhancing their motivation and self-efficacy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Information Science and Technology"
            }
          ],
          "personId": 127314
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127276
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127240
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127322
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127355
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127315
        }
      ]
    },
    {
      "id": 127363,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "ChainForge: An Open-source Visual Programming Environment for Prompt Engineering",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616660"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-2713",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Prompt engineering for large language models (LLMs) is a critical to effectively leverage their capabilities. However, due to the inherent stochastic and opaque nature of LLMs, prompt engineering is far from an exact science. Crafting prompts that elicit the desired responses still requires a lot of trial and error to gain a nuanced understanding of a model's strengths and limitations for one's specific task context and target application. To support users in sensemaking around the outputs of LLMs, we create ChainForge, an open-source visual programming environment for prompt engineering. ChainForge is publicly available, both on the web (https://chainforge.ai) and as a locally installable Python package hosted on PyPI. We detail some features of ChainForge and how we iterated the design in response to internal and external feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "John A. Paulson School of Engineering & Applied Sciences"
            }
          ],
          "personId": 127226
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": ""
            }
          ],
          "personId": 127353
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Harvard",
              "dsl": "Insight and Interaction Lab"
            }
          ],
          "personId": 127247
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "SEAS"
            }
          ],
          "personId": 127209
        }
      ]
    },
    {
      "id": 127364,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "How To Eat Garlic Without Causing Bad Breath",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616659"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3175",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "We study taste reproduction using taste sensors, with a particular emphasis on accurately measuring and reproducing the taste of garlic while reducing the associated bad breath.  We measured and reproduced the taste of garlic using a combination of odorless substances. We also developed a fork integrated with a mechanism for releasing the aroma of allicin to provide controlled exposure to the nose for a complete eating experience. Through our experiments, we successfully achieved identical taste and smell experiences, effectively eliminating the occurrence of bad breath. To validate our findings, participants tasted a dish of spaghetti and reported that the culinary experience was very similar to that of normal garlic-infused dishes, with no detectable bad breath.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 127271
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 127286
        }
      ]
    },
    {
      "id": 127365,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Edible Lenticular Lens Design System",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616656"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3053",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Lenticular lenses are widely known as optical elements that change color depending on the viewing angle. By realizing this phenomenon in edible materials, it is possible to create a new gastronomic experience that significantly changes the appearance of food. In this study, we propose a system that supports the workflow from the design to the fabrication of edible lenticular lenses. The proposed system consists of lenticular lens design software and fabrication hardware. Users can design a visual effect of lenticular lenses by software simulation and fabricate the lenses by the knife cutting method using the hardware of the proposed system. In this study, the fabricated lenses were compared with the rendered ones. Furthermore, we confirm that the fabrication of the lenses is highly accurate and requires a short time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 127344
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 127323
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 127286
        }
      ]
    },
    {
      "id": 127366,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Electric Salt: Tableware Design for Enhancing Taste of Low-Salt Foods",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616626"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-2422",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "This study focused on an optimal tableware design using electro-taste technology to enhance saltiness. Based on various design and usability considerations, we created prototypes and conducted interviews with users to obtain suggestions for the optimal design, and identified the requirements for designing electro-taste tableware.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 127286
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Graduate School of Advanced Mathematical Sciences",
              "dsl": "Meiji University"
            }
          ],
          "personId": 127224
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Kirin Holdings Company, Limited",
              "dsl": ""
            }
          ],
          "personId": 127237
        }
      ]
    },
    {
      "id": 127367,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "VizPI: A Real-Time Visualization Tool for Enhancing Peer Instruction in Large-Scale Programming Lectures",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616632"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-1572",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Peer instruction (PI) has shown significant potential in facilitating student engagement and collaborative learning. However, the implementation of PI for large-scale programming lectures has proven challenging due to difficulties in monitoring student engagement, discussion topics, and code changes. This paper introduces VizPI, an interactive web tool that enables instructors to conduct, monitor, and assess PI for programming exercises in real-time. With features that visualize the progress of student discussions and code submissions, VizPI allows for more effective oversight of PI activities and the provision of personalized feedback at scale. Our work aims to transform the pedagogical approach to PI in programming education, making it more engaging and adaptable to student needs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 127254
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 127329
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": ""
            }
          ],
          "personId": 127347
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 126428
        }
      ]
    },
    {
      "id": 127368,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Waste Genie: A Web-Based Educational Technology for Sustainable Waste Management",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616696"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3878",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "In this work, we proposed and designed a web-based educational technology to support sustainable living. It is called Waste Genie. Waste Genie integrates formal and informal learning features to help users learn about waste management. A field trial was conducted to evaluate its effectiveness, and the results indicated the platform was well-received by the users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Clara",
              "institution": "Santa Clara University",
              "dsl": ""
            }
          ],
          "personId": 127222
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Clara",
              "institution": "Santa Clara University ",
              "dsl": "CS"
            }
          ],
          "personId": 127282
        }
      ]
    },
    {
      "id": 127369,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Personal Situated Analytics (PSA) for Sensemaking in Recorded Meetings",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616697"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-2866",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Applications in immersive environments have gained popularity for training, learning, and recreational tasks. Due to the increasing availability of sensors and data-capturing devices, research is extending the use of immersive environments to support visual analytics processes, including sensemaking and strategic immersion for interaction and task completion. In this work, we propose Personal Situated Analytics (PSA) framework to embed users into recorded meetings with support for multiple degrees of immersion in the\r\nReality-Virtuality spectrum. Our proposed framework encompasses various stages such as tracking, data capturing, data cleaning, data synchronization, prototype building, and deploying the final product to end-user hardware. We evaluate this framework on a data\r\nanalysis scenario between human subjects and a conversational AI agent. Our pilot study (n=12) using this framework compares user experiences when using two different devices: Hololens2 and Quest2.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois at Chicago",
              "dsl": "Electronic Visualization laboratory"
            }
          ],
          "personId": 127263
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois at Chicago",
              "dsl": "Electronic Visualization Laboratory"
            }
          ],
          "personId": 127293
        }
      ]
    },
    {
      "id": 127370,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "AutoSurveyGPT: GPT-Enhanced Automated Literature Discovery",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616648"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3711",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "In this work, we introduce AutoSurveyGPT, a novel framework for literature discovery. Designed to accommodate brief user-provided descriptions of academic papers, ideas, or proposals, this system is capable of autonomously extracting keywords for subsequent exploration within scholarly search engines. By leveraging large language model like GPT-4, the system further evaluates the relevance of the retrieved papers to the user-provided idea. This process, based on examining the introduction and related work sections, drives a repeating cycle of creating new keywords and finding more papers. The system generates a list of related papers, effectively aiding researchers in their search for relevant work. The open-source code for this tool is available on GitHub https://github.com/a554b554/AutoSurveyGPT.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 126380
        }
      ]
    },
    {
      "id": 127371,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Laseroma: A Small-Sized, Light-Weight, and Low-Cost Olfactory Display Releasing Multiple Odors through Pointed Heating",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616691"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-6789",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "In this paper, we propose Laseroma, a tiny, lightweight, and low-cost Olfactory Display (OD) that can release a maximum of eleven types of odors through pointed heating. Laseroma can be easily attached to Head-Mounted Devices (HMDs) for creating smell-enhanced and controllable Immersive Experiences. Laseroma mainly consists of (1) a laser diode for rapidly generating the pointed heat, (2) a modular and replaceable odor strip which carries eleven miniature odor reservoirs, and (3) a stepper motor for rotating the odor strip for switching the target aroma. We illustrated and articulated the system design, including the hardware, structural mechanism, materials, and fabrication process. We also conducted a preliminary study by testing the scent release performance through the perceived time and odor-release duration of a single scent container.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Donghua University",
              "dsl": "College of Fashion and Design"
            }
          ],
          "personId": 127235
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Donghua University",
              "dsl": "College of Fashion and Design"
            }
          ],
          "personId": 127321
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Donghua University",
              "dsl": "College of Fashion and Design"
            }
          ],
          "personId": 127232
        }
      ]
    },
    {
      "id": 127372,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "EChat: An Emotion-Aware Adaptive UI for a Messaging App",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616698"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-1577",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "While online forums provide a convenient platform for people to interact anonymously with others who share similar interests, they have to deal with large amounts of hate speech and inappropriate content, often posted by users in the heat of the moment. This can have a negative impact on the psychological state of other forum users and moderators, who are tasked to identify and delete such content. We investigate a preventative approach to this problem with the design of EChat, a proof-of-concept augmentation to online forums that helps users attend to their emotional state. The user's current emotional state is detected using facial emotion recognition, and the aesthetics of the UI are adapted to reflect this emotion. In case of an emotion with negative valence such as anger or sadness, the UI aesthetic is gradually transitioned to one that evokes a more positive emotion. Semi-structured interviews with EChat users confirm the potential of emotion-aware design to reduce hateful content, and also highlight important design considerations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127334
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Madhya Pradesh",
              "city": "Jabalpur",
              "institution": "PDPM Indian Institute of Information Technology Design and Manufacturing Jabalpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 127308
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127291
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 127307
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126965
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 126262
        }
      ]
    },
    {
      "id": 127373,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Visualizing Spacecraft Magnetic Fields on the Web and in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616618"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-7635",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Spacecraft magnetic fields are very complex in nature, and they must be understood and minimized, as they can mask or even mimic signals of interest. Current approaches simulate the 3D nature of the magnetic field and then visualize it in 2D images. However, limited 2D views hide much of the information in the complex spacecraft magnetic field. We describe a prototype system that allows for both an interactive 3D web experience as well as an immersive virtual reality (VR) experience to view and manipulate the 3D spacecraft magnetic field, allowing engineers and scientists to trace field lines, real-time in 3D. A preliminary user study validates the usefulness of the tool and guides further development.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127288
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory",
              "dsl": ""
            }
          ],
          "personId": 127243
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127219
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127208
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127295
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127231
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127262
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127350
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127302
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127319
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127309
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127343
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pasadena",
              "institution": "Jet Propulsion Laboratory, California Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 127305
        }
      ]
    },
    {
      "id": 127374,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "TaleMate: Collaborating with Voice Agents for Parent-Child Joint Reading Experiences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616699"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-8894",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Joint reading is a key activity for early learners, with caregiver-child interactions such as questioning and feedback playing an essential role in children‚Äôs cognitive and linguistic development. However, for some parents, actively engaging children in storytelling can be challenging. To address this, we introduce TaleMate‚Äîa platform designed to enhance shared reading by leveraging conversational agents that have been shown to support children‚Äôs engagement and learning. TaleMate enables a dynamic, participatory reading experience where parents and children can choose which characters they wish to embody. Moreover, the system navigates the challenges posed by digital reading tools, such as decreased parent-child interaction, and builds upon the benefits of traditional and digital reading techniques. TaleMate offers an innovative approach to fostering early reading habits, bridging the gap between traditional joint reading practices and the digital reading landscape.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127290
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 127248
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "BLACKSBURG",
              "institution": "Virginia Polytechnic Institute & State University (Virginia Tech)",
              "dsl": "Human Development and Family Science"
            }
          ],
          "personId": 127261
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Human Development and Family Science"
            }
          ],
          "personId": 127325
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 126100
        }
      ]
    },
    {
      "id": 127375,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "SoundMist: Novel Interface for Spatial Auditory Experience",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616622"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-6596",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "We introduce a novel method called ‚Äòspraying sound‚Äô for immersive auditory and spatial experiences. Our prototype SoundMist disperses sound into the surrounding space, enhancing immersion and spatial perception. Through user tests with 11 participants, we demonstrate the effectiveness of this approach in enriching the auditory experience and expanding possibilities for spatial auditory interactions. This research opens up opportunities to explore immersive spatial experiences with sound and makes a contribution by proposing a novel method of sound-space interaction through a system that sprays sound in a spatial context.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Deajeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 127332
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejoen",
              "institution": "KAIST (Korea Advanced Institute of Science and Technology)",
              "dsl": "Industrial Design Department, College of Engineering"
            }
          ],
          "personId": 127287
        }
      ]
    },
    {
      "id": 127376,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Pneunocchio: A Playful Nose Augmentation for Facilitating Embodied Representation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616651"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-4893",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Prior research has offered a plethora of wearables centred around sensing bodily actions ranging from more explicit data, such as movement and physiological response, to implicit information, such as ocular and brain activity. Bodily augmentations that physically extend the user's body along with altering body schema and image have been proposed recently as well, owing to factors such as accessibility and improving communication. However, these attempts have usually consisted of uncomfortable interfaces that either restrict the user's movement or are intrusive in nature. In this work, we present Pneunocchio, a playful nose augmentation based on the lore of Pinocchio. Pneunocchio consists of a pneumatic-based inflatable that a user wears on their nose to play a game of two truths and a lie. With our work, we aim to explore expressive bodily augmentations that respond to a player's physiological state that can alter the perception of their body while serving as an expressive match for a current part of the body.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 126204
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 127220
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 127229
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 126612
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Clayton",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 126289
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human Centred Computing"
            }
          ],
          "personId": 127236
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "Faculty of Engineering and Information Technology"
            },
            {
              "country": "Netherlands",
              "state": "Noord Brabant",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 126313
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 125839
        }
      ]
    },
    {
      "id": 127377,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "SketchingRelatedWork: Finding and Organizing Papers through Inking a Node-Link Diagram",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616685"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-4057",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Writing an academic paper requires significant time and effort to find, read, and organize many related papers, which is a complex knowledge task. We present a novel interactive system that allows users to perform these tasks quickly and easily on the 2D canvas with pen and multitouch inputs. Our system turns users‚Äô sketches and handwriting into a node-link diagram of papers and citations that users can iteratively expand in situ toward constructing a coherent narrative when writing Related Work sections.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 127306
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126596
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 127275
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 125916
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126151
        }
      ]
    },
    {
      "id": 127378,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "VizAbility: Multimodal Accessible Data Visualization with Keyboard Navigation and Conversational Interaction",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616669"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-8731",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Data visualization serves as a crucial tool for communicating important information in our society. Yet, as visualizations grow more complex, they become less accessible to individuals with visual impairments. Traditional accessibility approaches like alternative text and data tables often fall short of capturing the full potential of data visualization. To bridge this gap, we introduce VizAbility, a novel multimodal accessible system that combines keyboard navigation with conventional interaction, enabling individuals with visual impairments to actively engage with and explore data visualizations. We built an LLM-based pipeline that classifies user queries and synthesizes underlying data, chart structure, user locality, and web-based information to answer the queries. Our preliminary evaluation using real-world questions from blind individuals demonstrates the significant potential of VizAbility.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Chestnut Hill",
              "institution": "Boston College",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 127245
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Chestnut Hill",
              "institution": "Boston College",
              "dsl": "Computer Science Dept"
            }
          ],
          "personId": 127345
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Chestnut Hill",
              "institution": "Boston College",
              "dsl": "computer science"
            }
          ],
          "personId": 127289
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Chestnut Hill",
              "institution": "Boston College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127233
        }
      ]
    },
    {
      "id": 127379,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616635"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-7363",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic hand gestures generated by virtual 3D models. In this paper, we present our open-source framework that utilizes Unreal Engine to synthesize realistic static and dynamic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time, effort, or cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures, improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the creation of a data set, our tool accelerates the development of gesture recognition systems for automotive and non-automotive applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "Saarland Informatics Campus\t",
              "dsl": ""
            }
          ],
          "personId": 127327
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbruecken",
              "institution": "Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 127210
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": "Cognitive Assistants"
            }
          ],
          "personId": 127336
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbr√ºcken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 126473
        }
      ]
    },
    {
      "id": 127380,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Improving Mobile Reading Experiences while Walking Through Automatic Adaptations and Prompted Customization",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616666"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-7825",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Increasingly more people are consuming information on the go, and yet walking can significantly affect the ability to read text documents on mobile devices. In this work, we propose a system that automatically detects when a user is walking while reading on mobile devices to suggest automatic adaptations and recommendations to improve reading experiences. The user can also customize these suggested adaptations in real time, which our system uses to offer future recommendations. We ran a preliminary user study to evaluate our prototype and identify challenges and opportunities of mixed-initiative adaptations for reading on the go.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 127342
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 127212
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 127269
        }
      ]
    },
    {
      "id": 127381,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "E4UnityIntegration-MIT: An Open-Source Unity Plug-in for Collecting Physiological Data Using Empatica E4 during Gameplay",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616627"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-7827",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Physiological measurement of player experience (PX) during gameplay has been of increasing interest within game research circles. A commonly-used non-invasive wearable device for physiological measurement is the Empatica E4 wristband, which offers multiple physiological metrics, ranging from electrodermal activity to heart rate. That said, the E4's integration with popular game engines such as Unity 3D presents certain challenges due to non-obvious critical bugs in the library and limited documentation applicability within the Unity context. In this paper, we present an open-source Unity plug-in designed to mitigate the challenges associated with integrating the E4 into Unity projects: E4UnityIntegration-MIT. The plug-in exposes the E4's API for interfacing with Unity C# scripts, thereby enabling realtime data collection and monitoring. E4UnityIntegration-MIT also provides the affordance of saving the E4 data into an external file for data analysis purposes. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Computer Science and Artificial Intelligence Laboratory"
            }
          ],
          "personId": 127316
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Computer Science and Artificial Intelligence Laboratory"
            }
          ],
          "personId": 127238
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 127221
        }
      ]
    },
    {
      "id": 127382,
      "typeId": 13116,
      "durationOverride": 30,
      "title": "The Ultimate Interface",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23c-5013",
      "source": "PCS",
      "trackId": 12421,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127845
      ],
      "eventIds": [],
      "abstract": "This talk will explore the possibilities of what emerging AI architectures could mean for interaction in delivering the ultimate interface that is fully generated by AI and not designed at all. My perspective and examples are based on personal opinions, informed by observations and experience gained from building advanced interactions over the last few decades. While I aim to provide insights, speculations, and suggest some essential conditions for creating such an ultimate interface, my insights may not be exhaustive. I invite open discussion and debate on this subject, recognizing that the conditions that I propose might be necessary but perhaps not sufficient in themselves.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Archetype AI",
              "dsl": "Archetype AI"
            }
          ],
          "personId": 127239
        }
      ]
    },
    {
      "id": 127383,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Introducing Augmented Post-it: An AR Prototype for Engaging Body Movements in Online GPT-Supported Brainstorming",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616693"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-5203",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Recent web-based brainstorming tools utilize pre-trained language models to support ideation. However, body movements such as walking and arm motion, which are known to enhance creativity, are generally limited in web-based brainstorming. Thus, we propose ‚ÄúAugmented Post-it,‚Äù a novel augmented reality (AR) interaction system that enables body movements while brainstorming using a generative pre-trained transformer (GPT). With Augmented Post-it, ideas uttered by users are structured via GPT and extended into a spatial representation that encourages divergent thinking. This study contributes to future GPT-based AR brainstorming by extending the thinking ability of users in ways that engage their body movements to enhance creativity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku-ku",
              "institution": "Kogakuin University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku-ku",
              "institution": "Kogakuin University",
              "dsl": ""
            }
          ],
          "personId": 127340
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku-ku",
              "institution": "Kogakuin university",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku-ku",
              "institution": "Kogakuin university",
              "dsl": ""
            }
          ],
          "personId": 127338
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Psychology"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Psychology"
            }
          ],
          "personId": 127328
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 127256
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku-ku",
              "institution": "Kogakuin University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku-ku",
              "institution": "Kogakuin University",
              "dsl": ""
            }
          ],
          "personId": 127244
        }
      ]
    },
    {
      "id": 127384,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Event-Based Pupil Tracking Using Bright and Dark Pupil Effect",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616657"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-4874",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Real-time high-speed gaze estimation can enable next-generation gaze-based interaction. The event camera, which captures intensity variations at high frequency, has been employed to this end. However, pupil tracking based only on events is difficult because events are sparse and limited. We propose high-speed pupil tracking using an event camera based on the bright and dark pupil effect. Two illumination sources generate events in the pupil area, and the pupil center is determined in real time at over 2000 Hz without requiring complete image from the events. We implemented gaze target estimation using smooth-pursuit eye movements and confirmed high-speed pupil tracking that may reduce the delay in gaze-based interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hiratsuka",
              "institution": "Tokai University",
              "dsl": ""
            }
          ],
          "personId": 127349
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hiratsuka",
              "institution": "Tokai University",
              "dsl": ""
            }
          ],
          "personId": 127227
        }
      ]
    },
    {
      "id": 127385,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Using LLMs to Customize the UI of Webpages",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616671"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3700",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "LLMs have capabilities to understand natural language and code, which makes them a great candidate for user-driven customization of webpages. A process that focuses on natural language can be useful for those who are less technologically literate. In this paper, we explore the potential of using LLMs to modify webpages, and what kinds of opportunities and challenges that come with it. We observe that specific prompts referring to color or targeted components can succeed, vague requests and any complex website tend to perform poorly.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 127253
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 126631
        }
      ]
    },
    {
      "id": 127386,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "FluencyAR: Augmented Reality Language Immersion",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616670"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-1005",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "FluencyAR is an augmented reality second language learning tool centered around the concepts of language immersion and self-talk. For many second language learners, advancing into upper levels of fluency can be difficult without sufficient opportunities to practice. Traditional solutions of tutoring or finding exchange partners are often inconvenient or limiting. FluencyAR provides situational conversation practice in highly self-directed practice sessions that imitate environments where the target language is dominant. We utilize augmented reality to allow users to practice their target language with immediate feedback at any time, and from any location. Using ChatGPT and the physical space of the user, we can produce unique and challenging conversation prompts relative to a user's surroundings, ensuring that sessions remain interesting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 127294
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 127274
        }
      ]
    },
    {
      "id": 127387,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Augmented Photogrammetry: 3D Object Scanning and Appearance Editing in Mobile Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616638"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-2858",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "We present a novel approach, Augmented Photogrammetry, for scanning and editing the appearance of physical objects in augmented reality (AR). Our work provides a user-friendly and efficient technique for enabling customizable appearance modifications in real time on arbitrary objects scanned from a user's physical environment. We accomplish this by integrating Structure from Motion (SfM), instance segmentation, and machine learning into a unified pipeline. Our streamlined process enables users to easily select a physical object and specify its desired appearance. We believe our mobile AR approach holds promise for applications in interior design, virtual prototyping, and content creation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Goleta",
              "institution": "University of California, Santa Barbara ",
              "dsl": ""
            }
          ],
          "personId": 127339
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 126965
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 126262
        }
      ]
    },
    {
      "id": 127388,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Chandelier: Interaction Design With Surrounding Mid-Air Tangible Interface",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616695"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-7150",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Chandelier is a mid-air tangible interface where a user is surrounded in the center by 120 pendants that levitate independently and orbit in 5 concentric circumferences, where each pendant is touch-enabled and color-changeable by default.\r\nWe explore interactions with Chandelier such as change blindness and repurposing formations from immersive experiences to mitigate the limitation of the hardware systems.\r\nWe discuss the extent of Surrounding Mid-Air interactions in tangible interfaces and the design factors that could be brought into experiences of future levitation interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 126513
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 126499
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 127214
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "Electrical engineering",
              "dsl": "National Taiwan University"
            }
          ],
          "personId": 127218
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 126522
        }
      ]
    },
    {
      "id": 127389,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Representing the Timbre of Traditional Musical Instruments Based On Contemporary Instrumental Samples Using DDSP",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616678"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3195",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "This project explores the potential of Differentiable Digital Signal Processing (DDSP) to represent and synthesize the timbre of five different notes of the Korean traditional musical instrument, Geomungo, using digital instrumental samples of the bass guitar, which has a similar mechanism to produce the sound. To evaluate the feasibility and quality of the digital recreation process, we compared hand-played Geomungo audio samples with digitally recreated audio samples using DDSP. The MFCC, spectral contrast, chroma features, and raw signal comparison, were used for assessment. Our findings show the possibility of applying DDSP to represent and synthesize the nuances of pitch and dynamics for expressive aspects of Geomungo's five different notes effectively. We also propose three audio features that can be used to evaluate the results quantitatively under the context of neural sound synthesis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology (UNIST) ",
              "dsl": "Expressive Computing Lab, Department of Design"
            }
          ],
          "personId": 127216
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology (UNIST)",
              "dsl": "Expressive Computing Lab, Department of Design"
            }
          ],
          "personId": 127250
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology (UNIST)",
              "dsl": "Expressive Computing Lab, Department of Design"
            }
          ],
          "personId": 127354
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology (UNIST)",
              "dsl": "Expressive Computing Lab, Department of Design"
            }
          ],
          "personId": 127260
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology (UNIST)",
              "dsl": "Expressive Computing Lab, Department of Design"
            }
          ],
          "personId": 127272
        }
      ]
    },
    {
      "id": 127390,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Touch'n'Draw: Rapid 3D Sketching with Fluent Bimanual Coordination",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616700"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-6064",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "In perspective drawing, designers express 3D shapes by drawing auxiliary lines that construct surfaces and drawing design curves on them. However, drawing auxiliary lines can be challenging, and too many of them can make the drawing difficult to understand. To address these issues, we present a novel 3D sketching system that allows the user to quickly and easily create instant auxiliary lines and instant sketch surfaces for drawing desired 3D curves with fluent bimanual touch and pen interactions. We produced a concept sketch using our system to showcase its potential usefulness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 125916
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126134
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126596
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126151
        }
      ]
    },
    {
      "id": 127391,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Sketching Proteins with Bare Hands in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616675"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-5494",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Recent developments in AI have made it possible to design new proteins that are crucial to meeting humanity‚Äôs needs. However, tools for exploring the 3D structures of proteins in the early stages of AI-based protein design are lacking, leading to many preventable trials and errors and much wasted time and efforts in the design process. To address this, we propose a novel VR interaction system that enables synthetic biologists to intuitively author the 3D structures of proteins with their bare hands.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 127306
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 126596
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 125916
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 126603
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Graduate School of Medical Science and Engineering"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Institute for Basic Science",
              "dsl": "Center for Biomolecular & Cellular Structure"
            }
          ],
          "personId": 127249
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 126151
        }
      ]
    },
    {
      "id": 127392,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Towards Trauma-Informed Data Donation of Sexual Experience in Online Dating to Improve Sexual Risk Detection AI",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616689"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-8429",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Sexual risk detection AI has been touted as a scalable solution for computer-mediated sexual violence. Data donation is a user-centered approach to producing ecologically valid datasets for sexual risk detection AI: voluntarily providing personal data that is representative of risk. However, the act of donating intimate sexual experience data can itself be traumatizing. We propose Ube: a trauma-informed sexual experience data donation app for online daters that is developed in collaboration with sexual violence experts and care practitioners. Cognitive walkthroughs of Ube with sexual violence experts elucidated several design approaches to mitigating retraumatization during data donation, including a conversational agent and mental health checks.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Rochester",
              "institution": "Oakland University",
              "dsl": ""
            }
          ],
          "personId": 127246
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Rochester",
              "institution": "Oakland University",
              "dsl": ""
            }
          ],
          "personId": 127304
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Rochester",
              "institution": "Oakland University",
              "dsl": ""
            }
          ],
          "personId": 127281
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Detroit",
              "institution": "Computer science",
              "dsl": "Wayne state University"
            }
          ],
          "personId": 127292
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Rochester",
              "institution": "Oakland University",
              "dsl": "School of Nursing"
            }
          ],
          "personId": 127335
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Rochester",
              "institution": "Oakland University",
              "dsl": "Psychology"
            }
          ],
          "personId": 127230
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Rochester",
              "institution": "Oakland University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 127313
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Detroit",
              "institution": "Wayne State University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 127341
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Flint",
              "institution": "University of Michigan-Flint",
              "dsl": ""
            }
          ],
          "personId": 127277
        }
      ]
    },
    {
      "id": 127393,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Sacriface: A Simple and Versatile Support Structure for 3D Printing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616649"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3894",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Digital fabrication serves as a potent instrument for facilitating interaction between the real and digital realms. However, the process is becoming increasingly complex amidst its development. We present Sacriface, a simple and versatile support structure for 3D printing which allows us to obtain further efficiency and flexibility in 3D printing. We simplified the strategy of expanding support structures taking advantage of a stable, growing overhang that gradually recovers its original shape as printing thickness increases. Sacriface is effortlessly designed and printable using an unmodified 3D printer, which enhances user customization.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL - Kyoto",
              "dsl": ""
            }
          ],
          "personId": 127259
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL - Kyoto",
              "dsl": ""
            }
          ],
          "personId": 127299
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL - Kyoto",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Okazaki",
              "institution": "National Institute for Basic Biology",
              "dsl": ""
            }
          ],
          "personId": 127207
        }
      ]
    },
    {
      "id": 127394,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Relay: A Collaborative UI Model for Design Handoff",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616624"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3652",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "The design handoff process refers to the stage in the user interface (UI) design process where a designer gives their finished design to a developer for implementation. However, design decisions are lost when developers struggle to interpret and implement the designer's original intent. To address this problem, we built a system called Relay that utilizes concrete UI models to capture design intent. To our knowledge, Relay is the first system described in the academic literature to take artifacts from an existing design tool and generate a UI model. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 127268
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Inc.",
              "dsl": ""
            }
          ],
          "personId": 127211
        }
      ]
    },
    {
      "id": 127395,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "A Multi-modal Toolkit to Support DIY Assistive Technology Creation for Blind and Low Vision People",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616646"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-9836",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "We design and build A11yBits, a tangible toolkit that empowers blind and low vision (BLV) people to easily create personalized do-it-yourself assistive technologies (DIY-ATs). A11yBits includes (1) a series of Sensing modules to detect both environmental information and user commands, (2) a set of Feedback modules to send multi-modal feedback, and (3) two Base modules (Sensing Base and Feedback Base) to power and connect the sensing and feedback modules. The toolkit enables accessible and easy assembly via a \"plug-and-play\" mechanism. BLV users can select and assemble their preferred modules to create personalized DIY-ATs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "School of Mechanical Engineering & Automation-BUAA",
              "dsl": "Beihang University"
            }
          ],
          "personId": 127217
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": ""
            }
          ],
          "personId": 127280
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": "Computational Media and Arts Thrust"
            }
          ],
          "personId": 125845
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Graphics Technology"
            }
          ],
          "personId": 126290
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 127310
        }
      ]
    },
    {
      "id": 127396,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "iTutor: A Generative Tutorial System for Teaching the Elders to Use Smartphone Applications",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616663"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-8900",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "We present iTutor, a generative tutorial system for promoting smartphone use proficiency among elders. iTutor is unique because it can dynamically generate tutorials based on current operation goals and UI context, which we achieved through leveraging prompt engineering to large language models (LLMs). Our evaluations showed potential for this approach, as we yielded 78.6% accuracy in the instruction generation process. We conclude by providing the roadmap for further development.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 127352
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 127351
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 127337
        }
      ]
    },
    {
      "id": 127397,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "An Interactive System for Drawing Cars in Perspective",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616701"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-6849",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "We propose a novel interactive system for drawing cars in perspective. Our harmonious set of pen and touch interactions based on traditional tools and techniques can help car designers naturally transition from 2D sketching to 3D sketching and allow them to sketch cars in 3D intuitively and iteratively. The pilot test shows that our system is easy to learn and use.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126134
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 125916
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126596
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 126151
        }
      ]
    },
    {
      "id": 127398,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "TrainerTap: Weightlifting Support System Prototype Simulating Personal Trainer's Tactile and Auditory Guidance",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616644"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-8908",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Working out alone at the gym often lacks the quality and intensity of exercises compared to the training session with a personal trainer. To narrow this gap, we introduce TrainerTap, which simulates the personal trainer's presence during solitary weightlifting workouts. TrainerTap replicates the trainer's manual interventions of tapping the trainee's body parts to capture their attention on target muscles and provides auditory guidance to support executing the movements at a consistent tempo.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 127324
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 127265
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 127257
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 127215
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 127264
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 127356
        }
      ]
    },
    {
      "id": 127399,
      "typeId": 13116,
      "durationOverride": 30,
      "title": "AGI is Coming... Is HCI Ready?",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23c-5732",
      "source": "PCS",
      "trackId": 12421,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127846
      ],
      "eventIds": [],
      "abstract": "We are at a transformational junction in computing, in the midst of an explosion in capabilities of foundational AI models that may soon match or exceed typical human abilities for a wide variety of cognitive tasks, a milestone often termed Artificial General Intelligence (AGI). Achieving AGI (or even closely approaching it) will transform computing, with ramifications permeating through all aspects of society. This is a critical moment not only for Machine Learning research, but also for the field of Human-Computer Interaction (HCI).\r\n\r\nIn this talk, I will define what I mean (and what I do NOT mean) by ‚ÄúAGI‚Äù (and related concepts, like superintelligence), and my journey from AGI skeptic to believing we are within five to ten years of reaching this milestone. I will then discuss how this new era of computing necessitates a new sociotechnical research agenda on methods and interfaces for studying and interacting with AGI. For instance, how can we extend status quo design and prototyping methods for envisioning novel experiences at the limits of our current imaginations? What novel interaction modalities might AGI (or superintelligence) enable (e.g., ‚ÄúESP‚Äù)? How do we create interfaces for computing systems that may intentionally or unintentionally deceive an end-user? How do we bridge the ‚Äúgulf of evaluation‚Äù when a system may arrive at an answer through methods that fundamentally differ from human mental models, or that may be too complex for an individual user to grasp? How do we evaluate technologies that may have unanticipated systemic side-effects on society when released into the wild?\r\n\r\nI will close by reflecting on the relationship between HCI and AI research. Typically, HCI and other sociotechnical domains are not considered as core to the ML research community as areas like model building. However, I argue that research on Human-AI Interaction and the societal impacts of AI is vital and central to this moment in computing history. HCI must not become a ‚Äúsecond class citizen‚Äù to AI, but rather be recognized as fundamental to ensuring the path to AGI and beyond is a beneficial one.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google DeepMind",
              "dsl": ""
            }
          ],
          "personId": 126393
        }
      ]
    },
    {
      "id": 127400,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "OperAR: Using an Augmented Reality Agent to Enhance Children‚Äôs Interactive Intangible Cultural Heritage Experience of the Peking Opera",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616690"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-9125",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "As a traditional Chinese theatrical performance, the Peking Opera is a world intangible cultural heritage. However, appreciating and understanding the traditional arts of Peking Opera requires rich life experience and knowledge about traditional Chinese culture, which has led to its lack of recognition and support from younger Chinese audiences. To expand the audience for the traditional Chinese cultural heritage of Peking Opera and stimulate young people‚Äôs interest in learning about this art form, we introduce ‚ÄúOperAR‚Äù as an augmented reality game system that teaches children about the history and culture of the Peking Opera and how to perform Peking Opera using a cartoon virtual agent and a set of physical cards based on Peking Opera costumes. The pilot study results show the potential benefits of OperAR in enhancing children‚Äôs interactive experiences in virtual scenarios in addition to stimulating their interest in learning about Peking Opera.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 125888
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 126461
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao Bowen Primary School",
              "dsl": ""
            }
          ],
          "personId": 127251
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 127326
        }
      ]
    },
    {
      "id": 127401,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "On-the-fly Editing of Emoji Elements for Mobile Messaging",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616630"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-9642",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "This poster presents an on-the-fly emoji editing method to extend the range of emotional expressions of off-the-shelf emojis when typing a message on a mobile. The proposed system allows for the quick editing of accessory elements of facial emojis via simple swipe or flick gestures to reflect the user's emotional intensity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Metropolitan University",
              "dsl": ""
            }
          ],
          "personId": 127303
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Metropolitan University",
              "dsl": "Department of Industrial Art"
            }
          ],
          "personId": 127311
        }
      ]
    },
    {
      "id": 127402,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Generative Facial Expressions and Eye Gaze Behavior from Prompts for Multi-Human-Robot Interaction",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616623"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-7588",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Nonverbal cues such as eye gaze and facial expressions play critical roles in conveying intent, regulating conversation, and fostering engagement. A robot‚Äôs ability to effectively deploy these behaviors can significantly enhance human-robot collaboration. We describe a simple zero-shot learning approach to generate facial expression and gaze shifting behaviors to control a social robot conversing with an individual or group. An initial prompt provides instructions to a pre-trained large language model on how the model can control a robot‚Äôs facial expression and eye gaze behaviors during a conversation. To demonstrate this, we describe a proof-of-concept implementation using the robot Furhat. This simple and easily customizable approach can be used to improve perception of a robot's social presence in multi-human-robot interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 127298
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Beavercreek",
              "institution": "Riverside Research",
              "dsl": "Open Innovation Center"
            }
          ],
          "personId": 127266
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lexington",
              "institution": "Riverside Research",
              "dsl": "Open Innovation Center"
            }
          ],
          "personId": 127225
        }
      ]
    },
    {
      "id": 127403,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Sonic Storyteller: Augmenting Oral Storytelling with Spatial Sound Effects",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616642"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-7587",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "Traditional oral storytelling has been predominantly limited to a live audience, augmented with voice acting, sound effects, and gestures. With new technologies, the reach of oral storytelling has expanded to allow remote listeners to tune in to radio programs and more recently to listen to podcasts and audio books, asynchronously. Digital tools offer easy-to-use applications that enable anyone the ability to not only narrate a story, but also to augment it in different ways, from adding sound effects in podcasts to narrating audio books with voice effects. In this work, we present a proof-of-concept system that allows oral storytellers to augment their storytelling with spatial sound effects. We present results from a preliminary study (n=12) that found the spatial sound effects to be vivid and immersive, indicating potential for semi-automated ways of augmenting oral storytelling. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UC Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 127312
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 126262
        }
      ]
    },
    {
      "id": 127404,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Virtual Buddy: Redefining Conversational AI Interactions for Individuals with Hand Motor Disabilities",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616680"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-6499",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Advances in artificial intelligence have transformed the paradigm of human-computer interaction, with the development of conversational AI systems playing a pivotal role. These systems employ technologies such as natural language processing and machine learning to simulate intelligent and human-like conversations. Driven by the personal experience of an individual with a neuromuscular disease who faces challenges with leaving home and contends with limited hand-motor control when operating digital systems, including conversational AI platforms, we propose a method aimed at enriching their interaction with conversational AI. Our prototype allows the creation of multiple agent personas based on hobbies and interests, to support topic-based conversations. In contrast with existing systems, such as Replika, that offer a 1:1 relation with a virtual agent, our design enables one-to-many relationships, easing the process of interaction for this individual by reducing the need for constant data input. We can imagine our prototype potentially helping others who are in a similar situation with reduced typing/input ability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 127300
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Gujarat",
              "city": "Gandhinagar",
              "institution": "National Institute of Design",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127297
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 125820
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 127223
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 126262
        }
      ]
    },
    {
      "id": 127405,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Reusing Cardboard for Packaging Boxes with a Computational Design System",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616692"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-9962",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129721,
        127865
      ],
      "eventIds": [],
      "abstract": "We propose a computational design system for packaging boxes, which enables reconstruction of cardboard into packaging structures that precisely fit the size of the objects to be shipped. Given a 3D model of the object and the structure of the cardboard to be reused, our system suggests the optimal net of a new box, which only requires minimum cuts and folds for fabrication by leveraging the edges of the input cardboard. To achieve this, we implemented a GUI visualizing the input models and the optimal net computed by our optimization algorithm. As a demonstration, we showed three design examples, with evaluation results of our method for reducing the length of cuts and folds.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Bunkyo-ku, Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127283
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127333
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127284
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127331
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Interfaculty Initiative in Information Studies"
            }
          ],
          "personId": 127234
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 127278
        }
      ]
    },
    {
      "id": 127406,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Sketchnote: Sketch-Based Visualization of Problem Decomposition in Block-Based Programming",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616654"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-8431",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "Block-based programming effectively supports processes based on the syntactic and conceptual knowledge of programming; however, its effectiveness is limited to processes that require strategic knowledge. To resolve the problem, we present Sketchnote, which visualizes the problem decomposition process with sketching and multi-layered structure in block-based programming. Sketchnote allows programmers to sketch in the block attachment slots before inserting the actual code blocks. The multi-layer structure of Sketchnote also visualizes the hierarchy of code while extending the brick metaphor of block-based programming.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 127252
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 126552
        }
      ]
    },
    {
      "id": 127407,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "Context-Aware Sit-Stand Desk for Promoting Healthy and Productive Behaviors",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616694"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-4195",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "To mitigate the risk of chronic diseases caused by prolonged sitting, sit-stand desks are promoted as an effective intervention to foster healthy behaviors among knowledge workers by allowing periodic posture switching between sitting and standing. However, conventional systems either let users manually switch the mode, and some research visited automated notification systems with pre-set time intervals. While this regular notification can promote healthy behaviors, such notification can act as external interruptions that hinder individuals' working productivity. Notably, knowledge workers are known to be reluctant to change their physical postures when concentrating. To address these issues, we propose considering work context based on their screen activities to encourage computer users to alternate their postures when it can minimize disruption, promoting healthy and productive behaviors. To that end, we are in the process of building a context-aware sit-stand desk that can promote healthy and productive behaviors. To that end, we have completed two modules: an application that monitors users' computer's ongoing activities and a sensor module that can measure the height of sit-stand desks for data collection. The collected data includes computer activities, measured desk height, and their willingness to switch to standing modes and will be used to build an LSTM prediction model to suggest optimal time points for posture changes, accompanied by appropriate desk height. In this work, we acknowledge previous relevant research, outline ongoing deployment efforts, and present our plan to validate the effectiveness of our approach via user studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127273
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 127258
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 127267
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 126100
        }
      ]
    },
    {
      "id": 127408,
      "typeId": 13095,
      "durationOverride": 30,
      "title": "AmplifiedCoaster: Virtual Roller Coaster Experience using Motorized Ramps and Personal Mobility Vehicle",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586182.3616662"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23b-3186",
      "source": "PCS",
      "trackId": 12422,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127867,
        129720
      ],
      "eventIds": [],
      "abstract": "A novel virtual reality (VR) ride consisting of a head-mounted display (HMD) and an electric wheelchair was developed for a virtual roller coaster experience. The VR ride was integrated with an electric wheeled ramp to amplify the perception of virtual ascent and descent. The ascending and descending motion on a slope was replicated continuously with varying curvatures. The system features an electric wheelchair and two motorized ramps with a fixed 10-degree angle, which the wheelchair can drive on and off to simulate ascending and descending in VR. Moreover, by adjusting the relative speed and position of the wheelchair and ramps, we can simulate pitch angle curvature in VR. We are investigating several methods for providing multiple ascent and descent experiences, and introduce one of them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Fujisawa",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 127242
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Fujisawa",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 126190
        }
      ]
    },
    {
      "id": 127828,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "C-PAK: Correcting and Completing Variable-length Prefix-based Abbreviated Keystrokes",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "1",
      "source": "CSV",
      "trackId": 12430,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127755
      ],
      "eventIds": [],
      "abstract": "Improving keystroke savings is a long-term goal of text input research. We present a study into the design space of an abbreviated style of text input called C-PAK (Correcting and completing variable-length Prefix-based Abbreviated Keystrokes) for text entry on mobile devices. Given a variable length and potentially inaccurate input string (e.g., 'li g t m'), C-PAK aims to expand it into a complete phrase (e.g., 'looks good to me'). We develop a C-PAK prototype keyboard, PhraseWriter, based on a current state-of-the-art mobile keyboard consisting of 1.3 million n-grams and 164,000 words. Using computational simulations on a large dataset of realistic input text, we found that, in comparison to conventional single-word suggestions, PhraseWriter improves the maximum keystroke savings rate by 6.7% (from 46.3% to 49.4,), reduces the word error rate by 14.7%, and is particularly advantageous for common phrases. We conducted a lab study of novice user behavior and performance which found that users could quickly utilize the C-PAK style abbreviations implemented in PhraseWriter, achieving a higher keystroke savings rate than forward suggestions (25% vs. 16%). Furthermore, they intuitively and successfully abbreviated more with common phrases. However, users had a lower overall text entry rate due to their limited experience with the system (28.5 words per minute vs. 37.7). We outline future technical directions to improve C-PAK over the PhraseWriter baseline, and further opportunities to study the perceptual, cognitive, and physical action trade-offs that underlie the learning curve of C-PAK systems.",
      "authors": [
        {
          "affiliations": [],
          "personId": 127833
        },
        {
          "affiliations": [],
          "personId": 127834
        },
        {
          "affiliations": [],
          "personId": 127837
        }
      ]
    },
    {
      "id": 127843,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "Marvista: Exploring the Design of a Human-AI Collaborative News Reading Tool",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "2",
      "source": "CSV",
      "trackId": 12430,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127758
      ],
      "eventIds": [],
      "abstract": "We explore the design of Marvista‚Äîa human-AI collaborative tool that employs a suite of natural language processing models to provide end-to-end support for reading online news articles. Before reading an article, Marvista helps a user plan what to read by filtering text based on how much time one can spend and what questions one is interested to find out from the article. During reading, Marvista helps the user reflect on their understanding of each paragraph with AI-generated questions. After reading, Marvista generates an explainable human-AI summary that combines both AI‚Äôs processing of the text, the user‚Äôs reading behavior, and user-generated data in the reading process. In contrast to prior work that offered (content-independent) interaction techniques or devices for reading, Marvista takes a human-AI collaborative approach that contributes text-specific guidance (content-aware) to support the entire reading process.",
      "authors": [
        {
          "affiliations": [],
          "personId": 127838
        },
        {
          "affiliations": [],
          "personId": 127840
        },
        {
          "affiliations": [],
          "personId": 127839
        },
        {
          "affiliations": [],
          "personId": 127836
        },
        {
          "affiliations": [],
          "personId": 127835
        },
        {
          "affiliations": [],
          "personId": 127841
        },
        {
          "affiliations": [],
          "personId": 127842
        }
      ]
    },
    {
      "id": 128173,
      "typeId": 13094,
      "durationOverride": 13,
      "title": "TaleStream: Supporting Story Ideation with Trope Knowledge",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3586183.3606807"
        },
        "Preview": {
          "title": "TaleStream: Supporting Story Ideation with Trope Knowledge",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cSo3hUCNo6w"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "uist23a-9569",
      "source": "PCS",
      "trackId": 12415,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127759
      ],
      "eventIds": [],
      "abstract": "Story ideation is a critical part of the story-writing process. It is challenging to support computationally due to its exploratory and subjective nature. Tropes, which are recurring narrative elements across stories, are essential in stories as they shape the structure of narratives and our understanding of them. In this paper, we propose to use tropes as an intermediate representation of stories to approach story ideation. We present TaleStream, a canvas system that uses tropes as building blocks of stories while providing steerable suggestions of story ideas in the form of tropes. Our trope suggestion methods leverage data from the tvtropes.org wiki. We find that 97\\% of the time, trope suggestions generated by our methods provide better story ideation materials than random tropes. Our system evaluation suggests that TaleStream can support writers‚Äô creative flow and greatly facilitates story development. Tropes, as a rich lexicon of narratives with available examples, play a key role in TaleStream and hold promise for story-creation support systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 128172
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 126168
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 128171
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Systems ",
              "dsl": "Adobe Research "
            }
          ],
          "personId": 128170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 126458
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 128169
        }
      ]
    },
    {
      "id": 130026,
      "typeId": 13139,
      "title": "A Wide-ranging Conversation with David Holz, Founder of Midjourney and Leap Motion",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3",
      "source": "CSV",
      "trackId": 12455,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127850
      ],
      "eventIds": [],
      "abstract": "David Holz is the founder and CEO of Midjourney. Previously, he was co-founder and CTO of Leap Motion. David contracted for NASA‚Äôs Langley Research Center and conducted neuroscience research while at the Max Planck Institute. He studied Applied Math at the University of North Carolina at Chapel Hill, ultimately leaving his PhD program to co-found Leap Motion in 2010. David is based here in San Francisco, California.",
      "authors": [
        {
          "affiliations": [],
          "personId": 130030
        }
      ]
    },
    {
      "id": 130027,
      "typeId": 13139,
      "durationOverride": 100,
      "title": "Cognitive Tools for Uncovering Useful Abstractions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "4",
      "source": "CSV",
      "trackId": 12455,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127851
      ],
      "eventIds": [],
      "abstract": "In the 17th century, the Cartesian coordinate system was groundbreaking. It exposed the unity between algebra and geometry, accelerating the development of the math that took humans to the moon. It was not just another concept, but a cognitive tool that people could wield to express abstract ideas in visual form, thereby expanding their capacity to think and generate new insights about a variety of other problems. Research in my lab aims to uncover the psychological mechanisms that explain how people have come to deploy these technologies in such innovative ways to learn, share knowledge, and create new things. In the first part of this talk, I will provide an overview of our recent work investigating drawing ‚Äî one of our most enduring and versatile tools. Across several empirical and computational studies, I‚Äôll argue that drawing not only provides a window into how we perceive and understand the visual world, but also accelerates humans' ability to learn and communicate useful abstractions. In the second part, I will describe an emerging line of work investigating how humans discover new abstractions when building physical structures, and externalize these abstractions to support planning and collaboration. I will close by noting the broader implications of embracing such complex, naturalistic behaviors for advancing theories of human cognition and enhancing real-world impact, including in AI and education.           Bio: Judy Fan is an Assistant Professor of Psychology at Stanford University. Research in her lab aims to reverse engineer the human cognitive toolkit, especially how people use physical representations of thought to learn, communicate, and solve problems. Towards this end, her lab employs converging approaches from cognitive science, computational neuroscience, and artificial intelligence. She previously held a faculty appointment at the University of California, San Diego, received her PhD in Psychology from Princeton University, and received her AB in Neurobiology and Statistics from Harvard College.",
      "authors": [
        {
          "affiliations": [],
          "personId": 130028
        }
      ]
    }
  ],
  "people": [
    {
      "id": 125749,
      "firstName": "Haldean",
      "lastName": "Brown",
      "middleInitial": "",
      "importedId": "1HKkwwiARiE_Btyvjlv0Sg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125750,
      "firstName": "Hariharan",
      "lastName": "Subramonyam",
      "middleInitial": "",
      "importedId": "8uB_Jqw_cfCMBk7zG0Bx_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125751,
      "firstName": "Nikolas",
      "lastName": "Martelaro",
      "middleInitial": "",
      "importedId": "9NGSZTdtf2HFcb1UzsbraQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125752,
      "firstName": "Ilene L",
      "lastName": "E",
      "middleInitial": "",
      "importedId": "nayJNXtLtZvaJeEEmal1GQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125753,
      "firstName": "XIWEI",
      "lastName": "XU",
      "middleInitial": "",
      "importedId": "wXbdlJqUwUAoqW-YUV51Dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125754,
      "firstName": "Tongyan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "pKhLEQmssxHsd7nafStK2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125755,
      "firstName": "Yongqi",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "fdPoAev3fSkNTE6hww0gCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125756,
      "firstName": "Rahul",
      "lastName": "Jain",
      "middleInitial": "",
      "importedId": "mfTaRRSgYeJ2PsLpZSedWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125757,
      "firstName": "Shobhit",
      "lastName": "Aggarwal",
      "middleInitial": "",
      "importedId": "0fitSRBzF2DcCHIb-glQVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125758,
      "firstName": "Po-Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "tZtA4J2ys46UB-kAIBzV7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125759,
      "firstName": "Zhiyuan",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "BZwHnAC2-PhEA0JIPm0G4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125760,
      "firstName": "Jack",
      "lastName": "Forman",
      "middleInitial": "",
      "importedId": "f8fRxS-izkHudU22fUFL9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125762,
      "firstName": "John",
      "lastName": "Stankovic",
      "middleInitial": "",
      "importedId": "gHvQThcCzQIVs2jR-0xi0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125764,
      "firstName": "Mihai",
      "lastName": "B√¢ce",
      "middleInitial": "",
      "importedId": "NM6QuG5wfFffsxGJuVOwng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125765,
      "firstName": "Stephen",
      "lastName": "Brade",
      "middleInitial": "",
      "importedId": "KVnKgrTeEwC-MBLdTGRLoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125766,
      "firstName": "Weinan",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "UKDrUKZr1jimKgMVC_LjBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125767,
      "firstName": "Michael",
      "lastName": "Wessely",
      "middleInitial": "",
      "importedId": "qRALpo5vu6f15uoXT7L8hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125768,
      "firstName": "Jingyu",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "D6Z-UGT8CuISvCYUNegRAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125769,
      "firstName": "Kerun",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "jJyC4CqDSckhMfq0JvMBtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125770,
      "firstName": "Stephen",
      "lastName": "MacNeil",
      "middleInitial": "",
      "importedId": "B4lp0htvBcqTBYIuQJUiGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125771,
      "firstName": "Shih Chin",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "iFmSboxmjunXyM1pLyTrGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125773,
      "firstName": "Vivien",
      "lastName": "Roussel",
      "middleInitial": "",
      "importedId": "ywYG5J8WaGsPv7UwT9vMgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125775,
      "firstName": "Xianfeng David",
      "lastName": "Gu",
      "middleInitial": "",
      "importedId": "eMWLACiedvOZQbupTWcdQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125776,
      "firstName": "Angela",
      "lastName": "Vujic",
      "middleInitial": "",
      "importedId": "2ui4YRbTYsRJMof5zAKxow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125777,
      "firstName": "Mert",
      "lastName": "Toka",
      "middleInitial": "",
      "importedId": "AfhL_hsjQzuGxt_XW3nwxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125779,
      "firstName": "Nick",
      "lastName": "Colonnese",
      "middleInitial": "",
      "importedId": "XXRxqHvvUkn4WQL000-5vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125780,
      "firstName": "Jun",
      "lastName": "Rekimoto",
      "middleInitial": "",
      "importedId": "2Guk0EWER7zUs-hWrmCSsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125782,
      "firstName": "Jas",
      "lastName": "Brooks",
      "middleInitial": "",
      "importedId": "k2LyUqEimjE87jfl9U9VgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125784,
      "firstName": "Tianren",
      "lastName": "Luo",
      "middleInitial": "",
      "importedId": "slLBKiGAjSYbXIuph7-uNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125785,
      "firstName": "Zhiming",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "evcQEtJTIGDkKUM_0i1kVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125787,
      "firstName": "Yiyue",
      "lastName": "Luo",
      "middleInitial": "",
      "importedId": "5CPGyyXKdVhEYOpiJhyCIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125788,
      "firstName": "Megan",
      "lastName": "Hofmann",
      "middleInitial": "",
      "importedId": "C82eVJWjVtjKXzredlQKUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125794,
      "firstName": "Jieshan",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "APD5zlTaGFwbTC-zXn94wg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125797,
      "firstName": "Sonia",
      "lastName": "Prashant",
      "middleInitial": "",
      "importedId": "p1GPgmKnpoN4PNxC6JVDyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125799,
      "firstName": "Yongquan",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "CYzpMBOdLBgpgus4tfzmbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125801,
      "firstName": "Geoffrey",
      "lastName": "Litt",
      "middleInitial": "",
      "importedId": "9oABTYbWck2AxhPd_XcFiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125802,
      "firstName": "Leandra",
      "lastName": "Tejedor",
      "middleInitial": "",
      "importedId": "vhhq_BAW4z88RcmK_wUb2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125803,
      "firstName": "Hsuanling",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "YNJVYFABSLFaOwJhSTG3bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125805,
      "firstName": "Takekazu",
      "lastName": "Kitagishi",
      "middleInitial": "",
      "importedId": "6nucnur3o2p85y4OdML8vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125808,
      "firstName": "Ziqi",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "lDMQQVdoOg-MN_SYo4_H9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125809,
      "firstName": "Jiaxi",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "paCdKgPp9f8C04BFR0sf7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125810,
      "firstName": "Peiquan",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "I0nDmLZp4qiWojgc6LFZvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125811,
      "firstName": "Zhengxiong",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "DStzi-JOW7O9thhtQLSNcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125812,
      "firstName": "Jiachen",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "JDr15_lCe-Y2xDSffyKs2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125813,
      "firstName": "Teng",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "tHIfaVnGk0hgRd0jb9BRUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125814,
      "firstName": "Jiening",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "lMcgvvEJGbHIvf4l9gAm7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125820,
      "firstName": "Arthur",
      "lastName": "Caetano",
      "middleInitial": "",
      "importedId": "igWWQQU6S_5tsJaeJgdEmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125823,
      "firstName": "Haruki",
      "lastName": "Takahashi",
      "middleInitial": "",
      "importedId": "valmwVaVJvEDw-u5ctqCxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125825,
      "firstName": "Kiran",
      "lastName": "Gani",
      "middleInitial": "",
      "importedId": "p_lnPgXkm_S8091Q4rxTwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125827,
      "firstName": "Zachary",
      "lastName": "Tatlock",
      "middleInitial": "",
      "importedId": "hgAl3l-evkOElEGuq8JvyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125828,
      "firstName": "Chenliang",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "-Jb-AOQ3k8HjouI2K1-zIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125829,
      "firstName": "Oliver",
      "lastName": "Schneider",
      "middleInitial": "",
      "importedId": "UdBI4I2fjfdOXDAydMLixw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125832,
      "firstName": "Hemant",
      "lastName": "Surale",
      "middleInitial": "Bhaskar",
      "importedId": "Tf-0w25xUThXdcQEPH-hqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125833,
      "firstName": "Ge",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "FWLlgLqOMq2xmLYJTe-zfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125835,
      "firstName": "Vidya",
      "lastName": "Setlur",
      "middleInitial": "",
      "importedId": "dxPeY6Jxn1PhelTJJyXnjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125836,
      "firstName": "Dasha",
      "lastName": "Shifrina",
      "middleInitial": "",
      "importedId": "3tJ7Z-9oA2Ur_n1KvxHMfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125837,
      "firstName": "Heng",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "-865vpVWyDPdYc6Vg7gpsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125838,
      "firstName": "Tiantian",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "mphlRbPmhnmAD5W59CJh3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125839,
      "firstName": "Florian",
      "lastName": "Mueller",
      "middleInitial": "‚ÄòFloyd‚Äô",
      "importedId": "l8cWCEh-tWQZbtxUv2Azag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125841,
      "firstName": "Mustafa Doga",
      "lastName": "Dogan",
      "middleInitial": "",
      "importedId": "iFn4fAUAYoPYRz95BeIWHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125843,
      "firstName": "Nimesha",
      "lastName": "Ranasinghe",
      "middleInitial": "",
      "importedId": "PGeCgLRrndebAml85w42sA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125844,
      "firstName": "Zhe",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "xEzSu3LrDkEvSMCHgbtdew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125845,
      "firstName": "Mingming",
      "lastName": "Fan",
      "middleInitial": "",
      "importedId": "Sf6s1OuvcuM-r64kIOrFFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125846,
      "firstName": "Ryo",
      "lastName": "Suzuki",
      "middleInitial": "",
      "importedId": "GN-jA9I12mj5OyFiQL9Cxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125849,
      "firstName": "Xiang 'Anthony'",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "KpO_0vxU-ZNKmw9QA3hTuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125851,
      "firstName": "Wange",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "tjLUX3dHCm4TvISCT5hIHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125852,
      "firstName": "Miroslav",
      "lastName": "Suzara",
      "middleInitial": "",
      "importedId": "dK_L0WkFIP2KVe_Mdp2K6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125854,
      "firstName": "Noor",
      "lastName": "Amin",
      "middleInitial": "",
      "importedId": "QUx2bOIpCwwnzhQn1kh3zg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125855,
      "firstName": "Aiguo",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "2AKtDVWV9tXgzewaU4LggQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125856,
      "firstName": "Zhenchang",
      "lastName": "Xing",
      "middleInitial": "",
      "importedId": "2W-eRE8FGO6UBoeBae5_Zw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125858,
      "firstName": "Minsuk",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "_bgOe1sWO1n7dyhSihLhxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125860,
      "firstName": "Abena",
      "lastName": "Boadi-Agyemang",
      "middleInitial": "",
      "importedId": "_-w_MCOmqA4WPMsH5Z3tiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125861,
      "firstName": "Hiroyuki",
      "lastName": "Manabe",
      "middleInitial": "",
      "importedId": "GPbdshGJqVFsnL-tpVbWZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125863,
      "firstName": "G√©ry",
      "lastName": "Casiez",
      "middleInitial": "",
      "importedId": "DIpebubG6g2BUtnvQQ6Pcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125864,
      "firstName": "Li-Yi",
      "lastName": "Wei",
      "middleInitial": "",
      "importedId": "wINqyPgzdC9m_wySH9U_XA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125865,
      "firstName": "Keiichi",
      "lastName": "Ihara",
      "middleInitial": "",
      "importedId": "UgbSt_yRJSagB9D8c1y05w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125866,
      "firstName": "Anusha",
      "lastName": "Withana",
      "middleInitial": "",
      "importedId": "80-gbgUjsm6bpgIfu0McLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125867,
      "firstName": "Jennifer",
      "lastName": "Jacobs",
      "middleInitial": "",
      "importedId": "dNmqJWscQmAhaZifgd-Bow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125868,
      "firstName": "Chia-Chen",
      "lastName": "Chi",
      "middleInitial": "",
      "importedId": "A1pl8of0SvwbMKG2tgiaaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125870,
      "firstName": "Zhicheng",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "zmSq2J9bmCXN13LDOegbrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125871,
      "firstName": "Jennifer",
      "lastName": "Mankoff",
      "middleInitial": "",
      "importedId": "t5zpMV-vR3b9i2C9JiCaWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125872,
      "firstName": "Caleb",
      "lastName": "Chan",
      "middleInitial": "C.",
      "importedId": "U3cdZZTSQJQJ4jKCp2TUag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125873,
      "firstName": "Kyzyl",
      "lastName": "Monteiro",
      "middleInitial": "",
      "importedId": "WHdTqYyms-c_XqcUEoblDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125874,
      "firstName": "Jackson",
      "lastName": "Rushing",
      "middleInitial": "",
      "importedId": "cEfG-WWys4Q0EHhw9SZHow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125875,
      "firstName": "Carrie",
      "lastName": "Cai",
      "middleInitial": "J",
      "importedId": "IpbG6xKKCWj1t6fVN2m8Qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125876,
      "firstName": "Percy",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "OG1BX_0XIzdWZn3lhaKOrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125878,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "aY56pNDJZ4aDpSLEQxb3_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125879,
      "firstName": "Mark",
      "lastName": "Meyer",
      "middleInitial": "",
      "importedId": "YahjNJjaCqM9Oc1TQw8qxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125881,
      "firstName": "Mike",
      "lastName": "Chen",
      "middleInitial": "Y.",
      "importedId": "lHUnsX0Tf6ZdyKH8WPjT9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125882,
      "firstName": "Zhenliang",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "MJ3yBKo4VHj_-4f3IfNEvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125884,
      "firstName": "Tan",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "emwcvxO4LNzr8Vh2u-222A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125885,
      "firstName": "Zhigeng",
      "lastName": "Pan",
      "middleInitial": "",
      "importedId": "XaJ1RbS9TBe51uyKRv9LcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125888,
      "firstName": "Hongning",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "upeRD5CVR9AS6hj5_L_-mQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125889,
      "firstName": "Eunice",
      "lastName": "Jun",
      "middleInitial": "",
      "importedId": "vqX_4F0zlxG1K9bAoTlgYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125891,
      "firstName": "Rubaiat Habib",
      "lastName": "Kazi",
      "middleInitial": "",
      "importedId": "UJXDaT6uZ4fNhdFsUVs55w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125893,
      "firstName": "Francois",
      "lastName": "Guimbretiere",
      "middleInitial": "",
      "importedId": "haFD2OgOdA3IY14UrV_GbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125896,
      "firstName": "J√ºrgen",
      "lastName": "Steimle",
      "middleInitial": "",
      "importedId": "s7t9OOHe8Q59g39YCU0IbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125897,
      "firstName": "Vivek",
      "lastName": "Jayaram",
      "middleInitial": "",
      "importedId": "tbIsGWt_CnXjySnd5tujzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125898,
      "firstName": "Edward",
      "lastName": "Misback",
      "middleInitial": "",
      "importedId": "Wtc4jsVVu9chDdl2fg7Czw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125899,
      "firstName": "Ken",
      "lastName": "Pfeuffer",
      "middleInitial": "",
      "importedId": "NrpIHYjQBiGY3BcwqIdH0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125900,
      "firstName": "Eytan",
      "lastName": "Adar",
      "middleInitial": "",
      "importedId": "RKBfX-gdSuZlPy6Y9_ZrWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125902,
      "firstName": "Juhi",
      "lastName": "Kedia",
      "middleInitial": "",
      "importedId": "ZKn79U6r6bXGIRSod5eCfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125903,
      "firstName": "Shan-Yuan",
      "lastName": "Teng",
      "middleInitial": "",
      "importedId": "Fr_MdyDQrCsSwLDzlppG2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125905,
      "firstName": "Tu",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "4vVdy4p9dNGozB2950m9oA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125907,
      "firstName": "Neil",
      "lastName": "Gershenfeld",
      "middleInitial": "",
      "importedId": "XJ4-FbJFtoQVUgB5KXVm5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125909,
      "firstName": "Svitlana",
      "lastName": "Midianko",
      "middleInitial": "",
      "importedId": "dLeuJxX8ZZVd_HzJh03rbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125910,
      "firstName": "Camila",
      "lastName": "Friedman-Gerlicz",
      "middleInitial": "",
      "importedId": "3SJsF0D_nsAiGbNih4y8jA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125915,
      "firstName": "Yang",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "H6pNZe1VMhZtmachw5ibrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125916,
      "firstName": "Taegyu",
      "lastName": "Jin",
      "middleInitial": "",
      "importedId": "77QhoxYk83C95LGab8EYaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125921,
      "firstName": "Ahmad",
      "lastName": "Taka",
      "middleInitial": "",
      "importedId": "Tvs1TKGuNUWHu8wtsKt3Xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125922,
      "firstName": "Lukas",
      "lastName": "Fritzsche",
      "middleInitial": "",
      "importedId": "ueRf4sxwsGZv0qrUC7D7yQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125923,
      "firstName": "Mackenzie",
      "lastName": "Leake",
      "middleInitial": "",
      "importedId": "iUeS_U98Vg4xQUr78-q7Kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125925,
      "firstName": "Wan-Chen",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "9dyfctixmaNcwtAuhWEs7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125926,
      "firstName": "Ata",
      "lastName": "Otaran",
      "middleInitial": "",
      "importedId": "U9wVMPpAy2L3eQAUJolIkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125927,
      "firstName": "Neil",
      "lastName": "Chulpongsatorn",
      "middleInitial": "",
      "importedId": "rSnV-hvldVQTSd1gTry7BQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125928,
      "firstName": "Yuna",
      "lastName": "Watanabe",
      "middleInitial": "",
      "importedId": "if46D19bcv4rC0A3GrJXew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125929,
      "firstName": "Chenhan",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "_Fg_-Yqg985ggG-4lunRig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125930,
      "firstName": "Wendy",
      "lastName": "Mackay",
      "middleInitial": "E.",
      "importedId": "ycUYXTFLv9qHpo3w--qmEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125931,
      "firstName": "Masahiko",
      "lastName": "Inami",
      "middleInitial": "",
      "importedId": "ymV92K5P9NKDPz9t2QmJBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125933,
      "firstName": "Xutong",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "Qvf_Vt1BMKwDTgM8-jlSFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125936,
      "firstName": "Marcel",
      "lastName": "Borowski",
      "middleInitial": "",
      "importedId": "SirLPJKaltPZsGk-S7n6uA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125938,
      "firstName": "Tianhong",
      "lastName": "Yu",
      "middleInitial": "Catherine",
      "importedId": "GjBvatlPKmFIpCliYjTNaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125939,
      "firstName": "Jennifer",
      "lastName": "Healey",
      "middleInitial": "",
      "importedId": "NID1xGqMZskcpeCvy0MZHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125940,
      "firstName": "Anika",
      "lastName": "Sayara",
      "middleInitial": "",
      "importedId": "0kTjKq6Iyn277y2QO08j-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125942,
      "firstName": "John",
      "lastName": "Chung",
      "middleInitial": "Joon Young",
      "importedId": "PGlpEVdS4_1vxTCbCETfCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125945,
      "firstName": "Zhipeng",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "3SAjQF1S2E3cLBzH2JmkTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125946,
      "firstName": "Violet Yinuo",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "KrwWgHfNrolI8NBfSU08Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125947,
      "firstName": "Nitzan",
      "lastName": "Bartov",
      "middleInitial": "",
      "importedId": "w-ZGis4Lb1QiSW-WQkX4FA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125948,
      "firstName": "Don Samitha",
      "lastName": "Elvitigala",
      "middleInitial": "",
      "importedId": "Hs_eu2OjQBPkrDYRgbKczA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125950,
      "firstName": "Conrad",
      "lastName": "Wyrick",
      "middleInitial": "",
      "importedId": "4X_BkdD8uClwt9zbwkph7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125953,
      "firstName": "Aditya",
      "lastName": "Gunturu",
      "middleInitial": "",
      "importedId": "L-fUZou7gDziFta9jz6h1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125954,
      "firstName": "Olivia",
      "lastName": "Seow",
      "middleInitial": "",
      "importedId": "hVzoC04I0qi9fO0MYKQ9IQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125955,
      "firstName": "Weihao",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "ZHUeIAVkrZR2YFY2WtTGNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125956,
      "firstName": "Damien",
      "lastName": "Masson",
      "middleInitial": "",
      "importedId": "iwL3ZEYDRFol0elEY4fiqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125957,
      "firstName": "Parastoo",
      "lastName": "Abtahi",
      "middleInitial": "",
      "importedId": "6wNA8DW0RAGuKiNjrv0V4w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125961,
      "firstName": "Emily",
      "lastName": "Chen",
      "middleInitial": "Lynn",
      "importedId": "BuHjevHjWdsECIbVVJjeUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125962,
      "firstName": "Zhijie",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "704AsxHq3GaLqRRJ4SDY9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125963,
      "firstName": "Azumi",
      "lastName": "Maekawa",
      "middleInitial": "",
      "importedId": "ZISiFBOX_xh3hkdZGyuq9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125965,
      "firstName": "Kangyu",
      "lastName": "Yuan",
      "middleInitial": "",
      "importedId": "_7AQrdVseA03hAesIVvchA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125970,
      "firstName": "Nicholas",
      "lastName": "Schiefer",
      "middleInitial": "",
      "importedId": "WCn7nvzYHoc2h_ZXi4WwoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125972,
      "firstName": "Hijung Valentina",
      "lastName": "Shin",
      "middleInitial": "",
      "importedId": "byDXDiMBq8Mwevc3dXtYkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125973,
      "firstName": "Daehwa",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "wofoprAgV6gzBo3Cazk5RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125974,
      "firstName": "Hiroshi",
      "lastName": "Ishii",
      "middleInitial": "",
      "importedId": "Q0WFFAjiC8YNNo97lIdtKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125976,
      "firstName": "Sidong",
      "lastName": "Feng",
      "middleInitial": "",
      "importedId": "So7Az2z7_DP0CjgJlfYAcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125978,
      "firstName": "Wenqiang",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "0O6BNlsDbXXN2ViCR8PiOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125979,
      "firstName": "Kentaro",
      "lastName": "Inui",
      "middleInitial": "",
      "importedId": "aqfraHWF1YNXHLD6HOPYjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125980,
      "firstName": "Nadine",
      "lastName": "El Nesr",
      "middleInitial": "",
      "importedId": "AwlRFEmaZVYNvJxpB5t1YQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125981,
      "firstName": "Xiaosong",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "xczrVCM2QsyuDYvNaosDNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125982,
      "firstName": "Aaron",
      "lastName": "Quigley",
      "middleInitial": "J",
      "importedId": "MibYPMlrFfkZJFjG9MoptQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125983,
      "firstName": "Marc",
      "lastName": "Teyssier",
      "middleInitial": "",
      "importedId": "9ExYnNlmMZIrFprSSixNHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125987,
      "firstName": "Mina",
      "lastName": "Huh",
      "middleInitial": "",
      "importedId": "3DJ9vRXdYSYSAX1SuhfR7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125988,
      "firstName": "Bjoern",
      "lastName": "Hartmann",
      "middleInitial": "",
      "importedId": "UeMY1SsBRzOg4bVxZSCmuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125989,
      "firstName": "Jenny",
      "lastName": "Gutierrez Villalobos",
      "middleInitial": "",
      "importedId": "ZG3viplaocc1q09pnoYAzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125990,
      "firstName": "Joey",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "-jmVWDQpUO-Qc7QV2zt-LA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125991,
      "firstName": "Hyeonsu",
      "lastName": "Kang",
      "middleInitial": "B",
      "importedId": "gCmsrQ1XRAePogWZ7jZMEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125992,
      "firstName": "Jeffrey",
      "lastName": "Heer",
      "middleInitial": "",
      "importedId": "foZlczN2NbdcrcVTe5EoTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125993,
      "firstName": "Toby",
      "lastName": "Li",
      "middleInitial": "Jia-Jun",
      "importedId": "il6-1O1vU0p_l7aSJcoJ4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125994,
      "firstName": "Kui",
      "lastName": "Ren",
      "middleInitial": "",
      "importedId": "_S83YGGPLcJFjYlUMa0HBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 125999,
      "firstName": "Shyamnath",
      "lastName": "Gollakota",
      "middleInitial": "",
      "importedId": "Zr7Y7Z4_815xPro-0r9GxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126001,
      "firstName": "Zhencan",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "T1uyyfi7Cqcrs2cGUS9QUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126002,
      "firstName": "Akifumi",
      "lastName": "Takahashi",
      "middleInitial": "",
      "importedId": "Up0Lt4f07m8L1USIs52wsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126003,
      "firstName": "Christoph",
      "lastName": "Gebhardt",
      "middleInitial": "",
      "importedId": "sqi8n0FfEM1tKtLA64ElSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126004,
      "firstName": "Marie",
      "lastName": "Muehlhaus",
      "middleInitial": "",
      "importedId": "I_MgBeWfIU2xMjTEBRpWDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126005,
      "firstName": "Zhongyi",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "SkyHXjYANOq5lqmLiB2IYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126006,
      "firstName": "Arata",
      "lastName": "Jingu",
      "middleInitial": "",
      "importedId": "W2mBVwJL8Bw6BocoiEZgvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126011,
      "firstName": "Yoonjoo",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "Yo8lKVaPLwDI_M2aCxRLSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126013,
      "firstName": "Xiao",
      "lastName": "Xie",
      "middleInitial": "",
      "importedId": "hosAhoy1tie7pj8sYrxWBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126016,
      "firstName": "Lichen",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "W4745EDKRs-oXp7ytry4YA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126017,
      "firstName": "Jingwen",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "J81OwzHjnRk3raOO1ut4xA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126019,
      "firstName": "Runhua",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "lrP5BKpYqUFCwQkubMlxQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126021,
      "firstName": "Zachary",
      "lastName": "Gordon",
      "middleInitial": "",
      "importedId": "v0InCFm-jWvix7D5tnR4bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126022,
      "firstName": "Parsa",
      "lastName": "Rajabi",
      "middleInitial": "",
      "importedId": "dh569qZCQYfVoHEDjiQdMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126023,
      "firstName": "Nayeemur",
      "lastName": "Rahman",
      "middleInitial": "",
      "importedId": "uoYEPMRTLVZDP5QukvINOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126024,
      "firstName": "Taejun",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "7Y8WEIZBkmKyDUuvx8UnqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126026,
      "firstName": "Michael",
      "lastName": "Nebeling",
      "middleInitial": "",
      "importedId": "7Kex_0dKVNSpniXyyTYpBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126027,
      "firstName": "Junkai",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "bfsJmLnS-G0LUeLQlm9R4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126028,
      "firstName": "Cuong",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "Zr6kWI7nXq7mFI0bN2adwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126031,
      "firstName": "Jane",
      "lastName": "Hoffswell",
      "middleInitial": "",
      "importedId": "GHs1unNX8EJNecXs_xATBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126033,
      "firstName": "Aakar",
      "lastName": "Gupta",
      "middleInitial": "",
      "importedId": "XMRioOOL5rNvcM8E9h2LDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126034,
      "firstName": "Chao-Jung",
      "lastName": "Lai",
      "middleInitial": "",
      "importedId": "QwcyJ-iQ-xrOc_bd9vuNSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126035,
      "firstName": "Yu",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "e8rWewnb2rSS1gJ7UAiipA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126036,
      "firstName": "Angel",
      "lastName": "Yuan",
      "middleInitial": "",
      "importedId": "Z-y7zcZtBxKr_UqYzw9VcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126037,
      "firstName": "Yilan",
      "lastName": "Tan",
      "middleInitial": "",
      "importedId": "oSMEGdQ-ky55BcIHHlyAgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126039,
      "firstName": "Erik",
      "lastName": "Harpstead",
      "middleInitial": "",
      "importedId": "fC62TjRnqP3QUTmmDS6MTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126040,
      "firstName": "Sangho",
      "lastName": "Suh",
      "middleInitial": "",
      "importedId": "LCIY9oz1aK9g8onEOgleUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126041,
      "firstName": "Alanson",
      "lastName": "Sample",
      "middleInitial": "P.",
      "importedId": "65d6FsWuwXJu0zdWm9Ye-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126042,
      "firstName": "Semina",
      "lastName": "Yi",
      "middleInitial": "",
      "importedId": "UIyUH9y6FUizApI1TD6bOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126043,
      "firstName": "Ching-Wen",
      "lastName": "Hung",
      "middleInitial": "",
      "importedId": "yddadgl3HkQf_5i5nWJjCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126045,
      "firstName": "Akshay",
      "lastName": "Kothakonda",
      "middleInitial": "",
      "importedId": "XUHMgL6Fxs081s22LFuwHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126046,
      "firstName": "Tongshuang",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "n1s8hPgat0T9AYyAsaCaKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126049,
      "firstName": "Andr√©",
      "lastName": "Zenner",
      "middleInitial": "",
      "importedId": "Y1e1zISAmrQLhVJEP-fAhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126051,
      "firstName": "Sageev",
      "lastName": "Oore",
      "middleInitial": "",
      "importedId": "hFwi6lm-7wvaZklPRp3fCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126054,
      "firstName": "Jason",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "AX66TvfSdtUHX3ZgCBIB1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126055,
      "firstName": "Xiaoying",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "3qGxucbmvfz93Ic_tR3G3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126058,
      "firstName": "Martin",
      "lastName": "Schmitz",
      "middleInitial": "",
      "importedId": "2TAkNV6pUi3rf_iBjrN1nQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126059,
      "firstName": "Zeyu",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "mUgdEK5HN9HRQbpOsze3_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126060,
      "firstName": "Tyler",
      "lastName": "Angert",
      "middleInitial": "",
      "importedId": "kVuHJY7c4eNiKmWHjJVlUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126062,
      "firstName": "Yingcai",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "578wgyxUrFOd78ek47YfAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126064,
      "firstName": "K.",
      "lastName": "Wu",
      "middleInitial": "D.",
      "importedId": "ryp3UsA6ZnBwcP7qqnubxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126065,
      "firstName": "Tucker",
      "lastName": "Rae-Grant",
      "middleInitial": "",
      "importedId": "iwk49FNAK0lL__P4iAob7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126066,
      "firstName": "Qinghua",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "mMPLx-07GA0OdGEZNqCTDg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126067,
      "firstName": "Jude",
      "lastName": "Rayan",
      "middleInitial": "",
      "importedId": "i8wKdZpH8Vf8t8hzaXkuhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126069,
      "firstName": "Jacob",
      "lastName": "Ritchie",
      "middleInitial": "",
      "importedId": "v_kerJX07RuWyOUaaR8soQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126070,
      "firstName": "Tae Soo",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "IjbVor6qHOuBPuQCQqxBgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126071,
      "firstName": "Dominik",
      "lastName": "Moritz",
      "middleInitial": "",
      "importedId": "oOa6nTJUGSU7MwETeWgNMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126072,
      "firstName": "Wojciech",
      "lastName": "Matusik",
      "middleInitial": "",
      "importedId": "gith0uoO8-bukEcs8c5MTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126074,
      "firstName": "Max",
      "lastName": "M√∂bus",
      "middleInitial": "",
      "importedId": "EXVwJNIrcIINQl-hjwtp-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126075,
      "firstName": "Zhenhui",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "QLB33PjBTuHKiEDUE3PJJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126076,
      "firstName": "Nancy",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "pSFpDicMfi9ARpgFMz1Fow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126077,
      "firstName": "Naomi",
      "lastName": "Yamashita",
      "middleInitial": "",
      "importedId": "tjRZsLvEfUihxl1jaoXegQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126078,
      "firstName": "Yuta",
      "lastName": "Itoh",
      "middleInitial": "",
      "importedId": "SL_W6xgAN8h-_ktvw92OUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126079,
      "firstName": "Lea",
      "lastName": "Albaugh",
      "middleInitial": "",
      "importedId": "HNFoktgk7b8PSTzNm6U1ZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126080,
      "firstName": "Aditya Shekhar",
      "lastName": "Nittala",
      "middleInitial": "",
      "importedId": "Etjddl9lyseaIki_U3rDrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126081,
      "firstName": "Yi Fei",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "G3LNupua1Os_nKq7EveZEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126082,
      "firstName": "Rosalie",
      "lastName": "Lin",
      "middleInitial": "Hsin-Ju",
      "importedId": "NkcQpa-GXV-esyUJqziX4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126084,
      "firstName": "Jiamou",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "4NzsGMDQ_3TLyqE9pjBXgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126085,
      "firstName": "Chen",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "zCnnOFfoIPPONoDgI9SgPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126086,
      "firstName": "David",
      "lastName": "Lindlbauer",
      "middleInitial": "",
      "importedId": "DYAQWrUdOzkbxikvLgQgHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126088,
      "firstName": "Michael",
      "lastName": "Bernstein",
      "middleInitial": "S.",
      "importedId": "batNJuLMTvp1yvs1G1gBHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126089,
      "firstName": "Vivian",
      "lastName": "Shen",
      "middleInitial": "",
      "importedId": "5ODM9ZUDc4NmwYwrd8M9QQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126090,
      "firstName": "Christoph",
      "lastName": "Johns",
      "middleInitial": "A.",
      "importedId": "oYYduzpzFmfHG0CBX3Fc1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126091,
      "firstName": "Nishan",
      "lastName": "Soni",
      "middleInitial": "",
      "importedId": "vcMNOIQSgC9z6Xo8csw1Ag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126092,
      "firstName": "Crist√≥bal",
      "lastName": "Valenzuela",
      "middleInitial": "",
      "importedId": "4FcesSJ2GNTs6nzi4T4bFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126093,
      "firstName": "Shilei",
      "lastName": "Cao",
      "middleInitial": "",
      "importedId": "lVp3C8VADpwOVs236FHuVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126094,
      "firstName": "Huaishu",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "cISP1gspzEJZEfcOkN6sIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126097,
      "firstName": "Patrick",
      "lastName": "Haertel",
      "middleInitial": "William",
      "importedId": "C0dqdF6VgMwERRLz_l6E_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126098,
      "firstName": "Mengjia",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "j2AVTjkRRacwaecKNtDgeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126100,
      "firstName": "Sang Won",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "6iT8RQIiG7T69Z5mNDGzNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126101,
      "firstName": "Trung",
      "lastName": "Bui",
      "middleInitial": "",
      "importedId": "WSmIJ8AMXFXeAyVX8km2Gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126102,
      "firstName": "Martin",
      "lastName": "Feick",
      "middleInitial": "",
      "importedId": "0D4Zr0Q_AnaOfaM9Sq655Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126109,
      "firstName": "Yu",
      "lastName": "Cai",
      "middleInitial": "",
      "importedId": "nTPymubHCTuyPKY1xUtcVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126112,
      "firstName": "Yan",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "3NQ6Y0uVeGa0gkKdHCgQGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126113,
      "firstName": "Zheng",
      "lastName": "Ning",
      "middleInitial": "",
      "importedId": "_BBAh_8tX7FS79E8oGRENQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126114,
      "firstName": "Gonzalo",
      "lastName": "Munilla-Garrido",
      "middleInitial": "",
      "importedId": "KyHzaXx-6ZPCfGV-MivyKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126116,
      "firstName": "Chao",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "lRqSOevZcJU1QEh2EnDvHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126117,
      "firstName": "Veronika",
      "lastName": "Domova",
      "middleInitial": "",
      "importedId": "CYPs8w2fZsgE7ra4FaOOkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126118,
      "firstName": "Eldon",
      "lastName": "Schoop",
      "middleInitial": "",
      "importedId": "E63zkZS6wXicR5EDWg6RXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126119,
      "firstName": "Michael",
      "lastName": "Malcolm",
      "middleInitial": "C",
      "importedId": "JAmOcEOA5mOvGRYZr-6Iow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126120,
      "firstName": "Hwajung",
      "lastName": "Hong",
      "middleInitial": "",
      "importedId": "YkLIi9h4rzvE5V9lmvuTsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126121,
      "firstName": "Jiaju",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "7b1cKp5CIrE7kyfrDhyh_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126122,
      "firstName": "Junwei",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "DtdbYJUo5jdHM5uG6jDNTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126123,
      "firstName": "Sean",
      "lastName": "Follmer",
      "middleInitial": "",
      "importedId": "GJ9yHhCWcHh0c45NNt6k6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126125,
      "firstName": "Anna",
      "lastName": "Feit",
      "middleInitial": "Maria",
      "importedId": "fRUBGy6UaA-NDMjP-V3WGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126128,
      "firstName": "Junyi",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "e55KVEA-R2fiLofj_Ci0fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126129,
      "firstName": "Dazhen",
      "lastName": "Deng",
      "middleInitial": "",
      "importedId": "vYyryKl_Lmaau_6EJ1qZ9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126130,
      "firstName": "Akarsh",
      "lastName": "Aurora",
      "middleInitial": "",
      "importedId": "5QOTHDigVDI4vCZJhoj-sA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126131,
      "firstName": "Kevin",
      "lastName": "Pu",
      "middleInitial": "",
      "importedId": "G0sYcmOgMBs-sF4VcCp5Bw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126134,
      "firstName": "Seung-Jun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "QN6mbJQKJHqrmrqm40QqJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126135,
      "firstName": "Amy",
      "lastName": "Karlson",
      "middleInitial": "",
      "importedId": "tDAoEFv9LFxFeVabFWke9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126137,
      "firstName": "Jasper",
      "lastName": "Tran O'Leary",
      "middleInitial": "",
      "importedId": "lS66sIrbFV1L7mJzsqUeDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126138,
      "firstName": "Zhaoyu",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "kUcP6GdN8Al7zjsJiHcPIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126139,
      "firstName": "Chen",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "surIJ_ZEjhsIR8Ud2Q2D5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126140,
      "firstName": "Hai",
      "lastName": "Dang",
      "middleInitial": "",
      "importedId": "sbimNJ2MBwJrSQvb3mLEVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126141,
      "firstName": "Yanhong",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "VaVwyiYlRgmAJP8LZayGgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126142,
      "firstName": "Xiaojun",
      "lastName": "Bi",
      "middleInitial": "",
      "importedId": "fpWqo1rUmyvEL7HUbPbz6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126143,
      "firstName": "Yudai",
      "lastName": "Tanaka",
      "middleInitial": "",
      "importedId": "bZXoz-mKCKaq3xd2DrrlTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126144,
      "firstName": "Xiaoyu",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "rniyJaqY2-kBzZ3H_2wwIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126145,
      "firstName": "Sylvain",
      "lastName": "Malacria",
      "middleInitial": "",
      "importedId": "-XkZNXaCRSB1PSPFzDJFkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126146,
      "firstName": "Ahmed",
      "lastName": "Katary",
      "middleInitial": "",
      "importedId": "NKnLu5RRnW5rq3Zc8BCnig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126147,
      "firstName": "Joshua",
      "lastName": "Horowitz",
      "middleInitial": "",
      "importedId": "iTuS5_8X5-FgS7ozV1T6WQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126148,
      "firstName": "Ranjodh",
      "lastName": "Dhaliwal",
      "middleInitial": "Singh",
      "importedId": "kdpJr1nemcwn8MBJXViCGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126149,
      "firstName": "Ali",
      "lastName": "Shtarbanov",
      "middleInitial": "",
      "importedId": "IiJn9F2aLgQwcehlassTgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126150,
      "firstName": "William",
      "lastName": "Duan",
      "middleInitial": "",
      "importedId": "dXvSODmCz5Y4kkxyISZhOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126151,
      "firstName": "Seok-Hyung",
      "lastName": "Bae",
      "middleInitial": "",
      "importedId": "mHgyrQnjuhRIke1ZUM_TkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126156,
      "firstName": "Xiaoyu",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "Fd0Ttm6DzWs8KHlzd4So7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126158,
      "firstName": "Shohei",
      "lastName": "Katakura",
      "middleInitial": "",
      "importedId": "OutkrIA7fjSxwAe3iWUIhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126160,
      "firstName": "Connor",
      "lastName": "Courtien",
      "middleInitial": "",
      "importedId": "9sLA0ncxiaGRVBJNv8ViMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126162,
      "firstName": "Kurt",
      "lastName": "Fleischer",
      "middleInitial": "",
      "importedId": "6akArrAquVRQqSL3yM4cTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126163,
      "firstName": "Jacob",
      "lastName": "Sayono",
      "middleInitial": "",
      "importedId": "v9Lu2VtE_UEcMmRqgpP3SA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126165,
      "firstName": "Siming",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "5Daj_PpanM-IpuGOrZcwzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126166,
      "firstName": "Tianyi",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "7taies-lvmk7BcDr022wLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126167,
      "firstName": "Oscar",
      "lastName": "Ariza",
      "middleInitial": "",
      "importedId": "CQ2OfOTiLWIRGojIzmkGSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126168,
      "firstName": "Maneesh",
      "lastName": "Agrawala",
      "middleInitial": "",
      "importedId": "vEgkfTRwqF0MI1NyYHSNKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126170,
      "firstName": "Hannah",
      "lastName": "Twigg-Smith",
      "middleInitial": "Rose",
      "importedId": "Xa--VOEHd48alXj_kMUuig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126171,
      "firstName": "Nathan",
      "lastName": "DeVrio",
      "middleInitial": "",
      "importedId": "c87oHJSEuNpDHogPqJA6gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126175,
      "firstName": "MING-CHUN",
      "lastName": "HUANG",
      "middleInitial": "",
      "importedId": "qXihLsiV3WJa7KiXPhEVxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126176,
      "firstName": "Samantha",
      "lastName": "Speer",
      "middleInitial": "",
      "importedId": "JPw0Bvx9y3V5OMf5kQXJvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126177,
      "firstName": "Junran",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "pa0ba8mAo_MupKDJxnosUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126179,
      "firstName": "Xiaoshuo",
      "lastName": "Yao",
      "middleInitial": "",
      "importedId": "dMkqgu7is4tqYs6iAxP7uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126180,
      "firstName": "Derek",
      "lastName": "Witzig",
      "middleInitial": "Alexander",
      "importedId": "qUS8Rvjy-0ud20DhumSIRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126182,
      "firstName": "Peiling",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "WAuTiM5ZgtuM7Q8rthGcZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126185,
      "firstName": "Mauricio",
      "lastName": "Sousa",
      "middleInitial": "",
      "importedId": "WRWNWFmuweggtf9ISKQDxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126186,
      "firstName": "Jens Emil",
      "lastName": "Gr√∏nb√¶k",
      "middleInitial": "Sloth",
      "importedId": "_xd7AYtHUdhPVmBzB19MvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126187,
      "firstName": "Kelsey",
      "lastName": "Turbeville",
      "middleInitial": "",
      "importedId": "AXeNPUMXjD9rYwlgYLbDWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126189,
      "firstName": "Bogoan",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "1k2XCnfJZiSreR-g6zEC3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126190,
      "firstName": "Yasuto",
      "lastName": "Nakanishi",
      "middleInitial": "",
      "importedId": "lQn0gxgvpBHyPgLRwrGkhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126193,
      "firstName": "Lining",
      "lastName": "Yao",
      "middleInitial": "",
      "importedId": "wy-8_BE-UutnZpdDwndBwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126194,
      "firstName": "Xinyu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "zP5-RPtG-Ste1c5d5TKA0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126195,
      "firstName": "Christopher",
      "lastName": "Collins",
      "middleInitial": "",
      "importedId": "lEpflhaARjgjLLfjRHiIoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126197,
      "firstName": "Shreyas",
      "lastName": "Nisal",
      "middleInitial": "",
      "importedId": "fuzO6YUl6cYX0ujP4RHD1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126199,
      "firstName": "Christian",
      "lastName": "Theobalt",
      "middleInitial": "",
      "importedId": "SVagDpkrMRD2mc_ihuKhRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126201,
      "firstName": "Romain",
      "lastName": "Nith",
      "middleInitial": "",
      "importedId": "Wo0hKV9EQ-0yaKNs_54ggA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126202,
      "firstName": "Andriy",
      "lastName": "Kanyuka",
      "middleInitial": "",
      "importedId": "9hW9hGdoyM40obUPcKb6eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126203,
      "firstName": "Jennifer",
      "lastName": "Kim",
      "middleInitial": "G",
      "importedId": "TgQEc3ore7X_lFwtsmlxaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126204,
      "firstName": "Aryan",
      "lastName": "Saini",
      "middleInitial": "",
      "importedId": "H1Rts5KoHFb2ZMPLWDo1iQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126207,
      "firstName": "Aniket",
      "lastName": "Kittur",
      "middleInitial": "",
      "importedId": "dvITl7CjKRGhLNC7tWO8aw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126208,
      "firstName": "Sarah",
      "lastName": "Ellenbogen",
      "middleInitial": "",
      "importedId": "kr-QgTo6u_Qgh0l1pPJSAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126209,
      "firstName": "Karthik",
      "lastName": "Ramani",
      "middleInitial": "",
      "importedId": "ltnbQF0vXB-RYkZ1s9rlgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126210,
      "firstName": "Kristen",
      "lastName": "Dorsey",
      "middleInitial": "",
      "importedId": "AOgbSB3d3C6t-3zxRlkCQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126211,
      "firstName": "Shumin",
      "lastName": "Zhai",
      "middleInitial": "",
      "importedId": "W_RCVI8Bqn5DhSURFBnu8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126213,
      "firstName": "Chunxu",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "K75Z5QZ2nfQAzUxafZrGiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126214,
      "firstName": "Masatoshi",
      "lastName": "Hamanaka",
      "middleInitial": "",
      "importedId": "WxMgc_YluJaF5Oj64yDhZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126215,
      "firstName": "Tarik",
      "lastName": "Hasic",
      "middleInitial": "",
      "importedId": "37zmX8TLChfCTgv-KC_XQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126216,
      "firstName": "Mose",
      "lastName": "Sakashita",
      "middleInitial": "",
      "importedId": "CW6zvbgdf7wjpW5hbFLDdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126217,
      "firstName": "Martin",
      "lastName": "Taraz",
      "middleInitial": "",
      "importedId": "imRPnat8cKoj8FJRMew_eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126218,
      "firstName": "Chia-An",
      "lastName": "Fan",
      "middleInitial": "",
      "importedId": "oLDLKHEz-WGChAQl57XpiA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126219,
      "firstName": "Sarah",
      "lastName": "Nicita",
      "middleInitial": "",
      "importedId": "-4zKv8lSsjwMmvL5FUMzGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126220,
      "firstName": "Sapna",
      "lastName": "Tayal",
      "middleInitial": "",
      "importedId": "vkvQdywNi8Bnb7NIhQRt-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126222,
      "firstName": "Xiaojuan",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "tOS6C4kPL7t30lwzz_2A4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126224,
      "firstName": "Artin",
      "lastName": "Saberpour Abadian",
      "middleInitial": "",
      "importedId": "gEQ6my8P80sGFdNzM2_hxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126225,
      "firstName": "Yunfan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "DgKz7Xu8iKu1YcHWi2miaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126226,
      "firstName": "Dawn",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "IPSlNsPba7ddldbQ0JPQ7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126231,
      "firstName": "Ranjitha",
      "lastName": "Kumar",
      "middleInitial": "",
      "importedId": "UBuTBCd1YWjMDDSb2Cc8cA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126234,
      "firstName": "Robert",
      "lastName": "Kovacs",
      "middleInitial": "",
      "importedId": "mqFUE8fjklknhHiBkQvhXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126235,
      "firstName": "Yuyu",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "F4qW8AI5RpIwwgQm6E_jrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126236,
      "firstName": "Chun",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "7Ogdmsb7v7aXGq5BHxNa5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126238,
      "firstName": "Patrick",
      "lastName": "Baudisch",
      "middleInitial": "",
      "importedId": "o7VwMA3PuVJclKFtwCL87A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126240,
      "firstName": "Dinmukhammed",
      "lastName": "Mukashev",
      "middleInitial": "",
      "importedId": "t91SBgLw2gJRVW7Y_mxHBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126241,
      "firstName": "Joyce",
      "lastName": "Passananti",
      "middleInitial": "E",
      "importedId": "c7PInKkt7W-PEerfRd88FQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126243,
      "firstName": "Feng",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "qpuybavMIX1nVNzqyZ2atg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126244,
      "firstName": "Feng",
      "lastName": "Tian",
      "middleInitial": "",
      "importedId": "e4x280wrNe6DjEzW6pjtvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126245,
      "firstName": "Lingyun",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "R7Nh49O-uyOOyzPxWd1VsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126247,
      "firstName": "Joe",
      "lastName": "Mullenbach",
      "middleInitial": "",
      "importedId": "hu_3T3tHybKMYRAzYdeKIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126248,
      "firstName": "Kui",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "GSKaaey9dj7GBaMAgny_ng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126250,
      "firstName": "Anyi",
      "lastName": "Rao",
      "middleInitial": "",
      "importedId": "f7f8uIt7O1MBP3qpw4tVrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126251,
      "firstName": "Cihan",
      "lastName": "Biyikli",
      "middleInitial": "",
      "importedId": "OrsJl-8caROZV3vJ2g9L1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126254,
      "firstName": "Alexandra",
      "lastName": "Ion",
      "middleInitial": "",
      "importedId": "nPzcOO-p9-FlXrBByTE0kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126255,
      "firstName": "Qihao",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "lNZQKKdJ9SnrSe_mYCIAbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126256,
      "firstName": "Mille",
      "lastName": "Lunding",
      "middleInitial": "Skovhus",
      "importedId": "z7U9k3V8W9dtOfEqd3nZrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126259,
      "firstName": "Florian",
      "lastName": "Strohm",
      "middleInitial": "",
      "importedId": "x7vuHPg6Xi56yVbbBu1WtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126260,
      "firstName": "Anhong",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "EE_pQo_cejBcQ4Xv5yZujw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126261,
      "firstName": "Camille",
      "lastName": "Gobert",
      "middleInitial": "",
      "importedId": "HytnajZMu-b-AssbZFIsvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126262,
      "firstName": "Misha",
      "lastName": "Sra",
      "middleInitial": "",
      "importedId": "Ud_BsQ2Tnwvc9smICZF5hg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126264,
      "firstName": "Gaurav",
      "lastName": "Jain",
      "middleInitial": "",
      "importedId": "SEPfc3LF6kZtFUhIFL4eEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126265,
      "firstName": "Qi",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "Gb_u7wSqq774w-KIItzvbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126267,
      "firstName": "Runlin",
      "lastName": "Duan",
      "middleInitial": "",
      "importedId": "PjWHvawpCYWVpuW0Dus61A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126268,
      "firstName": "Diogo",
      "lastName": "Luvizon",
      "middleInitial": "",
      "importedId": "7Omnvb2B_gNksoee0qnPBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126270,
      "firstName": "Jim",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "_NfCcrpoyXmNprJz4EMZUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126271,
      "firstName": "Jenny",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "7O7-atlVuuVP6dPkI0n2yQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126272,
      "firstName": "Wei-Hsin",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "m7_zf8XKu9Obxx1-eyey0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126273,
      "firstName": "Xing-Dong",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "qHjKmi-3r1K3BNbRMD2C6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126274,
      "firstName": "Juliete",
      "lastName": "Rossie",
      "middleInitial": "",
      "importedId": "U2mKltNbw4KJRMEA4440QQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126276,
      "firstName": "yachun",
      "lastName": "fan",
      "middleInitial": "",
      "importedId": "4mP3qf-nf4Wf1H-VjPsYZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126278,
      "firstName": "Abdulaziz",
      "lastName": "Alaboudi",
      "middleInitial": "",
      "importedId": "1eRgWiooJitKLNTz5Cp03A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126279,
      "firstName": "Zhao",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "t6LhIQo0K9Ve92OcK5qi-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126281,
      "firstName": "Kyu Won",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "KbDtps-CslAOpCjukWsfGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126283,
      "firstName": "Ira",
      "lastName": "Kemelmacher-Shlizerman",
      "middleInitial": "",
      "importedId": "s6OUEtkHGJAb87zXmErSIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126284,
      "firstName": "Zihan",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "-3vsEkPAM8r6kMbo3_2q4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126285,
      "firstName": "Samuel",
      "lastName": "Huron",
      "middleInitial": "",
      "importedId": "j4UIFeJ8EV61KrJgBLYtjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126286,
      "firstName": "Hita",
      "lastName": "Kambhamettu",
      "middleInitial": "",
      "importedId": "4O7mrvG6GjaqtXgHmZIYYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126287,
      "firstName": "Kyungsik",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "oVcowPaM8UEB0TFHi7Yztg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126288,
      "firstName": "Xingbo",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "ynSCVhtuQpQtFHEWO8znFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126289,
      "firstName": "Nathalie",
      "lastName": "Overdevest",
      "middleInitial": "",
      "importedId": "jBKNSFNdPUDd0nbrZDm-QQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126290,
      "firstName": "Liang",
      "lastName": "He",
      "middleInitial": "",
      "importedId": "n6CDwP8NP99UPUyk2xhhVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126292,
      "firstName": "Andreas",
      "lastName": "Fender",
      "middleInitial": "Rene",
      "importedId": "aGZO97mF9CBleMOKcUJ0Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126293,
      "firstName": "Pengrui",
      "lastName": "Quan",
      "middleInitial": "",
      "importedId": "6LCiSIzuCIzKagf4EJjnRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126295,
      "firstName": "Chenyang",
      "lastName": "Cai",
      "middleInitial": "",
      "importedId": "d3nYp7C2OmxDRM744XVDtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126297,
      "firstName": "Anthony",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "YGO4_l5XRHyTv-hGHbY83g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126298,
      "firstName": "Steve",
      "lastName": "Seitz",
      "middleInitial": "",
      "importedId": "5KB805mbtqDboRzSnZpuGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126302,
      "firstName": "Bryan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "HPOKEIUSZ5soMnVIZFkXqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126303,
      "firstName": "Linghao",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "ridamgubIS6DFow0anx5OQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126305,
      "firstName": "Stefanie",
      "lastName": "Mueller",
      "middleInitial": "",
      "importedId": "5vj196FvEHPKhaKPi53vKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126306,
      "firstName": "Chunyang",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "DKApI2jSxea4NOOuL6zZtg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126309,
      "firstName": "Keigo",
      "lastName": "Ushiyama",
      "middleInitial": "",
      "importedId": "EQyDh6coAYB_-Q29Fh5kPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126311,
      "firstName": "Robert",
      "lastName": "Xiao",
      "middleInitial": "",
      "importedId": "MG2oLsriVTMXVgzxme3DOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126312,
      "firstName": "Andreas",
      "lastName": "Bulling",
      "middleInitial": "",
      "importedId": "8yArQHb_3Ykoz836YJ3feQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126313,
      "firstName": "Elise",
      "lastName": "van den Hoven",
      "middleInitial": "",
      "importedId": "yEOYXDitD4SG38W22HELpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126314,
      "firstName": "Sixuan",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "fTsg3kfp7CNfajJmIfwdLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126315,
      "firstName": "Yasha",
      "lastName": "Iravantchi",
      "middleInitial": "",
      "importedId": "A79E5ZzjmfUTOdRBOspiFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126318,
      "firstName": "Christian",
      "lastName": "Holz",
      "middleInitial": "",
      "importedId": "iLPlU_z1dah9YY__uEeXsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126322,
      "firstName": "Qingyu",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "FX45bFFfs1DpUZn-LVy8vg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126325,
      "firstName": "Kylie",
      "lastName": "Peppler",
      "middleInitial": "A",
      "importedId": "pg7caKT5aY0WBlNS1XLbSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126327,
      "firstName": "Meng",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "jkq16zqYaKtFpXeLB-mPxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126328,
      "firstName": "Yi-Hao",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "mA32XskTkKZ9pUU0qJYRTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126329,
      "firstName": "Kristijan",
      "lastName": "Ivanƒçiƒá",
      "middleInitial": "",
      "importedId": "UGTUhn2d6mL8zBCIBcTUzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126331,
      "firstName": "Jeremy",
      "lastName": "Chu",
      "middleInitial": "",
      "importedId": "tyQDlBXrOABPn-6b6W5poA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126333,
      "firstName": "√áaƒüatay",
      "lastName": "Demiralp",
      "middleInitial": "",
      "importedId": "QwHcg0S4gD7FjGvzOUvihQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126334,
      "firstName": "Chris",
      "lastName": "Harrison",
      "middleInitial": "",
      "importedId": "s4DDgcdMxU0xmPJmhyUsNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126335,
      "firstName": "Pedro",
      "lastName": "Lopes",
      "middleInitial": "",
      "importedId": "vu7xKEsb0s2o9WvEByjw7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126337,
      "firstName": "Changyo",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "098kWELGyd9x1oRh_FKISA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126339,
      "firstName": "Ozgun",
      "lastName": "Kilic Afsar",
      "middleInitial": "",
      "importedId": "BERa-LgPbS4EK7duphD0yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126340,
      "firstName": "Ikkaku",
      "lastName": "Kawaguchi",
      "middleInitial": "",
      "importedId": "K5BHzMVJJdf5oz0xCO_wmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126341,
      "firstName": "Joey",
      "lastName": "O'Brien",
      "middleInitial": "",
      "importedId": "6lN5sM28SuvraCYSgetWAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126342,
      "firstName": "Rorik",
      "lastName": "Henrikson",
      "middleInitial": "",
      "importedId": "ZQOZjTLSmT2j4FG00gvAHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126343,
      "firstName": "Masatoshi",
      "lastName": "Hidaka",
      "middleInitial": "",
      "importedId": "2Mo2Pc5E-EN3T-hkClGx4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126344,
      "firstName": "Xun",
      "lastName": "Qian",
      "middleInitial": "",
      "importedId": "Mzd41Mu9XTzsFqvxf99-oA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126345,
      "firstName": "Madalina",
      "lastName": "Nicolae",
      "middleInitial": "Luciana",
      "importedId": "IRnhxDKl1XTcEk7-vwPL_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126346,
      "firstName": "Jun",
      "lastName": "Suzuki",
      "middleInitial": "",
      "importedId": "kaa84wZ3zAKUtXpaGqLEuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126348,
      "firstName": "Basel",
      "lastName": "Hindi",
      "middleInitial": "",
      "importedId": "cSinB5rgdfCAygGGznd6Fw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126349,
      "firstName": "Rui",
      "lastName": "Dong",
      "middleInitial": "",
      "importedId": "8J_x-z48prvNJPm2Ev6rIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126350,
      "firstName": "Takeshi",
      "lastName": "Naemura",
      "middleInitial": "",
      "importedId": "qc0uRc9kefvrfEWLoX_CJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126351,
      "firstName": "Nikhita",
      "lastName": "Joshi",
      "middleInitial": "",
      "importedId": "0jUgDgySag55ncDLi9_2nA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126353,
      "firstName": "Vimal",
      "lastName": "Mollyn",
      "middleInitial": "",
      "importedId": "9bLhzLgJ7rym4AIif7somA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126355,
      "firstName": "Alex",
      "lastName": "Mazursky",
      "middleInitial": "",
      "importedId": "7BMFQW2Ko9OVQblRuGVH3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126356,
      "firstName": "Daniel",
      "lastName": "Vogel",
      "middleInitial": "",
      "importedId": "VH8XpxsJEUlDmsjNcBbGOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126359,
      "firstName": "Haijun",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "qR18CZssIG8qNcFRCGcljg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126361,
      "firstName": "Tovi",
      "lastName": "Grossman",
      "middleInitial": "",
      "importedId": "y0gW9n9_FRp81xTibIb1pg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126368,
      "firstName": "Jo√£o",
      "lastName": "Evangelista Belo",
      "middleInitial": "Marcelo",
      "importedId": "dvHBJlfCWgQhrKbxlIjcRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126369,
      "firstName": "Jiacheng",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "krHDhT_AvNMP-uRouG7gAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126370,
      "firstName": "Carolyn",
      "lastName": "Ros√©",
      "middleInitial": "",
      "importedId": "FjS_lPChZmBVIKYjBfI8DQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126372,
      "firstName": "Tatsuki",
      "lastName": "Kuribayashi",
      "middleInitial": "",
      "importedId": "BvGLuNkLoFctLxy3yrkVsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126375,
      "firstName": "Shuo",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "hJ4ZjPexxHjEFtGfGQS1gA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126377,
      "firstName": "Dongwook",
      "lastName": "Yoon",
      "middleInitial": "",
      "importedId": "c8wLPbWPGUAAzs_nOML88g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126378,
      "firstName": "Nickolina",
      "lastName": "Yankova",
      "middleInitial": "",
      "importedId": "1LnqYmsPQrdYxyWulC9pAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126379,
      "firstName": "Kevin",
      "lastName": "Van",
      "middleInitial": "",
      "importedId": "-1XR47vNc8IjjQh4X6yKwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126380,
      "firstName": "Chang",
      "lastName": "Xiao",
      "middleInitial": "",
      "importedId": "zj-3Auk1zTBUkAHqgdGAYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126381,
      "firstName": "Nola",
      "lastName": "Rettenmaier",
      "middleInitial": "",
      "importedId": "R7D6CIInT6_E1jjNywrqkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126383,
      "firstName": "Charu",
      "lastName": "Mehra",
      "middleInitial": "",
      "importedId": "dECyk-rMmEgL_Np9e1LogQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126384,
      "firstName": "Peggy",
      "lastName": "Chi",
      "middleInitial": "",
      "importedId": "cibNVEBNbpclSQIzN8YrLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126385,
      "firstName": "Vivek",
      "lastName": "Nair",
      "middleInitial": "C",
      "importedId": "49_43B-CYIobOcKXE3DbVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126386,
      "firstName": "Kaitlyn",
      "lastName": "Beiler",
      "middleInitial": "",
      "importedId": "CTXYsv8i15z3Urm8cyqjYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126387,
      "firstName": "Hui",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "LGxgMYv7TRTuxqOm8dc_pw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126388,
      "firstName": "Yuqi",
      "lastName": "Yao",
      "middleInitial": "",
      "importedId": "R52g1bmO_yQxNF1Cv4TDIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126389,
      "firstName": "Zheng",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "bSbqVIDZ5T8bdWIRHGPy4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126393,
      "firstName": "Meredith",
      "lastName": "Morris",
      "middleInitial": "Ringel",
      "importedId": "5yIvWdXUvJUOaHe6nlRO7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126396,
      "firstName": "Yukun",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "wd6uKOk6-G7i7upP1eZr5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126398,
      "firstName": "Liwenhan",
      "lastName": "Xie",
      "middleInitial": "",
      "importedId": "NlG2YsNb5nvOv-ZWWyjNig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126399,
      "firstName": "Rebecca",
      "lastName": "Krosnick",
      "middleInitial": "",
      "importedId": "Knmjx1i42fye9HjoOLXXwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126400,
      "firstName": "Michael",
      "lastName": "Glueck",
      "middleInitial": "",
      "importedId": "-ppsuq2iT2ZebiGO940nrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126402,
      "firstName": "Ching-Yi",
      "lastName": "Tsai",
      "middleInitial": "",
      "importedId": "MOBgRF-g5hWFk7L_Iyn8Lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126403,
      "firstName": "Jianwei",
      "lastName": "Zheng",
      "middleInitial": "",
      "importedId": "9zWl8tMWPKJGfHhzeUWQ9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126404,
      "firstName": "Shwetha",
      "lastName": "Rajaram",
      "middleInitial": "",
      "importedId": "2OiZycHFv3YV4Uj-8Lqnrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126406,
      "firstName": "Conrad",
      "lastName": "Lempert",
      "middleInitial": "",
      "importedId": "R_LGhNf_v2hROWTcym2ojQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126407,
      "firstName": "Alex",
      "lastName": "B√§uerle",
      "middleInitial": "",
      "importedId": "-NhmFQPaf8N_DvhULs3bXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126408,
      "firstName": "Franziska",
      "lastName": "Roesner",
      "middleInitial": "",
      "importedId": "vfICWL9AiwTlMbRjHPG-bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126414,
      "firstName": "Amy",
      "lastName": "Pavel",
      "middleInitial": "",
      "importedId": "mWHY4paujYm0BGpPaankqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126415,
      "firstName": "Anastasis",
      "lastName": "Germanidis",
      "middleInitial": "",
      "importedId": "UVYbDHZmSbR6dupFBdoZLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126418,
      "firstName": "Lukas",
      "lastName": "Rambold",
      "middleInitial": "",
      "importedId": "dUJpoPPwNPXuFzChIzM7Aw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126419,
      "firstName": "Qixuan",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "GyaoGaJv6kyKxTGK67QZoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126420,
      "firstName": "Chao-Hsien",
      "lastName": "Ting",
      "middleInitial": "",
      "importedId": "9li2X1Sm7ELnnD6mjgopnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126421,
      "firstName": "Yun",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "iQR3kHR7ylO_mWSQf9bguw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126422,
      "firstName": "Matthew",
      "lastName": "Conlen",
      "middleInitial": "",
      "importedId": "ReBIMcd_OmL0iYpGEY6FTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126423,
      "firstName": "Muhammad",
      "lastName": "Abdullah",
      "middleInitial": "",
      "importedId": "uMJEArnyBaOHD0nBraCwfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126424,
      "firstName": "Yapeng",
      "lastName": "Tian",
      "middleInitial": "",
      "importedId": "B2UOMdZkw1tzkmNfohfXaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126427,
      "firstName": "Christopher",
      "lastName": "Pondoc",
      "middleInitial": "",
      "importedId": "PL-mXJR3zmsERtzkVtxzkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126428,
      "firstName": "Yan",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "nMaLQpu3pR6H8kYCJ1NR1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126430,
      "firstName": "Lifeng",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "_O3RnZit0TE7ApoSRvl8KA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126431,
      "firstName": "Cindy Hsin-Liu",
      "lastName": "Kao",
      "middleInitial": "",
      "importedId": "0MMz0gYWT7ZslB5DbhBWLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126433,
      "firstName": "Jessica",
      "lastName": "Hammer",
      "middleInitial": "",
      "importedId": "g-v6YNuEt4agYM8tA8qa2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126434,
      "firstName": "Mehrad",
      "lastName": "Faridan",
      "middleInitial": "",
      "importedId": "rGXs39D1nd_7hWuZn0WkJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126435,
      "firstName": "Jeremy",
      "lastName": "Warner",
      "middleInitial": "",
      "importedId": "oNrJkTdw-iAt5kv2fVK-3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126437,
      "firstName": "Brandon",
      "lastName": "Woodard",
      "middleInitial": "J",
      "importedId": "sqTBK-u4Et6MuGnhdKw-6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126439,
      "firstName": "Juho",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "20MXUK-HxVwenJX_VlbU8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126443,
      "firstName": "Craig",
      "lastName": "Shultz",
      "middleInitial": "",
      "importedId": "RhoQccaq8WOVcarf5yB4Lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126444,
      "firstName": "Bing-Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "-lrGWSm4EFKCsnZiKuIu3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126445,
      "firstName": "Eve",
      "lastName": "Hoggan",
      "middleInitial": "",
      "importedId": "rpxJ_Bg4EWuFi5wJqCMK1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126446,
      "firstName": "Zhengzhe",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "pnenrv-7MnVSj52a1Ksn8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126447,
      "firstName": "Yiwen",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "KhwBRtf4xPkFug2LyfmQ-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126449,
      "firstName": "Ian Iong",
      "lastName": "Lam",
      "middleInitial": "",
      "importedId": "gFO8AepqtOuH-Gbvv_oAaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126451,
      "firstName": "Jonathan",
      "lastName": "Bragg",
      "middleInitial": "",
      "importedId": "5juTx1AoMViy24GcPbDLsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126452,
      "firstName": "Haipeng",
      "lastName": "Mi",
      "middleInitial": "",
      "importedId": "BDw8B7BArYd3guR0WQqY0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126453,
      "firstName": "Joseph Chee",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "zuS02m6iwKI6qMyzZ93-Sg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126454,
      "firstName": "Shupei",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "xgacppVrVEP2XyrAeRzC-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126455,
      "firstName": "Dayoung",
      "lastName": "Jeong",
      "middleInitial": "",
      "importedId": "eheyKnFbbsYTpN8_LmFchA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126457,
      "firstName": "Amirhossein",
      "lastName": "H. Memar",
      "middleInitial": "",
      "importedId": "RHQNH32B875JdxVtqGFeGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126458,
      "firstName": "Ryan",
      "lastName": "Rossi",
      "middleInitial": "",
      "importedId": "2Y4JZCP0NlLH4-1OrbIqTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126459,
      "firstName": "Yixiao",
      "lastName": "Kang",
      "middleInitial": "",
      "importedId": "auC7vW2pWnwqU1KkoLzMcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126460,
      "firstName": "Yihong",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "8q47RrdlEpATt8XvPaIH3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126461,
      "firstName": "Jiajia",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "4m1AdDemtvQBjStVnw5ivg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126463,
      "firstName": "Vishal",
      "lastName": "Devireddy",
      "middleInitial": "",
      "importedId": "ETXGaryeyfpcjA_6sQZjkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126465,
      "firstName": "Beza",
      "lastName": "Desta",
      "middleInitial": "",
      "importedId": "OwF2Z5mtR3_LeTVwJu6V6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126466,
      "firstName": "Ludwig",
      "lastName": "Wall",
      "middleInitial": "Wilhelm",
      "importedId": "QWxrOXfbA7rWP6u9datXpw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126467,
      "firstName": "Jie",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "5pVE-rE4Z3zaqAEYBiKFAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126470,
      "firstName": "Tianyu",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "YU-ZtPbMYOviJULKrWbRBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126473,
      "firstName": "Antonio",
      "lastName": "Kr√ºger",
      "middleInitial": "",
      "importedId": "7BVGaBoskVivG5iBzHVi8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126474,
      "firstName": "Jamison",
      "lastName": "O'Keefe",
      "middleInitial": "John",
      "importedId": "YKeJrgsLGCt60kw29p8mxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126475,
      "firstName": "Jason",
      "lastName": "Moss",
      "middleInitial": "",
      "importedId": "U05ED2FkEigvPywRhVYiaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126477,
      "firstName": "Andrew",
      "lastName": "Head",
      "middleInitial": "",
      "importedId": "Vu2IqtRmC5v8JnkUlR3MjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126478,
      "firstName": "Aravind",
      "lastName": "Sagar",
      "middleInitial": "",
      "importedId": "aW3yIAyi7Cll1YW8RK5DrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126479,
      "firstName": "Zheng",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "xVBSzYnPJoh4PeNqkSURRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126482,
      "firstName": "Mani",
      "lastName": "Srivastava",
      "middleInitial": "",
      "importedId": "57HGlH_-oIx8dLzqz8tDmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126483,
      "firstName": "David Chuan-En",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "R4BeOlIwrD06tJ5OqLO0pA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126484,
      "firstName": "Helena",
      "lastName": "Lendowski",
      "middleInitial": "",
      "importedId": "QZ2DOqgDDSClbogldDDiUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126485,
      "firstName": "Anton",
      "lastName": "Wittig",
      "middleInitial": "",
      "importedId": "G-eq8kmi2ae58efEC1WtLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126486,
      "firstName": "Yining",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "PBKVo85S1WyQdiQOyFubKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126487,
      "firstName": "Mohamed",
      "lastName": "Kari",
      "middleInitial": "",
      "importedId": "Lrml1MyYjhpG-MTWVzgutQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126488,
      "firstName": "Jingyi",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "Z261FiwxZTVz9kmen0433w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126489,
      "firstName": "Raul",
      "lastName": "Sanchez-Reillo",
      "middleInitial": "",
      "importedId": "9taTNLkHoPPhoalTkRnGSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126491,
      "firstName": "Reinhard",
      "lastName": "Sch√ºtte",
      "middleInitial": "",
      "importedId": "OuapyOsflH_pUywhlBkgKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126493,
      "firstName": "Raj",
      "lastName": "Sodhi",
      "middleInitial": "",
      "importedId": "BgFQbW15rOo8wet41p4KSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126496,
      "firstName": "Ana",
      "lastName": "Garcia-Alonzo",
      "middleInitial": "P",
      "importedId": "c1PT0kyY0SSJwAN4L9A7jA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126498,
      "firstName": "Takumi",
      "lastName": "Ito",
      "middleInitial": "",
      "importedId": "BCMkzwOGOdtemvsaDOpq2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126499,
      "firstName": "Chiao",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "hkcGY5zjY8GDiuTTznLGBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126500,
      "firstName": "Yuan",
      "lastName": "Zeng",
      "middleInitial": "",
      "importedId": "UDuiMEYZK9b6RF3dk98bvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126501,
      "firstName": "Zhuo",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "LQe58EQWCjitvJ-xcLqt0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126502,
      "firstName": "Michel",
      "lastName": "Beaudouin-Lafon",
      "middleInitial": "",
      "importedId": "1ZHg1-epEdLXfFdVuqIaFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126503,
      "firstName": "Huadong",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "RdGflDPtLMoL9qKpvJXwRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126504,
      "firstName": "Paul",
      "lastName": "Streli",
      "middleInitial": "",
      "importedId": "iBBibiAD0taDgXYe3eG7Yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126505,
      "firstName": "Zisu",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "d0GUbZZwyM1aiK1DuEVYMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126506,
      "firstName": "Nadir",
      "lastName": "Weibel",
      "middleInitial": "",
      "importedId": "7z7qN_lZWxtdWHMuQ74weg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126509,
      "firstName": "Qiuyu",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "vm0q1Ia3g1yeAsGcaAdGxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126511,
      "firstName": "Sam",
      "lastName": "Bourgault",
      "middleInitial": "",
      "importedId": "5PCX6POWN5ddvLTD0W5abg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126513,
      "firstName": "Vivian",
      "lastName": "Chan",
      "middleInitial": "Hsinyueh",
      "importedId": "ilPCiWR_rNvPu0mRXYO_iw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126515,
      "firstName": "Christina",
      "lastName": "Simon",
      "middleInitial": "",
      "importedId": "9RSCtlbwDl6VrmQNFib26A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126516,
      "firstName": "Huamin",
      "lastName": "Qu",
      "middleInitial": "",
      "importedId": "oB-TfawP7djih5JUiWOXYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126518,
      "firstName": "Hehai",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "Lr1IWnRx4IyqjfqrtCICGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126519,
      "firstName": "Bryan",
      "lastName": "Min",
      "middleInitial": "",
      "importedId": "weZmMQkc5LxM_2ae9KnZTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126520,
      "firstName": "George",
      "lastName": "Fitzmaurice",
      "middleInitial": "",
      "importedId": "-i3KSjyZc-FL8pl5LcUMUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126522,
      "firstName": "Lung-Pan",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "eA4F65PMG-ZnFOFgV9JhsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126523,
      "firstName": "Jasmine",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "7zOfpMLFbNPM2fpIVwBnYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126525,
      "firstName": "Qiushi",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "qOFQrBymOEj5PIOrlG3bsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126527,
      "firstName": "Faraz",
      "lastName": "Faruqi",
      "middleInitial": "",
      "importedId": "smhcmuVPPGNqCG1o_uJ7dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126528,
      "firstName": "Jeffrey",
      "lastName": "Nichols",
      "middleInitial": "",
      "importedId": "xZQ-uByUZrRcJyhjqu8vfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126530,
      "firstName": "Chi",
      "lastName": "Hsia",
      "middleInitial": "",
      "importedId": "MNSlyERprMaTTmomLBNLpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126531,
      "firstName": "Steven",
      "lastName": "Dow",
      "middleInitial": "P.",
      "importedId": "8QHJbjDXFqKcG-dlzKVHwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126532,
      "firstName": "Pavel",
      "lastName": "Panchekha",
      "middleInitial": "",
      "importedId": "CDYhPGOJ-lTajBZ9PhFucw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126534,
      "firstName": "Jack",
      "lastName": "Jamieson",
      "middleInitial": "",
      "importedId": "tIvtHuZaYKO8ioxTnOwY2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126536,
      "firstName": "Amy",
      "lastName": "Zhang",
      "middleInitial": "X.",
      "importedId": "sA9xow8RflRH-dt1aYvZCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126537,
      "firstName": "Yuran",
      "lastName": "Ding",
      "middleInitial": "",
      "importedId": "Y2yRVKBzwb_llQ8R-ZHF2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126538,
      "firstName": "Weihao",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "RoLzoJqkMu9VTriOtCUc8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126540,
      "firstName": "Takuya",
      "lastName": "Yoshioka",
      "middleInitial": "",
      "importedId": "8J9r6_-HU-mMzJA3Ti7VkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126542,
      "firstName": "Jie",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "GXhelgwKXf7FCoJRItp-Mw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126545,
      "firstName": "Fraser",
      "lastName": "Anderson",
      "middleInitial": "",
      "importedId": "e9bhROme_xSCNu9BWxoUlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126546,
      "firstName": "Tony",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "LMRsSMIxqyzEFRqGcgEhMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126547,
      "firstName": "Yunyi",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "9lMZRMQRW9AOL2wSso2vbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126549,
      "firstName": "Frederik",
      "lastName": "Brudy",
      "middleInitial": "",
      "importedId": "OTF56JhFSF2TcE1z8ptV6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126550,
      "firstName": "Amira",
      "lastName": "Abdel Rahman",
      "middleInitial": "",
      "importedId": "jEqKcePllr74a0vmJu_Viw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126551,
      "firstName": "Kentaro",
      "lastName": "Yasu",
      "middleInitial": "",
      "importedId": "A7K1mK1GUhouzVCC5QCyRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126552,
      "firstName": "Woohun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "SJfYp_T2Hm0qxpZzutq2RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126553,
      "firstName": "Nora",
      "lastName": "Willett",
      "middleInitial": "S",
      "importedId": "FNm0DlOWUklLZKTPV2sBAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126556,
      "firstName": "Justin",
      "lastName": "Chan",
      "middleInitial": "",
      "importedId": "GX-qGY9CfzGz1dDFo2Mr5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126557,
      "firstName": "Arjun",
      "lastName": "Srinivasan",
      "middleInitial": "",
      "importedId": "MV-WjXqmu0Dqby-NNyRm3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126558,
      "firstName": "Ruijie",
      "lastName": "Geng",
      "middleInitial": "",
      "importedId": "UdAr_3HR05VhJJZgARgZTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126559,
      "firstName": "Brian",
      "lastName": "Smith",
      "middleInitial": "A.",
      "importedId": "DUgfJ2LEtD6TOu45EPbBYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126560,
      "firstName": "Jane",
      "lastName": "E",
      "middleInitial": "L",
      "importedId": "pexOf3_TPLCTA3pprxU4tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126561,
      "firstName": "Lap-Fai",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "Gcy8_8hfJ9OcqbzeMOPTFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126562,
      "firstName": "Ruidong",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "4gOWCTyy2zZpqRblr9GeDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126564,
      "firstName": "Scott",
      "lastName": "Hutchins",
      "middleInitial": "",
      "importedId": "Cl6g_opfQrR7e78H1Jm2aQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126565,
      "firstName": "Ruei-Che",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "fj-AI7DGbP0P11WOCt3b2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126569,
      "firstName": "Xin Yi Therese",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "bkQ_7UvgR0jq7cQ4V_7zdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126570,
      "firstName": "Raul",
      "lastName": "Garcia-Martin",
      "middleInitial": "",
      "importedId": "7OgoJJ9BNCG2SfSN_H026g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126571,
      "firstName": "Mark",
      "lastName": "Parent",
      "middleInitial": "",
      "importedId": "SUZ3dF9XkLkwGwPNXC1vMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126572,
      "firstName": "Ali",
      "lastName": "Zaidi",
      "middleInitial": "",
      "importedId": "vZKSV3d8CA8_s4HpLtiNFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126574,
      "firstName": "Ayumi",
      "lastName": "Ichikawa",
      "middleInitial": "",
      "importedId": "wYJYCY2C04uds3jRw43HqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126577,
      "firstName": "Melisa",
      "lastName": "Orta Martinez",
      "middleInitial": "",
      "importedId": "M8NABPvyJi83qxpVSDAuxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126583,
      "firstName": "Wenyao",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "8-1D0UIsGv_Az-LwrlIPKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126584,
      "firstName": "Bandhav",
      "lastName": "Veluri",
      "middleInitial": "",
      "importedId": "5-UO2u9KCNYV_s-hSinWkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126585,
      "firstName": "Scott",
      "lastName": "Hudson",
      "middleInitial": "E",
      "importedId": "7vKKoUAiJVNW0XZVoOd-TA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126588,
      "firstName": "Johannes",
      "lastName": "Schickling",
      "middleInitial": "",
      "importedId": "sEAbtZIyVHPspEqTAO8cLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126589,
      "firstName": "Thomas",
      "lastName": "Krolikowski",
      "middleInitial": "",
      "importedId": "NWPqKzjeT3DnZPcK4rlyDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126590,
      "firstName": "Minyi",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "4M0OoMToGxL_EolkVtMFAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126592,
      "firstName": "Tingyu",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "OW6mkxUkQ8ja21RxkLaW3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126594,
      "firstName": "Huiying",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "BMnvVuaGd3x4_MwHU1j9bw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126596,
      "firstName": "Joon Hyub",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "0oozWOxlyLLPrL4QxAFHxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126597,
      "firstName": "Thomas",
      "lastName": "LaToza",
      "middleInitial": "D.",
      "importedId": "_TWvEuZyPGvshMkF7WU7YQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126599,
      "firstName": "Liu",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "bRad-84udsgJeDwXSyOnsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126600,
      "firstName": "Clemens",
      "lastName": "Klokmose",
      "middleInitial": "Nylandsted",
      "importedId": "LDrt-dZ778vMa1oCGUhNhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126601,
      "firstName": "Daniel",
      "lastName": "Jackson",
      "middleInitial": "",
      "importedId": "RrvG2qHJZNeWuoYwUwhQDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126602,
      "firstName": "Amanda",
      "lastName": "Swearngin",
      "middleInitial": "",
      "importedId": "nYm9ZkLitPPwR9pP3NJroQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126603,
      "firstName": "Sang-Hyun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "pkmAxwavD-M_2Z9lxx3BRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126604,
      "firstName": "Eric",
      "lastName": "Rawn",
      "middleInitial": "",
      "importedId": "3TISutXYXZuCEA5KPma_4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126605,
      "firstName": "Hyunju",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "_lJEzf8OnOwU2xh8z_y1jw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126606,
      "firstName": "Brett",
      "lastName": "Saiki",
      "middleInitial": "",
      "importedId": "w-V82-plFSOVPC8-jPqQIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126607,
      "firstName": "Chuhan",
      "lastName": "Jiao",
      "middleInitial": "",
      "importedId": "fqEZ171TaTBDM4z-OvoqWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126609,
      "firstName": "Joon Sung",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "F1Jsyz-cSSkB-vvuBl8udg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126610,
      "firstName": "Pattie",
      "lastName": "Maes",
      "middleInitial": "",
      "importedId": "Apx00vbso3FewB_L4nWE_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126612,
      "firstName": "Rakesh",
      "lastName": "Patibanda",
      "middleInitial": "",
      "importedId": "xIfPgx5wQMXkmffqHxrMwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126614,
      "firstName": "Matthew",
      "lastName": "Beaudouin-Lafon",
      "middleInitial": "T",
      "importedId": "ebdyNZQJ-lQ6ZLX6zZkeFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126615,
      "firstName": "Nadya",
      "lastName": "Peek",
      "middleInitial": "",
      "importedId": "bCuJMThz4u2trsJEGdqTyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126616,
      "firstName": "Kevin",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "mgzmdTr9Lw44sz5PNrsLwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126617,
      "firstName": "Chen",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "sM7hymoFmg3nlkiQGfCugA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126618,
      "firstName": "Yichen",
      "lastName": "Xiang",
      "middleInitial": "",
      "importedId": "4Qa6rzFOcXtgYJbnNL1ymQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126620,
      "firstName": "James",
      "lastName": "McCann",
      "middleInitial": "",
      "importedId": "1pHDvh9aniGzRDtAYpOENw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126622,
      "firstName": "Sean",
      "lastName": "Liu",
      "middleInitial": "J.",
      "importedId": "SAnkmAQP_mCEU7Iq15O5cQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126624,
      "firstName": "Nianding",
      "lastName": "Ye",
      "middleInitial": "",
      "importedId": "cqZFn7MrdUtkPepbjY8MiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126625,
      "firstName": "Yuichi",
      "lastName": "Hiroi",
      "middleInitial": "",
      "importedId": "vNuqKst4mZKU8N_lQ1s5VA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126627,
      "firstName": "Matt",
      "lastName": "Latzke",
      "middleInitial": "",
      "importedId": "MsrhsuErN7wR4MU5kLx_zg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126628,
      "firstName": "Lawrence",
      "lastName": "Kim",
      "middleInitial": "H",
      "importedId": "000E7gUUvk4VT3DcPUnwug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126629,
      "firstName": "Borui",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "d7tTGGTv6ez4hV4OWGbN0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126630,
      "firstName": "Cedric",
      "lastName": "Honnet",
      "middleInitial": "",
      "importedId": "_REyw2ml7u4Wn8gl5W6u7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126631,
      "firstName": "Jeffrey",
      "lastName": "Bigham",
      "middleInitial": "P",
      "importedId": "8nDgzwamTG7TuXrNxpBBuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126632,
      "firstName": "Jesse",
      "lastName": "Gonzalez",
      "middleInitial": "T",
      "importedId": "8Nm5Qqbz7_muJwI4QIjFBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126634,
      "firstName": "Malek",
      "lastName": "Itani",
      "middleInitial": "",
      "importedId": "qwr4Z5iXclVp23PVS0q80A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126635,
      "firstName": "Noor",
      "lastName": "Hammad",
      "middleInitial": "",
      "importedId": "0IWiVTVwmiFZ2EW1GQf-wA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126638,
      "firstName": "Srishti",
      "lastName": "Palani",
      "middleInitial": "",
      "importedId": "l2VZObNbp4VGFOy2ibMNrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126639,
      "firstName": "Rishabh",
      "lastName": "Dabral",
      "middleInitial": "",
      "importedId": "xyS03bxX7BWKLf6LsMsA1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126640,
      "firstName": "Marion",
      "lastName": "Koelle",
      "middleInitial": "",
      "importedId": "kEXDR06q99XOOFE0vFpI8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126642,
      "firstName": "Xiaohua",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "jjiroOrUswWmt8g3AKnUwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126939,
      "firstName": "Yu",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "Fx1CQb-6LfsJEnBZRZnXFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126940,
      "firstName": "Emily",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "PHikwgXcldu_AnpE-yWkEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126941,
      "firstName": "Catherine Grevet",
      "lastName": "Delcourt",
      "middleInitial": "",
      "importedId": "yfljwTV1oeY_omSXkxqH_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126942,
      "firstName": "Rui",
      "lastName": "Sheng",
      "middleInitial": "",
      "importedId": "gvLXV8KxHMdtkhxB0YNxfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126943,
      "firstName": "Jiajing",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "UQHfdG8AR2kyztbUvjG0eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126944,
      "firstName": "Yonghao",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "08sHRsWjuUzj1EqQi_wrXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126945,
      "firstName": "ZhiXin",
      "lastName": "Jin",
      "middleInitial": "",
      "importedId": "FSoidoyaaCZgUEphgu-1UA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126946,
      "firstName": "Fangzheng",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "POqo747LgR9VBCo3iSdRyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126947,
      "firstName": "Sebastian",
      "lastName": "Gutierrez",
      "middleInitial": "",
      "importedId": "3W_mypcvyaCCS8nSeC7UmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126948,
      "firstName": "Megan",
      "lastName": "Hillis",
      "middleInitial": "E.",
      "importedId": "eWUWiZpzUTBMJzurPZT9zA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126949,
      "firstName": "Xu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "Y8ze6TZwMCxBhE58h8eHiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126950,
      "firstName": "Guanhong",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "TQB1eu2hLzvKoAJMZ6mY4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126951,
      "firstName": "Elahe",
      "lastName": "Soltanaghai",
      "middleInitial": "",
      "importedId": "wNnA7jrohjwTiuO1tEXVCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126952,
      "firstName": "Jun",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "7Gx-0rTm2_Wvt0dv2pa_oA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126953,
      "firstName": "Angelina",
      "lastName": "Zheng",
      "middleInitial": "J",
      "importedId": "PwES2RG8bvzqw_hdH0VZxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126954,
      "firstName": "Ruofei",
      "lastName": "Du",
      "middleInitial": "",
      "importedId": "R7ZrSPERot4-ssDFmZn9EQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126955,
      "firstName": "Shm",
      "lastName": "Almeda",
      "middleInitial": "Garanganao",
      "importedId": "xR2O1o1shoxNo6TSOBRLHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126956,
      "firstName": "Yuehe",
      "lastName": "Mao",
      "middleInitial": "",
      "importedId": "mcb_ffhGJohmKacjKTodMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126957,
      "firstName": "Shwetak",
      "lastName": "Patel",
      "middleInitial": "",
      "importedId": "IVnjxjYkN3trolk1IiWHlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126958,
      "firstName": "Anandghan",
      "lastName": "Waghmare",
      "middleInitial": "",
      "importedId": "T9RZYsEDPpgumkkV8i7ukQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126959,
      "firstName": "Joe",
      "lastName": "Paradiso",
      "middleInitial": "",
      "importedId": "mPgn9zhAqjthCQ5b5psNcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126960,
      "firstName": "Ruoyu",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "muYeOiemSY2lXg4yFSHJuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126961,
      "firstName": "Kristen",
      "lastName": "Wright",
      "middleInitial": "",
      "importedId": "O7mq4vXYhyBgQ8wtTQcFUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126962,
      "firstName": "Chien-Sheng",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "OIDMmvh5bDfl6_BfZi3WEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126963,
      "firstName": "Eujean",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "vZqpWsCUC4wuLr0MMFhLcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126964,
      "firstName": "Wenhao",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "oT3V3OnCu7JgVP49z6aMIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126965,
      "firstName": "Tobias",
      "lastName": "H√∂llerer",
      "middleInitial": "",
      "importedId": "SGQoxZb6MR9rznSB4d8OGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126966,
      "firstName": "Chih-Heng",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "ch0Io4g9sMGWRqdigEECXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126967,
      "firstName": "Sebastian",
      "lastName": "Rodriguez",
      "middleInitial": "S.",
      "importedId": "FrL_WEu6wLH-Z11C6ZXKhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126968,
      "firstName": "JangHyeon",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "AcrMsvy3TJl88Ut4dr2OFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126969,
      "firstName": "Samira",
      "lastName": "Pulatova",
      "middleInitial": "",
      "importedId": "PFlksBbY1sQzqSnp8BoRjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126970,
      "firstName": "Adarsh",
      "lastName": "Kowdle",
      "middleInitial": "",
      "importedId": "ZQx1xiwZldfgjvyCoQDNzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126971,
      "firstName": "Jason",
      "lastName": "Mayes",
      "middleInitial": "",
      "importedId": "3fjWHPovmLU_ghSQNw5_6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126972,
      "firstName": "Dimitri",
      "lastName": "Kanevsky",
      "middleInitial": "",
      "importedId": "KkWOQtfu0ehJ_KnMK10sEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126973,
      "firstName": "Kenta",
      "lastName": "Takagi",
      "middleInitial": "",
      "importedId": "DnCp34Lk7-rYgm0FqAVaww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126974,
      "firstName": "Mark",
      "lastName": "Sherwood",
      "middleInitial": "",
      "importedId": "tlb3_LHyhOJaLztrrtIdxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126975,
      "firstName": "Yan",
      "lastName": "Luo",
      "middleInitial": "",
      "importedId": "fzqDtYCBqOFs0VC0BPuB1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126976,
      "firstName": "Chia-Sheng",
      "lastName": "Hung",
      "middleInitial": "",
      "importedId": "J5DDlSuIex49ojRjXY5nSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126977,
      "firstName": "Xingyu",
      "lastName": "Liu",
      "middleInitial": "Bruce",
      "importedId": "BB-GhbxpYiO9H3ClxDe2YQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126978,
      "firstName": "Chenyang",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "hgKABqbLbuSrerR7uoGjzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126979,
      "firstName": "Leni",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "DXEUdTImwUYvpX37iWLbTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126980,
      "firstName": "Yair",
      "lastName": "Herbst",
      "middleInitial": "",
      "importedId": "5blZLNai6dbSrif_7qNTPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126981,
      "firstName": "Devin",
      "lastName": "Balkcom",
      "middleInitial": "",
      "importedId": "eNDj-fxEbQWKwS52lGZbZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126982,
      "firstName": "Lidiya",
      "lastName": "Murakhovs'ka",
      "middleInitial": "",
      "importedId": "ZDkeDzXhWUdBQiXdEgrWRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126983,
      "firstName": "Koji",
      "lastName": "Tsukada",
      "middleInitial": "",
      "importedId": "Mgenz4tho21hklGWVHrDEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126984,
      "firstName": "Andrew",
      "lastName": "Benton",
      "middleInitial": "",
      "importedId": "9ih_2YgKTBG8wQi2ZxwEqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126985,
      "firstName": "Peitong",
      "lastName": "Duan",
      "middleInitial": "",
      "importedId": "DctRVnS9-nqMKIFkxtBMqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126986,
      "firstName": "Erkin",
      "lastName": "Seker",
      "middleInitial": "",
      "importedId": "le-W_hSaUJfw_MeWdfDZdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126987,
      "firstName": "Frederik",
      "lastName": "Kjaer Soerensen",
      "middleInitial": "",
      "importedId": "kSYStNmHFvzsTc5IWcVoRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126988,
      "firstName": "Beituo",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "Cp0UwJe5I4828fLH2MxzTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126989,
      "firstName": "Krystle",
      "lastName": "Reagan",
      "middleInitial": "",
      "importedId": "RBkueyO2GlWphE0NjDFdvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126990,
      "firstName": "Da-Yuan",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "-zgqCtZmJO1iIX2rOe8FkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126991,
      "firstName": "Harrison",
      "lastName": "Goldstein",
      "middleInitial": "",
      "importedId": "RBcYzx0_YRqfOMRvaYzVUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126992,
      "firstName": "Jaewook",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "Ko2N9T8zAz-zPPa8nd9IBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126993,
      "firstName": "Li",
      "lastName": "Feng",
      "middleInitial": "",
      "importedId": "PMjwRWlfg9CFSacFN8sNNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126994,
      "firstName": "Eric",
      "lastName": "Shaffer",
      "middleInitial": "",
      "importedId": "Q9SckOIMZIfd1Nea72hqnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126995,
      "firstName": "David",
      "lastName": "Kraemer",
      "middleInitial": "J. M.",
      "importedId": "1snoe7AulcyuwRtTEeeHJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126996,
      "firstName": "Gage",
      "lastName": "Birchmeier",
      "middleInitial": "",
      "importedId": "grbwTAshGTkK1FGX2WfSCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126997,
      "firstName": "Hemanth",
      "lastName": "Kamana",
      "middleInitial": "",
      "importedId": "DCQx3PhOpyd3AWKpdaZyDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126998,
      "firstName": "Yosuke",
      "lastName": "Nakagawa",
      "middleInitial": "",
      "importedId": "5Df3zEcaDWB4TpECw14S-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 126999,
      "firstName": "Manaal",
      "lastName": "Mohammed",
      "middleInitial": "",
      "importedId": "Y7iGwLlHoNCOGa1vcB5jeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127000,
      "firstName": "Benjamin",
      "lastName": "Pierce",
      "middleInitial": "C.",
      "importedId": "iHNpREy5aNPpYY9fI9nn1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127001,
      "firstName": "Adrian",
      "lastName": "Rodriguez",
      "middleInitial": "",
      "importedId": "RzeCB4jS5u9h6FZSRexRvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127002,
      "firstName": "Devesh",
      "lastName": "Sarda",
      "middleInitial": "P",
      "importedId": "X0Jj6ja1_87Cht2u2Pw7mw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127003,
      "firstName": "Mathieu",
      "lastName": "Parvaix",
      "middleInitial": "",
      "importedId": "2xmQxmI36N4fkcHFjjtqLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127004,
      "firstName": "Liam",
      "lastName": "Chu",
      "middleInitial": "",
      "importedId": "c37LC_VY4IgK3IohCEecAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127005,
      "firstName": "Shuyi",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "ObYpTimAU_7UAofjnvz5jQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127006,
      "firstName": "Grace",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "MST-BQq5VMMwuGHLm33GpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127007,
      "firstName": "Tiansu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "evG39l0wdfxt3L2tEQZ91A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127008,
      "firstName": "Kyungeun",
      "lastName": "Jung",
      "middleInitial": "",
      "importedId": "QWyIWdlmJT23RVZGcWSPrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127009,
      "firstName": "Quan",
      "lastName": "Gu",
      "middleInitial": "",
      "importedId": "he9ElFkyID8X8X5vkenTug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127010,
      "firstName": "Amy",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "n5hkK08NNrf4NAu1wgKsZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127011,
      "firstName": "Alex",
      "lastName": "Olwal",
      "middleInitial": "",
      "importedId": "2JAYtPYSaIWyiAiP0PckPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127012,
      "firstName": "Kyunghwan",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "ttvfnkW7aulxHroeXJcfxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127013,
      "firstName": "Qijia",
      "lastName": "Shao",
      "middleInitial": "",
      "importedId": "FJRBJe-LA8j5H0-B8dls0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127014,
      "firstName": "Magnus",
      "lastName": "Frisk",
      "middleInitial": "",
      "importedId": "l94DG5ONh-_IeviJTWcVUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127015,
      "firstName": "Katia",
      "lastName": "Vega",
      "middleInitial": "",
      "importedId": "nUSPclWZt8TNmJtnExIPYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127016,
      "firstName": "Yao",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "hfyqXWLVehmx4FSo8OoJ_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127017,
      "firstName": "Dhruv",
      "lastName": "Jain",
      "middleInitial": "",
      "importedId": "si0FI_f98MJQTO6HrC85vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127018,
      "firstName": "Caiming",
      "lastName": "Xiong",
      "middleInitial": "",
      "importedId": "-9S3RAb_8yPgxQidYgDPBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127019,
      "firstName": "Haotian",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "pQ9dCF6qF5MDqHBbuMR2iQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127020,
      "firstName": "Xiuxiu",
      "lastName": "Yuan",
      "middleInitial": "",
      "importedId": "VqllyZb76qnHCqv0xAQS_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127021,
      "firstName": "Tong",
      "lastName": "Niu",
      "middleInitial": "",
      "importedId": "5m5eYNwUIJsqcjGqJX_ihw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127022,
      "firstName": "Jon",
      "lastName": "Froehlich",
      "middleInitial": "E.",
      "importedId": "WzZztLsZHV7nWIHWf-9_EA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127023,
      "firstName": "David",
      "lastName": "Gotz",
      "middleInitial": "",
      "importedId": "hN0mgfVeCe9KDSu01cgOJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127024,
      "firstName": "Liu",
      "lastName": "Ren",
      "middleInitial": "",
      "importedId": "aUDpIWTI4-jRw8RAD8sGkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127025,
      "firstName": "William",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "fjTLnVKspZAwf_8wukxqgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127026,
      "firstName": "Rohan",
      "lastName": "Nedungadi",
      "middleInitial": "Russel",
      "importedId": "Tqyp_iwIOgme39zuxs6cyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127027,
      "firstName": "Xia",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "iIlbtCfV0gKh-dAGUthtTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127028,
      "firstName": "Lihi",
      "lastName": "Zelnik-Manor",
      "middleInitial": "",
      "importedId": "CtVQEUgq-hkfP0eFW7fb_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127029,
      "firstName": "Jiexin",
      "lastName": "Ding",
      "middleInitial": "",
      "importedId": "3kDiV7d1h7ujieHzcihXug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127030,
      "firstName": "Ziyang",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "s1Cbu6mnPAw_0YrucrX8vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127031,
      "firstName": "Luca",
      "lastName": "Musk",
      "middleInitial": "",
      "importedId": "r0fZ4cQe67KCOj759rQiOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127032,
      "firstName": "Gabriela",
      "lastName": "Vega",
      "middleInitial": "",
      "importedId": "EzMxGxojNMIGO8HXEjYWmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127033,
      "firstName": "Michelle",
      "lastName": "Carney",
      "middleInitial": "",
      "importedId": "IGrMz_xuxSXqZKzKKQbGmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127034,
      "firstName": "Ana",
      "lastName": "Cardenas Gasca",
      "middleInitial": "",
      "importedId": "LlFV229UG2amnIlB0OjKcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127035,
      "firstName": "Brett",
      "lastName": "Saiki",
      "middleInitial": "",
      "importedId": "xso_29w6-3tv6LYZAybkbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127036,
      "firstName": "Can",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "Dn0yQWWCuIJ1z9SvYRIQ4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127037,
      "firstName": "Timothy",
      "lastName": "Aveni",
      "middleInitial": "J.",
      "importedId": "xEovVqAIvhB6tRnkvzyLqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127038,
      "firstName": "Geehyuk",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "5oayHmDaeFMZJ_onK64caA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127039,
      "firstName": "Sang Ho",
      "lastName": "Yoon",
      "middleInitial": "",
      "importedId": "V1nG1KeCiKE0S_kGh27BVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127040,
      "firstName": "Mads",
      "lastName": "Vejrup",
      "middleInitial": "",
      "importedId": "84B9tUZSr8OOcX2NSsSG4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127041,
      "firstName": "Alon",
      "lastName": "Wolf",
      "middleInitial": "",
      "importedId": "gKD7-anjr6C6sF7XIug4zg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127042,
      "firstName": "Tatsuya",
      "lastName": "Kawasaki",
      "middleInitial": "",
      "importedId": "Ok3k_3q_3NBKF-fbxEVLqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127043,
      "firstName": "Ram",
      "lastName": "Iyengar",
      "middleInitial": "",
      "importedId": "D4hI3YtLJKosaYpbR0QFYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127044,
      "firstName": "Chiong",
      "lastName": "Lai",
      "middleInitial": "",
      "importedId": "A728yLxc3ShOtdmLCXq_UQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127045,
      "firstName": "Jing",
      "lastName": "Jin",
      "middleInitial": "",
      "importedId": "gOckPYu3FfGXNFqAnaDqqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127046,
      "firstName": "Ashley",
      "lastName": "Martin",
      "middleInitial": "",
      "importedId": "1qnkeK7fjjFDqX21vJRF5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127047,
      "firstName": "Zhilan",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "H9ZvKZVERwZ2v2t7VQvckg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127048,
      "firstName": "Ishan",
      "lastName": "Chatterjee",
      "middleInitial": "",
      "importedId": "S8IFANAmyzzQ1FUZgZGddQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127049,
      "firstName": "Yijie",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "mrXDKKRv2c7OmXm628ratw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127050,
      "firstName": "Nan",
      "lastName": "Tian",
      "middleInitial": "",
      "importedId": "dLkm0CbjBslXMy3_GKJ19Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127051,
      "firstName": "Catherine",
      "lastName": "Delcourt",
      "middleInitial": "",
      "importedId": "VEGFA8mTv2qzY27lClLzEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127052,
      "firstName": "Takayoshi",
      "lastName": "Mochizuki",
      "middleInitial": "",
      "importedId": "wr9bQh8YFTLSEAUP3NPGlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127053,
      "firstName": "Na",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "1EJAqN08YRqf7U5cvybATg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127054,
      "firstName": "Haipeng",
      "lastName": "Mi",
      "middleInitial": "",
      "importedId": "xhJhgCfvYh1iyoqbNPme0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127055,
      "firstName": "J.D.",
      "lastName": "Zamfirescu-Pereira",
      "middleInitial": "",
      "importedId": "gUf0O7l2yHeEuT5VttA77w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127056,
      "firstName": "Lin",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "BJyvNekpPADMls3hk_Ekvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127057,
      "firstName": "Jingying",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "0C8YB9YFKDsnhc3IYBZNFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127058,
      "firstName": "Elizabeth",
      "lastName": "Brown",
      "middleInitial": "",
      "importedId": "pYVO2az9eLm8j5i61xFpFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127059,
      "firstName": "Philippe",
      "lastName": "Laban",
      "middleInitial": "",
      "importedId": "C4fWPqc-_A0aKTbvSAGa-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127060,
      "firstName": "Takahiro",
      "lastName": "Kusabuka",
      "middleInitial": "",
      "importedId": "J91DKejqXH_TUVCz4logKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127061,
      "firstName": "Nick",
      "lastName": "Feffer",
      "middleInitial": "",
      "importedId": "PDyv4OCNJ4R4SqLqsO8wnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127062,
      "firstName": "Samuel",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "dtT6WryqRZxURfpbGpGUVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127063,
      "firstName": "Zhengyu",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "1bqyGk54zLYWuoL5iQGVDg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127064,
      "firstName": "Gregory",
      "lastName": "Croisdale",
      "middleInitial": "Thomas",
      "importedId": "ZRvgmWs3HDJJuXfyIQx72A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127065,
      "firstName": "Armando",
      "lastName": "Fox",
      "middleInitial": "",
      "importedId": "BqzGchYK-3dmobTe_iOrkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127066,
      "firstName": "Ayaka",
      "lastName": "Ishii",
      "middleInitial": "",
      "importedId": "893h0ebCAyLrDOeI-aXqNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127067,
      "firstName": "Abena",
      "lastName": "Boadi-Agyemang",
      "middleInitial": "",
      "importedId": "S_Ho8VrarV7xcRpcwRqZ_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127068,
      "firstName": "Jingtao",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "CQiqi6tU1Gier1jlHqHC_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127069,
      "firstName": "Kosei",
      "lastName": "Kamata",
      "middleInitial": "",
      "importedId": "ZP3loXPRxDx20yXxXhxLBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127070,
      "firstName": "Vitaliy",
      "lastName": "Popov",
      "middleInitial": "",
      "importedId": "exp92HgW5RQwaGUQyBSb4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127071,
      "firstName": "Ziheng",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "ZptDVxRKBPpBD9gE6OGYXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127072,
      "firstName": "Qirui",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "AEH6LtM3NK0QSzfNwSIuBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127073,
      "firstName": "Hiroshi",
      "lastName": "Chigira",
      "middleInitial": "",
      "importedId": "hB1dzjgdqjRSTgOT4JxCNw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127074,
      "firstName": "Zhenhan",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "lrWY_V8MlZAyvKEH17YBuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127075,
      "firstName": "Sofia",
      "lastName": "Kobayashi",
      "middleInitial": "",
      "importedId": "r39zxnoE6mLPq0mPiCV5Ag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127076,
      "firstName": "Vladimir",
      "lastName": "Kirilyuk",
      "middleInitial": "",
      "importedId": "nRuxkyNNvWLN6CTHcEzsmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127077,
      "firstName": "Fuling",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "b9LLRFiTKRum9uD4l1Wptg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127078,
      "firstName": "Zhihao",
      "lastName": "Yao",
      "middleInitial": "",
      "importedId": "rk0MtRVSxp1z436zSwI5Ew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127079,
      "firstName": "Ruhan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "voiZA1MsJSHUFTpiJ1oudg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127080,
      "firstName": "Ping",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "7SVrPivYjvbMMdKDq4m5vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127081,
      "firstName": "Christine",
      "lastName": "Bassem",
      "middleInitial": "",
      "importedId": "PTQSF_UjfG8YrSHUhmgbmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127082,
      "firstName": "Artem",
      "lastName": "Dementyev",
      "middleInitial": "",
      "importedId": "nKv9r96T7YxBIeWjQKf00w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127083,
      "firstName": "Jiadi",
      "lastName": "Luo",
      "middleInitial": "",
      "importedId": "ZqDLchjC-xAUWGmnvcUSOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127084,
      "firstName": "Julien",
      "lastName": "Blanchet",
      "middleInitial": "",
      "importedId": "rZWQV0Mieh-fxqP-5MkZMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127085,
      "firstName": "Yeongji",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "2lYXN-yorgulRI2eusD47Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127086,
      "firstName": "Kyle",
      "lastName": "Heinz",
      "middleInitial": "",
      "importedId": "tPEw_bY1kQGRaOQlNDXGpw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127087,
      "firstName": "Jun",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "-fkUNYZLC9usPuKBbwNdZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127159,
      "firstName": "Ahad",
      "lastName": "Rauf",
      "middleInitial": "Mujtaba",
      "importedId": "coEEUKR259q-d5B0KWzGjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127160,
      "firstName": "Ran",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "XjHLZwrjJo6SplEsbuMorQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127161,
      "firstName": "Xiying",
      "lastName": "Bao",
      "middleInitial": "",
      "importedId": "8FzUco1U4yuNPEFGKnqTbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127162,
      "firstName": "Yubo",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "KjQX4-E28cRvOWR8Bc1ULg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127163,
      "firstName": "Jiahao",
      "lastName": "Ren",
      "middleInitial": "",
      "importedId": "6wr4AX2Dys8gI1rQQrNGyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127164,
      "firstName": "Daniel",
      "lastName": "Leithinger",
      "middleInitial": "",
      "importedId": "HY4LdUcyCCB7UUOt-6Mg1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127165,
      "firstName": "Mitchell",
      "lastName": "Gordon",
      "middleInitial": "L.",
      "importedId": "x3-FpFnEHX2sjmfr9v0cZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127166,
      "firstName": "Katherine",
      "lastName": "Song",
      "middleInitial": "W",
      "importedId": "wO87Fk4uuC6EthyAuj2Ibg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127167,
      "firstName": "Yi-Chun",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "MZSxNG3fcZC7Z1VvbuGQmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127168,
      "firstName": "Qinyi",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "Crh6J3CWW28OGo7s5iLTBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127169,
      "firstName": "YUTO",
      "lastName": "NAGAO",
      "middleInitial": "",
      "importedId": "jbvS6UxGQtcsEad7KD4mpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127170,
      "firstName": "Jaidev",
      "lastName": "Shriram",
      "middleInitial": "",
      "importedId": "Wpx7gy3s7Bjp3YGNAtJdbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127171,
      "firstName": "SOICHIRO",
      "lastName": "FUKUDA",
      "middleInitial": "",
      "importedId": "nlTQkD-CwB5h5eDfCx7U6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127172,
      "firstName": "Yumeng",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "3CAwffudEKBrMVV2IDxoSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127173,
      "firstName": "Eric",
      "lastName": "Acome",
      "middleInitial": "",
      "importedId": "WyvhWYIxivbxBIooqtNVhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127174,
      "firstName": "Wei-En",
      "lastName": "Tsai",
      "middleInitial": "",
      "importedId": "nB-7JBVyOFRauMnHqsyx3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127175,
      "firstName": "Toma",
      "lastName": "Itagaki",
      "middleInitial": "",
      "importedId": "UpV3PyYXXFCb8DXftL5YCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127176,
      "firstName": "DaEun",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "fPPayKAccH8mgg4X6Rr8yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127177,
      "firstName": "Jeongeon",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "yWqT8o9HN1Ge5XufQ7AhQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127178,
      "firstName": "Mar",
      "lastName": "Gonzalez-Franco",
      "middleInitial": "",
      "importedId": "WzJiSxt7y46Nu61rYGdhgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127179,
      "firstName": "Yihao",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "hsHCTV9-U58sAI6T7l6qjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127180,
      "firstName": "Saleema",
      "lastName": "Amershi",
      "middleInitial": "",
      "importedId": "aFgdwbLLdg1dw58VHcd6UA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127181,
      "firstName": "Richard",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "WtmeUVDjMhjFYE8kO7us6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127182,
      "firstName": "Lydia",
      "lastName": "Chilton",
      "middleInitial": "B",
      "importedId": "jM77DxTwyp1U8TtuVyYGCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127183,
      "firstName": "Sixuan",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "TO6m_YjFe7-IQHNRXYYxIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127184,
      "firstName": "Sanjayan",
      "lastName": "Pradeep Kumar Sreekala",
      "middleInitial": "",
      "importedId": "TFHD_YVCDHkQLzscQEaAhQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127207,
      "firstName": "Lana",
      "lastName": "Sinapayen",
      "middleInitial": "",
      "importedId": "YKfCJKfKfxmf8QkBkpXa6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127208,
      "firstName": "Lyle",
      "lastName": "Klyne",
      "middleInitial": "",
      "importedId": "uatsQnu8P-DO93BPCs5F5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127209,
      "firstName": "Elena",
      "lastName": "Glassman",
      "middleInitial": "L.",
      "importedId": "QmlJycOy4LH6PdhoUoRyEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127210,
      "firstName": "Robin",
      "lastName": "Zitt",
      "middleInitial": "Moritz",
      "importedId": "0wqK_tLMm9avX3RFR1UtqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127211,
      "firstName": "Kristopher",
      "lastName": "Giesing",
      "middleInitial": "",
      "importedId": "LiSG1QvewxT2VTqz3JCg3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127212,
      "firstName": "Tianyuan",
      "lastName": "Cai",
      "middleInitial": "",
      "importedId": "Zvs8XBxHQMpBBym5TeYNcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127213,
      "firstName": "Joseph",
      "lastName": "DeSimone",
      "middleInitial": "",
      "importedId": "Nu4DHbtAJuimO6XfdI_DeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127214,
      "firstName": "YuKai",
      "lastName": "Hung",
      "middleInitial": "",
      "importedId": "930PM67GfyT4SZLtpafZxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127215,
      "firstName": "Dong-Uk",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "qvZZCCaioLU0JrD85OFdeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127216,
      "firstName": "Yousang",
      "lastName": "Kwon",
      "middleInitial": "",
      "importedId": "dgKfDGRkIEV0YlBbwJG8OQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127217,
      "firstName": "Liwen",
      "lastName": "He",
      "middleInitial": "",
      "importedId": "dk0so2anhmF7dCmQKA4cVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127218,
      "firstName": "Jing-Yuan",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "M4gxA0IGD5QxZWYjm4lNmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127219,
      "firstName": "Justin",
      "lastName": "Williams",
      "middleInitial": "",
      "importedId": "16Cy9mzT3GXd5wb0EpcDeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127220,
      "firstName": "Srihari",
      "lastName": "Sridhar",
      "middleInitial": "",
      "importedId": "tjDbT1NLsEsk45r7qdDdqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127221,
      "firstName": "D. Fox",
      "lastName": "Harrell",
      "middleInitial": "",
      "importedId": "VmmmaQ3ePTPTDKxZksam4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127222,
      "firstName": "Qiming",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "Di5-6w3exgaWRt5sw60v_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127223,
      "firstName": "Alice",
      "lastName": "Zhong",
      "middleInitial": "",
      "importedId": "1gVUxjwYnycGk7C-rCe2eg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127224,
      "firstName": "Yoshinobu",
      "lastName": "Kaji",
      "middleInitial": "",
      "importedId": "waz0V9Ftgwiy96w8F-yAUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127225,
      "firstName": "Joseph",
      "lastName": "Salisbury",
      "middleInitial": "P",
      "importedId": "TgadV8k31-2hBlqV_0bbyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127226,
      "firstName": "Ian",
      "lastName": "Arawjo",
      "middleInitial": "",
      "importedId": "9Cmnmubftugec7sLlAw1QQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127227,
      "firstName": "Kentaro",
      "lastName": "Takemura",
      "middleInitial": "",
      "importedId": "Qo4Fel8PouXYkgjZ3_-djQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127228,
      "firstName": "Mingyi",
      "lastName": "Yuan",
      "middleInitial": "",
      "importedId": "rZrBl-Gbf-1Nl-PJ3FKPOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127229,
      "firstName": "Aarushi",
      "lastName": "Raheja",
      "middleInitial": "",
      "importedId": "7Tj0_6GfMLbVWcc-nubNlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127230,
      "firstName": "Melissa",
      "lastName": "McDonald",
      "middleInitial": "",
      "importedId": "E40edLre1v8UxIn_SDja4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127231,
      "firstName": "Hannes",
      "lastName": "Kraus",
      "middleInitial": "",
      "importedId": "EpBC1apKtEA-jABUPi57cg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127232,
      "firstName": "Yanan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "EHxmrNbFWOouU41TpQfyPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127233,
      "firstName": "Nam Wook",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "2FDLsIvlxIbUJ8rcEVDlhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127234,
      "firstName": "Ari",
      "lastName": "Hautasaari",
      "middleInitial": "",
      "importedId": "P5SqnmVPckUDvnD2CAO1cg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127235,
      "firstName": "Xiang",
      "lastName": "Fei",
      "middleInitial": "",
      "importedId": "qh0tlYpZqRoprzEuUNUa7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127236,
      "firstName": "Po-Yao (Cosmos)",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "8IiexJ5JfppD_orPuI3Y2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127237,
      "firstName": "Ai",
      "lastName": "Sato",
      "middleInitial": "",
      "importedId": "2HnB5yqv8CwHMwmtVaHaIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127238,
      "firstName": "Caglar",
      "lastName": "Yildirim",
      "middleInitial": "",
      "importedId": "tIWEMWirLPsVrj27SL7_Qw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127239,
      "firstName": "Ivan",
      "lastName": "Poupyrev",
      "middleInitial": "",
      "importedId": "fydKEOhTIOlOyZbSK7094w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127240,
      "firstName": "Hiroyuki",
      "lastName": "Saegusa",
      "middleInitial": "",
      "importedId": "kE4R_Fj-0V7Bevpwwm0M_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127241,
      "firstName": "James",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "fyoIt49wqMcReVVOJiMR4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127242,
      "firstName": "Shunta",
      "lastName": "Ito",
      "middleInitial": "",
      "importedId": "RavB4ttnWpTYjQlGU99wrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127243,
      "firstName": "Corey",
      "lastName": "Cochrane",
      "middleInitial": "",
      "importedId": "9hCCBlcjYhg-h1LpaXs0-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127244,
      "firstName": "Daigo",
      "lastName": "Misaki",
      "middleInitial": "",
      "importedId": "GOqHdRIDju9Q5gw5I1LX9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127245,
      "firstName": "Joshua",
      "lastName": "Gorniak",
      "middleInitial": "",
      "importedId": "AkhjT5oZHqbtOL-maQXKQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127246,
      "firstName": "Wenqi",
      "lastName": "Zheng",
      "middleInitial": "",
      "importedId": "WRw-oJI_JcoPoLVFYd6O6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127247,
      "firstName": "Martin",
      "lastName": "Wattenberg",
      "middleInitial": "",
      "importedId": "jpgssnTjnh_z0eSWwjLnYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127248,
      "firstName": "Sulakna",
      "lastName": "Karunaratna",
      "middleInitial": "",
      "importedId": "gFYmPkzutbN1og2dj7lYHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127249,
      "firstName": "Ho Min",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "67OadbQH69sXPUlwo48hgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127250,
      "firstName": "Seonuk",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "3K2ZMs3O0MQUePpwSVXzeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127251,
      "firstName": "Lian",
      "lastName": "Xue",
      "middleInitial": "",
      "importedId": "R34CC32xMGDsOBVw7fsu6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127252,
      "firstName": "Seung Hyeon",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "XA7m1O01eSXDrUNGLv4Jfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127253,
      "firstName": "Amanda",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "5-WVqj7se9UUG6tnddsYuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127254,
      "firstName": "Xiaohang",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "hZ0clmoUOvvE_12D7m_Kdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127255,
      "firstName": "Gabriel",
      "lastName": "Lipkowitz",
      "middleInitial": "",
      "importedId": "hsKOI7-Wl5vZVlVFUuIJkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127256,
      "firstName": "Xiao",
      "lastName": "Ge",
      "middleInitial": "",
      "importedId": "ZlsyKBKuS1JuN6U0tgyQkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127257,
      "firstName": "Yejin",
      "lastName": "Jang",
      "middleInitial": "",
      "importedId": "_ftgAYl1-YNEzVQQLYWFKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127258,
      "firstName": "Joseph",
      "lastName": "Bae",
      "middleInitial": "",
      "importedId": "OerfBVmNoo-0U_eYGnNGhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127259,
      "firstName": "Tomoki",
      "lastName": "Takahashi",
      "middleInitial": "",
      "importedId": "WHpJOZ6YqFLbPM1gzXBmLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127260,
      "firstName": "Juhyeok",
      "lastName": "Yoon",
      "middleInitial": "",
      "importedId": "HeNw9W4nE9v1bAhIHFHD5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127261,
      "firstName": "Jisun",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "RaHaZI9-NK11U89P13oOCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127262,
      "firstName": "Angelo Ryan",
      "lastName": "Soriano",
      "middleInitial": "",
      "importedId": "_XG4cUjY1SjqMmdV-xFG1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127263,
      "firstName": "Ashwini",
      "lastName": "Naik",
      "middleInitial": "",
      "importedId": "x88tpJGMYeCBaixMs-tctA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127264,
      "firstName": "Yurim",
      "lastName": "Son",
      "middleInitial": "",
      "importedId": "gb_S1nnYHW0qni1tGrmNVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127265,
      "firstName": "Chan Hu",
      "lastName": "Wie",
      "middleInitial": "",
      "importedId": "JL_-d6I-q28TSxKvSrHuag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127266,
      "firstName": "Virgil",
      "lastName": "Barnard",
      "middleInitial": "",
      "importedId": "ZbJCset5a-GRThbSkL0gPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127267,
      "firstName": "Sol",
      "lastName": "Lim",
      "middleInitial": "Ie",
      "importedId": "h8rrc_HJCt77z30KkE9ISQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127268,
      "firstName": "Tiffany",
      "lastName": "Knearem",
      "middleInitial": "",
      "importedId": "TIkWBBDxHceM2gsxpucG4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127269,
      "firstName": "Zoya",
      "lastName": "Bylinskii",
      "middleInitial": "",
      "importedId": "SKGl1g9FJ7_HgyLCCDx8zw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127270,
      "firstName": "Yanan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "E4gon_JOyGULd2nn4Hwq7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127271,
      "firstName": "Miku",
      "lastName": "Fukaike",
      "middleInitial": "",
      "importedId": "_tiE4VtvlyKV6JMt7y8rrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127272,
      "firstName": "Kyungho",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "PHcRA3RSrRE8eIKiBsXLoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127273,
      "firstName": "Donghan",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "oBbLhU_vIlGGsXpax_ibAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127274,
      "firstName": "Wesley",
      "lastName": "Willett",
      "middleInitial": "",
      "importedId": "nuvbgR_GITgGTYynblgdFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127275,
      "firstName": "Junwoo",
      "lastName": "Yoon",
      "middleInitial": "",
      "importedId": "PZ5gNvS277kEUpFF4jm3nw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127276,
      "firstName": "Shu",
      "lastName": "Sugita",
      "middleInitial": "",
      "importedId": "O0uH5VXglk48jVsSghQ--w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127277,
      "firstName": "Douglas",
      "lastName": "Zytko",
      "middleInitial": "",
      "importedId": "wBaXHC_z3iN3AIjBq93vzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127278,
      "firstName": "Yoshihiro",
      "lastName": "Kawahara",
      "middleInitial": "",
      "importedId": "7kMCYcI5yPwQRg4zZ61IkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127279,
      "firstName": "Alejandro",
      "lastName": "Aponte",
      "middleInitial": "",
      "importedId": "K4FjebgmBguoTfa2yq-LDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127280,
      "firstName": "Yifan",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "ECd0Kpfhaq7vklauKqCuuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127281,
      "firstName": "Isha",
      "lastName": "Datey",
      "middleInitial": "",
      "importedId": "CS0FY_NlARyfpArSUAygRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127282,
      "firstName": "Sharon",
      "lastName": "Hsiao",
      "middleInitial": "",
      "importedId": "NPDmzbA2JeMo61aeCvHYJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127283,
      "firstName": "Kazuki",
      "lastName": "Koyama",
      "middleInitial": "",
      "importedId": "i_2xlt7fST1pOAM1Dzey4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127284,
      "firstName": "Ken",
      "lastName": "Takaki",
      "middleInitial": "",
      "importedId": "aUMpjyUeFeyjUicQc6lgPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127285,
      "firstName": "Hebo",
      "lastName": "Gong",
      "middleInitial": "",
      "importedId": "88XoV5YqQUZqA9nR7BKJ6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127286,
      "firstName": "Homei",
      "lastName": "Miyashita",
      "middleInitial": "",
      "importedId": "8SUm8tZAhEI_3G3BIXTBiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127287,
      "firstName": "Chang Hee",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "u82yL9LFGB_k-6FC_tQBOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127288,
      "firstName": "Benjamin",
      "lastName": "Nuernberger",
      "middleInitial": "",
      "importedId": "r8sFyLGaKN9iJ0laVn77Sw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127289,
      "firstName": "Donglai",
      "lastName": "Wei",
      "middleInitial": "",
      "importedId": "ypzyqxkzHW8BbfTxRf2j8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127290,
      "firstName": "Daniel",
      "lastName": "Vargas-Diaz",
      "middleInitial": "",
      "importedId": "Iidpza8ccvh7GrK1Vd-G_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127291,
      "firstName": "Sherry",
      "lastName": "Chen",
      "middleInitial": "X",
      "importedId": "S7lKZOvrUvsc2zP1pnnUnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127292,
      "firstName": "Xiangyu",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "RZ6z6PEURotP3dpaxVltgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127293,
      "firstName": "Andrew",
      "lastName": "Johnson",
      "middleInitial": "E",
      "importedId": "3JuebO_PEKnoJqzDUtFMUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127294,
      "firstName": "Shanna",
      "lastName": "Hollingworth",
      "middleInitial": "Li Ching",
      "importedId": "Vj3mugwA1B3QVFc_dZCv0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127295,
      "firstName": "Andreas",
      "lastName": "Gottscholl",
      "middleInitial": "",
      "importedId": "7aDzg0Zha4nrvAckk_PxZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127296,
      "firstName": "Lora",
      "lastName": "Oehlberg",
      "middleInitial": "",
      "importedId": "GgOZkpsAdy2_ES3FSLq7NQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127297,
      "firstName": "Purav",
      "lastName": "Bhardwaj",
      "middleInitial": "",
      "importedId": "7LVR3Hougy68h0Q8c91Qog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127298,
      "firstName": "Gabriel J.",
      "lastName": "Serfaty",
      "middleInitial": "",
      "importedId": "rek-cPuEycsqtK9cisumOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127299,
      "firstName": "Yusuke",
      "lastName": "Sakai",
      "middleInitial": "",
      "importedId": "TH_wXOk2l69tbF1Nm7Ja-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127300,
      "firstName": "Atieh",
      "lastName": "Taheri",
      "middleInitial": "",
      "importedId": "ukNxpQ7B8WS_71YushGeIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127301,
      "firstName": "Yifan",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "plrdwOZS_8ZpB-nN5AVjqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127302,
      "firstName": "Chi-Chien Nelson",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "QVMviFNnVrS8-WSyUVOshQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127303,
      "firstName": "Chizu",
      "lastName": "Nishimori",
      "middleInitial": "",
      "importedId": "yBUTCe-4jCFYA1Jl_YEc-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127304,
      "firstName": "Emma",
      "lastName": "Walquist",
      "middleInitial": "",
      "importedId": "s1VyABrqCAWaiFEeD0-1wg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127305,
      "firstName": "Carol A.",
      "lastName": "Raymond",
      "middleInitial": "",
      "importedId": "UnXYlsnwjVLowcLCjmspQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127306,
      "firstName": "Donghyeok",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "mNbt3XdHuQTmH-e_jkjtfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127307,
      "firstName": "Avinash Ajit",
      "lastName": "Nargund",
      "middleInitial": "",
      "importedId": "jlx7syMKl68Udj0hAbsuwQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127308,
      "firstName": "Viral",
      "lastName": "Doshi",
      "middleInitial": "Niraj",
      "importedId": "qEhA4964co9hoTIywyLNMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127309,
      "firstName": "Edward C.",
      "lastName": "Gonzales",
      "middleInitial": "",
      "importedId": "KXhdB5QS87wknorMo7DHOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127310,
      "firstName": "Yuhang",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "grxEwavZH6KBaCspy1-laA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127311,
      "firstName": "Tomohiko",
      "lastName": "Mukai",
      "middleInitial": "",
      "importedId": "2fS1g7JDtKvAfgRWDjDywg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127312,
      "firstName": "Austin",
      "lastName": "Mac",
      "middleInitial": "",
      "importedId": "4vJWFIBO7jT-afVWS_4juA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127313,
      "firstName": "Michele",
      "lastName": "Parkhill",
      "middleInitial": "",
      "importedId": "kN8gCZV9hmZDry3DMysSFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127314,
      "firstName": "Takashi",
      "lastName": "Murayama",
      "middleInitial": "",
      "importedId": "vwFRFOxW0mA9qFnRj6gA7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127315,
      "firstName": "Shuichi",
      "lastName": "Sakai",
      "middleInitial": "",
      "importedId": "g4O25Icw3I0PRdW6nZh5rA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127316,
      "firstName": "Eduard",
      "lastName": "de Vidal Flores",
      "middleInitial": "",
      "importedId": "zp-hG9H77KmGp3yEqmFa7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127317,
      "firstName": "Xinyan",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "lTBsCOJxUG8zXgtFiXziwQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127318,
      "firstName": "Junxian",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "frwiKyxIejIK66pyeS_a8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127319,
      "firstName": "Katherine",
      "lastName": "Dang",
      "middleInitial": "",
      "importedId": "bgFuno21xBaPR9rzLX9_3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127320,
      "firstName": "Xinyi",
      "lastName": "Liao",
      "middleInitial": "",
      "importedId": "QsA-DCLtSk65CrFEExPxeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127321,
      "firstName": "Yujing",
      "lastName": "Tian",
      "middleInitial": "",
      "importedId": "3oqoCKX_50xOgLaZNj4KeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127322,
      "firstName": "Junichiro",
      "lastName": "Kadomoto",
      "middleInitial": "",
      "importedId": "Ye50LQGEOhRzRURPbdvSLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127323,
      "firstName": "Shuto",
      "lastName": "Murakami",
      "middleInitial": "",
      "importedId": "Azavso1-9TmlZ1nRbsriqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127324,
      "firstName": "Hye-Young",
      "lastName": "Jo",
      "middleInitial": "",
      "importedId": "brQtwrC_s6tc5BWS7hRuAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127325,
      "firstName": "Koeun",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "ZJhtt8i0fR7Q9WfKOtwmBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127326,
      "firstName": "Yajing",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "EeaqtxxOzCtb6qzPn_vYPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127327,
      "firstName": "Amr",
      "lastName": "Gomaa",
      "middleInitial": "",
      "importedId": "oYch6aFzsGIjQYAVOHPepA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127328,
      "firstName": "Chunchen",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "_YSLEVFf9KhbGA688r3plg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127329,
      "firstName": "Xi",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "h3qlTmznGGS7RLMkGpKADw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127330,
      "firstName": "Lauryn",
      "lastName": "Anderson",
      "middleInitial": "",
      "importedId": "lRBIdjrMf-_udv-QRXN08g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127331,
      "firstName": "Yasushi",
      "lastName": "Kawase",
      "middleInitial": "",
      "importedId": "uJHsosKXkjTiQj1-iS269A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127332,
      "firstName": "Jongik",
      "lastName": "Jeon",
      "middleInitial": "",
      "importedId": "XXVwAujG0GRWKXZvkFuW5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127333,
      "firstName": "Koya",
      "lastName": "Narumi",
      "middleInitial": "",
      "importedId": "KolI4fgGxwAfs0V5a8igoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127334,
      "firstName": "Radha",
      "lastName": "Kumaran",
      "middleInitial": "",
      "importedId": "9PbkzvNVJINm2yccDAF-uQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127335,
      "firstName": "Kelly",
      "lastName": "Berishaj",
      "middleInitial": "",
      "importedId": "487DT9EIKsvIA_bsssQlbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127336,
      "firstName": "Guillermo",
      "lastName": "Reyes",
      "middleInitial": "",
      "importedId": "reL5BLFrhSTgKuYkbNtnCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127337,
      "firstName": "Chen",
      "lastName": "Ye",
      "middleInitial": "",
      "importedId": "ceXFUQNRFybM0plNkL9FUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127338,
      "firstName": "ryoya",
      "lastName": "tamura",
      "middleInitial": "",
      "importedId": "W4WHlhxYPHzA73cjISVffw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127339,
      "firstName": "Daniel",
      "lastName": "Lohn",
      "middleInitial": "",
      "importedId": "fEiWhs488FPyC0RXRUq8NQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127340,
      "firstName": "yuya",
      "lastName": "aikawa",
      "middleInitial": "",
      "importedId": "yvOVeW8htFaVu-Un39XEUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127341,
      "firstName": "Dongxiao",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "TSALstDbkPnBIaQVMdA_9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127342,
      "firstName": "Junhan",
      "lastName": "Kong",
      "middleInitial": "",
      "importedId": "_Vr6TuQoVWta4jCdlRMJug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127343,
      "firstName": "Neil",
      "lastName": "Murphy",
      "middleInitial": "",
      "importedId": "2kXkZcDJF2xNC2QgWHLybg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127344,
      "firstName": "Takegi",
      "lastName": "Yoshimoto",
      "middleInitial": "",
      "importedId": "IaHqh-UCBTqdc4HdVs5GZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127345,
      "firstName": "Jacob",
      "lastName": "Ottiger",
      "middleInitial": "",
      "importedId": "pswTlLnQj5SIaYJ2k--Psw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127346,
      "firstName": "Zhitong",
      "lastName": "Cui",
      "middleInitial": "",
      "importedId": "u3V-MF07QYO2XoMa6iBJxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127347,
      "firstName": "Sam",
      "lastName": "Wong",
      "middleInitial": "",
      "importedId": "txiS6LJPjtJ41V5Gs_LICQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127348,
      "firstName": "Eric",
      "lastName": "Shaqfeh",
      "middleInitial": "",
      "importedId": "tOdBypUd1r5aRsZfjilBYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127349,
      "firstName": "Tatsuya",
      "lastName": "Kagemoto",
      "middleInitial": "",
      "importedId": "nX5GpT1r3bJVyIyz-nErAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127350,
      "firstName": "Pablo S.",
      "lastName": "Narvaez",
      "middleInitial": "",
      "importedId": "o6eXYkRT6NRyYuDzmKHTkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127351,
      "firstName": "Zi",
      "lastName": "Ye",
      "middleInitial": "",
      "importedId": "HivD9YYFjPsD9YD6Lu17Hg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127352,
      "firstName": "Ruishi",
      "lastName": "Zou",
      "middleInitial": "",
      "importedId": "X4oW29wPBoymAADGA47sKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127353,
      "firstName": "Priyan",
      "lastName": "Vaithilingam",
      "middleInitial": "",
      "importedId": "2ScSMpEz4tNamzQW_oQ0qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127354,
      "firstName": "Taeyoung",
      "lastName": "Ko",
      "middleInitial": "",
      "importedId": "biY-9gzG238i1wj92Yb6vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127355,
      "firstName": "Hidetsugu",
      "lastName": "Irie",
      "middleInitial": "",
      "importedId": "RZbOowAYpvEIUIyuLiTtZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127356,
      "firstName": "Yoonji",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "7ZL4CwOeL_cHQkpl0HXqyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 127833,
      "firstName": "Tianshi",
      "lastName": "Li",
      "importedId": "0000-0003-0877-5727",
      "source": "CSV",
      "affiliations": [
        {
          "country": "USA",
          "state": "PA",
          "city": "Pittsburgh",
          "institution": "Carnegie Mellon University"
        }
      ]
    },
    {
      "id": 127834,
      "firstName": "Philip",
      "lastName": "Quinn",
      "importedId": "0000-0002-6340-0568",
      "source": "CSV",
      "affiliations": [
        {
          "country": "USA",
          "state": "CA",
          "institution": "Google"
        }
      ]
    },
    {
      "id": 127835,
      "firstName": "Tong",
      "lastName": "Niu",
      "importedId": "5m5eYNwUIJsqcjGqJX_ihw",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 127836,
      "firstName": "Philippe",
      "lastName": "Laban",
      "importedId": "C4fWPqc-_A0aKTbvSAGa-A",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 127837,
      "firstName": "Shumin",
      "lastName": "Zhai",
      "importedId": "W_RCVI8Bqn5DhSURFBnu8g",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 127838,
      "firstName": "Xiang 'Anthony'",
      "lastName": "Chen",
      "importedId": "KpO_0vxU-ZNKmw9QA3hTuw",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 127839,
      "firstName": "Lidiya",
      "lastName": "Murakhovs'ka",
      "importedId": "ZDkeDzXhWUdBQiXdEgrWRQ",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 127840,
      "firstName": "Chien-Sheng",
      "lastName": "Wu",
      "importedId": "OIDMmvh5bDfl6_BfZi3WEA",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 127841,
      "firstName": "Wenhao",
      "lastName": "Liu",
      "importedId": "oT3V3OnCu7JgVP49z6aMIw",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 127842,
      "firstName": "Caiming",
      "lastName": "Xiong",
      "importedId": "-9S3RAb_8yPgxQidYgDPBw",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128169,
      "firstName": "Franck",
      "lastName": "Dernoncourt",
      "middleInitial": "",
      "importedId": "G3locUstcVP70v6v5hwLaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128170,
      "firstName": "Nedim",
      "lastName": "Lipka",
      "middleInitial": "",
      "importedId": "k_-2OTZI-GiHo5D4hbc6uQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128171,
      "firstName": "Alexa",
      "lastName": "Siu",
      "middleInitial": "F",
      "importedId": "c_RIBNUYfShSfRI1R6qP0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128172,
      "firstName": "Jean-Pe√Øc",
      "lastName": "Chou",
      "middleInitial": "",
      "importedId": "OklrqEVeShlCI4KGRpYGzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 129931,
      "firstName": "Yukang",
      "lastName": "Yan",
      "importedId": "0001000",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129932,
      "firstName": "Steve",
      "lastName": "Feiner",
      "importedId": "0001005",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129933,
      "firstName": "Gierad",
      "lastName": "Laput",
      "importedId": "0001004",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129935,
      "firstName": "Valkyrie",
      "lastName": "Savage",
      "importedId": "0001002",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129936,
      "firstName": "Bala",
      "lastName": "Kumaravel",
      "importedId": "0001001",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 130028,
      "firstName": "Judy",
      "lastName": "Fan",
      "importedId": "0001007",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "Stanford University"
        }
      ]
    },
    {
      "id": 130029,
      "firstName": "Jeff",
      "lastName": "Han",
      "importedId": "0001006",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "perceptiveIO"
        }
      ]
    },
    {
      "id": 130030,
      "firstName": "David",
      "lastName": "Holz",
      "importedId": "0001008",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "Midjourney"
        }
      ]
    }
  ],
  "recognitions": [
    {
      "id": 10054,
      "name": "People Choice Awards",
      "iconName": "heart"
    }
  ]
}