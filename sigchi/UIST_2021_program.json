{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10059,
    "shortName": "UIST",
    "year": 2021,
    "startDate": 1633824000000,
    "endDate": 1634169600000,
    "name": "UIST 2021",
    "fullName": "34th Annual ACM Symposium on User Interface Software & Technology",
    "url": "https://uist.acm.org/uist2021/",
    "location": "Virtual",
    "timeZoneOffset": -420,
    "timeZoneName": "America/Los_Angeles",
    "logoUrl": "https://files.sigchi.org/conference/logo/ecae02c0-05b8-f418-5bbb-f4dce29344bc.png"
  },
  "sponsors": [
    {
      "id": 10159,
      "name": "Autodesk",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/772ee938-18d3-12be-fb9e-96c2ce80b3d2.png",
      "levelId": 10115,
      "order": 4
    },
    {
      "id": 10160,
      "name": "Apple",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/82f5ea91-3eaf-90dc-4fc8-6b6f1ec6bbe2.png",
      "levelId": 10115,
      "order": 3
    },
    {
      "id": 10161,
      "name": "Adobe",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/1f7c957b-d5b0-3a87-25e4-2efab21a5aca.png",
      "levelId": 10115,
      "order": 2
    },
    {
      "id": 10162,
      "name": "Intel",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/2bf5777e-e13d-4686-5c95-71e86ab48587.png",
      "levelId": 10116,
      "order": 5
    },
    {
      "id": 10163,
      "name": "Toyota Research Institute",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/6a97bae1-a3c0-0d56-509c-76129a5c8635.png",
      "levelId": 10116,
      "order": 8
    },
    {
      "id": 10164,
      "name": "INTUITIVE",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/f4db47fb-b66a-7049-ecaf-0345310ea507.png",
      "levelId": 10116,
      "order": 6
    },
    {
      "id": 10173,
      "name": "Microsoft",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/572e15df-4278-b795-3b84-d8b915255a87.png",
      "levelId": 10116,
      "order": 7
    },
    {
      "id": 10174,
      "name": "Facebook Reality Labs (FRL)",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/da161285-6635-7e3d-5494-1ac25900585e.png",
      "levelId": 10113,
      "order": 1
    }
  ],
  "sponsorLevels": [
    {
      "id": 10093,
      "name": "Sponsors",
      "rank": 5,
      "isDefault": true
    },
    {
      "id": 10113,
      "name": "Platinum Sponsors",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10114,
      "name": "Gold Sponsors",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10115,
      "name": "Silver Sponsors",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10116,
      "name": "Bronze Sponsors",
      "rank": 4,
      "isDefault": false
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 10537,
      "name": "Gallifrey",
      "typeId": 11756,
      "setup": "Theatre",
      "capacity": "500",
      "note": "Papers Room 1"
    },
    {
      "id": 10538,
      "name": "Coruscant",
      "typeId": 11756,
      "setup": "Theatre",
      "capacity": "500",
      "note": "Papers Room 2"
    },
    {
      "id": 10539,
      "name": "Trantor",
      "typeId": 11756,
      "setup": "Theatre",
      "capacity": "500",
      "note": "Papers Room 3"
    },
    {
      "id": 10540,
      "name": "Ansible",
      "typeId": 11757,
      "setup": "Theatre",
      "capacity": "500",
      "note": "Plenary Room"
    },
    {
      "id": 10541,
      "name": "Multiverse",
      "typeId": 11752,
      "setup": "Special",
      "capacity": "1000",
      "note": "Demo & Poster Space"
    },
    {
      "id": 10542,
      "name": "Talos IV",
      "typeId": 11752,
      "setup": "Special",
      "capacity": "200",
      "note": "Ohyay Room"
    },
    {
      "id": 10543,
      "name": "Zebulon",
      "typeId": 11756,
      "setup": "Theatre",
      "capacity": "500",
      "note": "Tech Check & Backup Space"
    },
    {
      "id": 10544,
      "name": "who2chat",
      "typeId": 11752,
      "setup": "Special",
      "capacity": "200",
      "note": "who2chat"
    }
  ],
  "tracks": [
    {
      "id": 11291,
      "name": "UIST 2021 Demos",
      "typeId": 11891
    },
    {
      "id": 11292,
      "name": "UIST 2021 Papers",
      "typeId": 11756
    },
    {
      "id": 11293,
      "name": "UIST 2021 Posters",
      "typeId": 11892
    },
    {
      "id": 11331,
      "typeId": 11753
    },
    {
      "id": 11332,
      "typeId": 11757
    },
    {
      "id": 11333,
      "typeId": 11753
    },
    {
      "id": 11334,
      "typeId": 11916
    },
    {
      "id": 11335,
      "typeId": 11920
    },
    {
      "id": 11336,
      "typeId": 11921
    },
    {
      "id": 11337,
      "typeId": 11919
    },
    {
      "id": 11338,
      "typeId": 11918
    },
    {
      "id": 11339,
      "typeId": 11917
    },
    {
      "id": 11340,
      "typeId": 11922
    },
    {
      "id": 11341,
      "name": "UIST 2021 Doctoral Symposium",
      "typeId": 11923
    },
    {
      "id": 11342,
      "name": "UIST 2021 Visions",
      "typeId": 11916
    }
  ],
  "contentTypes": [
    {
      "id": 11749,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 60
    },
    {
      "id": 11750,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 11751,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 11752,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 11753,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 60,
      "displayName": "Invited Talks"
    },
    {
      "id": 11754,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 60
    },
    {
      "id": 11755,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 60,
      "displayName": "Panels"
    },
    {
      "id": 11756,
      "name": "Paper",
      "color": "#08519c",
      "duration": 15,
      "displayName": "Papers"
    },
    {
      "id": 11757,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 30
    },
    {
      "id": 11758,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 11891,
      "name": "Demos",
      "color": "#ff7a00",
      "duration": 60
    },
    {
      "id": 11892,
      "name": "Posters",
      "color": "#32d923",
      "duration": 60
    },
    {
      "id": 11916,
      "name": "Vision Talk",
      "color": "#6baed6",
      "duration": 60
    },
    {
      "id": 11917,
      "name": "Ask Me Anything",
      "color": "#ff99ca",
      "duration": 57
    },
    {
      "id": 11918,
      "name": "Social Hour",
      "color": "#ff99ca",
      "duration": 52
    },
    {
      "id": 11919,
      "name": "Lightning Talk",
      "color": "#ff99ca",
      "duration": 0
    },
    {
      "id": 11920,
      "name": "Student Innovation Contest",
      "color": "#8e008b",
      "duration": 60
    },
    {
      "id": 11921,
      "name": "Social Meetup",
      "color": "#ff99ca",
      "duration": 60
    },
    {
      "id": 11922,
      "name": "Sponsored Talk",
      "color": "#7d6bef",
      "duration": 60,
      "displayName": "Sponsored Talks"
    },
    {
      "id": 11923,
      "name": "Doctoral Symposium",
      "color": "#8e008b",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 12144,
      "type": "SESSION",
      "startDate": 1633966200000,
      "endDate": 1633971600000
    },
    {
      "id": 12145,
      "type": "SESSION",
      "startDate": 1633971600000,
      "endDate": 1633975200000
    },
    {
      "id": 12146,
      "type": "SESSION",
      "startDate": 1633975200000,
      "endDate": 1633978800000
    },
    {
      "id": 12147,
      "type": "SESSION",
      "startDate": 1633978800000,
      "endDate": 1633982400000
    },
    {
      "id": 12148,
      "type": "SESSION",
      "startDate": 1634055300000,
      "endDate": 1634058000000
    },
    {
      "id": 12149,
      "type": "SESSION",
      "startDate": 1634058000000,
      "endDate": 1634061600000
    },
    {
      "id": 12150,
      "type": "SESSION",
      "startDate": 1634061600000,
      "endDate": 1634065200000
    },
    {
      "id": 12151,
      "type": "SESSION",
      "startDate": 1634065200000,
      "endDate": 1634068800000
    },
    {
      "id": 12152,
      "type": "SESSION",
      "startDate": 1634000400000,
      "endDate": 1634004000000
    },
    {
      "id": 12153,
      "type": "SESSION",
      "startDate": 1634004000000,
      "endDate": 1634007600000
    },
    {
      "id": 12154,
      "type": "SESSION",
      "startDate": 1634007600000,
      "endDate": 1634011200000
    },
    {
      "id": 12155,
      "type": "SESSION",
      "startDate": 1634026500000,
      "endDate": 1634029200000
    },
    {
      "id": 12156,
      "type": "SESSION",
      "startDate": 1634029200000,
      "endDate": 1634032800000
    },
    {
      "id": 12157,
      "type": "SESSION",
      "startDate": 1634032800000,
      "endDate": 1634036400000
    },
    {
      "id": 12158,
      "type": "SESSION",
      "startDate": 1634036400000,
      "endDate": 1634040000000
    },
    {
      "id": 12159,
      "type": "SESSION",
      "startDate": 1634086800000,
      "endDate": 1634090400000
    },
    {
      "id": 12160,
      "type": "SESSION",
      "startDate": 1634090400000,
      "endDate": 1634094000000
    },
    {
      "id": 12161,
      "type": "SESSION",
      "startDate": 1634094000000,
      "endDate": 1634097600000
    },
    {
      "id": 12162,
      "type": "SESSION",
      "startDate": 1634112900000,
      "endDate": 1634115600000
    },
    {
      "id": 12163,
      "type": "SESSION",
      "startDate": 1634115600000,
      "endDate": 1634119200000
    },
    {
      "id": 12164,
      "type": "SESSION",
      "startDate": 1634119200000,
      "endDate": 1634122800000
    },
    {
      "id": 12165,
      "type": "SESSION",
      "startDate": 1634122800000,
      "endDate": 1634126400000
    },
    {
      "id": 12166,
      "type": "SESSION",
      "startDate": 1634140800000,
      "endDate": 1634144400000
    },
    {
      "id": 12167,
      "type": "SESSION",
      "startDate": 1634144400000,
      "endDate": 1634148000000
    },
    {
      "id": 12168,
      "type": "SESSION",
      "startDate": 1634148000000,
      "endDate": 1634151600000
    },
    {
      "id": 12169,
      "type": "SESSION",
      "startDate": 1634151600000,
      "endDate": 1634155200000
    },
    {
      "id": 12170,
      "type": "SESSION",
      "startDate": 1634083200000,
      "endDate": 1634086800000
    },
    {
      "id": 12172,
      "type": "SESSION",
      "startDate": 1634173200000,
      "endDate": 1634176800000
    },
    {
      "id": 12173,
      "type": "SESSION",
      "startDate": 1634176800000,
      "endDate": 1634180400000
    },
    {
      "id": 12175,
      "type": "SESSION",
      "startDate": 1634198400000,
      "endDate": 1634202000000
    },
    {
      "id": 12176,
      "type": "SESSION",
      "startDate": 1634202000000,
      "endDate": 1634205600000
    },
    {
      "id": 12177,
      "type": "SESSION",
      "startDate": 1634205600000,
      "endDate": 1634209200000
    },
    {
      "id": 12178,
      "type": "SESSION",
      "startDate": 1634209200000,
      "endDate": 1634214600000
    },
    {
      "id": 12187,
      "type": "SESSION",
      "startDate": 1633982400000,
      "endDate": 1633986000000
    },
    {
      "id": 12188,
      "type": "SESSION",
      "startDate": 1634011200000,
      "endDate": 1634014800000
    },
    {
      "id": 12189,
      "type": "SESSION",
      "startDate": 1634040000000,
      "endDate": 1634043600000
    },
    {
      "id": 12190,
      "type": "SESSION",
      "startDate": 1634068800000,
      "endDate": 1634072400000
    },
    {
      "id": 12191,
      "type": "SESSION",
      "startDate": 1634097600000,
      "endDate": 1634101200000
    },
    {
      "id": 12192,
      "type": "SESSION",
      "startDate": 1634126400000,
      "endDate": 1634130000000
    },
    {
      "id": 12194,
      "type": "SESSION",
      "startDate": 1634180400000,
      "endDate": 1634184000000
    },
    {
      "id": 12195,
      "type": "SESSION",
      "startDate": 1634214600000,
      "endDate": 1634218200000
    },
    {
      "id": 12196,
      "type": "SESSION",
      "startDate": 1634101200000,
      "endDate": 1634104800000
    }
  ],
  "sessions": [
    {
      "id": 66211,
      "name": "Opening Plenary & Keynote",
      "addons": {},
      "typeId": 11757,
      "roomId": 10540,
      "chairIds": [
        61230
      ],
      "contentIds": [
        66217,
        66208
      ],
      "timeSlotId": 12144
    },
    {
      "id": 66212,
      "name": "Keynote: Ludic Design for Accessibility",
      "addons": {},
      "typeId": 11757,
      "roomId": 10540,
      "chairIds": [
        60945
      ],
      "contentIds": [
        66210
      ],
      "timeSlotId": 12170
    },
    {
      "id": 66213,
      "name": "Closing Keynote & Plenary",
      "addons": {},
      "typeId": 11757,
      "roomId": 10540,
      "chairIds": [
        66584
      ],
      "contentIds": [
        66209,
        66218
      ],
      "timeSlotId": 12178
    },
    {
      "id": 66226,
      "name": "Lasting Impact Award Plenary",
      "addons": {},
      "typeId": 11753,
      "roomId": 10540,
      "chairIds": [
        66548
      ],
      "contentIds": [
        66225
      ],
      "timeSlotId": 12162
    },
    {
      "id": 66227,
      "name": "Townhall",
      "addons": {},
      "typeId": 11757,
      "roomId": 10540,
      "chairIds": [
        61230
      ],
      "contentIds": [
        66223
      ],
      "timeSlotId": 12166
    },
    {
      "id": 66228,
      "name": "UIST Visions #1 - sponsored by Autodesk",
      "addons": {},
      "typeId": 11916,
      "roomId": 10540,
      "chairIds": [
        66645
      ],
      "contentIds": [
        66628
      ],
      "timeSlotId": 12155
    },
    {
      "id": 66229,
      "name": "UIST Visions #2",
      "addons": {},
      "typeId": 11916,
      "roomId": 10540,
      "chairIds": [
        66645
      ],
      "contentIds": [
        66628
      ],
      "timeSlotId": 12148
    },
    {
      "id": 66234,
      "name": "Hand-y Stuff in AR/VR",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        61239
      ],
      "contentIds": [
        66576,
        66575,
        61337,
        61331,
        61408,
        61326
      ],
      "timeSlotId": 12145
    },
    {
      "id": 66235,
      "name": "Summarization & Semantics",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        60981
      ],
      "contentIds": [
        66564,
        66452,
        61386,
        61415,
        61350,
        61327
      ],
      "timeSlotId": 12145
    },
    {
      "id": 66236,
      "name": "Fabrication",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        66479
      ],
      "contentIds": [
        66551,
        66550,
        61371,
        61407,
        61416,
        61362
      ],
      "timeSlotId": 12145
    },
    {
      "id": 66237,
      "name": "Applications in Mixed Reality",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        61081
      ],
      "contentIds": [
        66556,
        66561,
        61406,
        61409,
        61329,
        61383
      ],
      "timeSlotId": 12146
    },
    {
      "id": 66238,
      "name": "Tasks & Tutorials",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66495
      ],
      "contentIds": [
        66567,
        66554,
        61324,
        61394,
        61402,
        61413
      ],
      "timeSlotId": 12146
    },
    {
      "id": 66239,
      "name": "Alternative Programming",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        66490
      ],
      "contentIds": [
        66452,
        66581,
        61376,
        61325,
        61382,
        61356
      ],
      "timeSlotId": 12146
    },
    {
      "id": 66240,
      "name": "Brushing, Talking, and Virtual Conferencing",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66482
      ],
      "contentIds": [
        66563,
        66578,
        61386,
        61350,
        61357
      ],
      "timeSlotId": 12153
    },
    {
      "id": 66241,
      "name": "Fabrics Meet Lasers",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66494
      ],
      "contentIds": [
        66573,
        66553,
        61366,
        61348,
        61346,
        61364
      ],
      "timeSlotId": 12152
    },
    {
      "id": 66242,
      "name": "Automotive and Proximate Interaction",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        61223
      ],
      "contentIds": [
        66559,
        66572,
        61330,
        61344,
        61339
      ],
      "timeSlotId": 12153
    },
    {
      "id": 66243,
      "name": "Reality: Augmented or Virtual?",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        61096
      ],
      "contentIds": [
        66565,
        66562,
        61408,
        61406,
        61329,
        61369
      ],
      "timeSlotId": 12152
    },
    {
      "id": 66244,
      "name": "Building Experiences in AR/VR",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66593
      ],
      "contentIds": [
        66581,
        66568,
        61337,
        61369,
        61383,
        61331
      ],
      "timeSlotId": 12156
    },
    {
      "id": 66245,
      "name": "Lasers, Sharks Not Included",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        60919
      ],
      "contentIds": [
        66553,
        66550,
        61371,
        61348,
        61346,
        61364
      ],
      "timeSlotId": 12157
    },
    {
      "id": 66246,
      "name": "Efficient Tasks For All",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66491
      ],
      "contentIds": [
        66567,
        66559,
        61402,
        61394,
        61324,
        61415
      ],
      "timeSlotId": 12156
    },
    {
      "id": 66247,
      "name": "Enhancing Complex Interactions",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        61283
      ],
      "contentIds": [
        66557,
        66564,
        61357,
        61327,
        61413,
        61387
      ],
      "timeSlotId": 12157
    },
    {
      "id": 66248,
      "name": "Fluids & Fabrication",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        60982
      ],
      "contentIds": [
        66574,
        66577,
        61407,
        61362,
        61366,
        61376
      ],
      "timeSlotId": 12156
    },
    {
      "id": 66249,
      "name": "Touring AR/VR...Sometimes in Cars",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66589
      ],
      "contentIds": [
        66568,
        66569,
        61326,
        61409,
        61344,
        61330
      ],
      "timeSlotId": 12157
    },
    {
      "id": 66250,
      "name": "Illustration and Information Management",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        60933
      ],
      "contentIds": [
        66571,
        66551,
        61323,
        61372,
        61347,
        61378
      ],
      "timeSlotId": 12149
    },
    {
      "id": 66251,
      "name": "Interacting Across And Through",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        66480
      ],
      "contentIds": [
        66580,
        66573,
        61381,
        61334,
        61341,
        61328
      ],
      "timeSlotId": 12150
    },
    {
      "id": 66252,
      "name": "More Haptics, Now With Modeling",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66486
      ],
      "contentIds": [
        66572,
        66552,
        61355,
        61379,
        61345
      ],
      "timeSlotId": 12150
    },
    {
      "id": 66253,
      "name": "Haptics",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        61040
      ],
      "contentIds": [
        66566,
        66579,
        61342,
        61400,
        61396,
        61414
      ],
      "timeSlotId": 12149
    },
    {
      "id": 66254,
      "name": "Wrangling Data for Authoring & Analysis",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        61262
      ],
      "contentIds": [
        66576,
        61393,
        61395,
        61340,
        61387
      ],
      "timeSlotId": 12150
    },
    {
      "id": 66255,
      "name": "Augmentation and Accessibility",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66478
      ],
      "contentIds": [
        66567,
        66557,
        61392,
        61370,
        61391,
        61404
      ],
      "timeSlotId": 12149
    },
    {
      "id": 66256,
      "name": "Gesture Input and Authoring",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        61028
      ],
      "contentIds": [
        66569,
        66566,
        61359,
        61365,
        61398
      ],
      "timeSlotId": 12160
    },
    {
      "id": 66257,
      "name": "Typing and Pointing",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66512
      ],
      "contentIds": [
        66568,
        66560,
        61389,
        61405,
        61385
      ],
      "timeSlotId": 12159
    },
    {
      "id": 66258,
      "name": "Fabrication & Games",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        61249
      ],
      "contentIds": [
        66560,
        66578,
        61363,
        61412,
        61401
      ],
      "timeSlotId": 12160
    },
    {
      "id": 66259,
      "name": "Head-Based Peripheral and Vibrotactile Interactions",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66484
      ],
      "contentIds": [
        66562,
        66575,
        61328,
        61380,
        61375
      ],
      "timeSlotId": 12159
    },
    {
      "id": 66260,
      "name": "Fabrication and Fabric",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        66485
      ],
      "contentIds": [
        66558,
        66565,
        61363,
        61412,
        61341
      ],
      "timeSlotId": 12177
    },
    {
      "id": 66261,
      "name": "Device Augmentation & Communication",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        61317
      ],
      "contentIds": [
        66578,
        66555,
        61334,
        61381,
        61392,
        61380
      ],
      "timeSlotId": 12176
    },
    {
      "id": 66262,
      "name": "Proximate Interactions and Haptics",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66493
      ],
      "contentIds": [
        66566,
        66554,
        61339,
        61400,
        61375,
        61368
      ],
      "timeSlotId": 12164
    },
    {
      "id": 66263,
      "name": "Even More Haptics",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        61200
      ],
      "contentIds": [
        66552,
        66572,
        61414,
        61396,
        61355,
        61335
      ],
      "timeSlotId": 12163
    },
    {
      "id": 66264,
      "name": "Working With Data",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        61016
      ],
      "contentIds": [
        66452,
        66576,
        61393,
        61395,
        61367,
        61340
      ],
      "timeSlotId": 12164
    },
    {
      "id": 66265,
      "name": "Healthcare & Information Management",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66609
      ],
      "contentIds": [
        66555,
        66551,
        61399,
        61361,
        61347,
        61378
      ],
      "timeSlotId": 12163
    },
    {
      "id": 66266,
      "name": "On-Body Interaction",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        61084
      ],
      "contentIds": [
        66579,
        66577,
        61374,
        61368,
        61335
      ],
      "timeSlotId": 12168
    },
    {
      "id": 66267,
      "name": "Pointing and BCI",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66498
      ],
      "contentIds": [
        66556,
        66571,
        61338,
        61388,
        61411,
        61410
      ],
      "timeSlotId": 12167
    },
    {
      "id": 66268,
      "name": "Motion Tracking",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        66518
      ],
      "contentIds": [
        66562,
        66577,
        61403,
        61390,
        61321,
        61399
      ],
      "timeSlotId": 12167
    },
    {
      "id": 66269,
      "name": "Text Entry and Dealing with Errors",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66515
      ],
      "contentIds": [
        61389,
        61352,
        61360,
        61358
      ],
      "timeSlotId": 12168
    },
    {
      "id": 66270,
      "name": "Exploring and Writing Code",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        60987
      ],
      "contentIds": [
        66581,
        66580,
        61384,
        61336,
        61353,
        61377
      ],
      "timeSlotId": 12168
    },
    {
      "id": 66271,
      "name": "Understanding and Modifying UI",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66483
      ],
      "contentIds": [
        66564,
        66579,
        61322,
        61343,
        61333,
        61351
      ],
      "timeSlotId": 12167
    },
    {
      "id": 66272,
      "name": "Tracking Devices, Eyes and Fingers",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66514
      ],
      "contentIds": [
        66574,
        66559,
        61390,
        61332,
        61349
      ],
      "timeSlotId": 12173
    },
    {
      "id": 66273,
      "name": "Interaction in the World",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        61099
      ],
      "contentIds": [
        66565,
        66569,
        61397,
        61374,
        61354
      ],
      "timeSlotId": 12172
    },
    {
      "id": 66274,
      "name": "Interactive Editing & Workflows",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66644
      ],
      "contentIds": [
        66571,
        66558,
        61382,
        61367
      ],
      "timeSlotId": 12173
    },
    {
      "id": 66275,
      "name": "Understanding UIs & Managing Epidemics",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66510
      ],
      "contentIds": [
        66557,
        61343,
        61322,
        61361
      ],
      "timeSlotId": 12172
    },
    {
      "id": 66276,
      "name": "Gestures & Sketching",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        61096
      ],
      "contentIds": [
        66561,
        66574,
        61404,
        61365,
        61398,
        61372
      ],
      "timeSlotId": 12175
    },
    {
      "id": 66277,
      "name": "Input & Output",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        61197
      ],
      "contentIds": [
        66563,
        66558,
        61359,
        61360,
        61405,
        61416
      ],
      "timeSlotId": 12164
    },
    {
      "id": 66278,
      "name": "Steering, Pointing, and Recovery",
      "addons": {},
      "typeId": 11756,
      "roomId": 10539,
      "chairIds": [
        66492
      ],
      "contentIds": [
        66550,
        66556,
        61385,
        61338,
        61345,
        61358
      ],
      "timeSlotId": 12163
    },
    {
      "id": 66279,
      "name": "Illustration, Games & Accessibility",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        60970
      ],
      "contentIds": [
        66560,
        66552,
        61323,
        61401,
        61370,
        61391
      ],
      "timeSlotId": 12175
    },
    {
      "id": 66280,
      "name": "Web Programming & Assembly",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66509
      ],
      "contentIds": [
        66561,
        61351,
        61333,
        61377,
        61356
      ],
      "timeSlotId": 12177
    },
    {
      "id": 66281,
      "name": "Live Interactive Programming",
      "addons": {},
      "typeId": 11756,
      "roomId": 10537,
      "chairIds": [
        66489
      ],
      "contentIds": [
        66553,
        61384,
        61336,
        61353,
        61325
      ],
      "timeSlotId": 12176
    },
    {
      "id": 66282,
      "name": "Sensing in Various Modalities",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        61305
      ],
      "contentIds": [
        66580,
        66573,
        61352,
        61403,
        61397,
        61354
      ],
      "timeSlotId": 12177
    },
    {
      "id": 66283,
      "name": "More Touch and Other Input Methods",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66511
      ],
      "contentIds": [
        66554,
        66563,
        61349,
        61379,
        61332,
        61410
      ],
      "timeSlotId": 12176
    },
    {
      "id": 66284,
      "name": "Tracking and Touch",
      "addons": {},
      "typeId": 11756,
      "roomId": 10538,
      "chairIds": [
        66496
      ],
      "contentIds": [
        66575,
        66555,
        61321,
        61388,
        61342,
        61411
      ],
      "timeSlotId": 12175
    },
    {
      "id": 66601,
      "name": "HCI @ Facebook Reality Labs: Augmenting Human Capabilities for the XR Future",
      "addons": {},
      "typeId": 11922,
      "roomId": 10543,
      "chairIds": [],
      "contentIds": [
        66600
      ],
      "timeSlotId": 12165
    }
  ],
  "events": [
    {
      "id": 66214,
      "name": "1A - Demo & Poster Session - sponsored by Adobe",
      "addons": {},
      "typeId": 11752,
      "roomId": 10541,
      "chairIds": [],
      "contentIds": [
        61610,
        61605,
        61615,
        61612,
        61606,
        61608,
        61618,
        61616,
        61622,
        61487,
        61486,
        61499,
        61493,
        61507,
        61504,
        61488,
        61497,
        61496,
        61514,
        61490,
        61498,
        61506,
        61515,
        66646
      ],
      "startDate": 1633978800000,
      "endDate": 1633982400000,
      "description": "Demo & Poster Session",
      "presenterIds": []
    },
    {
      "id": 66215,
      "name": "1B - Demo & Poster Session",
      "addons": {},
      "typeId": 11752,
      "roomId": 10541,
      "chairIds": [],
      "contentIds": [
        61602,
        61625,
        61619,
        61611,
        61603,
        61623,
        61613,
        61617,
        61624,
        61609,
        61487,
        61510,
        61518,
        61489,
        61505,
        61503,
        61516,
        61485,
        61490,
        61498
      ],
      "startDate": 1634007600000,
      "endDate": 1634011200000,
      "presenterIds": []
    },
    {
      "id": 66230,
      "name": "1C - Demo & Poster Session - sponsored by Intel",
      "addons": {},
      "typeId": 11752,
      "roomId": 10541,
      "chairIds": [],
      "contentIds": [
        61620,
        61604,
        61607,
        61601,
        61621,
        61614,
        61486,
        61499,
        61493,
        61507,
        61502,
        61511,
        61488,
        61500,
        61496,
        61508,
        61519,
        61516,
        61492,
        61506
      ],
      "startDate": 1634036400000,
      "endDate": 1634040000000,
      "presenterIds": []
    },
    {
      "id": 66231,
      "name": "2A - Demo, Poster, Student Innovation Contest Session - sponsored by Intuitive",
      "addons": {},
      "typeId": 11752,
      "roomId": 10541,
      "chairIds": [],
      "contentIds": [
        61603,
        61623,
        61620,
        61613,
        61617,
        61624,
        61604,
        61607,
        61609,
        61517,
        61483,
        61495,
        61518,
        61484,
        61482,
        61489,
        61500,
        61494,
        61503,
        61485,
        61492,
        61512,
        66460,
        66459,
        66467,
        66469,
        66468,
        66464,
        66453,
        66455,
        66454
      ],
      "startDate": 1634065200000,
      "endDate": 1634068800000,
      "presenterIds": []
    },
    {
      "id": 66232,
      "name": "2B - Demo, Poster, Student Innovation Contest Session",
      "addons": {},
      "typeId": 11752,
      "roomId": 10541,
      "chairIds": [],
      "contentIds": [
        61601,
        61621,
        61614,
        61606,
        61608,
        61616,
        61622,
        61517,
        61502,
        61511,
        61508,
        61501,
        61514,
        61513,
        61519,
        61515,
        66458,
        66456,
        66465,
        66453,
        66455
      ],
      "startDate": 1634094000000,
      "endDate": 1634097600000,
      "presenterIds": []
    },
    {
      "id": 66233,
      "name": "2C - Demo, Poster, Student Innovation Contest Session - sponsored by Toyota Research Institute (TRI)",
      "addons": {},
      "typeId": 11752,
      "roomId": 10541,
      "chairIds": [],
      "contentIds": [
        61610,
        61605,
        61615,
        61618,
        61612,
        61602,
        61625,
        61619,
        61611,
        61483,
        61495,
        61504,
        61510,
        61484,
        61482,
        61497,
        61494,
        61505,
        61501,
        61513,
        66646,
        66458,
        66456,
        66460,
        66459,
        66467,
        66465,
        66469,
        66468,
        66464,
        66454
      ],
      "startDate": 1634122800000,
      "endDate": 1634126400000,
      "presenterIds": []
    },
    {
      "id": 66499,
      "name": "1A - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1633982400000,
      "endDate": 1633986000000,
      "link": {},
      "presenterIds": []
    },
    {
      "id": 66500,
      "name": "2A - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1634068800000,
      "endDate": 1634072400000,
      "presenterIds": []
    },
    {
      "id": 66501,
      "name": "3A - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1634151600000,
      "endDate": 1634155200000,
      "presenterIds": []
    },
    {
      "id": 66502,
      "name": "1B - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1634011200000,
      "endDate": 1634014800000,
      "presenterIds": []
    },
    {
      "id": 66503,
      "name": "2B - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1634097600000,
      "endDate": 1634101200000,
      "presenterIds": []
    },
    {
      "id": 66504,
      "name": "3B - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1634180400000,
      "endDate": 1634184000000,
      "presenterIds": []
    },
    {
      "id": 66505,
      "name": "1C - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1634040000000,
      "endDate": 1634043600000,
      "presenterIds": []
    },
    {
      "id": 66506,
      "name": "2C - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1634126400000,
      "endDate": 1634130000000,
      "presenterIds": []
    },
    {
      "id": 66507,
      "name": "3C - Social Hour",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66466
      ],
      "startDate": 1634214600000,
      "endDate": 1634218200000,
      "presenterIds": []
    },
    {
      "id": 66582,
      "name": "2B - Ask Me Anything: How to design your thesis statement (and use it in your job talk)",
      "addons": {},
      "typeId": 11752,
      "roomId": 10543,
      "chairIds": [],
      "contentIds": [
        66570
      ],
      "startDate": 1634094000000,
      "endDate": 1634097600000,
      "presenterIds": []
    },
    {
      "id": 66583,
      "name": "1C - Ask Me Anything: Applying for Faculty Positions",
      "addons": {},
      "typeId": 11752,
      "roomId": 10543,
      "chairIds": [],
      "contentIds": [
        66457
      ],
      "startDate": 1634036400000,
      "endDate": 1634040000000,
      "presenterIds": []
    },
    {
      "id": 66602,
      "name": "1C - XR Social Meetup",
      "addons": {},
      "typeId": 11752,
      "roomId": 10543,
      "chairIds": [],
      "contentIds": [
        66596
      ],
      "startDate": 1634040000000,
      "endDate": 1634043600000,
      "presenterIds": []
    },
    {
      "id": 66603,
      "name": "1C - Connect to others with Who2chat!",
      "addons": {},
      "typeId": 11752,
      "roomId": 10544,
      "chairIds": [],
      "contentIds": [
        66461
      ],
      "startDate": 1633982400000,
      "endDate": 1633986000000,
      "presenterIds": []
    },
    {
      "id": 66604,
      "name": "2B - Connect to others with Who2chat!",
      "addons": {},
      "typeId": 11752,
      "roomId": 10544,
      "chairIds": [],
      "contentIds": [
        66461
      ],
      "startDate": 1634101200000,
      "endDate": 1634104800000,
      "presenterIds": []
    },
    {
      "id": 66605,
      "name": "3C - Connect to others with Who2chat!",
      "addons": {},
      "typeId": 11752,
      "roomId": 10544,
      "chairIds": [],
      "contentIds": [
        66461
      ],
      "startDate": 1634214600000,
      "endDate": 1634218200000,
      "presenterIds": []
    },
    {
      "id": 66610,
      "name": "2B - Diversity & Inclusion",
      "addons": {},
      "typeId": 11752,
      "roomId": 10540,
      "chairIds": [],
      "contentIds": [
        66595
      ],
      "startDate": 1634097600000,
      "endDate": 1634101200000,
      "presenterIds": []
    },
    {
      "id": 66611,
      "name": "2C - Diversity & Inclusion",
      "addons": {},
      "typeId": 11752,
      "roomId": 10540,
      "chairIds": [],
      "contentIds": [
        66595
      ],
      "startDate": 1634126400000,
      "endDate": 1634130000000,
      "presenterIds": []
    },
    {
      "id": 66647,
      "name": "Haptics SIG",
      "addons": {},
      "typeId": 11752,
      "roomId": 10542,
      "chairIds": [],
      "contentIds": [
        66594
      ],
      "startDate": 1634214600000,
      "endDate": 1634218200000,
      "presenterIds": []
    },
    {
      "id": 66648,
      "name": "1A - Tech Check",
      "addons": {},
      "typeId": 11752,
      "roomId": 10543,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1633964400000,
      "endDate": 1633977000000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66649,
      "name": "1B - Tech Check",
      "addons": {},
      "typeId": 11752,
      "roomId": 10543,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1633998600000,
      "endDate": 1634005800000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66650,
      "name": "1C - Tech Check",
      "addons": {},
      "typeId": 11752,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1634023800000,
      "endDate": 1634034600000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66651,
      "name": "2A - Tech Check",
      "addons": {},
      "typeId": 11752,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1634052600000,
      "endDate": 1634063400000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66652,
      "name": "2B - Tech Check",
      "addons": {},
      "typeId": 11752,
      "roomId": 10543,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1634081400000,
      "endDate": 1634092200000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66653,
      "name": "2C - Tech Check",
      "addons": {},
      "typeId": 11752,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1634110200000,
      "endDate": 1634121000000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66654,
      "name": "3A - Tech Check",
      "addons": {},
      "typeId": 11752,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1634139000000,
      "endDate": 1634149800000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66655,
      "name": "3B - Tech Check",
      "addons": {},
      "typeId": 11752,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1634171400000,
      "endDate": 1634182200000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66656,
      "name": "3C - Tech Check",
      "addons": {},
      "typeId": 11752,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1634196600000,
      "endDate": 1634207400000,
      "link": {},
      "description": "Extra Open Room for Tech Checks",
      "presenterIds": []
    },
    {
      "id": 66657,
      "name": "Doctoral Symposium",
      "typeId": 11752,
      "chairIds": [],
      "contentIds": [
        66627,
        66622,
        66621,
        66624,
        66623,
        66626,
        66625,
        66620,
        66619
      ],
      "startDate": 1633843800000,
      "endDate": 1633885200000,
      "presenterIds": []
    },
    {
      "id": 66658,
      "name": "2C - FabLunch",
      "addons": {},
      "typeId": 11752,
      "chairIds": [],
      "contentIds": [
        66462
      ],
      "startDate": 1634126400000,
      "endDate": 1634130000000,
      "presenterIds": []
    }
  ],
  "contents": [
    {
      "id": 61321,
      "typeId": 11756,
      "title": "HandyTrak: Recognizing the Holding Hand on a Commodity Smartphone from Body Silhouette Images",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474817"
        },
        "Presentation": {
          "duration": "603",
          "title": "HandyTrak: Recognizing the Holding Hand on a Commodity Smartphone from Body Silhouette Images",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=HvMk_zdELA8"
        },
        "Preview": {
          "duration": "30",
          "title": "HandyTrak: Recognizing the Holding Hand on a Commodity Smartphone from Body Silhouette Images",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hYJb-YnAZdQ"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66268,
        66284
      ],
      "eventIds": [],
      "abstract": "Understanding which hand a user holds a smartphone with can help improve the mobile interaction experience. For instance, the layout of the user interface (UI) can be adapted to the holding hand. In this paper, we present HandyTrak, an AI-powered software system that recognizes the holding hand on a commodity smartphone using body silhouette images captured by the front-facing camera. The silhouette images are processed and sent to a customized user-dependent deep learning model (CNN) to infer how the user holds the smartphone (left, right, or both hands). We evaluated our system on each participant's smartphone at five possible front camera positions in a user study with ten participants under two hand positions (in the middle and skewed) and three common usage cases (standing, sitting, and resting against a desk). The results showed that HandyTrak was able to continuously recognize the holding hand with an average accuracy of 89.03\\% (SD: 8.98\\%) at a 2 Hz sampling rate. We also discuss the challenges and opportunities to deploy HandyTrak on different commodity smartphones and potential applications in real-world scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computing and Information Science"
            }
          ],
          "personId": 61266
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell",
              "dsl": "Computing and Information Science"
            }
          ],
          "personId": 60990
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 61264
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "ITHACA",
              "institution": "Cornell University",
              "dsl": "Computing and Information Science"
            }
          ],
          "personId": 61057
        }
      ]
    },
    {
      "id": 61322,
      "typeId": 11756,
      "title": "HelpViz: Automatic Generation of Contextual Visual Mobile Tutorials from Text-Based Instructions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474812"
        },
        "Presentation": {
          "duration": "466",
          "title": "HelpViz: Automatic Generation of Contextual Visual Mobile Tutorials from Text-Based Instructions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=78NqbNhNimE"
        },
        "Preview": {
          "duration": "30",
          "title": "HelpViz: Automatic Generation of Contextual Visual Mobile Tutorials from Text-Based Instructions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=H6Em9-eAjwI"
        },
        "Video Figure": {
          "duration": "102",
          "title": "HelpViz: Automatic Generation of Contextual Visual Mobile Tutorials from Text-Based Instructions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=fvJjr533cvs"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66271,
        66275
      ],
      "eventIds": [],
      "abstract": "We present HelpViz, a tool for generating contextual visual mobile tutorials from text-based instructions that are abundant on the web. HelpViz transforms text instructions to graphical tutorials in batch, by extracting a sequence of actions from each text instruction through an instruction parsing model, and executing the extracted actions on a simulation infrastructure that manages an array of Android emulators. The automatic execution of each instruction produces a set of graphical and structural assets, including images, videos, and metadata such as clicked elements for each step. HelpViz then synthesizes a tutorial by combining parsed text instructions with the generated assets, and contextualizes the tutorial to user interaction by tracking the user's progress and highlighting the next step. Our experiments with HelpViz indicate that our pipeline improved tutorial execution robustness and that  participants preferred tutorials generated by HelpViz over text-based instructions. HelpViz promises a cost-effective approach for generating contextual visual tutorials for mobile interaction at scale.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 61177
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61202
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61088
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61225
        }
      ]
    },
    {
      "id": 61323,
      "typeId": 11756,
      "title": "Automated Accessory Rigs for Layered 2D Character Illustrations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474809"
        },
        "Presentation": {
          "duration": "472",
          "title": "Automated Accessory Rigs for Layered 2D Character Illustrations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=pq8yr21DP2g"
        },
        "Preview": {
          "duration": "30",
          "title": "Automated Accessory Rigs for Layered 2D Character Illustrations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=u4PjmhQAmwg"
        },
        "Video Figure": {
          "duration": "179",
          "title": "Automated Accessory Rigs for Layered 2D Character Illustrations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=zX9ZrgHAomw"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66250,
        66279
      ],
      "eventIds": [],
      "abstract": "Mix-and-match character creation tools enable users to quickly produce 2D character illustrations by combining various predefined accessories, like clothes and hairstyles, which are represented as separate, interchangeable artwork layers. However, these accessory layers are often designed to fit only the default body artwork, so users cannot modify the body without manually updating all the accessory layers as well. To address this issue, we present a method that captures and preserves important relationships between artwork layers so that the predefined accessories adapt with the character's body. We encode these relationships with four types of constraints that handle common interactions between layers: (1) occlusion, (2) attachment at a point, (3) coincident boundaries, and (4) overlapping regions. A rig is a set of constraints that allow a motion or deformation specified on the body to transfer to the accessory layers. We present an automated algorithm for generating such a rig for each accessory layer, but also allow users to select which constraints to apply to specific accessories. We demonstrate how our system supports a variety of modifications to body shape and pose using artwork from mix-and-match data sets.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 61200
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe",
              "dsl": "Adobe Research"
            }
          ],
          "personId": 61108
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 61270
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 61300
        }
      ]
    },
    {
      "id": 61324,
      "typeId": 11756,
      "title": "Automatic Instructional Video Creation from a Markdown-Formatted Tutorial",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474778"
        },
        "Presentation": {
          "duration": "420",
          "title": "Automatic Instructional Video Creation from a Markdown-Formatted Tutorial",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WmrZ7PUjyuM"
        },
        "Preview": {
          "duration": "30",
          "title": "Automatic Instructional Video Creation from a Markdown-Formatted Tutorial",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=oq4RuqEb6Eg"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66238,
        66246
      ],
      "eventIds": [],
      "abstract": "We introduce HowToCut, an automatic approach that converts a Markdown-formatted tutorial into an interactive video that presents the visual instructions with a synthesized voiceover for narration. HowToCut extracts instructional content from a multimedia document that describes a step-by-step procedure. Our method selects and converts text instructions to a voiceover. It makes automatic editing decisions to align the narration with edited visual assets, including step images, videos, and text overlays. We derive our video editing strategies from an analysis of 125 web tutorials and apply Computer Vision techniques to the assets. To enable viewers to interactively navigate the tutorial, HowToCut's conversational UI presents instructions in multiple formats upon user commands. We evaluated our automatically-generated video tutorials through user studies (N=20) and validated the video quality via an online survey (N=93). The evaluation shows that our method was able to effectively create informative and useful instructional videos from a web tutorial document for both reviewing and following.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61088
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61077
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Bruno",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61147
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Google",
              "dsl": "Google Research"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technolgy",
              "dsl": "College of Computing"
            }
          ],
          "personId": 61289
        }
      ]
    },
    {
      "id": 61325,
      "typeId": 11756,
      "title": "Situated Live Programming for Human-Robot Collaboration",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474773"
        },
        "Presentation": {
          "duration": "553",
          "title": "Situated Live Programming for Human-Robot Collaboration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=FKEwt-F3y78"
        },
        "Preview": {
          "duration": "30",
          "title": "Situated Live Programming for Human-Robot Collaboration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jq9bk3LMdNo"
        },
        "Video Figure": {
          "duration": "304",
          "title": "Situated Live Programming for Human-Robot Collaboration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uQCptZvWDug"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66239,
        66281
      ],
      "eventIds": [],
      "abstract": "We present situated live programming for human-robot collaboration, an approach that enables users with limited programming experience to program collaborative applications for human-robot interaction. Allowing end users, such as shop floor workers, to program collaborative robots themselves would make it easy to “retask” robots from one process to another, facilitating their adoption by small and medium enterprises. Our approach builds on the paradigm of trigger-action programming (TAP) by allowing end users to create rich interactions through simple trigger-action pairings. It enables end users to iteratively create, edit, and refine a reactive robot program while executing partial programs. This live programming approach enables the user to utilize the task space and objects by incrementally specifying situated trigger-action pairs, substantially lowering the barrier to entry for programming or reprogramming robots for collaboration. We instantiate situated live programming in an authoring system where users can create trigger-action programs by annotating an augmented video feed from the robot’s perspective and assign robot actions to trigger conditions. We evaluated this system in a study where participants (n = 10) developed robot programs for solving collaborative light-manufacturing tasks. Results showed that users with little programming experience were able to program HRC tasks in an interactive fashion and our situated live programming approach further supported individualized strategies and workflows. We conclude by discussing opportunities and limitations of the proposed approach, our system implementation, and our study and discuss a roadmap for expanding this approach to a broader range of tasks and applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 61149
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin- Madison",
              "dsl": ""
            }
          ],
          "personId": 61011
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": ""
            }
          ],
          "personId": 61206
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": ""
            }
          ],
          "personId": 60949
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 61072
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 61146
        }
      ]
    },
    {
      "id": 61326,
      "typeId": 11756,
      "title": "X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474782"
        },
        "Presentation": {
          "duration": "471",
          "title": "X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uQZ-PleQ9bY"
        },
        "Preview": {
          "duration": "30",
          "title": "X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=t15ZPjxyZCg"
        },
        "Video Figure": {
          "duration": "185",
          "title": "X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WHsG7fisdqM"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66234,
        66249
      ],
      "eventIds": [],
      "abstract": "X-Rings is a novel hand-mounted 360 degree shape display for Virtual Reality that renders objects in 3D and responds to user-applied touch and grasping force. Designed as a modular stack of motor-driven expandable rings (5.7-7.7 cm diameter), X-Rings renders radially-symmetric surfaces graspable by the user's whole hand. The device is strapped to the palm, allowing the fingers to freely make and break contact with the device. Capacitance sensors and motor current sensing provide estimates of finger touch states and gripping force. We present the results of a user study evaluating participants’ ability to associate device-rendered shapes with visually-rendered objects as well as a demo application that allows users to freely interact with a variety of objects in a virtual environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Mechanical Engineering"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61171
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 60921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61179
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": "Microsoft Research"
            }
          ],
          "personId": 61181
        }
      ]
    },
    {
      "id": 61327,
      "typeId": 11756,
      "title": "SymbolFinder: Brainstorming Diverse Symbols Using Local Semantic Networks",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474757"
        },
        "Presentation": {
          "duration": "532",
          "title": "SymbolFinder: Brainstorming Diverse Symbols Using Local Semantic Networks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=5qO6XF_nwCg"
        },
        "Preview": {
          "duration": "30",
          "title": "SymbolFinder: Brainstorming Diverse Symbols Using Local Semantic Networks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ILEXGrjMn28"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66235,
        66247
      ],
      "eventIds": [],
      "abstract": "Visual symbols are the building blocks for visual communication. They convey abstract concepts like reform and participation quickly and effectively. When creating graphics with symbols, novice designers often struggle to brainstorm multiple, diverse symbols because they fixate on a few associations instead of broadly exploring different aspects of the concept. We present SymbolFinder, an interactive tool for finding visual symbols for abstract concepts. SymbolFinder molds symbol-finding into a recognition rather than recall task by introducing the user to diverse clusters of words associated with the concept. Users can dive into these clusters to find related, concrete objects that symbolize the concept. We evaluate SymbolFinder with two studies: a comparative user study, demonstrating that SymbolFinder helps novices find more unique symbols for abstract concepts with significantly less effort than a popular image database and a case study demonstrating how SymbolFinder helped design students create visual metaphors for three cover illustrations of news articles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 61311
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 61139
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 61090
        }
      ]
    },
    {
      "id": 61328,
      "typeId": 11756,
      "title": "ModularHMD: A Reconfigurable Mobile Head-Mounted Display Enabling Ad-hoc Peripheral Interactions with the Real World",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474738"
        },
        "Presentation": {
          "duration": "569",
          "title": "ModularHMD: A Reconfigurable Mobile Head-Mounted Display Enabling Ad-hoc Peripheral Interactions with the Real World",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=a3TxeANQQVg"
        },
        "Preview": {
          "duration": "30",
          "title": "ModularHMD: A Reconfigurable Mobile Head-Mounted Display Enabling Ad-hoc Peripheral Interactions with the Real World",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=OQgjK7h4Gc8"
        },
        "Video Figure": {
          "duration": "260",
          "title": "ModularHMD: A Reconfigurable Mobile Head-Mounted Display Enabling Ad-hoc Peripheral Interactions with the Real World",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=zVrEkRfo42Y"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66251,
        66259
      ],
      "eventIds": [],
      "abstract": "We propose ModularHMD, a new mobile head-mounted display concept, which adopts a modular mechanism and allows a user to perform ad-hoc peripheral interaction with real-world devices or people during VR experiences. ModularHMD is comprised of a central HMD and three removable module devices installed in the periphery of the HMD cowl. Each module has four main states: occluding, extended VR view, video see-through (VST), and removed/reused. Among different combinations of module states, a user can quickly setup the necessary HMD forms, functions, and real-world visions for ad-hoc peripheral interactions without removing the headset. For instance, an HMD user can see her surroundings by switching a module into the VST mode. She can also physically remove a module to obtain direct peripheral visions of the real world. The removed module can be reused as an instant interaction device (e.g., touch keyboards) for subsequent peripheral interactions. Users can end the peripheral interaction and revert to a full VR experience by re-mounting the module. We design ModularHMD’s configuration and peripheral interactions with real-world objects and people. We also implement a proof-of-concept prototype of ModularHMD to validate its interactions capabilities through a user study. Results show that ModularHMD is an effective solution that enables both immersive VR and ad-hoc peripheral interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 60997
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 61302
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 60979
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Miyagi",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 61165
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 61211
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 60966
        }
      ]
    },
    {
      "id": 61329,
      "typeId": 11756,
      "title": "FaraPy: An Augmented Reality Feedback System for Facial Paralysis using Action Unit Intensity Estimation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474803"
        },
        "Presentation": {
          "duration": "443",
          "title": "FaraPy: An Augmented Reality Feedback System for Facial Paralysis using Action Unit Intensity Estimation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KO0sIsufeSo"
        },
        "Preview": {
          "duration": "30",
          "title": "FaraPy: An Augmented Reality Feedback System for Facial Paralysis using Action Unit Intensity Estimation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1F7j17jU0oo"
        },
        "Video Figure": {
          "duration": "137",
          "title": "FaraPy: An Augmented Reality Feedback System for Facial Paralysis using Action Unit Intensity Estimation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=oHMtfEdkfUY"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66237,
        66243
      ],
      "eventIds": [],
      "abstract": "Facial paralysis is the most common facial nerve disorder. It causes functional and aesthetic deficits that often lead to emotional distress and affect psychosocial well-being.\r\nOne form of treatment is mirror therapy, which has shown potential but has several mirror-related drawbacks that limit its effectiveness.\r\nWe propose FaraPy, the first mobile augmented reality mirror therapy system for facial paralysis that provides real-time feedback and tracks user \r\nprogress over time. We developed an efficient convolutional neural network to detect muscle activations and intensities as users perform facial exercises in front of a mobile device camera.\r\nOur model outperforms the state-of-the-art model on benchmark data for detecting facial action unit intensity. Our user study (n=20) shows high user satisfaction and greater preference for our interactive system over traditional mirror therapy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61176
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 60970
        }
      ]
    },
    {
      "id": 61330,
      "typeId": 11756,
      "title": "Strives: String-based Force Feedback for Automotive Engineering",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474790"
        },
        "Presentation": {
          "duration": "574",
          "title": "Strives: String-based Force Feedback for Automotive Engineering",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PzOnGy5RptQ"
        },
        "Preview": {
          "duration": "30",
          "title": "Strives: String-based Force Feedback for Automotive Engineering",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=c6Iv9W7rCpE"
        },
        "Video Figure": {
          "duration": "239",
          "title": "Strives: String-based Force Feedback for Automotive Engineering",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_7rT4nhcMgM"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66242,
        66249
      ],
      "eventIds": [],
      "abstract": "The large potential of force feedback devices for interacting in Virtual Reality (VR) has been illustrated in a plethora of research prototypes. Yet, these devices are still rarely used in practice and it remains an open challenge how to move this research into practice. To that end, we contribute a participatory design study on the use of haptic feedback devices in the automotive industry. Based on a 10-month observing process with 13 engineers, we developed STRIVE, a string-based haptic feedback device. In addition to the design of STRIVE, this process led to a set of requirements for introducing haptic devices into industrial settings, which center around a need for flexibility regarding forces, comfort, and mobility. We evaluated STRIVE with 16 engineers in five different day-to-day automotive VR use cases. The main results show an increased level of trust and perceived safety as well as further challenges towards moving haptics research into practice.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sindelfingen",
              "institution": "Mercedes-Benz AG",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "VISUS"
            }
          ],
          "personId": 61170
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sindelfingen",
              "institution": "Mercedes-Benz AG",
              "dsl": ""
            }
          ],
          "personId": 60953
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sindelfingen",
              "institution": "Mercedes-Benz AG",
              "dsl": ""
            }
          ],
          "personId": 61296
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Hochschule der Medien - University of Applied Sciences",
              "dsl": "Faculty Information and Communication"
            }
          ],
          "personId": 61189
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "VISUS"
            }
          ],
          "personId": 61142
        }
      ]
    },
    {
      "id": 61331,
      "typeId": 11756,
      "title": "ScaffoldSketch: Accurate Industrial Design Drawing in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474756"
        },
        "Presentation": {
          "duration": "394",
          "title": "ScaffoldSketch: Accurate Industrial Design Drawing in VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=iNNJhuzCAe8"
        },
        "Preview": {
          "duration": "30",
          "title": "ScaffoldSketch: Accurate Industrial Design Drawing in VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=634rFT6m2eI"
        },
        "Video Figure": {
          "duration": "221",
          "title": "ScaffoldSketch: Accurate Industrial Design Drawing in VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qRCtafIhyZY"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66234,
        66244
      ],
      "eventIds": [],
      "abstract": "We present an approach to in-air design drawing based on the two-stage approach common in 2D design drawing practice. The primary challenge to 3D drawing in air is the accuracy of users' strokes. Beautifying or auto-correcting an arbitrary drawing in 2D or 3D is challenging due to ambiguities stemming from many possible interpretations of a stroke. A similar challenge appears when drawing freehand on paper in the real world. 2D design drawing practice (as taught in industrial design school) addresses this by decomposing the process of creating realistic 2D projections of 3D shapes. Designers first create scaffold or construction lines. When drawing shape or structure curves, designers are guided by the scaffolds. Our key insight is that accurate industrial design drawing in 3D becomes tractable when decomposed into auto-correcting scaffold strokes, which have simple relationships with one another, followed by auto-correcting shape strokes with respect to the scaffold strokes. We demonstrate our approach's effectiveness with an expert study involving industrial designers.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 61166
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 61010
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia tech",
              "dsl": ""
            }
          ],
          "personId": 61100
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 60965
        }
      ]
    },
    {
      "id": 61332,
      "typeId": 11756,
      "title": "Evaluating the Effects of Saccade Types and Directions on Eye Pointing Tasks",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474818"
        },
        "Presentation": {
          "duration": "579",
          "title": "Evaluating the Effects of Saccade Types and Directions on Eye Pointing Tasks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YVnMowbuH7Q"
        },
        "Preview": {
          "duration": "30",
          "title": "Evaluating the Effects of Saccade Types and Directions on Eye Pointing Tasks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=dcZlP-UthwI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66272,
        66283
      ],
      "eventIds": [],
      "abstract": "With the portable and affordable gaze input devices being marketed for end users, gaze-based interactions were getting increasingly popular. Unfortunately, the understanding about the dominant task of gaze input, i.e. eye pointing task, was still not sufficient although a performance model had been specifically proposed in previous study because of that 1) the original model was based on a specific circular target condition, without the ability to predict the performance of acquiring conventional rectangular targets and that 2) there was a lack of explanation from the perspective of the anatomical structure of the eyes. In this paper, we proposed a 2D extension to take account of more general target conditions. Carrying out two experiments, we evaluated the effectiveness of the new model and furthermore we found that the index of difficulty that we redefined for 2D eye pointing (IDeye) was able to properly reflect the asymmetrical impacts of target width and height, and consequently the IDeye model could more accurately and properly predict the performance when acquiring 2D targets than Fitts' law, no matter what kind of saccades or eye orientations (i.e. saccadic eye movement directions) was employed to acquire the desired targets. According to the results, we provided more useful implications and recommendations for gaze-based applications.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Renmin University of China",
              "dsl": "School of Information "
            }
          ],
          "personId": 61164
        }
      ]
    },
    {
      "id": 61333,
      "typeId": 11756,
      "title": "ETNA: Harvesting Action Graphs from Websites",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474752"
        },
        "Presentation": {
          "duration": "474",
          "title": "ETNA: Harvesting Action Graphs from Websites",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Cp-Tewc10Co"
        },
        "Preview": {
          "duration": "30",
          "title": "ETNA: Harvesting Action Graphs from Websites",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8Ed2QQ-vbtA"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66271,
        66280
      ],
      "eventIds": [],
      "abstract": "Knowledge bases, such as Google knowledge graph, contain millions of entities (people, places, etc.) and billions of facts about them. While much is known about entities, little is known about the actions these entities relate to. On the other hand, the Web has lots of information about human tasks. A website for restaurant reservations, for example, implicitly knows about various restaurant-related actions (making reservations, delivering food, etc.), the inputs these actions require and their expected output; it can also be automated to execute those actions. To harvest action knowledge from websites, we propose Etna. Users demonstrate how to accomplish various tasks in a website, and Etna constructs an action-state model of the website visualized as an action graph. An action graph includes definitions of tasks and actions, knowledge about their start/end states and execution scripts for their automation. We report on our experience in building action-state models of many commercial websites and use cases that leveraged them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 60993
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 61163
        }
      ]
    },
    {
      "id": 61334,
      "typeId": 11756,
      "title": "Bit Whisperer: Enabling Ad-hoc, Short-range, Walk-Up-and-Share Data Transmissions via Surface-restricted Acoustics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3477980"
        },
        "Presentation": {
          "duration": "505",
          "title": "Bit Whisperer: Enabling Ad-hoc, Short-range, Walk-Up-and-Share Data Transmissions via Surface-restricted Acoustics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8zQGbK82zII"
        },
        "Preview": {
          "duration": "30",
          "title": "Bit Whisperer: Enabling Ad-hoc, Short-range, Walk-Up-and-Share Data Transmissions via Surface-restricted Acoustics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=TX7f5QXtxXM"
        },
        "Video Figure": {
          "duration": "187",
          "title": "Bit Whisperer: Enabling Ad-hoc, Short-range, Walk-Up-and-Share Data Transmissions via Surface-restricted Acoustics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=y0afQ1lE0GQ"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66251,
        66261
      ],
      "eventIds": [],
      "abstract": "Bluetooth requires device pairing to ensure security in data transmission, encumbering a number of ad-hoc, transactional interactions that require both ease-of-use and ``good enough'' security: e.g., sharing contact information or secure links to people nearby.\r\nWe introduce Bit Whisperer, an ad-hoc short-range wireless communication system that enables ``walk up and share'' data transmissions with ``good enough'' security. Bit Whisperer transmits data to proximate devices co-located on a solid surface through high frequency, inaudible acoustic signals. The physical surface has two benefits: it enhances acoustic signal transmission by reflecting sound waves as they propagate; and, it makes the domain of communication visible, helping users identify exactly with whom they are sharing data without prior pairing.\r\nThrough a series of technical evaluations, we demonstrate that Bit Whisperer is robust for common use-cases and secure against likely threats.\r\nWe also implement three example applications to demonstrate the utility of Whisperer: 1-to-1 local contact sharing, 1-to-N private link sharing to open a secure group chat, and 1-to-N local device authentication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61097
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 60998
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Electrical and Computer Engineering"
            }
          ],
          "personId": 60907
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Woodruff School of Mechanical Engineering"
            }
          ],
          "personId": 60947
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Woodruff School of Mechanical Engineering"
            }
          ],
          "personId": 61261
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 61250
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 60973
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Dept. of Electrical and Computer Engineering"
            }
          ],
          "personId": 61297
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61093
        }
      ]
    },
    {
      "id": 61335,
      "typeId": 11756,
      "title": "VHP: Vibrotactile Haptics Platform for On-body Applications",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474772"
        },
        "Presentation": {
          "duration": "551",
          "title": "VHP: Vibrotactile Haptics Platform for On-body Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ksck4Uf4dKQ"
        },
        "Preview": {
          "duration": "30",
          "title": "VHP: Vibrotactile Haptics Platform for On-body Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=6qrksaX_YEo"
        },
        "Video Figure": {
          "duration": "239",
          "title": "VHP: Vibrotactile Haptics Platform for On-body Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=LObvG7znTtw"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66263,
        66266
      ],
      "eventIds": [],
      "abstract": "Wearable vibrotactile devices have many potential applications, including sensory substitution for accessibility and notifications. Currently, vibrotactile experimentation is done using large lab setups. However, most practical applications require standalone on-body devices and integration into small form factors. Such integration is time-consuming and requires expertise. \r\n\r\nWith a goal to democratize wearable haptics we introduce VHP, a vibrotactile haptics platform. It includes a low-power miniature electronics board that can drive up to 12 independent channels of haptic signals with arbitrary waveforms at a 2 kHz sampling rate. The platform can drive vibrotactile actuators including linear resonant actuators and voice coils. The control hardware is battery-powered and programmable, and has multiple input options, including serial and Bluetooth, as well as the ability to synthesize haptic signals internally. We developed current-based loading sensing, thus allowing for unique features such as actuator auto-classification, and skin-contact quality sensing. Our technical evaluations showed that the system met all our initial design criteria and is an improvement over prior methods as it allows all-day wear, has low latency, has battery life between 3 and 25 hours, and can run 12 actuators simultaneously. \r\n\r\nWe demonstrate unique applications that would be time-consuming to develop without the VHP platform. We show that VHP can be used as bracelet, sleeve and phone-case form factors. The bracelet was programmed with an audio-to-tactile interface and was successfully worn for multiple days over months by developers. To facilitate more use of this platform, we open-source our design and plan to make the hardware widely available. We hope this work will motivate the use and study of vibrotactile all-day wearable devices. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61125
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 60916
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 60925
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 60954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": "Research"
            }
          ],
          "personId": 61116
        }
      ]
    },
    {
      "id": 61336,
      "typeId": 11756,
      "title": "reCode: A Lightweight Find-and-Replace Interaction in the IDE for Transforming Code by Example",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474748"
        },
        "Presentation": {
          "duration": "369",
          "title": "reCode: A Lightweight Find-and-Replace Interaction in the IDE for Transforming Code by Example",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_GQ8E7EMMws"
        },
        "Preview": {
          "duration": "30",
          "title": "reCode: A Lightweight Find-and-Replace Interaction in the IDE for Transforming Code by Example",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=fMdHK9UrgQ4"
        },
        "Video Figure": {
          "duration": "127",
          "title": "reCode: A Lightweight Find-and-Replace Interaction in the IDE for Transforming Code by Example",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nN2iHKePYy8"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66270,
        66281
      ],
      "eventIds": [],
      "abstract": "Software developers frequently confront a recurring challenge of making systematic code transformations---similar but not entirely identical code changes in many places---in their integrated development environments. Through formative interviews (n=7), we found that developers were aware of many tools intended to help with code transformations, but often made their changes manually because these tools required too much expertise or effort to be able to use effectively. To address these needs, we built an extension for Visual Studio Code, called reCode. reCode improves the familiar find-and-replace experience by allowing the developer to specify a straightforward search term to identify relevant locations, and then demonstrate their intended changes by simply typing a change directly in the editor. Using programming by example, reCode automatically learns a more general code transformation and displays these transformations as before-and-after differences inline, with clickable actions to interactively accept, reject, or refine the proposed changes. In our usability study (n=12), developers reported that this mixed-initiative, example-driven experience is intuitive, complements their existing workflow, and offers a unified approach to conveniently tackle a variety of common yet frustrating scenarios for code transformations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Institute for Software Research"
            }
          ],
          "personId": 61247
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 61217
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            }
          ],
          "personId": 61159
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 60999
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 61031
        }
      ]
    },
    {
      "id": 61337,
      "typeId": 11756,
      "title": "GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474769"
        },
        "Presentation": {
          "duration": "466",
          "title": "GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_y5wPQVBmJQ"
        },
        "Preview": {
          "duration": "30",
          "title": "GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=DYlX4wzBTsk"
        },
        "Video Figure": {
          "duration": "303",
          "title": "GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PJhGGtj6q9o"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66234,
        66244
      ],
      "eventIds": [],
      "abstract": "The freehand gesture is an essential input modality for modern Augmented Reality (AR) user experiences. However, developing AR applications with customized hand interactions remains a challenge for end-users. Therefore, we propose GesturAR, an end-to-end authoring tool that supports users to create in-situ freehand AR applications through embodied demonstration and visual programming. During authoring, users can intuitively demonstrate the customized gesture inputs while referring to the spatial and temporal context. Based on the taxonomy of gestures in AR, we proposed a hand interaction model which maps the gesture inputs to the reactions of the AR contents. Thus, users can author comprehensive freehand applications using trigger-action visual programming and instantly experience the results in AR. Further, we demonstrate multiple application scenarios enabled by GesturAR, such as interactive virtual objects, robots, and avatars, room-level interactive AR spaces, embodied AR presentations, etc. Finally, we evaluate the performance and usability of GesturAR through a user study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 60930
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 61048
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette ",
              "institution": "Purdue University",
              "dsl": "School of Electrical and Computer Engineering"
            }
          ],
          "personId": 61260
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette ",
              "institution": "Purdue University ",
              "dsl": "School of Mechanical Engineering "
            }
          ],
          "personId": 60910
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "C Design Lab"
            }
          ],
          "personId": 61014
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 60931
        }
      ]
    },
    {
      "id": 61338,
      "typeId": 11756,
      "title": "Relevance and Applicability of Hardware-independent Pointing Transfer Functions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474767"
        },
        "Presentation": {
          "duration": "481",
          "title": "Relevance and Applicability of Hardware-independent Pointing Transfer Functions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Ih6p0Zrkces"
        },
        "Preview": {
          "duration": "30",
          "title": "Relevance and Applicability of Hardware-independent Pointing Transfer Functions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VcyPOyyFrhI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66267,
        66278
      ],
      "eventIds": [],
      "abstract": "Pointing transfer functions remain predominantly expressed in pixels per input counts, which can generate different visual pointer behaviors with different input and output devices; we show in a first controlled experiment that even small hardware differences impact pointing performance with functions defined in this manner. We also demonstrate the applicability of \"hardware-independent'' transfer functions defined in physical units. \r\nWe explore two methods to maintain hardware-independent pointer performance in operating systems that require hardware-dependent definitions: scaling them to the resolutions of the input and output devices, or selecting the OS acceleration setting that produces the closest visual behavior. In a second controlled experiment, we adapted a baseline function to different screen and mouse resolutions using both methods, and the resulting functions provided equivalent performance. Lastly, we provide a tool to calculate equivalent transfer functions between hardware setups, allowing users to match pointer behavior with different devices, and researchers to tune and replicate experiment conditions. Our work emphasizes, and hopefully facilitates, the idea that operating systems should have the capability to formulate pointing transfer functions in physical units, and to adjust them automatically to hardware setups.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Inria",
              "dsl": "Loki team"
            }
          ],
          "personId": 61029
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Inria",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 61114
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Université de Lille",
              "dsl": "UMR 9189 - CRIStAL"
            },
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Inria",
              "dsl": ""
            }
          ],
          "personId": 61007
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Villeneuve-d'Ascq",
              "institution": "Inria Lille - Nord Europe",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Univ. Lille, UMR 9189 - CRIStAL",
              "dsl": ""
            }
          ],
          "personId": 61271
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Inria",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "University of Lille",
              "dsl": "CRIStAL -- UMR 9189"
            }
          ],
          "personId": 61284
        }
      ]
    },
    {
      "id": 61339,
      "typeId": 11756,
      "title": "Midair Balloon Interface: A Soft and Lightweight Midair Object for Proximate Interactions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474786"
        },
        "Presentation": {
          "duration": "506",
          "title": "Midair Balloon Interface: A Soft and Lightweight Midair Object for Proximate Interactions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=d-Vqkf0KhCE"
        },
        "Preview": {
          "duration": "30",
          "title": "Midair Balloon Interface: A Soft and Lightweight Midair Object for Proximate Interactions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=HPYi-YWVaeY"
        },
        "Video Figure": {
          "duration": "93",
          "title": "Midair Balloon Interface: A Soft and Lightweight Midair Object for Proximate Interactions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=brzwyvltFu8"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66242,
        66262
      ],
      "eventIds": [],
      "abstract": "This paper introduces a midair balloon interface, a fast and soft interactive object in mid-air. Our approach tackles the trade-off between safety and speed by controlling a soft helium-filled balloon with external actuators and sensors. We developed a prototype system that uses airborne ultrasound phased arrays to propel a balloon and high-speed stereo cameras to track its motion. This configuration realizes both a high thrust/weight ratio and such a soft body that is safe-to-collide. We describe a sight-based interaction and a touch-based interaction that leverage the safety and speed of a midair balloon interface. A sight-based interaction allows the user to keep the object inside her/his view within reach by controlling a balloon to follow the direction of the user's face. A touch-based interaction allows the user to manipulate the object directly with his/her hand, issue a command by moving his/her finger on the surface, and receive vibrotactile feedback produced by vibrating the balloon with amplitude-modulated ultrasound. We describe the implementation and evaluation of the prototype and explore the application scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Frontier Sciences"
            }
          ],
          "personId": 61083
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": "Frontier Sciences"
            }
          ],
          "personId": 61301
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": "Frontier Sciences"
            }
          ],
          "personId": 61150
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": "Frontier Sciences"
            }
          ],
          "personId": 61184
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": "Frontier Sciences"
            }
          ],
          "personId": 61304
        }
      ]
    },
    {
      "id": 61340,
      "typeId": 11756,
      "title": "Snowy: Recommending Utterances for Conversational Visual Analysis",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474792"
        },
        "Presentation": {
          "duration": "603",
          "title": "Snowy: Recommending Utterances for Conversational Visual Analysis",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JGg5w0MsIDc"
        },
        "Preview": {
          "duration": "30",
          "title": "Snowy: Recommending Utterances for Conversational Visual Analysis",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uFX1zVTNKDQ"
        },
        "Video Figure": {
          "duration": "304",
          "title": "Snowy: Recommending Utterances for Conversational Visual Analysis",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=t8fleP2OvrI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66254,
        66264
      ],
      "eventIds": [],
      "abstract": "Natural language interfaces (NLIs) have become a prevalent medium for conducting visual data analysis, enabling people with varying levels of analytic experience to ask questions of and interact with their data. While there have been notable improvements with respect to language understanding capabilities in these systems, fundamental user experience and interaction challenges including the lack of analytic guidance (i.e., knowing what aspects of the data to consider) and discoverability of natural language input (i.e., knowing how to phrase input utterances) persist. To address these challenges, we investigate utterance recommendations that contextually provide analytic guidance by suggesting data features (e.g., attributes, values, trends) while implicitly making users aware of the types of phrasings that an NLI supports. We present SNOWY, a prototype system that generates and recommends utterances for visual analysis based on a combination of data interestingness metrics and language pragmatics. Through a preliminary user study, we found that utterance recommendations in SNOWY support conversational visual analysis by guiding the participants' analytic workflows and making them aware of the system's language interpretation capabilities. Based on the feedback and observations from the study, we discuss potential implications and considerations for incorporating recommendations in future NLIs for visual analysis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 61256
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 60981
        }
      ]
    },
    {
      "id": 61341,
      "typeId": 11756,
      "title": "PocketView: Through-Fabric Information Displays",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474766"
        },
        "Presentation": {
          "duration": "430",
          "title": "PocketView: Through-Fabric Information Displays",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7VpRMvzLH8o"
        },
        "Preview": {
          "duration": "30",
          "title": "PocketView: Through-Fabric Information Displays",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=FRnzNRt5UxU"
        },
        "Video Figure": {
          "duration": "242",
          "title": "PocketView: Through-Fabric Information Displays",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=IQ1f2N0jcTk"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66251,
        66260
      ],
      "eventIds": [],
      "abstract": "People often have to remove their phone from an inaccessible location like a pocket to view things like notifications and directions. We explore the idea of viewing such information through the fabric of a pocket using low resolution bright LED matrix displays. A survey confirms viewing information on inaccessible phones is desirable, and establishes types of pockets in garments worn by respondents and what objects are typically put in pockets. A technical evaluation validates that LED light can shine through many common garment fabrics. Based on these results, functional hardware prototypes are constructed to demonstrate different form factors of through-fabric display devices, such as a phone, wallet, a key fob, a pen, and earbud headphone case. A simple interaction vocabulary for viewing key information on these devices is described, and the social and technical aspects of the approach are discussed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 61267
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            }
          ],
          "personId": 61312
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 61055
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            }
          ],
          "personId": 61197
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 61053
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 61059
        }
      ]
    },
    {
      "id": 61342,
      "typeId": 11756,
      "title": "TouchPose: Hand Pose Prediction, Depth Estimation, and Touch Classification from Capacitive Images",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474801"
        },
        "Presentation": {
          "duration": "292",
          "title": "TouchPose: Hand Pose Prediction, Depth Estimation, and Touch Classification from Capacitive Images",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=73bKS0i5TRA"
        },
        "Preview": {
          "duration": "30",
          "title": "TouchPose: Hand Pose Prediction, Depth Estimation, and Touch Classification from Capacitive Images",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=CGnS18uGwC4"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66253,
        66284
      ],
      "eventIds": [],
      "abstract": "Today’s touchscreen devices commonly detect the coordinates of user input through capacitive sensing. Yet, these coordinates are the mere 2D manifestations of the more complex 3D configuration of the whole hand—a sensation that touchscreen devices so far remain oblivious to. In this work, we introduce the problem of reconstructing a 3D hand skeleton from capacitive images, which encode the sparse observations captured by touch sensors. These low-resolution images represent intensity mappings that are proportional to the distance to the user’s fingers and hands. \r\n\r\nWe present the first dataset of capacitive images with corresponding depth maps and 3D hand pose coordinates, comprising 65,374 aligned records from 10 participants. We introduce our supervised method TouchPose, which learns a 3D hand model and a corresponding depth map using a cross-modal trained embedding from capacitive images in our dataset. We quantitatively evaluate TouchPose’s accuracy in touch classification, depth estimation, and 3D joint reconstruction, showing that our model generalizes to hand poses it has never seen during training and can infer joints that lie outside the touch sensor’s volume.\r\n\r\nEnabled by TouchPose, we demonstrate a series of interactive apps and novel interactions on multitouch devices. These applications show TouchPose’s versatile capability to serve as a general purpose model, operating independent of use-case, and establishing 3D hand pose as an integral part of the input dictionary for application designers and developers. We also release our dataset, code, and model to enable future work in this domain.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 61081
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61208
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61028
        }
      ]
    },
    {
      "id": 61343,
      "typeId": 11756,
      "title": "Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474763"
        },
        "Presentation": {
          "duration": "512",
          "title": "Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=AuFMhUJZhA4"
        },
        "Preview": {
          "duration": "30",
          "title": "Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=d3y_p4eUecU"
        },
        "Video Figure": {
          "duration": "155",
          "title": "Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7sWsjV8EbFE"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66271,
        66275
      ],
      "eventIds": [],
      "abstract": "Automated understanding of user interfaces (UIs) from their pixels can improve  accessibility, enable task automation, and facilitate interface design without relying on developers to comprehensively provide metadata.\r\nA first step is to infer what UI elements exist on a screen, but current approaches are limited in how they infer how those elements are semantically grouped into structured interface definitions.\r\nIn this paper, we motivate the problem of screen parsing, the task of predicting UI elements and their relationships from a screenshot.\r\nWe describe our implementation of screen parsing and provide an effective training procedure that optimizes its performance.\r\nIn an evaluation comparing the accuracy of the generated output, we find that our implementation significantly outperforms current systems (up to 23%).\r\nFinally, we show three example applications that are facilitated by screen parsing: (i) UI similarity search, (ii) accessibility enhancement, and (iii) code generation from UI screenshots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 61215
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Apple Inc",
              "dsl": ""
            }
          ],
          "personId": 61174
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Cupertino",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 61230
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Cupertino",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 61307
        }
      ]
    },
    {
      "id": 61344,
      "typeId": 11756,
      "title": "SoundsRide: Affordance-Synchronized Music Mixing for In-Car Audio Augmented Reality",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474739"
        },
        "Presentation": {
          "duration": "488",
          "title": "SoundsRide: Affordance-Synchronized Music Mixing for In-Car Audio Augmented Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=11h3OIBw2lw"
        },
        "Preview": {
          "duration": "30",
          "title": "SoundsRide: Affordance-Synchronized Music Mixing for In-Car Audio Augmented Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=tRqZnFnS6d4"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66242,
        66249
      ],
      "eventIds": [],
      "abstract": "Music is a central instrument in video gaming to attune a player's attention to the current atmosphere and increase their immersion in the game. \r\nWe transfer the idea of scene-adaptive music to car drives and propose SoundsRide, an in-car audio augmented reality system that mixes music in real-time synchronized with sound affordances along the ride.\r\nAfter exploring the design space of affordance-synchronized music, we design SoundsRide to temporally and spatially align high-contrast events on the route, e.g., highway entrances or tunnel exits, with high-contrast events in music, e.g., song transitions or beat drops, for any recorded and annotated GPS trajectory by a three-step procedure.\r\nIn real-time, SoundsRide 1) estimates temporal distances to events on the route, 2) fuses these novel estimates with previous estimates in a cost-aware music-mixing plan, and 3) if necessary, re-computes an updated mix to be propagated to the audio output. \r\nTo minimize user-noticeable updates to the mix, SoundsRide fuses new distance information with a filtering procedure that chooses the best updating strategy given the last music-mixing plan, the novel distance estimations, and the system parameterization.\r\nWe technically evaluate SoundsRide and conduct a user evaluation with 8 participants to gain insights into how users perceive SoundsRide in terms of mixing, affordances, and synchronicity. We find that SoundsRide can create captivating music experiences and positively as well as negatively influence subjectively perceived driving safety, depending on the mix and user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Dr. Ing. h. c. F. Porsche AG",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "Chair for Integrated Information Systems"
            }
          ],
          "personId": 61201
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Porsche AG",
              "dsl": ""
            }
          ],
          "personId": 60975
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Porsche AG",
              "dsl": ""
            }
          ],
          "personId": 61130
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Porsche AG",
              "dsl": ""
            }
          ],
          "personId": 60977
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Essen",
              "institution": "Institute for Computer Science and Business Information Systems",
              "dsl": "University Duisburg-Essen"
            }
          ],
          "personId": 61204
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61028
        }
      ]
    },
    {
      "id": 61345,
      "typeId": 11756,
      "title": "Variance and Distribution Models for Steering Tasks",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474811"
        },
        "Presentation": {
          "duration": "552",
          "title": "Variance and Distribution Models for Steering Tasks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=unPW2waw0kQ"
        },
        "Preview": {
          "duration": "30",
          "title": "Variance and Distribution Models for Steering Tasks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=b6a5rDmxRjk"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66252,
        66278
      ],
      "eventIds": [],
      "abstract": "Steering law reveals a linear relationship between the movement time MT and the index of difficulty ID in trajectory-based steering tasks. However, it does not relate the variance or distribution of MT to ID. In this paper, we propose and evaluate models that predict the variance and distribution of MT based on ID for steering tasks. We first propose a quadratic variance model which reveals that the variance of MT is quadratically related to ID with the linear coefficient being 0. Empirical evaluation on a new and a previously collected dataset show that the quadratic variance model accounts for between 78% and 97% of variance of observed MT variances; it outperforms other model candidates such as linear and constant models; adding the linear coefficient leads to no improvement on the model fitness. The variance model enables predicting the distribution of MT given ID:  we can use the variance model to predict the variance (or scale) parameter and Steering law to predict the mean (or location) parameter of a distribution. We have evaluated six types of distributions for predicting the distribution of MT.  Our investigation also shows that positively skewed distribution such as Gamma, Lognormal, Exponentially Modified Gaussian (ExGaussian), and Extreme value distributions outperformed the symmetric distribution such as Gaussian and truncated Gaussian distribution in predicting the MT distribution, and Gamma distribution performed slightly better than other positively skewed distributions. Overall, our research advances the MT prediction of steering tasks from a point estimate to variance and distribution estimates, which provides a more complete understanding of steering behavior and quantifies the uncertainty of MT prediction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland - College Park",
              "dsl": ""
            }
          ],
          "personId": 61162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61168
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Capital University of Economics and Business",
              "dsl": "School of Management and Engineering"
            }
          ],
          "personId": 61169
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kochi",
              "city": "Kami",
              "institution": "Kochi University of Technology",
              "dsl": "School of Information"
            }
          ],
          "personId": 61183
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61281
        }
      ]
    },
    {
      "id": 61346,
      "typeId": 11756,
      "title": "Roadkill: Nesting Laser-Cut Objects for Fast Assembly",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474799"
        },
        "Presentation": {
          "duration": "416",
          "title": "Roadkill: Nesting Laser-Cut Objects for Fast Assembly",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nynWnvwA6Vc"
        },
        "Preview": {
          "duration": "30",
          "title": "Roadkill: Nesting Laser-Cut Objects for Fast Assembly",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=zvoX52dMYxs"
        },
        "Video Figure": {
          "duration": "289",
          "title": "Roadkill: Nesting Laser-Cut Objects for Fast Assembly",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0_lZJ15lnqw"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66241,
        66245
      ],
      "eventIds": [],
      "abstract": "We present Roadkill, a software tool that converts 3D models to 2D cutting plans for laser cutting—such that the resulting layouts allow for fast assembly. Roadkill achieves this by putting all relevant information into the cutting plan: (1) Thumbnails indicate which area of the model a set of parts belongs to. (2) Parts with exposed finger joints are easy to access, thereby suggesting to start assembly here. (3) Openings in the sheet act as jigs, affording assembly within the sheet. (4) Users continue assembly by inserting what has already been assembled into parts that are immediately adjacent or are pointed to by arrows. Roadkill maximizes the number of joints rendered in immediate adjacency by breaking down models into “subassemblies.” Within a subassembly, Roadkill holds the parts together using break-away tabs. (5) Users complete subassemblies according to their labels 1, 2, 3…, following 1  > 1 links to insert subassemblies into other subassemblies, until all parts come together. In our user study, Roadkill allowed participants to assemble layouts 2.4 times faster than layouts generated by a traditional pair-wise labeling of plates.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61138
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso-Plattner-Institute",
              "dsl": ""
            }
          ],
          "personId": 60942
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 61303
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institut",
              "dsl": ""
            }
          ],
          "personId": 60960
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "University of Potsdam",
              "dsl": "Hasso Plattner Institute"
            }
          ],
          "personId": 60927
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61023
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60955
        }
      ]
    },
    {
      "id": 61347,
      "typeId": 11756,
      "title": "MedKnowts: Unified Documentation and Information Retrievalfor Electronic Health Records",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474814"
        },
        "Presentation": {
          "duration": "473",
          "title": "MedKnowts: Unified Documentation and Information Retrievalfor Electronic Health Records",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7UmeI6c54P0"
        },
        "Preview": {
          "duration": "30",
          "title": "MedKnowts: Unified Documentation and Information Retrievalfor Electronic Health Records",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uqFOsPuRiS8"
        },
        "Video Figure": {
          "duration": "270",
          "title": "MedKnowts: Unified Documentation and Information Retrievalfor Electronic Health Records",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=IHyqkKHTsb8"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66250,
        66265
      ],
      "eventIds": [],
      "abstract": "Clinical documentation can be transformed by Electronic Health Records, yet the documentation process is still a tedious, time-consuming, and error-prone process. Clinicians are faced with multi-faceted requirements and fragmented interfaces for information exploration and documentation. These challenges are only exacerbated in the Emergency Department---clinicians often see 35 patients in one shift, during which they have to synthesize an often previously unknown patient’s medical records in order to reach a tailored diagnosis and treatment plan. To better support this information synthesis, clinical documentation tools must enable rapid contextual access to the patient’s medical record. MedKnowts is an integrated note-taking editor and information retrieval system which unifies the documentation and search process and provides concise synthesized concept-oriented slices of the patient’s medical record. MedKnowts automatically captures structured data while still allowing users the flexibility of natural language. MedKnowts leverages this structure to enable easier parsing of long notes, auto-populated text, and proactive information retrieval, easing the documentation burden.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "CSAIL",
              "dsl": "Haystack"
            }
          ],
          "personId": 61154
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Clinical Machine Learning Group"
            }
          ],
          "personId": 61025
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Clinical Machine Learning Group"
            }
          ],
          "personId": 61018
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "BIDMC",
              "dsl": ""
            }
          ],
          "personId": 61070
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Clinical Machine Learning Group"
            }
          ],
          "personId": 60976
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Haystack Group"
            }
          ],
          "personId": 61280
        }
      ]
    },
    {
      "id": 61348,
      "typeId": 11756,
      "title": "AutoAssembler: Automatic Reconstruction of Laser-Cut 3D Models ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474776"
        },
        "Presentation": {
          "duration": "491",
          "title": "AutoAssembler: Automatic Reconstruction of Laser-Cut 3D Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=kl79ENYbmgk"
        },
        "Preview": {
          "duration": "30",
          "title": "AutoAssembler: Automatic Reconstruction of Laser-Cut 3D Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=6xAwMb0rTuo"
        },
        "Video Figure": {
          "duration": "330",
          "title": "AutoAssembler: Automatic Reconstruction of Laser-Cut 3D Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=aEX2en9fRfo"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66241,
        66245
      ],
      "eventIds": [],
      "abstract": "Recent research showed how to import laser cut 3D models encoded in the form of 2D cutting plans into a 3D editor (assembler^3), which allows users to perform parametric manipulations on such models. In contrast to assembler^3 , which requires users to perform this process manually, we present autoAssembler, which performs this process automatically. AutoAssembler uses a beam search algorithm to search possible ways of assembling plates. It uses joints on these plates to combine them into assembly candidates. It thereby preferably pursues candidates (1) that have no intersecting plates, (2) that fit into a small bounding box, (3) that use plates whose joints fit together well, (4) that do not add many unpaired joints, (5) that make use of constraints posed by other plates, and (6) that conform to symmetry axes of the plates. This works for models that have at least one edge joint (finger or t-joint). In our technical evaluation, we imported 66 models using autoAssembler. AutoAssembler assembled 79% of those models fully automatically; another 18% of models required on average 2.7 clicks of post-processing, for an overall success rate of 97%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61023
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61115
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61046
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61120
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61182
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61303
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61127
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60955
        }
      ]
    },
    {
      "id": 61349,
      "typeId": 11756,
      "title": "Identifying Contact Fingers on Touch Sensitive Surfaces by Ring-Based Vibratory Communication",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474745"
        },
        "Presentation": {
          "duration": "347",
          "title": "Identifying Contact Fingers on Touch Sensitive Surfaces by Ring-Based Vibratory Communication",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qqDjjy-rrSQ"
        },
        "Preview": {
          "duration": "30",
          "title": "Identifying Contact Fingers on Touch Sensitive Surfaces by Ring-Based Vibratory Communication",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8mSVkcOlmzU"
        },
        "Video Figure": {
          "duration": "239",
          "title": "Identifying Contact Fingers on Touch Sensitive Surfaces by Ring-Based Vibratory Communication",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0W5vaDaKaMc"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66272,
        66283
      ],
      "eventIds": [],
      "abstract": "As computing paradigms shift toward mobile and ubiquitous interaction, there is an increasing demand for wearable interfaces supporting multifaceted input in smart living environments. In this regard, we introduce a system that identifies contact fingers using vibration as a modality of communication. We investigate the vibration characteristics of the communication channels involved and simulate the transmission of vibration sequences. In the simulation, we test and refine modulation and demodulation methods to design vibratory communication protocols that are robust to environmental noises and can detect multiple simultaneous contact fingers. As a result, we encode an on-off keying sequence with a unique carrier frequency to each finger and demodulate the sequences by applying cross-correlation. We verify the communication protocols in two environments, laboratory and cafe, where the resulting highest accuracy was 93 % and 90.5 %, respectively. Our system achieves over 91 % accuracy in identifying seven contact states from three fingers while wearing only two actuator rings with the aid of a touch screen. Our findings shed light on diversifying touch interactions on rigid surfaces by means of vibratory communication. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology (POSTECH)",
              "dsl": "Interaction Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology (POSTECH)",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 61194
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyungsangbuk",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology",
              "dsl": "Interaction Laboratory"
            }
          ],
          "personId": 61134
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Electrical Engineering"
            }
          ],
          "personId": 61141
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongbuk",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology (POSTECH)",
              "dsl": "Computer Science and Engineering / Interaction Laboratory "
            }
          ],
          "personId": 61119
        }
      ]
    },
    {
      "id": 61350,
      "typeId": 11756,
      "title": "Hierarchical Summarization for Longform Spoken Dialog",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474771"
        },
        "Presentation": {
          "duration": "338",
          "title": "Hierarchical Summarization for Longform Spoken Dialog",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=3H17Pyr4vag"
        },
        "Preview": {
          "duration": "30",
          "title": "Hierarchical Summarization for Longform Spoken Dialog",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=-s3L31uT8YA"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66235,
        66240
      ],
      "eventIds": [],
      "abstract": "Every day we are surrounded by spoken dialog. This medium delivers rich diverse streams of information auditorily; however, systematically understanding dialog can often be non-trivial. Despite the pervasiveness of spoken dialog, automated speech understanding and quality information extraction remains markedly poor, especially when compared to written prose. Furthermore, compared to understanding text, auditory communication poses many additional challenges such as speaker disfluencies, informal prose styles, and lack of structure. These concerns all demonstrate the need for a distinctly speech tailored interactive system to help users understand and navigate the spoken language domain. While individual automatic speech recognition (ASR) and text summarization methods already exist, they are imperfect technologies; neither consider user purpose and intent nor address spoken language induced complications. Consequently, we design a two stage ASR and text summarization pipeline and propose a set of semantic segmentation and merging algorithms to resolve these speech modeling challenges. Our system enables users to easily browse and navigate content as well as recover from errors in these underlying technologies. Finally, we present an evaluation of the system which highlights user preference for hierarchical summarization as a tool to quickly skim audio and identify content of interest to the user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 61056
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 60906
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 60936
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 61090
        }
      ]
    },
    {
      "id": 61351,
      "typeId": 11756,
      "title": "Umitation: Retargeting UI Behavior Examples for Website Design",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474796"
        },
        "Presentation": {
          "duration": "435",
          "title": "Umitation: Retargeting UI Behavior Examples for Website Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=DjITGNAwWGc"
        },
        "Preview": {
          "duration": "30",
          "title": "Umitation: Retargeting UI Behavior Examples for Website Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GdiwJaJ59sQ"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66271,
        66280
      ],
      "eventIds": [],
      "abstract": "Interface designers often refer to UI behavior examples found in the wild (e.g., commercial websites) for reference or design inspiration. While past research has looked at retargeting interface and webpage design, limited work has explored the challenges in retargeting interactive visual behaviors. We introduce Umitation, a system that helps designers extract, edit, and adapt example front-end UI behaviors to target websites. Umitation can also help designers specify the desired behaviors and reconcile their intended interaction details with their existing UI. In a qualitative evaluation, we found evidence that Umitation helps participants extract and retarget dynamic front-end UI behavior examples quickly and expressively.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61144
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61062
        }
      ]
    },
    {
      "id": 61352,
      "typeId": 11756,
      "title": "Just Speak It: Minimize Cognitive Load for Text Editing with a Smart Voice Assistant",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474795"
        },
        "Presentation": {
          "duration": "430",
          "title": "Just Speak It: Minimize Cognitive Load for Text Editing with a Smart Voice Assistant",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jrXTRfpBYSA"
        },
        "Preview": {
          "duration": "30",
          "title": "Just Speak It: Minimize Cognitive Load for Text Editing with a Smart Voice Assistant",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=RdVGDatWl9U"
        },
        "Video Figure": {
          "duration": "60",
          "title": "Just Speak It: Minimize Cognitive Load for Text Editing with a Smart Voice Assistant",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=BP_d6YyCbn8"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66269,
        66282
      ],
      "eventIds": [],
      "abstract": "Entering text precisely by voice, users might encounter colloquial inserts, inappropriate wording, and recognition errors, which brings difficulties to voice editing. Users need to locate the errors and then correct them. In eyes-free scenarios, this select-modify mode brings a cognitive burden and a risk of error. This paper introduces neural networks and pre-trained models to understand users' revision intention based on semantics, reducing the need for the information from users' statements. We present two strategies. One is to remove the colloquial inserts automatically. The other is to allow users to edit by just speaking out the target words without having to say the context and the incorrect text. Accordingly, our approach can predict whether to insert or replace, the incorrect text to replace, and the position to insert. We implement these strategies in SmartEdit, an eyes-free voice input agent controlled with earphone buttons. The evaluation shows that our techniques reduce the cognitive load and decrease the average failure rate by 54.1\\% compared to descriptive command or re-speaking.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 61219
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 61047
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 60951
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 61019
        }
      ]
    },
    {
      "id": 61353,
      "typeId": 11756,
      "title": "Sporq: An Interactive Environment for Exploring Code Using Query-by-Example",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474737"
        },
        "Presentation": {
          "duration": "604",
          "title": "Sporq: An Interactive Environment for Exploring Code Using Query-by-Example",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=axssrecq99U"
        },
        "Preview": {
          "duration": "30",
          "title": "Sporq: An Interactive Environment for Exploring Code Using Query-by-Example",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=M2982bFNGbk"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66270,
        66281
      ],
      "eventIds": [],
      "abstract": "There has been widespread adoption of IDEs and powerful tools for program analysis.\r\nHowever, programmers still find it difficult to conveniently analyze their code for custom patterns.\r\nSuch systems either provide inflexible interfaces or require\r\nknowledge of complex query languages and compiler internals. In this paper, we present \\tool, a tool that allows\r\ndevelopers to mine their codebases for a range of patterns, including bugs, code smells, and violations of coding\r\nstandards. \\tool{} offers an interactive environment in which the user highlights program elements, and the system\r\nresponds by identifying other parts of the codebase with similar patterns. The programmer can then provide feedback\r\nwhich enables the system to rapidly infer the programmer's intent. Internally, our system is driven by high-fidelity\r\nrelational program representations and algorithms to synthesize database queries from examples. Our experiments and user\r\nstudies with a VS Code extension indicate that \\tool{}\r\nreduces the effort needed by programmers to write custom analyses and discover bugs in large codebases.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "Computer and Information Science"
            }
          ],
          "personId": 61213
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": ""
            }
          ],
          "personId": 61002
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 60917
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": ""
            }
          ],
          "personId": 60971
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": ""
            }
          ],
          "personId": 61095
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 60908
        }
      ]
    },
    {
      "id": 61354,
      "typeId": 11756,
      "title": "Per Garment Capture and Synthesis for Real-time Virtual Try-on",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474762"
        },
        "Presentation": {
          "duration": "557",
          "title": "Per Garment Capture and Synthesis for Real-time Virtual Try-on",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vh9waquHevA"
        },
        "Preview": {
          "duration": "30",
          "title": "Per Garment Capture and Synthesis for Real-time Virtual Try-on",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2nkEQlU3WOc"
        },
        "Video Figure": {
          "duration": "199",
          "title": "Per Garment Capture and Synthesis for Real-time Virtual Try-on",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=bLSiQAOlh-Q"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66273,
        66282
      ],
      "eventIds": [],
      "abstract": "Virtual try-on is a promising application of computer graphics and human computer interaction that can have a profound real-world impact especially during this pandemic.\r\nExisting image-based works try to synthesize a try-on image from a single image of a target garment, but it inherently limits the ability to react to possible interactions.\r\nIt is difficult to reproduce the change of\r\nwrinkles caused by pose and body size change, as well as pulling and stretching of the garment by hand.\r\nIn this paper, we propose an alternative per garment capture and synthesis workflow to handle such rich interactions by training the model with many systematically captured images.\r\nOur workflow is composed of two parts: garment capturing and clothed person image synthesis.\r\nWe designed an actuated mannequin and an efficient capturing process that collects the detailed deformations of the target garments under diverse body sizes and poses.\r\nFurthermore, we proposed to use a custom-designed measurement garment, and we captured paired images of the measurement garment and the target garments.\r\nWe then learn a mapping between the measurement garment and the target garments using deep image-to-image translation.\r\nThe customer can then try on the target garments interactively during online shopping.\r\nThe proposed workflow requires certain manual labor, but we believe that the cost is acceptable given that the retailers are already paying significant costs for hiring professional photographers and models, stylists, and editors to take photographs for promotion.\r\nOur method can remove the need of hiring these costly professionals.\r\nWe evaluated the effectiveness of the proposed system with ablation studies and quality comparison with previous virtual try-on methods.\r\nWe perform a user study to show our promising virtual try-on performances.\r\nMoreover, we also demonstrate that we use our method for changing virtual costumes in video conferences.\r\nFinally, we provide the collected dataset as the cloth dataset parameterized by various viewing angles, body poses, and sizes.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hongo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 61314
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Graduate School of Information Science and Technology",
              "dsl": "The University of Tokyo"
            }
          ],
          "personId": 61122
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 60939
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 61027
        }
      ]
    },
    {
      "id": 61355,
      "typeId": 11756,
      "title": "Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474747"
        },
        "Presentation": {
          "duration": "604",
          "title": "Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qEUdwZURXGY"
        },
        "Preview": {
          "duration": "30",
          "title": "Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2nppa8iAMzg"
        },
        "Video Figure": {
          "duration": "303",
          "title": "Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=SHwPztlMDpI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66252,
        66263
      ],
      "eventIds": [],
      "abstract": "We propose a new class of haptic devices that provide haptic sensations by delivering liquid-stimulants to the user’s skin; we call this chemical haptics. Upon absorbing these stimulants, which contain safe and small doses of key active ingredients, receptors in the user’s skin are chemically triggered, rendering distinct haptic sensations. We identified five chemicals that can render lasting haptic sensations: tingling (sanshool), numbing (lidocaine), stinging (cinnamaldehyde), warming (capsaicin), and cooling (menthol). To enable the application of our novel approach in a variety of settings (such as VR), we engineered a self-contained wearable that can be worn anywhere on the user’s skin (e.g., face, arms, legs). Implemented as a soft silicone patch, our device uses micropumps to push the liquid stimulants through channels that are open to the user’s skin, enabling topical stimulants to be absorbed by the skin as they pass through. Our approach presents two unique benefits. First, it enables sensations, such as numbing, not possible with existing haptic devices. Second, our approach offers a new pathway, via the skin’s chemical receptors, for achieving multiple haptic sensations using a single actuator, which would otherwise require combining multiple actuators (e.g., Peltier, vibration motors, electro-tactile stimulation). We evaluated our approach by means of two studies. In our first study, we characterized the temporal profiles of sensations elicited by each chemical. Using these insights, we designed five interactive VR experiences utilizing chemical haptics, and in our second user study, participants rated these VR experiences with chemical haptics as more immersive than without. Finally, as the first work exploring the use of chemical haptics on the skin, we offer recommendations to designers for how they may employ our approach for their interactive experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61224
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago ",
              "institution": "University of Chicago ",
              "dsl": ""
            }
          ],
          "personId": 60909
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 60933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61040
        }
      ]
    },
    {
      "id": 61356,
      "typeId": 11756,
      "title": "Assuage: Assembly Synthesis Using A Guided Exploration",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474740"
        },
        "Presentation": {
          "duration": "568",
          "title": "Assuage: Assembly Synthesis Using A Guided Exploration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1UrRaQMOEM8"
        },
        "Preview": {
          "duration": "30",
          "title": "Assuage: Assembly Synthesis Using A Guided Exploration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=H9i1ZKykAdo"
        },
        "Video Figure": {
          "duration": "309",
          "title": "Assuage: Assembly Synthesis Using A Guided Exploration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=tpMDSW_7yFc"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66239,
        66280
      ],
      "eventIds": [],
      "abstract": "Assembly programming is challenging, even for experts. Program synthesis, as an alternative to manual implementation, has the potential to enable both expert and non-expert users to generate programs in an automated fashion. However, current tools and techniques are unable to synthesize assembly programs larger than a few instructions. We present Assuage: ASsembly Synthesis Using A Guided Exploration, which is a parallel interactive assembly synthesizer that engages the user as an active collaborator, enabling synthesis to scale beyond current limits. Using Assuage, users can provide two types of semantically meaningful hints that expedite synthesis and allow for exploration of multiple possibilities simultaneously. Assuage exposes information about the underlying synthesis process using multiple representations to help users guide synthesis. We conducted a within-subjects study with twenty-one participants working on assembly programming tasks. With Assuage, participants with a wide range of expertise were able to achieve significantly higher success rates, perceived less subjective workload, and preferred the usefulness and usability of Assuage over a state of the art synthesis tool.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61316
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": ""
            }
          ],
          "personId": 61126
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": ""
            }
          ],
          "personId": 61065
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "The University of British Columbia",
              "dsl": ""
            }
          ],
          "personId": 61155
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "SEAS"
            }
          ],
          "personId": 61133
        }
      ]
    },
    {
      "id": 61357,
      "typeId": 11756,
      "title": "GazeChat: Enhancing Virtual Conferences with Gaze Awareness and Interactive 3D Photos",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474785"
        },
        "Presentation": {
          "duration": "531",
          "title": "GazeChat: Enhancing Virtual Conferences with Gaze Awareness and Interactive 3D Photos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Fnl8hxxgGVQ"
        },
        "Preview": {
          "duration": "30",
          "title": "GazeChat: Enhancing Virtual Conferences with Gaze Awareness and Interactive 3D Photos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=f7nZrSsqEao"
        },
        "Video Figure": {
          "duration": "223",
          "title": "GazeChat: Enhancing Virtual Conferences with Gaze Awareness and Interactive 3D Photos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=6cyPh9jM6vQ"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66240,
        66247
      ],
      "eventIds": [],
      "abstract": "Communication software such as Clubhouse and Zoom has evolved to be an integral part of many people's daily lives. However, due to network bandwidth constraints and concerns about privacy, cameras in video conferencing are often turned off by participants. This leads to a situation in which people can only see each others' profile images, which is essentially an audio-only experience. Even when switched on, video feeds do not provide accurate cues as to who is talking to whom. This paper introduces GazeChat, a remote communication system that visually represents users as gaze-aware 3D profile photos. This satisfies users' privacy needs while keeping online conversations engaging and efficient. GazeChat uses a single webcam to track whom any participant is looking at, then uses neural rendering to animate all participants' profile images so that participants appear to be looking at each other. We have conducted a remote user study (N=16) to evaluate GazeChat in three conditions: audio conferencing with profile photos, GazeChat, and video conferencing. Based on the results of our user study, we conclude that GazeChat maintains the feeling of presence while preserving more privacy and requiring lower bandwidth than video conferencing, provides a greater level of engagement than to audio conferencing, and helps people to better understand the structure of their conversation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "Future Reality Lab, Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "Future Reality Lab, Department of Computer Science"
            }
          ],
          "personId": 61092
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": ""
            }
          ],
          "personId": 61237
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61102
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Google",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61238
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Dept of Computer Science, NYU",
              "dsl": "Future Reality Lab"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Dept of Computer Science, NYU",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 61045
        }
      ]
    },
    {
      "id": 61358,
      "typeId": 11756,
      "title": "False Positives vs. False Negatives: The Effects of Recovery Time and Cognitive Costs on Input Error Preference",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474735"
        },
        "Presentation": {
          "duration": "522",
          "title": "False Positives vs. False Negatives: The Effects of Recovery Time and Cognitive Costs on Input Error Preference",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=tqIV2-kcZwQ"
        },
        "Preview": {
          "duration": "30",
          "title": "False Positives vs. False Negatives: The Effects of Recovery Time and Cognitive Costs on Input Error Preference",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4PJmvYmpMAA"
        },
        "Video Figure": {
          "duration": "192",
          "title": "False Positives vs. False Negatives: The Effects of Recovery Time and Cognitive Costs on Input Error Preference",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=bUcTFflvaqI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66269,
        66278
      ],
      "eventIds": [],
      "abstract": "Existing approaches to trading off false positive versus false negative errors in input recognition are based on imprecise ideas of how these errors affect user experience that are unlikely to hold for all situations. To inform dynamic approaches to setting such a tradeoff, two user studies were conducted on how relative preference for false positive versus false negative errors is influenced by differences in the temporal cost of error recovery, and high-level task factors (time pressure, multi-tasking). Participants completed a tile selection task in which false positive and false negative errors were injected at a fixed rate, and the temporal cost to recover from each of the two types of error was varied, and then indicated a preference for one error type or the other, and a frustration rating for the task. Responses indicate that the temporal costs of error recovery can drive both frustration and relative error type preference, and that participants exhibit a bias against false positive errors, equivalent to ~1.5 seconds or more of added temporal recovery time. Several explanations for this bias were revealed, including that false positive errors impose a greater attentional demand on the user, and that recovering from false positive errors imposes a task switching cost.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 61282
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs: Research",
              "dsl": ""
            }
          ],
          "personId": 61004
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 61286
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Facebook",
              "dsl": ""
            }
          ],
          "personId": 61272
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 61104
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61062
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 61094
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61315
        }
      ]
    },
    {
      "id": 61359,
      "typeId": 11756,
      "title": "Bi-3D:  Bi-Manual Pen-and-Touch Interaction for 3D Manipulation on Tablets",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474741"
        },
        "Presentation": {
          "duration": "525",
          "title": "Bi-3D:  Bi-Manual Pen-and-Touch Interaction for 3D Manipulation on Tablets",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=zyzuODg10Kw"
        },
        "Preview": {
          "duration": "30",
          "title": "Bi-3D:  Bi-Manual Pen-and-Touch Interaction for 3D Manipulation on Tablets",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=S5yhBOymZko"
        },
        "Video Figure": {
          "duration": "161",
          "title": "Bi-3D:  Bi-Manual Pen-and-Touch Interaction for 3D Manipulation on Tablets",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0aSgu3Z_BzY"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66256,
        66277
      ],
      "eventIds": [],
      "abstract": "Tablets are attractive for design work anywhere, but 3D manipulations are notoriously difficult. We explore how engaging the stylus and multi-touch in concert can render such tasks easier. We introduce Bi-3D, an interaction concept where  touch gestures are combined with 2D pen commands for 3D manipulation. For example, for a fast and intuitive 3D drag & drop technique: the pen drags the object on-screen, and parallel pinch-to-zoom moves it in the third dimension. In this paper, we describe the Bi-3D design space, crossing two-handed input and the degrees-of-freedom (DOF) of 3D manipulation and navigation tasks. We demonstrate sketching and manipulation  tools in a prototype 3D design application, where users can fluidly combine 3D operations through alternating and parallel use of the modalities. We evaluate the core technique, bi-manual 3DOF input, against widget and mid-air baselines in an object movement task. We find that Bi-3D is a fast and practical way for multi-dimensional manipulation of graphical objects, promising to facilitate 3D design on stylus and tablet devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61049
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 61061
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 61091
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Neubiberg",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            }
          ],
          "personId": 61190
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bayern",
              "city": "Munich",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            }
          ],
          "personId": 61234
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 61277
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            }
          ],
          "personId": 61098
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            }
          ],
          "personId": 61021
        }
      ]
    },
    {
      "id": 61360,
      "typeId": 11756,
      "title": "Voice and Touch Based Error-tolerant Multimodal Text Editing and Correction for Smartphones",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474742"
        },
        "Presentation": {
          "duration": "546",
          "title": "Voice and Touch Based Error-tolerant Multimodal Text Editing and Correction for Smartphones",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=iXIydlpzP_g"
        },
        "Preview": {
          "duration": "30",
          "title": "Voice and Touch Based Error-tolerant Multimodal Text Editing and Correction for Smartphones",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=U4_judUynN8"
        },
        "Video Figure": {
          "duration": "297",
          "title": "Voice and Touch Based Error-tolerant Multimodal Text Editing and Correction for Smartphones",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KipuIATUca0"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66269,
        66277
      ],
      "eventIds": [],
      "abstract": "Editing operations such as cut, copy, paste, and correcting errors in typed text is often tedious and challenging to perform on smartphones. In this paper, we present VT, a voice and touch-based multi-modal text editing and correction method for smartphones. To edit text with VT, the user glides over a text fragment with a finger and dictates a command, such as \"bold\" to change the format of the fragment, or the user can tap inside a text area and speak a command such as \"highlight this paragraph\" to edit the text. For text correcting, the user taps approximately at the area of erroneous text fragment and dictates the new content for substitution or insertion. VT combines touch and voice inputs with language context such as language model and phrase similarity to infer a user's editing intention, which can handle ambiguities and noisy input signals. It is a great advantage over the existing error correction methods (e.g., iOS’s Voice Control) which require precise cursor control or text selection. Our evaluation shows that VT significantly improves the efficiency of text editing and text correcting on smartphones over the touch-only method and the iOS’s Voice Control method. Our user studies showed that VT reduced the text editing time by 30.80%, and text correcting time by 29.97% over the touch-only method., and reduced the text editing time by 30.81%, and text correcting time by 47.96% over the iOS’s Voice Control method. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61034
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 60980
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61039
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 60932
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61281
        }
      ]
    },
    {
      "id": 61361,
      "typeId": 11756,
      "title": "Planning Epidemic Interventions with EpiPolicy",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474794"
        },
        "Presentation": {
          "duration": "511",
          "title": "Planning Epidemic Interventions with EpiPolicy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xXfHnqZ9EVE"
        },
        "Preview": {
          "duration": "30",
          "title": "Planning Epidemic Interventions with EpiPolicy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=f7tJf7hhHeQ"
        },
        "Video Figure": {
          "duration": "273",
          "title": "Planning Epidemic Interventions with EpiPolicy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YgbCgttxgDE"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66265,
        66275
      ],
      "eventIds": [],
      "abstract": "Model-driven policymaking for epidemic control is a challenging collaborative process. It begins when a team of public-health officials, epidemiologists, and economists construct a reasonably predictive disease model representative of the team’s region of interest as a function of its unique socio-economic and demographic characteristics. As the team considers possible interventions such as school closures, social distancing, vaccination drives, etc., they need to simultaneously model each intervention’s effect on disease spread and economic cost. The team then engages in an extensive what-if analysis process to determine a cost-effective policy: a schedule of when, where and how extensively each intervention should be applied. This policymaking process is often an iterative and laborious programming-intensive effort where parameters are introduced and refined, model and intervention behaviors are modified, and schedules changed. We have designed and developed EpiPolicy to support this effort. \r\nEpiPolicy is a policy aid and epidemic simulation tool that supports the mathematical specification and simulation of disease and population models, the programmatic specification of interventions and the declarative construction of schedules. EpiPolicy’s design supports a separation of concerns in the modeling process and enables capabilities such as the iterative and automatic exploration of intervention plans with Monte Carlo simulations to find a cost-effective one. We report expert feedback on EpiPolicy. In general, experts found EpiPolicy’s capabilities powerful and transformative, when compared with their current practice.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 60991
        },
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 61233
        },
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 61050
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Eco Health Alliance",
              "dsl": ""
            }
          ],
          "personId": 60935
        },
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 61198
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "New York University",
              "dsl": ""
            }
          ],
          "personId": 61042
        }
      ]
    },
    {
      "id": 61362,
      "typeId": 11756,
      "title": "FabHydro: Printing Interactive Hydraulic Devices with an Affordable SLA 3D Printer",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474751"
        },
        "Presentation": {
          "duration": "703",
          "title": "FabHydro: Printing Interactive Hydraulic Devices with an Affordable SLA 3D Printer",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4zfKQmxLfPI"
        },
        "Preview": {
          "duration": "30",
          "title": "FabHydro: Printing Interactive Hydraulic Devices with an Affordable SLA 3D Printer",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KIshy4aPCvk"
        },
        "Video Figure": {
          "duration": "123",
          "title": "FabHydro: Printing Interactive Hydraulic Devices with an Affordable SLA 3D Printer",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cpg3fEtZdf0"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66236,
        66248
      ],
      "eventIds": [],
      "abstract": "We introduce FabHydro, a set of rapid and low-cost methods to prototype interactive hydraulic devices based on an off-the-shelf 3D printer and flexible photosensitive resin. \r\nWe first present printer settings and custom support structures to warrant the successful print of flexible and deformable objects. \r\nWe then demonstrate two printing methods to seal the transmission fluid inside these deformable structures: the Submerged Printing process that seals the liquid resin without manual assembly, and the Printing with Plugs method that allows the use of different transmission fluids without modification to the printer. \r\nFollowing the printing methods, we report a design space with a range of 3D printable primitives, including the hydraulic generator, transmitter, and actuator.\r\nTo demonstrate the feasibility of our approaches and the breadth of new designs that they enable, we showcase a set of examples from a printed robotic gripper that can be operated at a distance to a mobile phone stand that serves as a status reminder by repositioning the user's phone.\r\nWe conclude with a discussion of our approach's limitations and possible future improvements.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61220
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61248
        }
      ]
    },
    {
      "id": 61363,
      "typeId": 11756,
      "title": "Trusscillator: a System for Fabricating Human-Scale Human-Powered Oscillating Devices",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474807"
        },
        "Presentation": {
          "duration": "580",
          "title": "Trusscillator: a System for Fabricating Human-Scale Human-Powered Oscillating Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xvPJKSQlvL0"
        },
        "Preview": {
          "duration": "30",
          "title": "Trusscillator: a System for Fabricating Human-Scale Human-Powered Oscillating Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=kyaL_wG0rJI"
        },
        "Video Figure": {
          "duration": "287",
          "title": "Trusscillator: a System for Fabricating Human-Scale Human-Powered Oscillating Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=g3xVaJlhXsQ"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66258,
        66260
      ],
      "eventIds": [],
      "abstract": "Trusscillator is an end-to-end system that allows non-engineers to create human-scale human-powered devices that perform oscillatory movements, such as playground equipment, workout devices, and interactive kinetic installations. While recent research has been focusing on generating mechanisms that produce specific movement-path, without considering the required energy for the motion (kinematic approach), Trusscillator supports users in designing mechanisms that recycle energy in the system in the form of oscillating mechanisms (dynamic approach), specifically with the help of coil-springs. The presented system features a novel set of tools tailored for designing the dynamic experience of the motion. These tools allow designers to focus on user experience-specific aspects, such as motion range, tempo, and effort while abstracting away the underlying technicalities of eigenfrequencies, spring constants, and energy. Since the forces involved in the resulting devices can be high, Trusscillator helps users to fabricate from steel, by picking out appropriate steal springs, generating part lists, and by producing stencils and welding jigs that help weld with precision. To validate our system, we designed, built, and tested a series of unique playground equipment featuring 2-4 degrees of movement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 60948
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61127
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61075
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60984
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61178
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61292
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "University of Potsdam",
              "dsl": "Hasso Plattner Institute"
            }
          ],
          "personId": 60927
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60955
        }
      ]
    },
    {
      "id": 61364,
      "typeId": 11756,
      "title": "Daedalus in the Dark: Designing for Non-Visual Accessible Construction of Laser-Cut Architecture",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474754"
        },
        "Presentation": {
          "duration": "432",
          "title": "Daedalus in the Dark: Designing for Non-Visual Accessible Construction of Laser-Cut Architecture",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JIr96tEXdcc"
        },
        "Preview": {
          "duration": "30",
          "title": "Daedalus in the Dark: Designing for Non-Visual Accessible Construction of Laser-Cut Architecture",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=mKV39PAnKn4"
        },
        "Video Figure": {
          "duration": "242",
          "title": "Daedalus in the Dark: Designing for Non-Visual Accessible Construction of Laser-Cut Architecture",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0LE2zqfwbYw"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66241,
        66245
      ],
      "eventIds": [],
      "abstract": "Design tools and research for laser-cut architectures have been widely explored in the past decade. However, this discussion has mostly revolved around technical and structural design questions instead of another essential element of laser-cut models - assembly - a process that relies heavily on visual affordance of components, therefore less accessible to blind or low vision (BLV) people. To close this gap, we co-designed with 7 BLV people to examine their assembly experience with different laser-cut architecture. Distilled from their feedback, we proposed several design heuristics and guidelines for Daedalus, a generative design tool that can produce tactile aids for laser-cut assembly given a few high-level manual inputs. We validate the proposed aids in a user study with 8 new BLV participants. Our results revealed that BLV users can manage laser-cut assembly more efficiently with Daedalus. Going forth from this design iteration, we discuss implications for future research on accessible laser-cut assembly.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61268
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61309
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61113
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "NTU IoX Center"
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "NTU IoX Center"
            }
          ],
          "personId": 61187
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            }
          ],
          "personId": 61052
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61161
        }
      ]
    },
    {
      "id": 61365,
      "typeId": 11756,
      "title": "GestuRING: A Web-based Tool for Designing Gesture Input with Rings, Ring-Like, and Ring-Ready Devices",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474780"
        },
        "Presentation": {
          "duration": "417",
          "title": "GestuRING: A Web-based Tool for Designing Gesture Input with Rings, Ring-Like, and Ring-Ready Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=-ZXk4nRy3mU"
        },
        "Preview": {
          "duration": "30",
          "title": "GestuRING: A Web-based Tool for Designing Gesture Input with Rings, Ring-Like, and Ring-Ready Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=mTwKgFV2FJs"
        },
        "Video Figure": {
          "duration": "34",
          "title": "GestuRING: A Web-based Tool for Designing Gesture Input with Rings, Ring-Like, and Ring-Ready Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=HxoJMiEwJWM"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66256,
        66276
      ],
      "eventIds": [],
      "abstract": "Despite an exciting area with many promises for innovations in wearable interactive systems, research on interaction techniques for smart rings lacks structured knowledge and readily-available resources for designers to systematically attain such innovations. In this work, we conduct a systematic literature review of ring-based gesture input, from which we extract key results and a large set of gesture commands for ring, ring-like, and ring-ready devices. We use these findings to deliver GestuRING, our web-based tool to support design of ring-based gesture input. GestuRING features a searchable gesture-to-function dictionary of 579 records with downloadable numerical data files and an associated YouTube video library. These resources are meant to assist the community in attaining further innovations in ring-based gesture input for interactive systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "University Stefan cel Mare of Suceava",
              "dsl": "MintViz Lab"
            }
          ],
          "personId": 60974
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava ",
              "institution": "Universitatea Ștefan cel Mare ",
              "dsl": ""
            }
          ],
          "personId": 61232
        }
      ]
    },
    {
      "id": 61366,
      "typeId": 11756,
      "title": "OmniFiber: Integrated Fluidic Fiber Actuators for Weaving Movement-based Interactions into the ‘Fabric of Everyday Life’",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474802"
        },
        "Presentation": {
          "duration": "512",
          "title": "OmniFiber: Integrated Fluidic Fiber Actuators for Weaving Movement-based Interactions into the ‘Fabric of Everyday Life’",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=OK-FqyxhvGY"
        },
        "Preview": {
          "duration": "30",
          "title": "OmniFiber: Integrated Fluidic Fiber Actuators for Weaving Movement-based Interactions into the ‘Fabric of Everyday Life’",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=TNUijAqXzLs"
        },
        "Video Figure": {
          "duration": "269",
          "title": "OmniFiber: Integrated Fluidic Fiber Actuators for Weaving Movement-based Interactions into the ‘Fabric of Everyday Life’",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xtlL-0eb0xc"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66241,
        66248
      ],
      "eventIds": [],
      "abstract": "Fiber -- a primitive yet ubiquitous form of material -- intertwines with our bodies and surroundings, from constructing our fibrous muscles that enable our movement, to forming fabrics that intimately interface with our skin. In soft robotics and advanced materials science research, actuated fibers are gaining interest as thin, flexible materials that can morph in response to external stimuli. In this paper, we build on fluidic artificial muscles research to develop OmniFiber - a soft, line-based material system for designing movement-based interactions. We devised actuated thin (ø 𝑜𝑢𝑡𝑒𝑟 < 1.8 mm) fluidic fibers with integrated soft sensors that exhibit perceivably strong forces, up to 19 N at 0.5 MPa, and a high speed of linear actuation peaking at 150mm/s. These allow to flexibly weave them into everyday tangible interactions; including on-body haptic devices for embodied learning, synchronized tangible interfaces for remote communication, and robotic crafting for expressivity. The design of such interactive capabilities is supported by OmniFiber’s design space, accessible fabrication pipeline, and a fluidic I/O control system to bring omni-functional fluidic fibers to the HCI toolbox of interactive morphing materials.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Soma Design Lab"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 61041
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 61287
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "MIT Media Lab | Tangible Media Group"
            }
          ],
          "personId": 61118
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 60923
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 60913
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Konstfack",
              "dsl": "Department of Arts and Crafts"
            }
          ],
          "personId": 61005
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "John A. Paulson School of Engineering and Applied Sciences"
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Materials Engineering and Science"
            }
          ],
          "personId": 61285
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Ångström Laboratory",
              "dsl": "Department of Chemistry"
            }
          ],
          "personId": 61117
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Soma Design Lab"
            }
          ],
          "personId": 60915
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 61143
        }
      ]
    },
    {
      "id": 61367,
      "typeId": 11756,
      "title": "Marcelle: Composing Interactive Machine Learning Workflows and Interfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474734"
        },
        "Presentation": {
          "duration": "516",
          "title": "Marcelle: Composing Interactive Machine Learning Workflows and Interfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=gkMnUl2OZ-Y"
        },
        "Preview": {
          "duration": "30",
          "title": "Marcelle: Composing Interactive Machine Learning Workflows and Interfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JMS8WUEnxdQ"
        },
        "Video Figure": {
          "duration": "295",
          "title": "Marcelle: Composing Interactive Machine Learning Workflows and Interfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=bIx1z-r_XDs"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66264,
        66274
      ],
      "eventIds": [],
      "abstract": "Human-centered approaches to machine learning have established theoretical foundations, design principles and interaction techniques to facilitate end-user interaction with machine learning systems. Yet, general-purpose toolkits supporting the design of interactive machine learning systems are still missing, despite their potential to foster reuse, appropriation and collaboration between different stakeholders including developers, machine learning experts, designers and end users. In this paper, we present an architectural model for toolkits dedicated to the design of human interactions with machine learning. The architecture is built upon a modular collection of interactive components that can be composed to build interactive machine learning workflows, using reactive pipelines and composable user interfaces. We introduce Marcelle, a toolkit for the design of human interactions with machine learning that implements this model.  We illustrate Marcelle with two implemented case studies: (1) a HCI researcher conducts user studies to understand novice interaction with machine learning, and (2) a machine learning expert and a clinician collaborate to develop a skin cancer diagnosis system. Finally, we discuss our experience with the toolkit, along with its limitation and perspectives.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "LISN, Université Paris-Saclay, CNRS",
              "dsl": ""
            }
          ],
          "personId": 60968
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Gif-sur-Yvette",
              "institution": "Université Paris Saclay",
              "dsl": "LISN"
            }
          ],
          "personId": 61058
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, CNRS, ISIR",
              "dsl": ""
            }
          ],
          "personId": 61221
        }
      ]
    },
    {
      "id": 61368,
      "typeId": 11756,
      "title": "DextrEMS: Achieving Dexterity in Electrical Muscle Stimulation by Combining it with Brakes",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474759"
        },
        "Presentation": {
          "duration": "578",
          "title": "DextrEMS: Achieving Dexterity in Electrical Muscle Stimulation by Combining it with Brakes",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GQh9YpTV8gg"
        },
        "Preview": {
          "duration": "30",
          "title": "DextrEMS: Achieving Dexterity in Electrical Muscle Stimulation by Combining it with Brakes",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=arxIyor4Hq4"
        },
        "Video Figure": {
          "duration": "292",
          "title": "DextrEMS: Achieving Dexterity in Electrical Muscle Stimulation by Combining it with Brakes",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=LZo7uORVTDk"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66262,
        66266
      ],
      "eventIds": [],
      "abstract": "Electrical muscle stimulation (EMS) is an emergent technique that miniaturizes force feedback, especially popular for untethered haptic devices, such as mobile gaming, VR, or AR. However, the actuation displayed by interactive systems based on EMS is coarse and imprecise. EMS systems mostly focus on inducing movements in large muscle groups such as legs, arms, and wrists; whereas individual finger poses, which would be required, for example, to actuate a user’s fingers to fingerspell even the simplest letters in sign language, are not possible. The lack of dexterity in EMS stems from two fundamental limitations: (1) lack of independence: when a particular finger is actuated by EMS, the current runs through nearby muscles, causing unwanted actuation of adjacent fingers; and, (2) unwanted oscillations: while it is relatively easy for EMS to start moving a finger, it is very hard for EMS to stop and hold that finger at a precise angle; because, to stop a finger, virtually all EMS systems contract the opposing muscle, typically achieved via controllers (e.g., PID)—unfortunately, even with the best controller tuning, this often results in unwanted oscillations. To tackle these limitations, we propose dextrEMS, an EMS-based haptic device featuring mechanical brakes attached to each finger joint. The key idea behind dextrEMS is that while the EMS actuates the fingers, it is our mechanical brake that stops the finger in a precise position. Moreover, it is also the brakes that allow dextrEMS to select which fingers are moved by EMS, eliminating unwanted movements by preventing adjacent fingers from moving. We implemented dextrEMS as an untethered haptic device, weighing only 68g, that actuates eight finger joints independently (metacarpophalangeal and proximal interphalangeal joints for four fingers), which we demonstrate in a wide range of haptic applications, such as assisted fingerspelling, a piano tutorial, guitar tutorial, and a VR game. Finally, in our technical evaluation, we found that dextrEMS outperformed EMS alone by doubling its independence and reducing unwanted oscillations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 60924
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61196
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61073
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61040
        }
      ]
    },
    {
      "id": 61369,
      "typeId": 11756,
      "title": "Rapido: Prototyping Interactive AR Experiences through Programming by Demonstration",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474774"
        },
        "Presentation": {
          "duration": "468",
          "title": "Rapido: Prototyping Interactive AR Experiences through Programming by Demonstration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nnMeLnikL6Y"
        },
        "Preview": {
          "duration": "30",
          "title": "Rapido: Prototyping Interactive AR Experiences through Programming by Demonstration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_VjVyrOJZRw"
        },
        "Video Figure": {
          "duration": "372",
          "title": "Rapido: Prototyping Interactive AR Experiences through Programming by Demonstration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9HsMWb71siQ"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66243,
        66244
      ],
      "eventIds": [],
      "abstract": "Programming by Demonstration (PbD) is a well-known technique that allows non-programmers to describe interactivity by performing examples of the expected behavior, but it has not been extensively explored for AR. We present Rapido, a novel early-stage prototyping tool to create fully interactive mobile AR prototypes from non-interactive video prototypes using PbD. In Rapido, designers use a mobile AR device to record a video prototype to capture context, sketch assets, and demonstrate interactions. They can demonstrate touch inputs, animation paths, and rules to, e.g., have a sketch follow the focus area of the device or the user's world-space touches. Simultaneously, a live website visualizes an editable overview of all the demonstrated examples and infers a state machine of the user flow. Our key contribution is a method that enables designers to turn a video prototype into an executable state machine through PbD. The designer switches between these representations to interactively refine the final interactive prototype. We illustrate the power of Rapido's approach by prototyping the main interactions of three popular AR mobile applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Digital Design and Information Studies"
            }
          ],
          "personId": 61288
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61319
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Digital Design and Information Studies"
            }
          ],
          "personId": 61291
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 61188
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 61035
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe",
              "dsl": "Adobe Research"
            }
          ],
          "personId": 61105
        }
      ]
    },
    {
      "id": 61370,
      "typeId": 11756,
      "title": "NavStick: Making Video Games Blind-Accessible via the Ability to Look Around",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474768"
        },
        "Presentation": {
          "duration": "540",
          "title": "NavStick: Making Video Games Blind-Accessible via the Ability to Look Around",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=oAu_Q_2YU_E"
        },
        "Preview": {
          "duration": "30",
          "title": "NavStick: Making Video Games Blind-Accessible via the Ability to Look Around",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=NGbY6J0gSUg"
        },
        "Video Figure": {
          "duration": "72",
          "title": "NavStick: Making Video Games Blind-Accessible via the Ability to Look Around",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WE6tskxaOxw"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66255,
        66279
      ],
      "eventIds": [],
      "abstract": "Video games remain largely inaccessible to visually impaired people (VIPs). Today’s blind-accessible games are highly simplified renditions of what sighted players enjoy, and they do not give VIPs the same freedom to look around and explore game worlds on their own terms. In this work, we introduce NavStick, an audio-based tool for looking around within virtual environments, with the aim of making 3D adventure video games more blind-accessible. NavStick repurposes a game controller's thumbstick to allow VIPs to survey what is around them via line-of-sight. In a user study, we compare NavStick with traditional menu-based surveying for different navigation tasks and find that VIPs were able to form more accurate mental maps of their environment with NavStick than with menu-based surveying. In an additional exploratory study, we investigate NavStick in the context of a representative 3D adventure game. Our findings reveal several implications for blind-accessible games, and we close by discussing these.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 60928
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61193
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61255
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Electrical Engineering"
            }
          ],
          "personId": 61231
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61111
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New Paltz",
              "institution": "State University of New York at New Paltz",
              "dsl": ""
            }
          ],
          "personId": 61278
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 61259
        }
      ]
    },
    {
      "id": 61371,
      "typeId": 11756,
      "title": "SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474733"
        },
        "Presentation": {
          "duration": "604",
          "title": "SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=fxD5GEMQ8kk"
        },
        "Preview": {
          "duration": "30",
          "title": "SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Gw8XxyGFy-Y"
        },
        "Video Figure": {
          "duration": "357",
          "title": "SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=LVYWWin1KeM"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66236,
        66245
      ],
      "eventIds": [],
      "abstract": "Laser cutter users face difficulties distinguishing between visually similar materials. This can lead to problems, such as using the wrong power/speed settings or accidentally cutting hazardous materials. To support users, we present SensiCut, an integrated material sensing platform for laser cutters.SensiCut enables material awareness beyond what users are able to see and reliably differentiates among similar-looking types. It achieves this by detecting materials' surface structures using speckle sensing and deep learning.\r\n\r\nSensiCut consists of a compact hardware add-on for laser cutters and a user interface that integrates material sensing into the laser cutting workflow. In addition to improving the traditional workflow and its safety, SensiCut enables new applications, such as automatically partitioning designs when engraving on multi-material objects or adjusting their geometry based on the kerf of the identified material.\r\n\r\nWe evaluate SensiCut's accuracy for different types of materials under different sheet orientations and illumination conditions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 60982
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61026
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61265
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Computer Science"
            }
          ],
          "personId": 60941
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61017
        }
      ]
    },
    {
      "id": 61372,
      "typeId": 11756,
      "title": "SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474791"
        },
        "Presentation": {
          "duration": "607",
          "title": "SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4p77oo2Xz1k"
        },
        "Preview": {
          "duration": "30",
          "title": "SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9Ihznuo7_S4"
        },
        "Video Figure": {
          "duration": "289",
          "title": "SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=RdxJ17TqhAU"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66250,
        66276
      ],
      "eventIds": [],
      "abstract": "Head shapes play an important role in 3D character design. In this work, we propose SimpModeling, a novel sketch-based system for helping users, especially amateur users, easily model 3D animalmorphic heads - a prevalent kind of head in character design. Although sketching provides an easy way to depict desired shapes, it is challenging to infer dense geometric information from sparse line drawings. Recently, deepnet-based approaches have been taken to address this challenge and try to produce rich geometric details from very few strokes. However, while such methods reduce users' workload, they would cause less controllability of target shapes. This is mainly due to the uncertainty of the neural prediction. Our system tackles this issue and provides good controllability from three aspects: 1) we separate coarse shape design and geometric detail specification into two stages and respectively provide different sketching means; 2) in coarse shape designing, sketches are used for both shape inference and geometric constraints to determine global geometry, and in geometric detail crafting, sketches are used for carving surface details;  3) in both stages, we use the advanced implicit-based shape inference methods, which have strong ability to handle the domain gap between freehand sketches and synthetic ones used for training. Experimental results confirm the effectiveness of our method and the usability of our interactive system. We also contribute to a dataset of high-quality 3D animal heads, which are manually created by artists. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "The Chinese University of Hong Kong (Shenzhen)",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Shenzhen Research Institute of Big Data",
              "dsl": ""
            }
          ],
          "personId": 61038
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong ",
              "dsl": ""
            }
          ],
          "personId": 61318
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "The Chinese University of Hong Kong (Shenzhen)",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen ",
              "institution": "Shenzhen Research Insititute of Big Data",
              "dsl": ""
            }
          ],
          "personId": 61079
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Anhui",
              "city": "Hefei",
              "institution": "University of Science and Technology of China",
              "dsl": "Graphics&Geometric Computing Laboratory (GCL)"
            }
          ],
          "personId": 61137
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shengzhen",
              "institution": "The Chinese University of Hong Kong (Shenzhen)",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Shenzhen Research Insititute of Big Data",
              "dsl": ""
            }
          ],
          "personId": 60959
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 60978
        }
      ]
    },
    {
      "id": 61374,
      "typeId": 11756,
      "title": "LipNotif: Use of Lips as a Non-Contact Tactile Notification Interface Based on Ultrasonic Tactile Presentation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474732"
        },
        "Presentation": {
          "duration": "397",
          "title": "LipNotif: Use of Lips as a Non-Contact Tactile Notification Interface Based on Ultrasonic Tactile Presentation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=iMXmoTHZ-Q8"
        },
        "Preview": {
          "duration": "30",
          "title": "LipNotif: Use of Lips as a Non-Contact Tactile Notification Interface Based on Ultrasonic Tactile Presentation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=R7ag_K9o0dY"
        },
        "Video Figure": {
          "duration": "79",
          "title": "LipNotif: Use of Lips as a Non-Contact Tactile Notification Interface Based on Ultrasonic Tactile Presentation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4dkOTClyWFo"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66266,
        66273
      ],
      "eventIds": [],
      "abstract": "We propose LipNotif, a non-contact tactile notification system that uses airborne ultrasound tactile presentation to lips. Lips are suitable for non-contact tactile notifications because they have high tactile sensitivity comparable to the palms, are less occupied in daily life, and are constantly exposed outward. LipNotif uses tactile patterns to intuitively convey information to users, allowing them to receive notifications using only their lips, without sight, hearing, or hands. We developed a prototype system that automatically recognizes the position of the lips and presents non-contact tactile sensations. Two experiments were conducted to evaluate the feasibility of LipNotif. In the first experiment, we found that directional information can be notified to the lips with an average accuracy of ± 11.1° in the 120° horizontal range. In the second experiment, we could elicit significantly different affective responses by changing the stimulus intensity. The　experimental results indicated that LipNotif is practical for conveying directions, emotions, and combinations of them. LipNotif can be applied for various purposes, such as notifications during work, calling in the waiting room, and tactile feedback in automotive user interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 61175
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": "Shinoda & Makino Lab"
            }
          ],
          "personId": 61068
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": "Frontier Sciences"
            }
          ],
          "personId": 61150
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": "Frontier Sciences"
            }
          ],
          "personId": 61184
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "The University of Tokyo",
              "dsl": "Frontier Sciences"
            }
          ],
          "personId": 61304
        }
      ]
    },
    {
      "id": 61375,
      "typeId": 11756,
      "title": "Weirding Haptics: In-Situ Prototyping of Vibrotactile Feedback in Virtual Reality through Vocalization",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474797"
        },
        "Presentation": {
          "duration": "400",
          "title": "Weirding Haptics: In-Situ Prototyping of Vibrotactile Feedback in Virtual Reality through Vocalization",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VxCFqKRw5rQ"
        },
        "Preview": {
          "duration": "30",
          "title": "Weirding Haptics: In-Situ Prototyping of Vibrotactile Feedback in Virtual Reality through Vocalization",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=gPJeSAwhd1k"
        },
        "Video Figure": {
          "duration": "260",
          "title": "Weirding Haptics: In-Situ Prototyping of Vibrotactile Feedback in Virtual Reality through Vocalization",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=zH4GLHALrLk"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66259,
        66262
      ],
      "eventIds": [],
      "abstract": "Effective haptic feedback in virtual reality (VR) is an essential element for creating convincing immersive experiences. To design such feedback, state-of-the-art VR setups provide APIs for programmatically generating controller vibration patterns. While tools for designing vibrotactile feedback keep evolving, they often require expert knowledge and rarely support direct manipulation methods for mapping feedback to user interactions within the VR environment. To address these challenges, we contribute a novel concept called Weirding Haptics, that supports fast-prototyping by leveraging the user's voice to design such feedback while manipulating virtual objects in-situ. Through a pilot study (N = 9) focusing on how tactile experiences are vocalized during object manipulation, we identify spatio-temporal mappings and supporting features needed to produce intended vocalizations. To study our concept, we built a VR design tool informed by the results of the pilot study. This tool enables users to design tactile experiences using their voice while manipulating objects, provides a set of modifiers for fine-tuning the created experiences in VR, and allows to rapidly compare various experiences by feeling them. Results from a validation study (N = 8) show that novice hapticians can vocalize experiences and refine their designs with the fine-tuning modifiers to match their intentions. We conclude our work by discussing uncovered design implications for direct manipulation and vocalization of vibrotactile feedback in immersive virtual environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 61148
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 61103
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Peer",
              "institution": "Polygoat",
              "dsl": ""
            }
          ],
          "personId": 61195
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 60989
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 61110
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 61186
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 61044
        }
      ]
    },
    {
      "id": 61376,
      "typeId": 11756,
      "title": "Taxon: a Language for Formal Reasoning with Digital Fabrication Machines",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474779"
        },
        "Presentation": {
          "duration": "330",
          "title": "Taxon: a Language for Formal Reasoning with Digital Fabrication Machines",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=IaG-zhIa7yw"
        },
        "Preview": {
          "duration": "30",
          "title": "Taxon: a Language for Formal Reasoning with Digital Fabrication Machines",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vBHVcY6WrUI"
        },
        "Video Figure": {
          "duration": "238",
          "title": "Taxon: a Language for Formal Reasoning with Digital Fabrication Machines",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8Sqy3IwKGOY"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66239,
        66248
      ],
      "eventIds": [],
      "abstract": "Digital fabrication machines for makers have expanded access to manufacturing processes such as 3D printing, laser cutting, and milling. While digital models encode the data necessary for a ma- chine to manufacture an object, understanding the trade-offs and limitations of the machines themselves is crucial for successful production. Yet, this knowledge is not codified and must be gained through experience, which limits both adoption of and creative exploration with digital fabrication tools. To formally represent machines, we present Taxon, a language that encodes a machine’s high-level characteristics, physical composition, and performable actions. With this programmatic foundation, makers can develop rules of thumb that filter for appropriate machines for a given job and verify that actions are feasible and safe. We integrate the language with a browser-based system for simulating and experimenting with machine workflows. The system lets makers engage with rules of thumb and enrich their understanding of machines. We evaluate Taxon by representing several machines from both common practice and digital fabrication research. We find that while Taxon does not exhaustively describe all machines, it provides a starting point for makers and HCI researchers to develop tools for reasoning about and making decisions with machines.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 61279
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 61074
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 60937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 61299
        }
      ]
    },
    {
      "id": 61377,
      "typeId": 11756,
      "title": "Shapir: Standardizing and Democratizing Access to Web APIs",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474822"
        },
        "Presentation": {
          "duration": "587",
          "title": "Shapir: Standardizing and Democratizing Access to Web APIs",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xShuyadoiNA"
        },
        "Preview": {
          "duration": "30",
          "title": "Shapir: Standardizing and Democratizing Access to Web APIs",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=v1cjXiinbqM"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66270,
        66280
      ],
      "eventIds": [],
      "abstract": "Today, many web sites offer third-party access to their data through web APIs. \r\nBut manually encoding URLs with arbitrary endpoints, parameters,  authentication handshakes, and pagination, among other things, makes API use challenging and laborious for programmers, and untenable for novices.  In addition, each site offers its own idiosyncratic data model, properties, and methods that a new user must learn, even when the sites manage the same common types of information as many others.  \r\n\r\nIn this work, we show how working with web APIs can be dramatically simplified by describing the APIs using a standardized, machine-readable ontology.  By surveying a statistical sample of web APIs, we develop a simple ontology that can effectively describe the core functionality of nearly all of them. We then present Shapir, a system that includes a graphical, form-based authoring tool for the API description, from which Shapir can automatically generate a standardized JavaScript library for accessing data on the web site as objects with readable and writeable properties. This enables programmers to access data without learning the details of each API, and indeed allows them to use the same unchanged code for multiple web sites.\r\nWe then integrate Shapir with Mavo, an HTML language extension for describing web applications declaratively, to also empower plain-HTML authors to access these APIs.  \r\nIn our lab evaluation, we found that programmers are able to accomplish program data management tasks that require multiple API requests 5.6 times faster on average using the Shapir generic library than using the popular Swagger API integration library. Using our Mavo-Shapir integration, even non-programmers were able to build functioning data management applications that access multiple web APIs in just 4 minutes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 61020
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 61157
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 61280
        }
      ]
    },
    {
      "id": 61378,
      "typeId": 11756,
      "title": "KondoCloud: Improving Information Management in Cloud Storage via Recommendations Based on File Similarity",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474736"
        },
        "Presentation": {
          "duration": "522",
          "title": "KondoCloud: Improving Information Management in Cloud Storage via Recommendations Based on File Similarity",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=X01m15JahE4"
        },
        "Preview": {
          "duration": "30",
          "title": "KondoCloud: Improving Information Management in Cloud Storage via Recommendations Based on File Similarity",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=zD3-2ELdiDA"
        },
        "Video Figure": {
          "duration": "188",
          "title": "KondoCloud: Improving Information Management in Cloud Storage via Recommendations Based on File Similarity",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=v0bMUUkr1kg"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66250,
        66265
      ],
      "eventIds": [],
      "abstract": "Users face many challenges in keeping their personal file collections organized. While current file-management interfaces help users retrieve files in disorganized repositories, they do not aid in organization. Pertinent files can be difficult to find, and files that should have been deleted may remain. To help, we designed KondoCloud, a file-browser interface for personal cloud storage. KondoCloud makes machine learning-based recommendations of files users may want to retrieve, move, or delete. These recommendations leverage the intuition that similar files should be managed similarly.\r\n\r\nWe developed and evaluated KondoCloud through two complementary online user studies. In our Observation Study, we logged the actions of 69 participants who spent 30 minutes manually organizing their own Google Drive repositories. We identified high-level organizational strategies, including moving related files to newly created sub-folders and extensively deleting files. To train the classifiers that underpin KondoCloud's recommendations, we had participants label whether pairs of files were similar and whether they should be managed similarly. In addition, we extracted ten metadata and content features from all files in participants' repositories. Our logistic regression classifiers all achieved F1 scores of 0.72 or higher. In our Evaluation Study, 62 participants used KondoCloud either with or without recommendations. Roughly half of participants accepted a non-trivial fraction of recommendations, and some participants accepted nearly all of them. Participants who were shown the recommendations were more likely to delete related files located in different directories. They also generally felt the recommendations improved efficiency. Participants who were not shown recommendations nonetheless manually performed about a third of the actions that would have been recommended.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61085
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 60934
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 60905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61257
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 60992
        }
      ]
    },
    {
      "id": 61379,
      "typeId": 11756,
      "title": "Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474800"
        },
        "Presentation": {
          "duration": "590",
          "title": "Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hdSmTzECnqc"
        },
        "Preview": {
          "duration": "30",
          "title": "Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=pLVi9Sy9vO8"
        },
        "Video Figure": {
          "duration": "200",
          "title": "Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ve4cEryFY9g"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66252,
        66283
      ],
      "eventIds": [],
      "abstract": "We propose a haptic device that alters the perceived softness of real rigid objects without requiring to instrument the objects. Instead, our haptic device works by restricting the user’s fingerpad lateral deformation via a hollow frame that squeezes the sides of the fingerpad. This causes the fingerpad to become bulgier than it originally was—when users touch an object’s surface with their now-restricted fingerpad, they feel the object to be softer than it is. To illustrate the extent of softness illusion induced by our device, touching the tip of a wooden chopstick will feel as soft as a rubber eraser. Our haptic device operates by pulling the hollow frame using a motor. Unlike most wearable haptic devices, which cover up the user’s fingerpad to create force sensations, our device creates softness while leaving the center of the fingerpad free, which allows the users to feel most of the object they are interacting with. This makes our device a unique contribution to altering the softness of everyday objects, creating “buttons” by softening protrusions of existing appliances or tangibles, or even, altering the softness of handheld props for VR. Finally, we validated our device through two studies: (1) a psychophysics study showed that the device brings down the perceived softness of any object between 50A-90A to around 40A (on Shore A hardness scale); and (2) a user study demonstrated that participants preferred our device for interactive applications that leverage haptic props, such as making a VR prop feel softer or making a rigid 3D printed remote control feel softer on its button.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61073
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61040
        }
      ]
    },
    {
      "id": 61380,
      "typeId": 11756,
      "title": "MotionRing: Creating Illusory Tactile Motion around the Head using 360° Vibrotactile Headbands",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474781"
        },
        "Presentation": {
          "duration": "508",
          "title": "MotionRing: Creating Illusory Tactile Motion around the Head using 360° Vibrotactile Headbands",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hQoMXI0lp2U"
        },
        "Preview": {
          "duration": "30",
          "title": "MotionRing: Creating Illusory Tactile Motion around the Head using 360° Vibrotactile Headbands",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xwVJpen41bI"
        },
        "Video Figure": {
          "duration": "110",
          "title": "MotionRing: Creating Illusory Tactile Motion around the Head using 360° Vibrotactile Headbands",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_QW--FV7oXo"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66259,
        66261
      ],
      "eventIds": [],
      "abstract": "We present MotionRing, a vibrotactile headband that creates illusory tactile motion around the head by controlling the timing of a 1-D, 360° sparse array of vibration motors. Its unique ring shape enables symmetric and asymmetric haptic motion experiences, such as when users pass through a medium and when an object passes nearby in any direction. We first conducted a perception study to understand how factors such as vibration motor timing, spacing, duration, intensity, and head region affect the perception of apparent tactile motion. Results showed that illusory tactile motion around the head can be achieved with 12 and 16 vibration motors with angular speed between 0.5-4.9 revolutions per second. We developed a symmetric and an asymmetric tactile motion pattern to enhance the experience of teleportation in VR and dodging footballs, respectively. We conducted a user study to compare the experience of MotionRing vs. static vibration patterns and visual-only feedback. Results showed that illusory tactile motion significantly improved users' perception of directionality and enjoyment of motion events, and was most preferred by users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61293
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei City",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61185
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 60920
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "Department of Computer Science & Information Engineering",
              "dsl": "National Taiwan University"
            }
          ],
          "personId": 61128
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61310
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61024
        }
      ]
    },
    {
      "id": 61381,
      "typeId": 11756,
      "title": "AirConstellations: In-Air Device Formations for Cross-Device Interaction via Multiple Spatially-Aware Armatures",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474820"
        },
        "Presentation": {
          "duration": "501",
          "title": "AirConstellations: In-Air Device Formations for Cross-Device Interaction via Multiple Spatially-Aware Armatures",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=o6wccaWaNCQ"
        },
        "Preview": {
          "duration": "30",
          "title": "AirConstellations: In-Air Device Formations for Cross-Device Interaction via Multiple Spatially-Aware Armatures",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WVEEsiXM6d4"
        },
        "Video Figure": {
          "duration": "289",
          "title": "AirConstellations: In-Air Device Formations for Cross-Device Interaction via Multiple Spatially-Aware Armatures",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nqN7FJfIKew"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66251,
        66261
      ],
      "eventIds": [],
      "abstract": "AirConstellations supports a unique semi-fixed style of cross-device interactions via multiple self-spatially-aware armatures to which users can easily attach (or detach) tablets and other devices. In particular, AirConstellations affords highly flexible and dynamic device formations where the users can bring multiple devices together in-air – with 2-5 armatures poseable in 7DoF within the same workspace – to suit the demands of their current task, social situation, app scenario, or mobility needs. This affords an interaction metaphor where relative orientation, proximity, attaching (or detaching) devices, and continuous movement into and out of ad-hoc ensembles can drive context-sensitive interactions. Yet all devices remain self-stable in useful configurations even when released in mid-air. \r\nWe explore flexible physical arrangement, feedforward of transition options, and layering of devices in-air across a variety of multi-device app scenarios. These include video conferencing with flexible arrangement of the person-space of multiple remote participants around a shared task-space, layered and tiled device formations with overview+detail and shared-to-personal transitions, and flexible composition of UI panels and tool palettes across devices for productivity applications. A preliminary interview study highlights user reactions to AirConstellations, such as for minimally disruptive device formations, easier physical transitions, and balancing \"seeing and being seen\" in remote work.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "UCL Interaction Centre"
            }
          ],
          "personId": 61210
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61236
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61028
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": ""
            }
          ],
          "personId": 61218
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61123
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 61249
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 60983
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 61226
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": ""
            }
          ],
          "personId": 61003
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61305
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 60921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61199
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 60940
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61136
        }
      ]
    },
    {
      "id": 61382,
      "typeId": 11756,
      "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474804"
        },
        "Presentation": {
          "duration": "430",
          "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=SrhgD4w45BA"
        },
        "Preview": {
          "duration": "30",
          "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=iF1gp3ta4ro"
        },
        "Video Figure": {
          "duration": "205",
          "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=-79GF_e_blo"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66239,
        66274
      ],
      "eventIds": [],
      "abstract": "In many engineering disciplines such as circuit board, chip, and mechanical design, a hardware description language (HDL) approach provides important benefits over direct manipulation interfaces by supporting concepts like abstraction and generator meta-programming.\r\nWhile several such HDLs have emerged recently and promised power and flexibility, they also present challenges -- especially to designers familiar with current graphical workflows.\r\nIn this work, we investigate an IDE approach to provide a graphical editor for a board-level circuit design HDL.\r\nUnlike GUI builders which convert an entire diagram to code, we instead propose generating equivalent HDL from individual graphical edit actions.\r\nBy keeping code as the primary design input, we preserve the full power of the underlying HDL, while remaining useful even to advanced users.\r\nWe discuss our concept, design considerations such as performance, system implementation, and report on the results of an exploratory remote user study with four experienced hardware designers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 60956
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 61037
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 60912
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 60986
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 61076
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 61131
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 61153
        }
      ]
    },
    {
      "id": 61383,
      "typeId": 11756,
      "title": "SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474750"
        },
        "Presentation": {
          "duration": "610",
          "title": "SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7NLicZYRuIQ"
        },
        "Preview": {
          "duration": "30",
          "title": "SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ZR5MFh2jKIU"
        },
        "Video Figure": {
          "duration": "327",
          "title": "SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=l3wdOw5eiGc"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66237,
        66244
      ],
      "eventIds": [],
      "abstract": "We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated for MR systems to be beneficial for end users. We contribute an approach that formulates this challenge as a combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. To achieve this, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users' current task, layout factors, and spatio-temporal consistency to previous layouts. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic associations into account, our approach decreased the number of manual interface adaptations by 33\\%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Swarthmore",
              "institution": "Swarthmore College, Swarthmore, Pennsylvania, United States",
              "dsl": "Swarthmore College"
            }
          ],
          "personId": 61235
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 61096
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 61173
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 61019
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 61239
        }
      ]
    },
    {
      "id": 61384,
      "typeId": 11756,
      "title": "Ten Million Users and Ten Years Later: Python Tutor's Design Guidelines for Building Scalable and Sustainable Research Software in Academia",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474819"
        },
        "Presentation": {
          "duration": "181",
          "title": "Ten Million Users and Ten Years Later: Python Tutor's Design Guidelines for Building Scalable and Sustainable Research Software in Academia",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=najKuOqdmbA"
        },
        "Preview": {
          "duration": "30",
          "title": "Ten Million Users and Ten Years Later: Python Tutor's Design Guidelines for Building Scalable and Sustainable Research Software in Academia",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=HUU5bDL4p3s"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66270,
        66281
      ],
      "eventIds": [],
      "abstract": "Research software is often built as prototypes that never get widespread usage and are left unmaintained after a few papers get published. To counteract this trend, we propose a method for building research software with scale and sustainability in mind so that it can organically grow a large userbase and enable longer-term research. To illustrate this method, we present the design and implementation of Python Tutor (pythontutor.com), a code visualization tool that is, to our knowledge, one of the most widely-used pieces of research software developed within a university lab. Over the past decade, it has been used by over ten million people in over 180 countries. It has also contributed to 55 publications from 35 research groups in 13 countries. We distilled lessons from working on Python Tutor into three sets of design guidelines: 1) user experience design for scale and sustainability, 2) software architecture design for long-term sustainability, and 3) designing a sustainable software development workflow within academia. These guidelines can enable a student to create long-lasting software that reaches many users and facilitates research from many independent groups.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Cognitive Science"
            }
          ],
          "personId": 60944
        }
      ]
    },
    {
      "id": 61385,
      "typeId": 11756,
      "title": "Do We Need a Faster Mouse? Empirical Evaluation of Asynchronicity-Induced Jitter",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474783"
        },
        "Presentation": {
          "duration": "599",
          "title": "Do We Need a Faster Mouse? Empirical Evaluation of Asynchronicity-Induced Jitter",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lFktO_91eGc"
        },
        "Preview": {
          "duration": "30",
          "title": "Do We Need a Faster Mouse? Empirical Evaluation of Asynchronicity-Induced Jitter",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GIHtNP6UfLg"
        },
        "Video Figure": {
          "duration": "94",
          "title": "Do We Need a Faster Mouse? Empirical Evaluation of Asynchronicity-Induced Jitter",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=djCLZ6qEVuA"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66257,
        66278
      ],
      "eventIds": [],
      "abstract": "In gaming, accurately rendering input signals on a display is crucial, both spatially and temporally. However, the asynchronicity between the input and output signal frequencies results in unstable responses called \"jitter.\" A recent research modeled this jitter mathematically; however, the effect of jitter on human performance is unknown. In this study, we investigated the empirical effect of asynchronicity-induced jitter using a state-of-the-art high-performance mouse and monitor device. In the first part, perceptual user experience under different jitter levels was examined using the ISO 4120:2004 triangle test protocol, and a jitter of over 0.3 ms could be perceived by sensitive subjects. In the second part, we measured the pointing task performance for different jitter levels using the ISO 9241-9 (i.e., Fitts' law) test, and found that the pointing performance was unaffected up to a jitter of 1 ms. Finally, we recommended display and mouse combinations based on our results, which indicated the need for a higher mouse polling rate than that of the current standard 1000-Hz USB mouse.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daegu",
              "institution": "Daegu Gyeongbuk Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 60985
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daegu",
              "institution": "Daegu Gyeongbuk Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 61205
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daegu",
              "institution": "Daegu Gyeongbuk Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 61099
        }
      ]
    },
    {
      "id": 61386,
      "typeId": 11756,
      "title": "Urban Brush: Intuitive and Controllable Urban Layout Editing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474787"
        },
        "Presentation": {
          "duration": "603",
          "title": "Urban Brush: Intuitive and Controllable Urban Layout Editing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Vzcq73o2jh4"
        },
        "Preview": {
          "duration": "30",
          "title": "Urban Brush: Intuitive and Controllable Urban Layout Editing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ABmnaLB7-Do"
        },
        "Video Figure": {
          "duration": "293",
          "title": "Urban Brush: Intuitive and Controllable Urban Layout Editing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=yFMD1NYEa2o"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66235,
        66240
      ],
      "eventIds": [],
      "abstract": "Efficient urban layout generation is an interesting and important problem in many applications dealing with computer graphics and entertainment. We introduce a novel framework for intuitive and controllable small and large-scale urban layout editing. The key inspiration comes from the observation that cities develop in small incremental changes e.g., a building is replaced, or a new road is created. We introduce a set of atomic operations that consistently modify the city. For example, two buildings are merged, a block is split in two, etc. Our second inspiration comes from volumetric editings, such as clay manipulation, where the manipulated material is preserved. The atomic operations are used in interactive brushes that consistently modify the urban layout. The city is populated with agents. Like volume transfer, the brushes attract or repulse the agents, and blocks can be merged and populated with smaller buildings. We also introduce a large-scale brush that repairs a part of the city by learning style as distributions of orientations and intersections. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61227
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61051
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Polytechnique",
              "dsl": "Computer Science"
            }
          ],
          "personId": 60969
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "Ile-de-France",
              "city": "Palaiseau",
              "institution": "Ecole Polytechnique, IP Paris",
              "dsl": "LIX, CNRS "
            }
          ],
          "personId": 60918
        }
      ]
    },
    {
      "id": 61387,
      "typeId": 11756,
      "title": "Space, Time, and Choice: A unified approach to flexible personal scheduling ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474764"
        },
        "Presentation": {
          "duration": "527",
          "title": "Space, Time, and Choice: A unified approach to flexible personal scheduling",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=DQI5JJiPUjA"
        },
        "Preview": {
          "duration": "30",
          "title": "Space, Time, and Choice: A unified approach to flexible personal scheduling",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=m96jF81kATI"
        },
        "Video Figure": {
          "duration": "262",
          "title": "Space, Time, and Choice: A unified approach to flexible personal scheduling",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=J4KZkCJLxFI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66247,
        66254
      ],
      "eventIds": [],
      "abstract": "In the context of increasingly busy lives and mobility constraints, we present a unified space-time approach to support flexible personal scheduling. We distill an analysis of the design requirements of interactive space-time scheduling into a single coherent workflow where users can manipulate a rich vocabulary of spatio-temporal parameters, and plan/explore itineraries that satisfy or optimize the resulting space-time constraints. We demonstrate our approach using a proof-of-concept mobile application that enables exploration of the inter-connected continuum between task scheduling (temporal), and multi-destination route mapping (spatial). We evaluate the application with a user study involving an itinerary reproduction task and a free-form planning task. We also provide usage scenarios illustrating the potential of our approach in various contexts and tasks. Results suggest that our approach fills an important gap between route mapping and calendar scheduling, suggesting a new research direction in personal planning interface design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61313
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science and Statistical Sciences"
            }
          ],
          "personId": 60996
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61151
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61069
        }
      ]
    },
    {
      "id": 61388,
      "typeId": 11756,
      "title": "Modeling Touch Point Distribution with Rotational Dual Gaussian Model",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474816"
        },
        "Presentation": {
          "duration": "393",
          "title": "Modeling Touch Point Distribution with Rotational Dual Gaussian Model",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Bg9J0xyF6Zs"
        },
        "Preview": {
          "duration": "30",
          "title": "Modeling Touch Point Distribution with Rotational Dual Gaussian Model",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=gZ5bYYYPpPg"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66267,
        66284
      ],
      "eventIds": [],
      "abstract": "Touch point distribution models are important tools for designing touchscreen interfaces. In this paper, we investigate how the finger movement direction affects the touch point distribution, and how to account for it in modeling. We propose the Rotational Dual Gaussian model, a refinement and generalization of the Dual Gaussian model, to account for the finger movement direction in predicting touch point distribution. In this model, the major axis of the prediction ellipse of the touch point distribution is along the finger movement direction, and the minor axis is perpendicular to the finger movement direction. We also propose using projected target width and height, in lieu of nominal target width and height to model touch point distribution. Evaluation on three empirical datasets shows that the new model reflects the observation that the touch point distribution is elongated along the finger movement direction, and outperforms the original Dual Gaussian Model in all prediction tests. Compared with the original Dual Gaussian model, the Rotational Dual Gaussian model reduces the RMSE of touch error rate prediction from 8.49% to 4.95%, and more accurately predicts the touch point distribution in target acquisition. Using the Rotational Dual Gaussian model can also improve the soft keyboard decoding accuracy on smartwatches.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 61258
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 60932
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61039
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61281
        }
      ]
    },
    {
      "id": 61389,
      "typeId": 11756,
      "title": "iText: Hands-free Text Entry on an Imaginary Keyboard for Augmented Reality Systems",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474788"
        },
        "Presentation": {
          "duration": "480",
          "title": "iText: Hands-free Text Entry on an Imaginary Keyboard for Augmented Reality Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2j4efVrdHq0"
        },
        "Preview": {
          "duration": "30",
          "title": "iText: Hands-free Text Entry on an Imaginary Keyboard for Augmented Reality Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=v0aF2A0Xt3s"
        },
        "Video Figure": {
          "duration": "133",
          "title": "iText: Hands-free Text Entry on an Imaginary Keyboard for Augmented Reality Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=O1TAogyZ2Fs"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66257,
        66269
      ],
      "eventIds": [],
      "abstract": "Text entry is an important and frequent task in interactive devices including augmented reality head-mounted displays (AR HMDs). In current AR HMDs, there are still two main open challenges to overcome for efficient and usable text entry: arm fatigue due to mid-air input and visual occlusion because of their small see-through displays. To address these challenges, we present iText, a technique for AR HMDs that is hands-free and is based on an imaginary (invisible) keyboard. We first show that it is feasible and practical to use an imaginary keyboard on AR HMDs. Then, we evaluated its performance and usability with three hands-free selection mechanisms: eye blinks (E-Type), dwell (D-Type), and swipe gestures (G-Type). Our results show that users could achieve an average text entry speed of 11.95, 9.03 and 9.84 words per minutes (WPM) with E-Type, D-Type, and G-Type, respectively. Given that iText with E-Type outperformed the other two selection mechanisms in text entry rate and subjective feedback, we ran a third, 5-day study. Our results show that iText with E-Type can achieve an average text entry rate of 13.76 WPM with a mean word error rate of 1.5\\%. In short, iText can enable efficient eyes-free text entry and can be useful for various application scenarios in AR HMDs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": "Computer Science and Software Engineering"
            }
          ],
          "personId": 61015
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 60938
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": "Department of Computing"
            }
          ],
          "personId": 61129
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 61212
        }
      ]
    },
    {
      "id": 61390,
      "typeId": 11756,
      "title": "ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474805"
        },
        "Presentation": {
          "duration": "457",
          "title": "ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Vz91HHVu92A"
        },
        "Preview": {
          "duration": "30",
          "title": "ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=-MyDp9Z-CkI"
        },
        "Video Figure": {
          "duration": "169",
          "title": "ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JBHjbzPDzn8"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66268,
        66272
      ],
      "eventIds": [],
      "abstract": "3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm*60cm*60cm space and 22.1 mm in the 30cm*30cm*30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 61101
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 61253
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 61096
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Information School"
            }
          ],
          "personId": 60963
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 61019
        }
      ]
    },
    {
      "id": 61391,
      "typeId": 11756,
      "title": "An Aligned Rank Transform Procedure for Multifactor Contrast Tests",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474784"
        },
        "Presentation": {
          "duration": "475",
          "title": "An Aligned Rank Transform Procedure for Multifactor Contrast Tests",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=t2BNFdp93C8"
        },
        "Preview": {
          "duration": "30",
          "title": "An Aligned Rank Transform Procedure for Multifactor Contrast Tests",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=OEgMoy_hFjU"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66255,
        66279
      ],
      "eventIds": [],
      "abstract": "Data from multifactor HCI experiments often violates the assumptions of parametric tests (i.e., nonconforming data). The Aligned Rank Transform (ART) has become a popular nonparametric analysis in HCI that can find main and interaction effects in nonconforming data, but leads to incorrect results when used to conduct post hoc contrast tests. We created a new algorithm called ART-C for conducting contrast tests within the ART paradigm and validated it on 72,000 synthetic data sets. Our results indicate that ART-C does not inflate Type I error rates, unlike contrasts based on ART, and that ART-C has more statistical power than a t-test, Mann-Whitney U test, Wilcoxon signed-rank test, and ART. We also extended an open-source tool called ARTool with our ART-C algorithm for both Windows and R. Our validation had some limitations (e.g., only six distribution types, no mixed factorial designs, no random slopes), and data drawn from Cauchy distributions should not be analyzed with ART-C.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 61243
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": "Computer Science and Communication"
            }
          ],
          "personId": 60911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Manhattan",
              "institution": "Kansas State U",
              "dsl": ""
            }
          ],
          "personId": 61066
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 61067
        }
      ]
    },
    {
      "id": 61392,
      "typeId": 11756,
      "title": "Adroid: Augmenting Hands-on Making with a Collaborative Robot",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474749"
        },
        "Presentation": {
          "duration": "395",
          "title": "Adroid: Augmenting Hands-on Making with a Collaborative Robot",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jICf8EcOr7U"
        },
        "Preview": {
          "duration": "30",
          "title": "Adroid: Augmenting Hands-on Making with a Collaborative Robot",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=x-zAx3TsZjo"
        },
        "Video Figure": {
          "duration": "164",
          "title": "Adroid: Augmenting Hands-on Making with a Collaborative Robot",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=BKZopeNAvsY"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66255,
        66261
      ],
      "eventIds": [],
      "abstract": "Adroid enables users to borrow precision and accuracy from a robotic arm when using hand-held tools.  When a tool is mounted to the robot, the user can hold and move the tool directly-Adroid measures the user's applied forces and commands the robot to move in response.  Depending on the tool and scenario, Adroid can selectively restrict certain motions. In the resulting interaction, the robot acts like a virtual \"jig\" which constrains the tool’s motion; augmenting the user's accuracy, technique, and strength, while not diminishing their agency during open-ended fabrication tasks. We complement these hands-on interactions with projected augmented reality for visual feedback about the state of the system. We show how tools augmented by Adroid can support hands-on making and discuss how it can be configured to support other tasks within and beyond fabrication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "Computer Sciences"
            }
          ],
          "personId": 61254
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": ""
            }
          ],
          "personId": 61295
        }
      ]
    },
    {
      "id": 61393,
      "typeId": 11756,
      "title": "Idyll Studio: A Structured Editor for Authoring Interactive & Data-Driven Articles",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474731"
        },
        "Presentation": {
          "duration": "516",
          "title": "Idyll Studio: A Structured Editor for Authoring Interactive & Data-Driven Articles",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=OQQgezM6avw"
        },
        "Preview": {
          "duration": "30",
          "title": "Idyll Studio: A Structured Editor for Authoring Interactive & Data-Driven Articles",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1SmsrTC8Z1k"
        },
        "Video Figure": {
          "duration": "303",
          "title": "Idyll Studio: A Structured Editor for Authoring Interactive & Data-Driven Articles",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ebO6tYguHV4"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66254,
        66264
      ],
      "eventIds": [],
      "abstract": "Interactive articles are an effective medium of communication in education, journalism, and scientific publishing, yet are created using complex general-purpose programming tools. We present \\textit{Idyll Studio}, a structured editor for authoring and publishing interactive and data-driven articles. We extend the Idyll framework to support reflective documents, which can inspect and modify their underlying program at runtime, and show how this functionality can be used to reify the constituent parts of a reactive document model---components, text, state, and styles---in an expressive, interoperable, and easy-to-learn graphical interface. In a study with 18 diverse participants, all could perform basic editing and composition, use datasets and variables, and specify relationships between components. Most could choreograph interactive visualizations and dynamic text, although some struggled with advanced uses requiring unstructured code editing. Our findings suggest \\textit{Idyll Studio} lowers the threshold for non-experts to create interactive articles and allows experts to rapidly specify a wide range of article designs.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 61132
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Idyll Collective",
              "dsl": ""
            }
          ],
          "personId": 61135
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Idyll Collective",
              "dsl": ""
            }
          ],
          "personId": 61209
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 61156
        }
      ]
    },
    {
      "id": 61394,
      "typeId": 11756,
      "title": "App-Based Task Shortcuts for Virtual Assistants",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474808"
        },
        "Presentation": {
          "duration": "589",
          "title": "App-Based Task Shortcuts for Virtual Assistants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=79eO74e7cQM"
        },
        "Preview": {
          "duration": "30",
          "title": "App-Based Task Shortcuts for Virtual Assistants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9Wdjpz268L0"
        },
        "Video Figure": {
          "duration": "124",
          "title": "App-Based Task Shortcuts for Virtual Assistants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4GZOGIwTC24"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66238,
        66246
      ],
      "eventIds": [],
      "abstract": "Virtual assistants like Google Assistant and Siri often interface with external apps when they cannot directly perform a task. Currently, developers must manually expose the capabilities of their apps to virtual assistants, using App Actions on Android or Shortcuts on iOS. This paper presents SAVANT, a system that automatically generates task shortcuts for virtual assistants by mapping user intents and entities to relevant UI screens in apps. For a given natural language task (e.g., ``send money to Joe''), SAVANT leverages text and semantic information contained within a UI to identify relevant screens, and intent modeling to parse and map entities (e.g., ``Joe'') to required UI inputs. Therefore, SAVANT allows virtual assistants to interface with apps and handle new tasks without requiring any developer effort. To evaluate SAVANT, we performed a user study to identify common tasks users perform with virtual assistants. We then demonstrate that SAVANT can find relevant app screens for those tasks and autocomplete the UI inputs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois Urbana-Champaign",
              "dsl": "Computer Science"
            }
          ],
          "personId": 60958
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois, Urbana-Champaign",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 61273
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "User Testing, Inc.",
              "dsl": ""
            }
          ],
          "personId": 61191
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 60945
        }
      ]
    },
    {
      "id": 61395,
      "typeId": 11756,
      "title": "Unravel: A Fluent Code Explorer for Data Wrangling",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474744"
        },
        "Presentation": {
          "duration": "297",
          "title": "Unravel: A Fluent Code Explorer for Data Wrangling",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wJ77e39XVEs"
        },
        "Preview": {
          "duration": "30",
          "title": "Unravel: A Fluent Code Explorer for Data Wrangling",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ERrrVXkO_do"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66254,
        66264
      ],
      "eventIds": [],
      "abstract": "Data scientists have adopted a popular design pattern in programming called the fluent interface for composing data wrangling code. The fluent interface works by combining multiple transformations on a data table---or dataframes---with a single chain of expressions, which produces an output. Although fluent code promotes legibility, the intermediate dataframes are lost, forcing data scientists to unravel the chain through tedious code edits and re-execution. Existing tools for data scientists do not allow easy exploration or support understanding of fluent code. To address this gap, we designed a tool called Unravel that enables structural edits via drag-and-drop and toggle switch interactions to help data scientists explore and understand fluent code. Data scientists can apply simple structural edits via drag-and-drop and toggle switch interactions to reorder  and (un)comment lines. To help data scientists understand fluent code, Unravel provides function summaries and always-on visualizations highlighting important changes to a dataframe. We discuss the design motivations behind Unravel and how it helps understand and explore fluent code. In a first-use study with 14 data scientists, we found that Unravel facilitated diverse activities such as validating assumptions about the code or data, exploring alternatives, and revealing function behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Raleigh",
              "institution": "North Carolina State University",
              "dsl": ""
            }
          ],
          "personId": 61121
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 61031
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Raleigh",
              "institution": "North Carolina State University",
              "dsl": ""
            }
          ],
          "personId": 61080
        }
      ]
    },
    {
      "id": 61396,
      "typeId": 11756,
      "title": "Retargeted Self-Haptics for Increased Immersion in VR without Hand Instrumentation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474810"
        },
        "Presentation": {
          "duration": "527",
          "title": "Retargeted Self-Haptics for Increased Immersion in VR without Hand Instrumentation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=oFRZhm3BzJM"
        },
        "Preview": {
          "duration": "30",
          "title": "Retargeted Self-Haptics for Increased Immersion in VR without Hand Instrumentation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_6dOVmh4LXk"
        },
        "Video Figure": {
          "duration": "220",
          "title": "Retargeted Self-Haptics for Increased Immersion in VR without Hand Instrumentation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=V8d6Hm2QgxU"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66253,
        66263
      ],
      "eventIds": [],
      "abstract": "Today’s consumer virtual reality (VR) systems offer immersive graphics and audio, but haptic feedback is rudimentary –  delivered through controllers with vibration feedback or is non-existent (i.e., the hands operating freely in the air). In this paper, we explore an alternative, highly mobile and controller-free approach to haptics, where VR applications utilize the user’s own body to provide physical feedback. To achieve this, we warp (retarget) the locations of a user’s hands such that one hand serves as a physical surface or prop for the other hand. For example, a hand holding a virtual nail can serve as a physical backstop for a hand that is virtually hammering, providing a sense of impact in an air-borne and uninstrumented experience. To illustrate this rich design space, we implemented twelve interactive demos across three haptic categories. We conclude with a user study from which we draw design recommendations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 61032
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 61036
        }
      ]
    },
    {
      "id": 61397,
      "typeId": 11756,
      "title": "VEmotion: Using Driving Context for Indirect Emotion Prediction in Real-Time",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474775"
        },
        "Presentation": {
          "duration": "592",
          "title": "VEmotion: Using Driving Context for Indirect Emotion Prediction in Real-Time",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9S2ILPsmseM"
        },
        "Preview": {
          "duration": "30",
          "title": "VEmotion: Using Driving Context for Indirect Emotion Prediction in Real-Time",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hi2VwV5htzI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66273,
        66282
      ],
      "eventIds": [],
      "abstract": "Detecting emotions while driving remains a challenge in Human-Computer Interaction. Current methods to estimate the driver's experienced emotions use physiological sensing (e.g., skin-conductance, electroencephalography), speech, or facial expressions. However, drivers need to use wearable devices, perform explicit voice interaction, or require robust facial expressiveness. We present VEmotion (Virtual Emotion Sensor), a novel method to predict driver emotions in an unobtrusive way using contextual smartphone data. VEmotion analyzes information including traffic dynamics, environmental factors, in-vehicle context, and road characteristics to implicitly classify driver emotions. We demonstrate the applicability in a real-world driving study (N=12) to evaluate the emotion prediction performance. Our results show that VEmotion outperforms facial expressions by 29% in a person-dependent classification and by 8.5%  in a person-independent classification. We discuss how VEmotion enables empathic car interfaces to sense the driver's emotions and will provide in-situ interface adaptations on-the-go.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Porsche AG",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Ludwig-Maximilian University",
              "dsl": ""
            }
          ],
          "personId": 60977
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 61216
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Porsche AG",
              "dsl": ""
            }
          ],
          "personId": 60975
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 61001
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Dr. Ing. h. c. F. Porsche AG",
              "dsl": ""
            }
          ],
          "personId": 61201
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Porsche AG",
              "dsl": ""
            }
          ],
          "personId": 61130
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 61306
        }
      ]
    },
    {
      "id": 61398,
      "typeId": 11756,
      "title": "SGToolkit: An Interactive Gesture Authoring Toolkit for Embodied Conversational Agents",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474789"
        },
        "Presentation": {
          "duration": "603",
          "title": "SGToolkit: An Interactive Gesture Authoring Toolkit for Embodied Conversational Agents",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qClSOtLiVlc"
        },
        "Preview": {
          "duration": "30",
          "title": "SGToolkit: An Interactive Gesture Authoring Toolkit for Embodied Conversational Agents",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xmFsT_e2BXw"
        },
        "Video Figure": {
          "duration": "272",
          "title": "SGToolkit: An Interactive Gesture Authoring Toolkit for Embodied Conversational Agents",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qsK3p1S_K48"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66256,
        66276
      ],
      "eventIds": [],
      "abstract": "Non-verbal behavior is essential for embodied agents like social robots, virtual avatars, and digital humans. Existing behavior authoring approaches including keyframe animation and motion capture are too expensive to use when there are numerous utterances requiring gestures. Automatic generation methods show promising results, but their output quality is not satisfactory yet, and it is hard to modify outputs as a gesture designer wants. We introduce a new gesture generation toolkit, named SGToolkit, which gives a higher quality output than automatic methods and is more efficient than manual authoring. For the toolkit, we propose a neural generative model that synthesizes gestures from speech and accommodates fine-level pose controls and coarse-level style controls from users. The user study with 24 participants showed that the toolkit is favorable over manual authoring, and the generated gestures were also human-like and appropriate to input speech. The SGToolkit is platform agnostic, and the code is available at https://github.com/ai4r/SGToolkit.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute (ETRI)",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": " Korea Advanced Institute of Science and Technology (KAIST)",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 61000
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "HCI Lab, School of Computing"
            }
          ],
          "personId": 60952
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 61207
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "ETRI",
              "dsl": ""
            }
          ],
          "personId": 60962
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 61087
        }
      ]
    },
    {
      "id": 61399,
      "typeId": 11756,
      "title": "EIT-kit: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474758"
        },
        "Presentation": {
          "duration": "602",
          "title": "EIT-kit: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ILT1Ny4yzik"
        },
        "Preview": {
          "duration": "30",
          "title": "EIT-kit: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uU2XiFKAbek"
        },
        "Video Figure": {
          "duration": "388",
          "title": "EIT-kit: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Z4drUWcueHA"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66265,
        66268
      ],
      "eventIds": [],
      "abstract": "In this paper, we propose EIT-kit, an electrical impedance tomography toolkit for designing and fabricating health and motion sensing devices. EIT-kit contains (1) an extension to a 3D editor for personalizing the form factor of electrode arrays and electrode distribution, (2) a customized EIT sensing motherboard for performing the measurements, (3) a microcontroller library that automates signal calibration and facilitates data collection, and (4) an image reconstruction library for mobile devices for interpolating and visualizing the measured data. Together, these EIT-kit components allow for applications that require 2- or 4-terminal setups, up to 64 electrodes, and single or multiple (up to four) electrode arrays simultaneously. \r\n\r\nWe motivate the design of each component of EIT-kit with a formative study, and conduct a technical evaluation of the data fidelity of our EIT measurements. We demonstrate the design space that EIT-kit enables by showing various applications in health as well as motion sensing and control. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61275
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61152
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": " Massachusetts Institute of Technology ",
              "dsl": "MIT CSAIL"
            }
          ],
          "personId": 61078
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 60967
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 60988
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts General Hospital",
              "dsl": ""
            }
          ],
          "personId": 60922
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts General Hospital ",
              "dsl": ""
            }
          ],
          "personId": 61290
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61017
        }
      ]
    },
    {
      "id": 61400,
      "typeId": 11756,
      "title": "HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474821"
        },
        "Presentation": {
          "duration": "447",
          "title": "HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cFq5JNtXSKo"
        },
        "Preview": {
          "duration": "30",
          "title": "HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=HTiZgOESJyQ"
        },
        "Video Figure": {
          "duration": "159",
          "title": "HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ErMj8VdlkyI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66253,
        66262
      ],
      "eventIds": [],
      "abstract": "HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user's hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 60987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 60921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": "Microsoft Research"
            }
          ],
          "personId": 61181
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado, Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 61172
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61179
        }
      ]
    },
    {
      "id": 61401,
      "typeId": 11756,
      "title": "Game Illusionization: A Workflow for Applying Optical Illusions to Video Games",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474824"
        },
        "Presentation": {
          "duration": "542",
          "title": "Game Illusionization: A Workflow for Applying Optical Illusions to Video Games",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=kmdORoUV00Q"
        },
        "Preview": {
          "duration": "30",
          "title": "Game Illusionization: A Workflow for Applying Optical Illusions to Video Games",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=BjKVeE8oyzA"
        },
        "Video Figure": {
          "duration": "340",
          "title": "Game Illusionization: A Workflow for Applying Optical Illusions to Video Games",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=SCowM-91uZY"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66258,
        66279
      ],
      "eventIds": [],
      "abstract": "Optical illusions have been brought into recent video games to enhance gaming experiences. \r\n  However, a large corpus of optical illusions remains unused while few games incorporate illusions seamlessly.\r\n  To mitigate the gap, we propose a workflow to guide game designers to apply optical illusions to their video games, i.e., making more illusion games. \r\n  In particular, our workflow consists of 5 stages: (1) choosing a game object, (2) searching for a matching illusion, (3) selecting an illusion mechanic, (4) integrating the selected illusion into the game, and (5) optionally explicit revealing the illusion. \r\n  To facilitate our workflow, we provide a tag database with 163 illusions that are labeled by their in-game visual elements and desired effects. \r\n  We also provide example editing interfaces of 6 illusions for game designers. \r\n  We walk through our workflow and showcase 6 resulting illusion games. \r\n  We implemented these 6 games (with and without illusion) and conducted a 12-participant study to gain a preliminary understanding of how illusions enhance gaming experiences. \r\n  To evaluate our workflow, we invited 6 game designers and 6 experienced players to follow our workflow and design their own illusion games, where 3 experienced game designers completed 2-week in-depth developments.\r\n  We report their games, qualitative feedback and discuss reflection on our workflow, database, and editing interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 60926
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61263
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61298
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61251
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61228
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61089
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei City",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61022
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61223
        }
      ]
    },
    {
      "id": 61402,
      "typeId": 11756,
      "title": "Tabs.do: Task-Centric Browser Tab Management",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474777"
        },
        "Presentation": {
          "duration": "483",
          "title": "Tabs.do: Task-Centric Browser Tab Management",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=UrXMVkqfYbg"
        },
        "Preview": {
          "duration": "30",
          "title": "Tabs.do: Task-Centric Browser Tab Management",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ZedYkS9DHnE"
        },
        "Video Figure": {
          "duration": "251",
          "title": "Tabs.do: Task-Centric Browser Tab Management",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uVVvZ8yyj_U"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66238,
        66246
      ],
      "eventIds": [],
      "abstract": "Despite the increasing complexity and scale of people’s online activities, browser interfaces have stayed largely the same since tabs were introduced in major browsers nearly 20 years ago. The gap between simple tab-based browser interfaces and the complexity of users’ tasks can lead to serious adverse effects -- commonly referred to as \"tab overload.\" This paper introduces a Chrome extension called Tabs.do, which explores bringing a task-centric approach to the browser, helping users to group their tabs into tasks and then organize, prioritize, and switch between those tasks fluidly. To lower the cost of importing, Tabs.do uses machine learning to make intelligent suggestions for grouping users’ open tabs into task bundles by exploiting behavioral and semantic features. We conducted a field deployment study where participants used Tabs.do with their real-life tasks in the wild, and showed that Tabs.do can decrease tab clutter, enabled users to create rich task structures with lightweight interactions, and allowed participants to context-switch among tasks more efficiently.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "HCII and LTI",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 61167
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 61294
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Human-Computer Interaction Institute",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 60964
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Human-Computer Interaction Institute",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 61071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 60972
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 60929
        }
      ]
    },
    {
      "id": 61403,
      "typeId": 11756,
      "title": "MoiréBoard: A Stable, Accurate and Low-cost Camera Tracking Method",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474793"
        },
        "Presentation": {
          "duration": "559",
          "title": "MoiréBoard: A Stable, Accurate and Low-cost Camera Tracking Method",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Sq2xyzZ95uU"
        },
        "Preview": {
          "duration": "30",
          "title": "MoiréBoard: A Stable, Accurate and Low-cost Camera Tracking Method",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=u0bPYICHOkE"
        },
        "Video Figure": {
          "duration": "257",
          "title": "MoiréBoard: A Stable, Accurate and Low-cost Camera Tracking Method",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VjFi9xDMP1o"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66268,
        66282
      ],
      "eventIds": [],
      "abstract": "Camera tracking is an essential building block in a myriad of HCI applications. For example, commercial VR devices are equipped with dedicated hardware, such as laser-emitting beacon stations, to enable accurate tracking of VR headsets. However, this hardware remains costly. On the other hand, low-cost solutions such as IMU sensors and visual markers exist, but they suffer from large tracking errors. In this work, we bring high accuracy and low cost together to present MoiréBoard, a new 3-DOF camera position tracking method that leverages a seemingly irrelevant visual phenomenon, the moiré effect. Based on a systematic analysis of the moiré effect under camera projection, MoiréBoard requires no power nor camera calibration. It can be easily made at a low cost (e.g., through 3D printing), ready to use with any stock mobile devices with a camera. Its tracking algorithm is computationally efficient, able to run at a high frame rate. Although it is simple to implement, it tracks devices at high accuracy, comparable to the state-of-the-art commercial VR tracking systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 60914
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 60943
        }
      ]
    },
    {
      "id": 61404,
      "typeId": 11756,
      "title": "Tilt-Explore: Making Tilt Gestures Usable for Low-Vision Smartphone Users",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474813"
        },
        "Presentation": {
          "duration": "661",
          "title": "Tilt-Explore: Making Tilt Gestures Usable for Low-Vision Smartphone Users",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hAisk5puENk"
        },
        "Preview": {
          "duration": "30",
          "title": "Tilt-Explore: Making Tilt Gestures Usable for Low-Vision Smartphone Users",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cHLBIoucnxY"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66255,
        66276
      ],
      "eventIds": [],
      "abstract": "People with low vision interact with smartphones using assistive technologies like screen magnifiers, which provide built-in touch gestures to pan and zoom onscreen content. These gestures are often cumbersome and require bimanual interaction. Of particular interest is panning gestures, which are issued frequently, which involve 2- or 3-finger dragging. This paper aims to utilize tilt-based interaction as a single-handed alternative to built-in panning gestures. To that end, we first identified our design space from the literature and conducted an exploratory user study with 12 low-vision participants to understand key challenges. Among many findings, the study revealed that built-in panning gestures are error-prone, and most tilt-based interaction techniques are designed for sighted users, which low vision users struggle to use as-is. We addressed these challenges by adapting low-vision users' interaction behavior and proposed Tilt-Explore, a new screen magnifier mode that enables tilt-to-pan. A second study with 16 low-vision participants revealed that, compared to built-in gestures, the participants were significantly less error-prone; and for lower magnification scale (e.g., <4x), they were significantly more efficient with Tilt-Explore. These findings indicate Tilt-Explore is a promising alternative to built-in panning gestures.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology "
            }
          ],
          "personId": 61245
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park ",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 61276
        }
      ]
    },
    {
      "id": 61405,
      "typeId": 11756,
      "title": "TypeBoard: Identifying Unintentional Touch on Pressure-Sensitive Touchscreen Keyboards",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474770"
        },
        "Presentation": {
          "duration": "482",
          "title": "TypeBoard: Identifying Unintentional Touch on Pressure-Sensitive Touchscreen Keyboards",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nUyms-nZn2c"
        },
        "Preview": {
          "duration": "30",
          "title": "TypeBoard: Identifying Unintentional Touch on Pressure-Sensitive Touchscreen Keyboards",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lk91bHGZKaE"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66257,
        66277
      ],
      "eventIds": [],
      "abstract": "Text input is essential in tablet computer interaction. However, tablet software keyboards face the problem of misrecognizing unintentional touch, which affects efficiency and usability. In this paper, we proposed TypeBoard, a pressure-sensitive touchscreen keyboard that prevents unintentional touches. The TypeBoard allows users to rest their fingers on the touchscreen, which changes the user behavior: on average, users generate 40.83 unintentional touches every 100 keystrokes. The TypeBoard prevents unintentional touch with an accuracy of 98.88%. A typing study showed that the TypeBoard reduced fatigue (𝑝 < 0.005) and typing errors (𝑝 < 0.01), and improved the touchscreen keyboard’ typing speed by 11.78% (𝑝 < 0.005). As users could touch the screen without triggering responses, we added tactile landmarks on the TypeBoard, allowing users to locate the keys by the sense of touch. This feature further improves the typing speed, outperforming the ordinary tablet keyboard by 21.19% (𝑝 < 0.001). Results show that pressure-sensitive touchscreen keyboards can prevent unintentional touch, improving usability from many aspects, such as avoiding fatigue, reducing errors, and mediating touch typing on tablets.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology "
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology "
            }
          ],
          "personId": 61242
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 60951
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 61012
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
              "dsl": "Department of Computer Science and Technology, Tsinghua University, Beijing, China"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
              "dsl": "Department of Computer Science and Technology, Tsinghua University, Beijing, China"
            }
          ],
          "personId": 61112
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 61019
        }
      ]
    },
    {
      "id": 61406,
      "typeId": 11756,
      "title": "HoloBoard: a Large-format Immersive Teaching Board based on Pseudo HoloGraphics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474761"
        },
        "Presentation": {
          "duration": "450",
          "title": "HoloBoard: a Large-format Immersive Teaching Board based on Pseudo HoloGraphics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=dScyD3m8c8Q"
        },
        "Preview": {
          "duration": "30",
          "title": "HoloBoard: a Large-format Immersive Teaching Board based on Pseudo HoloGraphics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=RPQfR-Q5yJs"
        },
        "Video Figure": {
          "duration": "313",
          "title": "HoloBoard: a Large-format Immersive Teaching Board based on Pseudo HoloGraphics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=afXr11Sd438"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66237,
        66243
      ],
      "eventIds": [],
      "abstract": "In this paper, we present HoloBoard, an interactive large-format pseduo-holographic display system for lecture based classes. With its unique properties of immersive visual display and transparent screen, we designed and implemented a rich set of novel interaction techniques like immersive presentation, role-play, and lecturing behind the scene that are potentially valuable for lecturing in class. We conducted a controlled experimental study to compare a HoloBoard class with a normal class through measuring students’ learning outcomes and three dimensions of engagement (i.e., behavioral, emotional, and cognitive engagement). We used pre-/post- knowledge tests and multimodal learning analytics to measure students’ learning outcomes and learning experiences. Results indicated that the lecture-based class utilizing HoloBoard lead to slightly better learning outcomes and a significantly higher level of student engagement.    Given the results, we discussed the impact of HoloBoard as an immersive media in the classroom setting and suggest several design implications for deploying HoloBoard in immersive teaching practices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Lenovo Research",
              "dsl": ""
            }
          ],
          "personId": 60995
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 60961
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Lenovo Research",
              "dsl": ""
            }
          ],
          "personId": 61064
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61241
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 61252
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "--无--",
              "city": "Beijing",
              "institution": "lenovo research",
              "dsl": ""
            }
          ],
          "personId": 61417
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 61418
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Lenovo Research",
              "dsl": ""
            }
          ],
          "personId": 61419
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Lenovo Research",
              "dsl": ""
            }
          ],
          "personId": 61520
        }
      ]
    },
    {
      "id": 61407,
      "typeId": 11756,
      "title": "MetaSense: Integrating Sensing Capabilities into Mechanical Metamaterial",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474806"
        },
        "Presentation": {
          "duration": "570",
          "title": "MetaSense: Integrating Sensing Capabilities into Mechanical Metamaterial",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=pem2U1bWCH8"
        },
        "Preview": {
          "duration": "30",
          "title": "MetaSense: Integrating Sensing Capabilities into Mechanical Metamaterial",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lKdEos_CjMQ"
        },
        "Video Figure": {
          "duration": "99",
          "title": "MetaSense: Integrating Sensing Capabilities into Mechanical Metamaterial",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wXYzZ9x0_GE"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66236,
        66248
      ],
      "eventIds": [],
      "abstract": "In this paper, we present a method to integrate sensing capabilities into 3D printable metamaterial structures comprised of cells, which enables the creation of monolithic input devices for HCI. We accomplish this by converting select opposing cell walls within the metamaterial device into electrodes, thereby creating capacitive sensors. When a user interacts with the object and applies a force, the distance and overlapping area between opposing cell walls change, resulting in a measurable capacitance variation. \r\n  \r\nTo help designers create interactive metamaterial devices, we contribute a design and fabrication pipeline based on multi-material 3D printing. Our 3D editor automatically places conductive cells in locations that are most affected by deformation during interaction and thus are most suitable as sensors. On export, our editor creates two files, one for conductive and one for non-conductive cell walls, which designers can fabricate on a multi-material 3D printer. Our applications show that designers can create metamaterial devices that sense various interactions, including sensing acceleration, binary state, shear, and magnitude and direction of applied force.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 61283
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61107
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Media Lab",
              "dsl": "MIT"
            }
          ],
          "personId": 61308
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 60913
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61017
        }
      ]
    },
    {
      "id": 61408,
      "typeId": 11756,
      "title": "So Predictable! Continuous 3D Hand Trajectory Prediction in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474753"
        },
        "Presentation": {
          "duration": "455",
          "title": "So Predictable! Continuous 3D Hand Trajectory Prediction in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=H-xq6renfaE"
        },
        "Preview": {
          "duration": "30",
          "title": "So Predictable! Continuous 3D Hand Trajectory Prediction in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=USbv2mYD7Nw"
        },
        "Video Figure": {
          "duration": "103",
          "title": "So Predictable! Continuous 3D Hand Trajectory Prediction in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wmIVw5a5kJI"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66234,
        66243
      ],
      "eventIds": [],
      "abstract": "We contribute a novel user- and activity-independent kinematics-based regressive model for continuously predicting ballistic hand movements in virtual reality (VR).\r\nCompared to prior work on end-point prediction, continuous hand trajectory prediction in VR enables an early estimation of future events such as collisions between the user’s hand and virtual objects such as UI widgets. \r\nWe developed and validated our prediction model through a user study with 20 participants. \r\nThe study collected hand motion data with a 3D pointing task and a gaming task with three popular VR games. \r\nResults show that our model can achieve a low Root Mean Square Error (RMSE) of 0.80 cm, 0.85 cm and 3.15 cm from future hand positions ahead of 100 ms, 200 ms and 300 ms respectively across all the users and activities. \r\nIn pointing tasks, our predictive model achieves an average angular error of 4.0° and 1.5° from the true landing position when 50% and 70% of the way through the movement. \r\nA follow-up study showed that the model can be applied to new users and new activities without further training. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Sydney",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 60946
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Darlington",
              "institution": "University of Sydney",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Sri Lanka",
              "state": "",
              "city": "Moratuwa",
              "institution": "University of Moratuwa",
              "dsl": "Department of Electronic and Telecommunication Engineering"
            }
          ],
          "personId": 61063
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Offenbach am Main",
              "institution": "Honda Research Institute Europe",
              "dsl": ""
            }
          ],
          "personId": 61214
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "The University of Sydney Nano Institute (Sydney Nano)"
            }
          ],
          "personId": 61009
        }
      ]
    },
    {
      "id": 61409,
      "typeId": 11756,
      "title": "Route Tapestries: Navigating 360 Virtual Tour Videos Using Slit-Scan Visualizations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474746"
        },
        "Presentation": {
          "duration": "478",
          "title": "Route Tapestries: Navigating 360 Virtual Tour Videos Using Slit-Scan Visualizations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0uAhUxJXgpc"
        },
        "Preview": {
          "duration": "30",
          "title": "Route Tapestries: Navigating 360 Virtual Tour Videos Using Slit-Scan Visualizations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=6AtGcoCY87U"
        },
        "Video Figure": {
          "duration": "247",
          "title": "Route Tapestries: Navigating 360 Virtual Tour Videos Using Slit-Scan Visualizations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=fP9A7sqEYDY"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66237,
        66249
      ],
      "eventIds": [],
      "abstract": "An increasingly popular way of experiencing remote places is by viewing 360 virtual tour videos, which show the surrounding view while traveling through an environment. However, finding particular locations in these videos can be difficult because current interfaces rely on distorted frame previews for navigation. To alleviate this usability issue, we propose Route Tapestries, continuous orthographic-perspective projection of scenes along camera routes. We first introduce an algorithm for automatically constructing Route Tapestries from a 360 video, inspired by the slit-scan photography technique. We then present a desktop video player interface using a Route Tapestry timeline for navigation. An online evaluation using a target-seeking task showed that Route Tapestries allowed users to locate targets 22% faster than with YouTube-style equirectangular previews and reduced the failure rate by 75% compared to a more conventional row-of-thumbnail strip preview. Our results highlight the value of reducing visual distortion and providing continuous visual contexts in previews for navigating \\vid virtual tour videos.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61241
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61109
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science "
            }
          ],
          "personId": 61317
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 61192
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 61013
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61062
        }
      ]
    },
    {
      "id": 61410,
      "typeId": 11756,
      "title": "Taming fNIRS-based BCI Input for Better Calibration and Broader Use",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474743"
        },
        "Presentation": {
          "duration": "512",
          "title": "Taming fNIRS-based BCI Input for Better Calibration and Broader Use",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=dMeB5F3DPHM"
        },
        "Preview": {
          "duration": "30",
          "title": "Taming fNIRS-based BCI Input for Better Calibration and Broader Use",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=mS7mlzUrVCU"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66267,
        66283
      ],
      "eventIds": [],
      "abstract": "Brain-computer interfaces (BCI) are an emerging technology with many potential applications. Functional near-infrared spectroscopy (fNIRS) can provide a convenient and unobtrusive real-time input for BCI. fNIRS is especially promising as a signal that could be used to automatically classify a user’s current cognitive workload. However, the data needed to train such a classifier is currently not widely available, difficult to collect, and difficult to interpret due to noise and cross-subject variation. A further challenge is the need for significant user-specific calibration. To address these issues, we introduce a new dataset gathered from 15 subjects and a new multi-stage supervised machine learning pipeline. Our approach learns from both observed data and augmented data derived from multiple subjects in its early stages, and then fine-tunes predictions to an individual subject in its last stage. We show promising gains in accuracy in a standard “n-back” cognitive workload classification task compared to baselines that use only subject-specific data or only group-level data, even when our approach is given much less subject-specific data. Even though these experiments analyzed the data retrospectively, we carefully removed anything from our process that could not have been done in real-time, because our process is targeted at future real-time operations. This paper contributes a new dataset, a new multi-stage training pipeline, results showing significant improvement compared to alternative pipelines, and a discussion of the implications for user interface design. Our complete dataset and software are publicly available at https://tufts-hci-lab.github.io/code_and_datasets/. We hope these results make fNIRS-based interactive brain input easier for a wide range of future researchers and designers to explore.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 60950
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "MEDFORD",
              "institution": "Tufts University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61240
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61082
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61030
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Department of Biomedical Engineering"
            }
          ],
          "personId": 61180
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 60957
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61033
        }
      ]
    },
    {
      "id": 61411,
      "typeId": 11756,
      "title": "MARS:  Nano-Power Battery-free Wireless Interface for Touch, Swipe and Speech Input",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474823"
        },
        "Presentation": {
          "duration": "389",
          "title": "MARS:  Nano-Power Battery-free Wireless Interface for Touch, Swipe and Speech Input",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PWe-JriWVVk"
        },
        "Preview": {
          "duration": "30",
          "title": "MARS:  Nano-Power Battery-free Wireless Interface for Touch, Swipe and Speech Input",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wPY-v1lW6fc"
        },
        "Video Figure": {
          "duration": "286",
          "title": "MARS:  Nano-Power Battery-free Wireless Interface for Touch, Swipe and Speech Input",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=CBdKuh-zUsE"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66267,
        66284
      ],
      "eventIds": [],
      "abstract": "Augmenting everyday surfaces with interaction sensing capability that is maintenance-free, low-cost (∼ $1), and in an appropriate form factor is a challenge with current technologies. MARS (Multi-channel Ambiently-powered Realtime Sensing) enables battery-free sensing and wireless communication of touch, swipe, and speech interactions by combining a nanowatt programmable oscillator with frequency-shifted analog backscatter communication. A zero-threshold voltage field-effect transistor (FET) is used to create an oscillator with a low startup voltage (∼500 mV) and current (< 2𝑢𝐴), whose frequency can be affected through changes in inductance or capacitance from the user interactions. Multiple MARS systems can operate in the same environment by tuning each oscillator circuit to a different frequency range. The nanowatt power budget allows the system to be powered directly through ambient energy sources like photodiodes or thermoelectric generators. We differentiate MARS from previous systems based on power requirements, cost, and part count and explore different interaction and activity sensing scenarios suitable for indoor environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61084
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61222
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61060
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61145
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61274
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61106
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Dept. of Electrical and Computer Engineering"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61297
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 60973
        }
      ]
    },
    {
      "id": 61412,
      "typeId": 11756,
      "title": "PneuSeries: 3D Shape Forming with Modularized Serial-Connected Inflatables",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474760"
        },
        "Presentation": {
          "duration": "596",
          "title": "PneuSeries: 3D Shape Forming with Modularized Serial-Connected Inflatables",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uS4eR1FgkOc"
        },
        "Preview": {
          "duration": "30",
          "title": "PneuSeries: 3D Shape Forming with Modularized Serial-Connected Inflatables",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=dwMac3k1ucI"
        },
        "Video Figure": {
          "duration": "163",
          "title": "PneuSeries: 3D Shape Forming with Modularized Serial-Connected Inflatables",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wKHyQeKUbbA"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66258,
        66260
      ],
      "eventIds": [],
      "abstract": "We present PneuSeries, a series of modularized inflatables where their inflation and deflation are propagated in-between stage by stage to form various shapes. The key component of PneuSeries is the bidirectional check valve that passively regulates the air flowing in/out from/to adjacent inflatables, allowing each of the inflatables to be inflated/deflated one by one through serial propagation. The form of the inflatable series thus is programmed by the sequential operations of a pump that push/pull the air in/out. In this paper, we explored the design of PneuSeries and implemented working prototypes as a proof of concept. In particular, we built PneuSeries with(1) modularized cubical, cuboidal, tetrahedral, prismatic, and custom inflatables to examine their shape forming, (2) fast assembly connectors to allow quick reconfiguration of the series, and (3) folding mechanism to reduce irregularity of the shrunken inflatables.We also evaluated the inflating and deflating time and the flow rate of the valve for simulating the inflating and deflating process and display the steps and time required to transform in our software. Finally, we demonstrate example objects that show the capability of PneuSeries and its potential applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61008
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61006
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61310
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 61223
        }
      ]
    },
    {
      "id": 61413,
      "typeId": 11756,
      "title": "Dynamic Guidance for Decluttering Photographic Compositions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474755"
        },
        "Presentation": {
          "duration": "496",
          "title": "Dynamic Guidance for Decluttering Photographic Compositions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7VmF5TemWEE"
        },
        "Preview": {
          "duration": "30",
          "title": "Dynamic Guidance for Decluttering Photographic Compositions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=za_Gw10aLQA"
        },
        "Video Figure": {
          "duration": "302",
          "title": "Dynamic Guidance for Decluttering Photographic Compositions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=33Z8kJbZeX4"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66238,
        66247
      ],
      "eventIds": [],
      "abstract": "Unwanted clutter in a photo can be incredibly distracting. However in the moment, photographers have so many things to simultaneously consider, it can be hard to catch every detail. Designers have long known the benefits of abstraction for seeing a more holistic view of their design. We wondered if, similarly, some form of image abstraction might be helpful for photographers as an alternative perspective or ``lens'' with which to see their image. Specifically, we wondered if such abstraction might draw the photographer's attention away from details in the subject to noticing objects in the background, such as unwanted clutter. We present our process for designing such a camera overlay, based on the idea of using abstraction to recognize clutter. Our final design uses object-based saliency and edge detection to highlight contrast along subject and image borders, outlining potential distractors in these regions. We describe the implementation and evaluation of a capture-time tool that interactively displays these overlays and find that the tool is helpful for making users more confident in their ability to take decluttered photos that clearly convey their intended story.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 61262
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Brooklyn",
              "institution": "Air, Inc",
              "dsl": ""
            }
          ],
          "personId": 61124
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "South San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 61158
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Interdisciplinary Center Herzliya",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 61203
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 61244
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 61043
        }
      ]
    },
    {
      "id": 61414,
      "typeId": 11756,
      "title": "Capturing Tactile Properties of Real Surfaces for Haptic Reproduction",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474798"
        },
        "Presentation": {
          "duration": "497",
          "title": "Capturing Tactile Properties of Real Surfaces for Haptic Reproduction",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qN9b-cAhRXc"
        },
        "Preview": {
          "duration": "30",
          "title": "Capturing Tactile Properties of Real Surfaces for Haptic Reproduction",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VNaLDiOjO84"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66253,
        66263
      ],
      "eventIds": [],
      "abstract": "Tactile feedback of an object's surface enables us to discern its material properties and affordances. This understanding is used in digital fabrication processes by creating objects with high-resolution surface variations to influence a user's tactile perception. As the design of such surface haptics commonly relies on knowledge from real-life experiences, it is unclear how to adapt this information for digital design methods. In this work, we investigate replicating the haptics of real materials. Using an existing process for capturing an object's microgeometry, we digitize and reproduce the stable surface information of a set of 15 fabric samples. In a psychophysical experiment, we evaluate the tactile qualities of our set of original samples and their replicas. From our results, we see that direct reproduction of surface variations is able to influence different psychophysical dimensions of the tactile perception of surface textures. While the fabrication process did not preserve all properties, our approach underlines that replication of surface microgeometries benefits fabrication methods in terms of haptic perception by covering a large range of tactile variations. Moreover, by changing the surface structure of a single fabricated material, its material perception can be influenced. We conclude by proposing strategies for capturing and reproducing digitized textures to better resemble the perceived haptics of the originals.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 61148
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "IST Austria",
              "dsl": ""
            }
          ],
          "personId": 61086
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Klosterneuburg",
              "institution": "IST Austria",
              "dsl": ""
            }
          ],
          "personId": 61160
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 61186
        }
      ]
    },
    {
      "id": 61415,
      "typeId": 11756,
      "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474765"
        },
        "Presentation": {
          "duration": "426",
          "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9ftQqNIJpVI"
        },
        "Preview": {
          "duration": "30",
          "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=3DJdcMbqgxU"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66235,
        66246
      ],
      "eventIds": [],
      "abstract": "Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios. We present Screen2Words, a novel screen summarization approach that automatically encapsulates essential information of a UI screen into a coherent language phrase. Summarizing mobile screens requires a holistic understanding of the multi-modal data of mobile UIs, including text, image, structures as well as UI semantics, motivating our multi-modal learning approach. We collected and analyzed a large-scale screen summarization dataset annotated by human workers. Our dataset contains more than 112k language summarization across ~22k unique UI screens. We then experimented with a set of deep models with different configurations. Our evaluation of these models with both automatic accuracy metrics and human rating shows that our approach can generate high-quality summaries for mobile screens. We demonstrate potential use cases of Screen2Words and open-source our dataset and model to lay the foundations for further bridging language and user interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61016
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61202
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 60994
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61320
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61062
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61225
        }
      ]
    },
    {
      "id": 61416,
      "typeId": 11756,
      "title": "Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474815"
        },
        "Presentation": {
          "duration": "549",
          "title": "Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=RSx_yFlM9Gg"
        },
        "Preview": {
          "duration": "30",
          "title": "Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4BpRXzF9G-k"
        },
        "Video Figure": {
          "duration": "260",
          "title": "Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=b4oTMnaxyuM"
        }
      },
      "trackId": 11292,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66236,
        66277
      ],
      "eventIds": [],
      "abstract": "In this paper, we present a method that makes 3D objects appear differently under different viewpoints. We accomplish this by 3D printing lenticular lenses across the curved surface of objects. By calculating the lens distribution and the corresponding surface color patterns, we can determine which appearance is shown to the user at each viewpoint.\r\n\r\nWe built a 3D editor that takes as input the 3D model, and the visual appearances, i.e. images, to show at different viewpoints. Our 3D editor then calculates the corresponding lens placements and underlying color pattern. On export, the user can use ray tracing to live preview the resulting appearance from each angle. The 3D model, color pattern, and lenses are then 3D printed in one pass on a multi-material 3D printer to create the final 3D object.\r\n\r\nTo determine the best fabrication parameters for 3D printing lenses, we printed lenses of different sizes and tested various post-processing techniques. To support a large number of different appearances, we compute the lens geometry that has the best trade-off between the number of viewpoints and the protrusion from the object geometry. Finally, we demonstrate our system in practice with a range of use cases for which we show the simulated and physical results side by side. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 61140
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61229
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61246
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 60919
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Architecture"
            }
          ],
          "personId": 61269
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61017
        }
      ]
    },
    {
      "id": 61482,
      "typeId": 11891,
      "title": "GenLine and GenForm: Two Tools for Interacting with Generative Language Models in a Code Editor",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480209"
        },
        "Video Figure": {
          "duration": "302",
          "title": "GenLine and GenForm: Two Tools for Interacting with Generative Language Models in a Code Editor",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KI2FhBFGOb0"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "macros",
        "generative models",
        "prompt programming",
        "code synthesis"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "A large, generative language model's output can be influenced through well-designed prompts, or text-based inputs that establish textual patterns that the model replicates in its output. These capabilities create new opportunities for novel interactions with large, generative language models. We present a macro system with two tools that allow users to invoke language model prompts as macros in a code editor. GenLine allows users to execute macros inline as they write code in the editor (e.g., ``Make an OK button'' produces the equivalent HTML). GenForm provides a form-like interface where the user provides input that is then transformed into multiple pieces of output at the same time (e.g., a description of web code is transformed into HTML, CSS, and JavaScript).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61471
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61474
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61437
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61463
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61440
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61436
        }
      ]
    },
    {
      "id": 61483,
      "typeId": 11891,
      "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474804"
        },
        "Presentation": {
          "duration": "430",
          "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=SrhgD4w45BA"
        },
        "Preview": {
          "duration": 30,
          "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=iF1gp3ta4ro"
        },
        "Project Page": {
          "annotation": "Project Page",
          "hideBeforeConference": false,
          "hideFromPublicJson": false,
          "hideOnConferenceStart": false,
          "isAvailableForRegisteredMembersOnly": false,
          "title": "Project Page",
          "type": "custom",
          "url": "https://github.com/berkeleyHCI/edg-ide"
        },
        "Video Figure": {
          "duration": "205",
          "title": "Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=IOe3En2P3Fs"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "integrated development environment (IDE)",
        "hardware description language (HDL)",
        "printed circuit board (PCB) design"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "In many engineering disciplines such as circuit board, chip, and mechanical design, a hardware description language (HDL) approach provides important benefits over direct manipulation interfaces by supporting concepts like abstraction and generator meta-programming.\r\nWhile several such HDLs have emerged recently and promised power and flexibility, they also present challenges -- especially to designers familiar with current graphical workflows.\r\nIn this work, we investigate an IDE approach to provide a graphical editor for a board-level circuit design HDL.\r\nUnlike GUI builders which convert an entire diagram to code, we instead propose generating equivalent HDL from individual graphical edit actions.\r\nBy keeping code as the primary design input, we preserve the full power of the underlying HDL, while remaining useful even to advanced users.\r\nWe discuss our concept, design considerations such as performance, system implementation, and report on the results of an exploratory remote user study with four experienced hardware designers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 60956
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 61037
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 60912
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 60986
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 61076
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 61131
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 61153
        }
      ]
    },
    {
      "id": 61484,
      "typeId": 11891,
      "title": "Eloquent: Improving Text Editing on Mobile",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480178"
        },
        "Video Figure": {
          "duration": "293",
          "title": "Eloquent: Improving Text Editing on Mobile",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_9YPm0EghvU"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Mobile",
        "text editing",
        "touch displays",
        "menus"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "We present Eloquent, an exploration of new interaction techniques for text editing on mobile. Our prototype combines techniques for targeting, selection, and command menus. We demonstrate preliminary results in the form of a text editor, whose design was informed by feedback from users of existing systems. With Eloquent, selecting and acting on text can be done with a single gesture for both novice and expert users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 61460
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "32al",
              "dsl": ""
            }
          ],
          "personId": 61433
        }
      ]
    },
    {
      "id": 61485,
      "typeId": 11891,
      "title": "HMK: Head-Mounted-Keyboard for Text Input in Virtual or Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480195"
        },
        "Video Figure": {
          "duration": "31",
          "title": "HMK: Head-Mounted-Keyboard for Text Input in Virtual or Augmented Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0Biw8adTIZo"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Text input",
        "VR/AR",
        "HMD",
        "keyboard"
      ],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "Text input is essential in a variety of uses in virtual and augmented reality (VR and AR). We present HMK: an effective text input method that mounts split keyboards on the left and right side of the head mounted display (HMD). Users who can touch-type are able to type using HMK by relying on their familiarity with the normal QWERTY keyboard. We develop custom keycaps to make it easier to find the home position. A study with three participants shows that users retain most of their normal keyboard typing skills. The participants achieved, on average, 34.7 words per minute (WPM) by the end of three days of use, retaining 81 percent of their regular entry speed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61423
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61480
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO",
              "dsl": ""
            }
          ],
          "personId": 61469
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61421
        }
      ]
    },
    {
      "id": 61486,
      "typeId": 11891,
      "title": "EIT-kit Demo: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480185"
        },
        "Video Figure": {
          "duration": "388",
          "title": "EIT-kit Demo: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ZFDRlQUYuyg"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "electrical impedance tomography",
        "health sensing",
        "electronic prototyping",
        "personal fabrication"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66230
      ],
      "abstract": "In this demo, we present EIT-kit, an electrical impedance tomography toolkit for designing and fabricating health and motion sensing devices. EIT-kit contains (1) an extension to a 3D editor for personalizing the form factor of the electrode arrays and the electrode distribution, (2) a customized EIT sensing motherboard that users can use to perform measurements, (3) a microcontroller library that automates electrical impedance measurements, and (4) an image reconstruction library for mobile devices for interpolating and then visualizing the measured data. Together, these allow for applications that require 2- or 4-terminal setups, up to 64-electrodes, and single or multiple (up to four) electrode arrays simultaneously.\r\n\r\nWe demonstrate each element of EIT-kit, as well as the design space that EIT-kit enables by showing various applications in health sensing as well as motion sensing and control. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61275
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61152
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61078
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 60967
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 60988
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts General Hospital",
              "dsl": ""
            }
          ],
          "personId": 60922
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts General Hospital ",
              "dsl": ""
            }
          ],
          "personId": 61290
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61017
        }
      ]
    },
    {
      "id": 61487,
      "typeId": 11891,
      "title": "ShiftTouch: Sheet-type Interface Extending Capacitive Touch Inputs with Minimal Screen Occlusion",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480174"
        },
        "Video Figure": {
          "duration": "83",
          "title": "ShiftTouch: Sheet-type Interface Extending Capacitive Touch Inputs with Minimal Screen Occlusion",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=np531IVbb9o"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Capacitive Touch Surface",
        "Conductive Ink",
        "Printed Interface."
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66215
      ],
      "abstract": "We present ShiftTouch, a sheet-type passive interface that provides multiple inputs for capacitive touch surfaces with minimal screen occlusion.  It consists of a conductive layer and a masking layer that partially insulates the conductive one.  ShiftTouch uses multiple linear electrodes to control the fine touch position.  The touch input is triggered under the electrodes when several adjacent electrodes are grounded simultaneously.  Although each input area shares some electrodes with neighboring input areas, the touch surface can identify the inputs from each input area by detecting the slight displacement of the touch position.  Our interface is simple yet effective in implementing multiple input areas while reducing screen occlusion compared to existing approaches using finger-sized electrodes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Yahoo Japan Corporation",
              "dsl": ""
            }
          ],
          "personId": 61452
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo University of Technology",
              "dsl": ""
            }
          ],
          "personId": 61466
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo University of Technology",
              "dsl": ""
            }
          ],
          "personId": 61426
        }
      ]
    },
    {
      "id": 61488,
      "typeId": 11891,
      "title": "Programmable Polarities: Actuating Interactive Prototypes with Programmable Electromagnets",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480198"
        },
        "Video Figure": {
          "duration": "164",
          "title": "Programmable Polarities: Actuating Interactive Prototypes with Programmable Electromagnets",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8F-MEuwspIM"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "tabletop mobile robots",
        "self-reconfigurable robots",
        "swarm user interfaces",
        "interactive devices",
        "fabrication"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66230
      ],
      "abstract": "This demo introduces a framework that uses programmable electromagnets as a method to rapidly prototype interactive objects. Our approach allows users to to quickly and inexpensively embed actuation mechanisms into otherwise static prototypes in order to make them dynamic and interactive. Underpinning the technique is the insight of using electromagnets to interchangeably create attractive and repulsive forces between adjacent parts, and programmatically setting their polarities in a way that allows objects to translate rotationally and linearly, respond haptically, assemble, and locomote.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61432
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61446
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61445
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 60987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61017
        }
      ]
    },
    {
      "id": 61489,
      "typeId": 11891,
      "title": "VoLearn: An Operable Motor Learning System with Auditory Feedback",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480186"
        },
        "Video Figure": {
          "duration": "222",
          "title": "VoLearn: An Operable Motor Learning System with Auditory Feedback",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xrB-iJl7K78"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Activity Recognition",
        "Wearable Computing"
      ],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "Previous motor learning systems rely on a vision-based workflow both from feed-forward and feedback process, which limits the application requirement and scenario. In this demo, we presented a novel cross-modal motor learning system named VoLearn. The\r\nnovice is able to interact with desired motion through a virtual 3D interface and obtain the audio feedback based on a personal smartphone. Both interactivity and user-accessibility of the designed system contribute to a wider range of applications and reduce the\r\nlimitations in the applied space as well.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Department of Computer Science and Information"
            }
          ],
          "personId": 61450
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama City",
              "institution": "Keio University",
              "dsl": "Department of Information and Computer Science"
            }
          ],
          "personId": 61431
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama City",
              "institution": "Keio University",
              "dsl": "Department of Information and Computer Science"
            }
          ],
          "personId": 61425
        }
      ]
    },
    {
      "id": 61490,
      "typeId": 11891,
      "title": "Single-sided Multi-layer Electric Circuit by Hot Stamping with 3D Printer",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480200"
        },
        "Video Figure": {
          "duration": "31",
          "title": "Single-sided Multi-layer Electric Circuit by Hot Stamping with 3D Printer",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8zmFLT_XwHQ"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Multi-layer electric circuit",
        "hot stamping",
        "transfer foil",
        "3D printer",
        "silver paste"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66215
      ],
      "abstract": "The spread of personal computer-aided fabrication has made it possible for individuals to create things that meet their personal needs. However, it is still not easy for an individual to prototype electronic circuits. Several methods have been proposed for quickly prototyping electronic circuits. Our solution is combining a 3D printer with transfer foil. This paper expands our original circuit printing method so as to fabricate single-sided multi-layer boards. We show that the proposed technique allows a 3D printer and two types of transfer foil to create single-sided multi-layer circuits.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61439
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61421
        }
      ]
    },
    {
      "id": 61492,
      "typeId": 11891,
      "title": "Designing Adaptive Tools for Motor Skill Training",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480205"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "adaptive learning",
        "physical interfaces",
        "motor-skill learning",
        "toolkit design"
      ],
      "sessionIds": [],
      "eventIds": [
        66230,
        66231
      ],
      "abstract": "We demonstrate the design of adaptive tools for motor skill training that use shape change to automatically vary task difficulty based on a learner's performance. Studies have shown that automatically-adaptive tools lead to significantly higher learning gains when compared to non-adaptive and manually-adaptive tools. We demonstrate the use of Adapt2Learn - a toolkit that supports designers in building adaptive training tools. Adapt2Learn auto-generates an algorithm that converts a learner's performance data into adaptation states during motor skill training. This algorithm, which maintains the training difficulty at the 'optimal challenge point', can be uploaded to the micro-controller to convert several shape-changing tools into adaptive tools for motor skill training. We demonstrate 7 prototypes of adaptive tools for motor-skill learning to show applications in sports, music, rehabilitation, and accessibility.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT CSAIL"
            }
          ],
          "personId": 61464
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61447
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61424
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61462
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61481
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61017
        }
      ]
    },
    {
      "id": 61493,
      "typeId": 11891,
      "title": "Towards a Generalized Acoustic Minimap for Visually Impaired Gamers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480177"
        },
        "Video Figure": {
          "duration": "164",
          "title": "Towards a Generalized Acoustic Minimap for Visually Impaired Gamers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=sqfVz-hgmao"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Audio navigation tools",
        "acoustic minimaps",
        "blind-accessible games",
        "visual impairments"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66230
      ],
      "abstract": "Video games created for visually impaired players (VIPs) remain inequivalent to those created for sighted players. Sighted players use minimaps within games to learn how their surrounding environment is laid out, but there is no effective analogue to the minimap for visually impaired players. A major accessibility challenge is to create a generalized, acoustic (non-visual) version of the minimap for VIPs. To address this challenge, we develop and investigate four acoustic minimap techniques which represent a breadth of ideas for how an acoustic minimap might work: a companion smartphone app, echolocation, a directional scanner, and a simple menu. Each technique is designed to communicate information about the area around the player within a game world, providing functionality analogous to a visual minimap but in acoustic form. We close by describing a user study that we are performing with these techniques to investigate the factors that are important in the design of acoustic minimap tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 60928
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 61456
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 61441
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 61472
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61468
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maine",
              "city": "Brunswick",
              "institution": "Bowdoin College",
              "dsl": ""
            }
          ],
          "personId": 61448
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell University, Cornell Tech",
              "dsl": ""
            }
          ],
          "personId": 61473
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 61429
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 61259
        }
      ]
    },
    {
      "id": 61494,
      "typeId": 11891,
      "title": "VHP: Vibrotactile Haptics Platform for On-body Applications",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474772"
        },
        "Presentation": {
          "duration": "551",
          "title": "VHP: Vibrotactile Haptics Platform for On-body Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ksck4Uf4dKQ"
        },
        "Preview": {
          "duration": 30,
          "title": "VHP: Vibrotactile Haptics Platform for On-body Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=6qrksaX_YEo"
        },
        "Video Figure": {
          "duration": -1,
          "title": "VHP: Vibrotactile Haptics Platform for On-body Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=LObvG7znTtw"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "Wearable vibrotactile devices have many potential applications, including sensory substitution for accessibility and notifications. Currently, vibrotactile experimentation is done using large lab setups. However, most practical applications require standalone on-body devices and integration into small form factors. Such integration is time-consuming and requires expertise.\r\n\r\nWith our goal to democratize wearable haptics we introduce VHP, a vibrotactile haptics platform. It includes a low-power miniature electronics board that can drive up to 12 independent channels of haptic signals with arbitrary waveforms at a 2 kHz sampling rate. The platform can drive vibrotactile actuators including linear resonant actuators and voice coils. The hardware is battery-powered and programmable, and has multiple input options, including serial and Bluetooth, as well as the ability to synthesize haptic signals internally. We developed current-based load sensing, thus allowing for unique features such as actuator auto-classification, and skin-contact quality sensing. Our technical evaluations showed that the system met all our initial design criteria and is an improvement over prior methods as it allows all-day wear, has low latency, has battery life between 3 and 25 hours, and can run 12 actuators simultaneously.\r\n\r\nWe demonstrate unique applications that would be time consuming to iterate without the VHP platform. We show that VHP can be used as bracelet, sleeve and phone-case form factors. The bracelet was programmed with an audio-to-tactile interface and was successfully worn for multiple days by developers. To facilitate more use of this platform, we open-source our design and plan to make the hardware widely available. We hope this work will motivate the use and study of vibrotactile all-day wearable devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 61125
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 60916
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 60925
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 60954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": "Research"
            }
          ],
          "personId": 61116
        }
      ]
    },
    {
      "id": 61495,
      "typeId": 11891,
      "title": "Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474800"
        },
        "Presentation": {
          "duration": "590",
          "title": "Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hdSmTzECnqc"
        },
        "Preview": {
          "duration": 30,
          "title": "Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=pLVi9Sy9vO8"
        },
        "Video Figure": {
          "duration": "200",
          "title": "Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1gCayd3LhHc"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "haptic illusion, compliance, wearable haptics, tactile, VR, props, AR, soft, rigid"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "We propose a haptic device that alters the perceived softness of real rigid objects without requiring to instrument the objects. Instead, our haptic device works by restricting the user’s fingerpad lateral deformation via a hollow frame that squeezes the sides of the fingerpad. This causes the fingerpad to become bulgier than it originally was—when users touch an object’s surface with their now-restricted fingerpad, they feel the object to be softer than it is. To illustrate the extent of softness illusion induced by our device, touching the tip of a wooden chopstick will feel as soft as a rubber eraser. Our haptic device operates by pulling the hollow frame using a motor. Unlike most wearable haptic devices, which cover up the user’s fingerpad to create force sensations, our device creates softness while leaving the center of the fingerpad free, which allows the users to feel most of the object they are interacting with. This makes our device a unique contribution to altering the softness of everyday objects, creating “buttons” by softening protrusions of existing appliances or tangibles, or even, altering the softness of handheld props for VR. Finally, we validated our device through two studies: (1) a psychophysics study showed that the device brings down the perceived softness of any object between 50A-90A to around 40A (on Shore A hardness scale); and (2) a user study demonstrated that participants preferred our device for interactive applications that leverage haptic props, such as making a VR prop feel softer or making a rigid 3D printed remote control feel softer on its button.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61073
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61040
        }
      ]
    },
    {
      "id": 61496,
      "typeId": 11891,
      "title": "ViObject: A Smartwatch-based Object Recognition System via Vibrations",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480182"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Wearable Sensing",
        "Object Recognition",
        "Vibration Intelligence"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66230
      ],
      "abstract": "Today, in order to start an interaction with most digital technology, we must perform some sort of action to indicate our intention, such as shaking a computer’s mouse to wake it or pressing a coffee maker’s start button for your morning cup of coffee. Our system\r\naims to help remove these currently necessary \"trigger actions\" and aims to support an array of applications to create borderless and fluid interactions between the technological world and our own. Our system as has the potential for application within the\r\nworld of accessible technology as well. The system we propose is a method of identifying held objects via a smartwatch’s accelerometer and gyroscope sensors. A preview demo video is available at https://youtu.be/1YCTzvjcJ18.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "CS"
            }
          ],
          "personId": 61435
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "CS"
            }
          ],
          "personId": 61475
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61434
        }
      ]
    },
    {
      "id": 61497,
      "typeId": 11891,
      "title": "Demonstration of FabHydro: 3D Printing Techniques for Interactive Hydraulic Devices with an Affordable SLA 3D Printer",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480180"
        },
        "Video Figure": {
          "duration": "123",
          "title": "Demonstration of FabHydro: 3D Printing Techniques for Interactive Hydraulic Devices with an Affordable SLA 3D Printer",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=BC9kXonqnEc"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Fabrication",
        "3D Printing",
        "Interaction",
        "Design",
        "Hydraulic"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66233
      ],
      "abstract": "In this demonstration, we showcase FabHydro, a set of rapid and low-cost techniques to prototype interactive hydraulic devices using off-the-shelf SLA 3D printers and flexible photosensitive resin. \r\nWe introduce two printing processes to seal the transmission fluid: the Submerged Printing process seals liquid resin inside the printed objects without assembly, and the Printing with Plug method which allows using variety of transmission fluid with no modification to the printer. \r\nWe showcase how a range of relevant 3D printable primitives, including hydraulic generator, transmitter, and actuator can be printed and combined to create interactive examples.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61220
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61248
        }
      ]
    },
    {
      "id": 61498,
      "typeId": 11891,
      "title": "Post-plant3: The Third Type of a Series of Non-humanoid Robots with an Embedded Physical Interaction",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480206"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Physical Human-robot Interaction",
        "Tangible User Interface",
        "Interaction Design"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66215
      ],
      "abstract": "Post-plant is a series of plant-like robots which communicate nonverbally through physical movements. Most of the social robots look like humans or animals. communicating with us by mimicking human speech and gestures. Inspired by plants, post-plant respond to touch instead of language. With post-plant as a starting point, robots of the future will communicate with us in their own way, without having to mimic human behaviors. Post-plant 3 is the third type of post-plant robot series.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul National University",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 61467
        }
      ]
    },
    {
      "id": 61499,
      "typeId": 11891,
      "title": "ScaffoldSketch: Accurate Industrial Design Drawing in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474756"
        },
        "Presentation": {
          "duration": "394",
          "title": "ScaffoldSketch: Accurate Industrial Design Drawing in VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=iNNJhuzCAe8"
        },
        "Preview": {
          "duration": 30,
          "title": "ScaffoldSketch: Accurate Industrial Design Drawing in VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=634rFT6m2eI"
        },
        "Video Figure": {
          "duration": "31",
          "title": "ScaffoldSketch: Accurate Industrial Design Drawing in VR",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=634rFT6m2eI"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66230
      ],
      "abstract": "We present an approach to in-air design drawing based on the two-stage approach common in 2D design drawing practice. The primary challenge to 3D drawing in air is the accuracy of users' strokes. Beautifying or auto-correcting an arbitrary drawing in 2D or 3D is challenging due to ambiguities stemming from many possible interpretations of a stroke. A similar challenge appears when drawing freehand on paper in the real world. 2D design drawing practice (as taught in industrial design school) addresses this by decomposing the process of creating realistic 2D projections of 3D shapes. Designers first create scaffold or construction lines. When drawing shape or structure curves, designers are guided by the scaffolds. Our key insight is that accurate industrial design drawing in 3D becomes tractable when decomposed into auto-correcting scaffold strokes, which have simple relationships with one another, followed by auto-correcting shape strokes with respect to the scaffold strokes. We demonstrate our approach's effectiveness with an expert study involving industrial designers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 61166
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 61010
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia tech",
              "dsl": ""
            }
          ],
          "personId": 61100
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 60965
        }
      ]
    },
    {
      "id": 61500,
      "typeId": 11891,
      "title": "MARS:  Nano-Power Battery-free Wireless Interface for Touch, Swipe and Speech Input",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474823"
        },
        "Presentation": {
          "duration": "389",
          "title": "MARS:  Nano-Power Battery-free Wireless Interface for Touch, Swipe and Speech Input",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PWe-JriWVVk"
        },
        "Preview": {
          "duration": 30,
          "title": "MARS:  Nano-Power Battery-free Wireless Interface for Touch, Swipe and Speech Input",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wPY-v1lW6fc"
        },
        "Video Figure": {
          "duration": "281",
          "title": "MARS:  Nano-Power Battery-free Wireless Interface for Touch, Swipe and Speech Input",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Sg1L73wQzLc"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Interaction",
        "Low-voltage",
        "Low power",
        "Flexible Electronics",
        "Backscatter",
        "Tangible",
        "Wireless",
        "Sensing"
      ],
      "sessionIds": [],
      "eventIds": [
        66230,
        66231
      ],
      "abstract": "Augmenting everyday surfaces with interaction sensing capability that is maintenance-free, low-cost (∼ $1), and in an appropriate form factor is a challenge with current technologies. MARS (Multi-channel Ambiently-powered Realtime Sensing) enables battery-free sensing and wireless communication of touch, swipe, and speech interactions by combining a nanowatt programmable oscillator with frequency-shifted analog backscatter communication. A zero-threshold voltage field-effect transistor (FET) is used to create an oscillator with a low startup voltage (∼500 mV) and current (< 2𝑢𝐴), whose frequency can be affected through changes in inductance or capacitance from the user interactions. Multiple MARS systems can operate in the same environment by tuning each oscillator circuit to a different frequency range. The nanowatt power budget allows the system to be powered directly through ambient energy sources like photodiodes or thermoelectric generators. We differentiate MARS from previous systems based on power requirements, cost, and part count and explore different interaction and activity sensing scenarios suitable for indoor environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technolgy",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61084
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61222
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61060
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61145
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61274
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 61106
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Dept. of Electrical and Computer Engineering"
            }
          ],
          "personId": 61297
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 60973
        }
      ]
    },
    {
      "id": 61501,
      "typeId": 11891,
      "title": "Demonstrating AutoAssembler: Automatic Reconstruction of Laser-Cut 3D Models",
      "addons": {
        "Video Figure": {
          "duration": "330",
          "title": "Demonstrating AutoAssembler: Automatic Reconstruction of Laser-Cut 3D Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=6nnv4nZhqVg"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66232,
        66233
      ],
      "abstract": "Recent research showed how to import laser cut 3D models encoded in the form of 2D cutting plans into a 3D editor (assembler3  [28]), which allows users to perform parametric manipulations on such models. In contrast to assembler3 , which requires users to perform this process manually, we present autoAssembler, which performs this process automatically. AutoAssembler uses a beam search algorithm to search possible ways of assembling plates. It uses joints on these plates to combine them into assembly candidates. It thereby preferably pursues candidates (1) that have no intersecting plates, (2) that fit into a small bounding box, (3) that uses plates whose joints fit together well, (4) that do not add many unpaired joints, (5) that make use of constraints posed by other plates, and (6) that conform to symmetry axes of the plates. This works for models that have at least one edge joint (finger or t-joint). In our technical evaluation, we imported 66 models using autoAssembler. AutoAssembler assembled 79% of those models fully automatically; another 18% of models required on average 2.7 clicks of post-processing, for an overall success rate of 97%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61023
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61115
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61046
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61120
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61182
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 61303
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61127
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60955
        }
      ]
    },
    {
      "id": 61502,
      "typeId": 11891,
      "title": "Creating Interactive Machine Learning Applications with Marcelle",
      "addons": {
        "Project Page": {
          "annotation": "Project Page",
          "hideBeforeConference": false,
          "hideFromPublicJson": false,
          "hideOnConferenceStart": false,
          "isAvailableForRegisteredMembersOnly": false,
          "title": "Project Page",
          "type": "custom",
          "url": "https://uist2021demos.marcelle.dev/"
        },
        "Video Figure": {
          "duration": "295",
          "title": "Creating Interactive Machine Learning Applications with Marcelle",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=SeGSaLR5GuE"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Interactive Machine Learning",
        "Machine Teaching",
        "Architectural  Model",
        "Toolkit"
      ],
      "sessionIds": [],
      "eventIds": [
        66230,
        66232
      ],
      "abstract": "This demo illustrates the use and features of Marcelle, an open source toolkit for programming for programming human interactions with machine learning pipelines. Marcelle’s architecture is built upon a modular collection of components with a unified interface that can be composed to form custom processing pipelines and user interfaces. This component-based architecture is extensible and facilitates reuse of pipelines and interaction techniques across projects. The architecture is built over web technologies to facilitate collaboration, and supports sharing of applications, data and models. This demo will demonstrate through examples available online: (1) how Marcelle’s API enables developers to quickly prototype interactive machine learning applications where training and inference run in a web page; (2) how Marcelle’s data store system and Python interoperability facilitates collaboration between various stakeholders, including machine learning experts, designers and end users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            }
          ],
          "personId": 60968
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Gif-sur-Yvette",
              "institution": "Université Paris Saclay",
              "dsl": "LISN"
            }
          ],
          "personId": 61058
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, CNRS, ISIR",
              "dsl": ""
            }
          ],
          "personId": 61221
        }
      ]
    },
    {
      "id": 61503,
      "typeId": 11891,
      "title": "Pronunciation Training through Articulation Motion Sensing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480187"
        },
        "Video Figure": {
          "duration": "313",
          "title": "Pronunciation Training through Articulation Motion Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=NxdBgyTpd6Y"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Pronunciation Training",
        "Acoustic Sensing",
        "Ultrasonic Sensing",
        "Acoustic Phonetics"
      ],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "Vowels are considered the essence of the syllable, which controls the articulation of each word uttered. However, articulation sensing has not been adequately evaluated. The challenging task is that the speech signal contains insufficient information for articulation analysis. It is difficult for users to improve their pronunciation only by getting scoring feedback on pronunciation assessments. We propose a  new method to simultaneously use two different acoustic signals (speech and ultrasonic) to recognize lip shape and tongue position. The system gives articulation feedback to a user, identifying the articulation of monophthongs in multiple languages. The proposed technique is implemented into an off-the-shelf smartphone to be more accessible.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Shenzhen",
              "institution": "Shenzhen University",
              "dsl": "College of Computer Science & Software Engineering"
            }
          ],
          "personId": 61443
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "GuangDong",
              "city": "ShenZhen",
              "institution": "ShenZhen University",
              "dsl": "College of Computer Science & Software Engineering"
            }
          ],
          "personId": 66288
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Shenzhen",
              "institution": "Shenzhen University",
              "dsl": "College of Computer Science & Software Engineering"
            }
          ],
          "personId": 61444
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Shenzhen University",
              "dsl": ""
            }
          ],
          "personId": 61477
        }
      ]
    },
    {
      "id": 61504,
      "typeId": 11891,
      "title": "Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474747"
        },
        "Presentation": {
          "duration": "604",
          "title": "Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qEUdwZURXGY"
        },
        "Preview": {
          "duration": 30,
          "title": "Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2nppa8iAMzg"
        },
        "Video Figure": {
          "duration": "303",
          "title": "Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2tiAaBN8Uw8"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66233
      ],
      "abstract": "We propose a new class of haptic devices that provide haptic sensations by delivering liquid-stimulants to the user’s skin; we call this chemical haptics. Upon absorbing these stimulants, which contain safe and small doses of key active ingredients, receptors in the user’s skin are chemically triggered, rendering distinct haptic sensations. We identified five chemicals that can render lasting haptic sensations: tingling (sanshool), numbing (lidocaine), stinging (cinnamaldehyde), warming (capsaicin), and cooling (menthol). To enable the application of our novel approach in a variety of settings (such as VR), we engineered a self-contained wearable that can be worn anywhere on the user’s skin (e.g., face, arms, legs). Implemented as a soft silicone patch, our device uses micropumps to push the liquid stimulants through channels that are open to the user’s skin, enabling topical stimulants to be absorbed by the skin as they pass through. Our approach presents two unique benefits. First, it enables sensations, such as numbing, not possible with existing haptic devices. Second, our approach offers a new pathway, via the skin’s chemical receptors, for achieving multiple haptic sensations using a single actuator, which would otherwise require combining multiple actuators (e.g., Peltier, vibration motors, electro-tactile stimulation). We evaluated our approach by means of two studies. In our first study, we characterized the temporal profiles of sensations elicited by each chemical. Using these insights, we designed five interactive VR experiences utilizing chemical haptics, and in our second user study, participants rated these VR experiences with chemical haptics as more immersive than without. Finally, as the first work exploring the use of chemical haptics on the skin, we offer recommendations to designers for how they may employ our approach for their interactive experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61224
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 60933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago ",
              "institution": "University of Chicago ",
              "dsl": ""
            }
          ],
          "personId": 60909
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61040
        }
      ]
    },
    {
      "id": 61505,
      "typeId": 11891,
      "title": "Demonstration of GestuRING, a Web Tool for Ring Gesture Input",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480199"
        },
        "Project Page": {
          "annotation": "Project Page",
          "hideBeforeConference": false,
          "hideFromPublicJson": false,
          "hideOnConferenceStart": false,
          "isAvailableForRegisteredMembersOnly": false,
          "title": "Project Page",
          "type": "custom",
          "url": "http://www.eed.usv.ro/~vatavu/projects/GestuRING/"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Smart rings",
        "gesture input",
        "GestuRING",
        "web tool",
        "applications"
      ],
      "sessionIds": [],
      "eventIds": [
        66215,
        66233
      ],
      "abstract": "We present use cases for GestuRING, our web-based tool providing access to 579 gesture-to-function mappings, companion YouTube videos, and numerical gesture recordings for input with smart rings. We illustrate how practitioners can employ GestuRING for the design of gesture sets for ring-based UIs by discussing two examples: (1) enhancing a smart ring application for users with motor impairments with new functions and corresponding gesture commands, and (2) identifying a gesture set for cross-device watch-ring input.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava ",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 61232
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 60974
        }
      ]
    },
    {
      "id": 61506,
      "typeId": 11891,
      "title": "Demonstrating HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480202"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "virtual reality",
        "encountered-type haptics",
        "tabletop mobile robots",
        "swarm user interfaces"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66230
      ],
      "abstract": "HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability— these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 60987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 60921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": "Microsoft Research"
            }
          ],
          "personId": 61181
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado, Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 61172
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61179
        }
      ]
    },
    {
      "id": 61507,
      "typeId": 11891,
      "title": "Demonstrating DextrEMS: Achieving Dexterity in Electrical Muscle Stimulation by Combining it with Brakes",
      "addons": {
        "Project Page": {
          "annotation": "Project Page",
          "hideBeforeConference": false,
          "hideFromPublicJson": false,
          "hideOnConferenceStart": false,
          "isAvailableForRegisteredMembersOnly": false,
          "title": "Project Page",
          "type": "custom",
          "url": "https://lab.plopes.org/dextrEMS/demo"
        },
        "Video Figure": {
          "duration": "292",
          "title": "Demonstrating DextrEMS: Achieving Dexterity in Electrical Muscle Stimulation by Combining it with Brakes",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9oXWCOu39Ik"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "electrical muscle stimulation",
        "exoskeleton",
        "dexterity",
        "force feedback",
        "haptics"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66230
      ],
      "abstract": "Electrical muscle stimulation (EMS) is an emergent technique that miniaturizes force feedback, especially popular for untethered haptic devices, such as mobile gaming, VR, or AR. However, the actuation displayed by interactive systems based on EMS is coarse and imprecise. EMS systems mostly focus on inducing movements in large muscle groups such as legs, arms, and wrists; whereas individual finger poses, which would be required, for example, to actuate a user’s fingers t o fingerspell even the simplest letters in sign language, are not possible. The lack of dexterity in EMS stems from two fundamental limitations: (1) lack of independence: when a particular finger is actuated by EMS, the current runs through nearby muscles, causing unwanted actuation of adjacent fingers; and, (2) unwanted oscillations: while it is relatively easy for EMS to start moving a finger, it is very hard for EMS to stop and hold that finger at a precise angle; because, to stop a finger, virtually all EMS systems contract the opposing muscle, typically achieved via controllers (e.g., PID)—unfortunately, even with the best controller tuning, this often results in unwanted oscillations. To tackle these limitations, we propose dextrEMS, an EMS-based haptic device featuring mechanical brakes attached to each finger joint. The key idea behind dextrEMS is that while the EMS actuates the fingers, it is our mechanical brake that stops the finger in a precise position. Moreover, it is also the brakes that allow dextrEMS to select which fingers are moved by EMS, eliminating unwanted movements by preventing adjacent fingers from moving. We implemented dextrEMS as an untethered haptic device, weighing only 68g, that actuates eight finger joints independently (metacarpophalangeal and proximal interphalangeal joints for four fingers), which we demonstrate in a wide range of haptic applications, such as assisted fingerspelling, a piano tutorial, guitar tutorial, and a VR game. Finally, in our technical evaluation, we found that dextrEMS outperformed EMS alone by doubling its independence and reducing unwanted oscillations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 60924
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61196
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61073
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 61040
        }
      ]
    },
    {
      "id": 61508,
      "typeId": 11891,
      "title": "ProbMap: Automatically constructing design galleries through feature extraction and semantic clustering",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480203"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "design galleries",
        "feature extraction",
        "design data",
        "search facets"
      ],
      "sessionIds": [],
      "eventIds": [
        66230,
        66232
      ],
      "abstract": "Making sense of large unstructured problem spaces is cognitively demanding. Structure can help, but adding structure to a problem space also takes significant effort. ProbMap is a novel application for automatically constructing a design gallery from unstructured text input. Given a list of problem statements, ProbMap extracts and semantically groups the stakeholders to construct hierarchical search facets which enable designers to more efficiently navigate the problem statements. We contribute a novel feature extraction algorithm using natural language processing and a technique for automatically constructing a design gallery. These stakeholders are grouped semantically by clustering stakeholders with higher pairwise similarity together. Preliminary trials show that these techniques, which mirror traditional design activities like stakeholder identification and affinity mapping, provide an initial structure to a large unstructured problem space. This resulted in similar features that would be extracted by humans and sensible clusters. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Temple University",
              "dsl": "Computer and Information Sciences"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Design Lab"
            }
          ],
          "personId": 61442
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "UC San Diego",
              "dsl": "Design Lab"
            }
          ],
          "personId": 61451
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Design Lab"
            }
          ],
          "personId": 61479
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 61465
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Design Lab"
            }
          ],
          "personId": 61454
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": "Dept of Cognitive Science"
            }
          ],
          "personId": 61422
        }
      ]
    },
    {
      "id": 61510,
      "typeId": 11891,
      "title": "OpenNFCSense: Open-Source Library for NFCSense",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480196"
        },
        "Project Page": {
          "annotation": "Project Page",
          "hideBeforeConference": false,
          "hideFromPublicJson": false,
          "hideOnConferenceStart": false,
          "isAvailableForRegisteredMembersOnly": false,
          "title": "Project Page",
          "type": "custom",
          "url": "https://github.com/howieliang/OpenNFCSense"
        },
        "Video Figure": {
          "duration": "295",
          "title": "OpenNFCSense: Open-Source Library for NFCSense",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=BnU6xsbqgMw"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "NFCSense",
        "API",
        "rapid prototyping",
        "tangible interaction"
      ],
      "sessionIds": [],
      "eventIds": [
        66215,
        66233
      ],
      "abstract": "OpenNFCSense is an open-source library for NFCSense, a system for detecting the movements of near-field communication (NFC) tags using a commodity, low-cost RC522 NFC reader. With a user-defined tag profile, the users can use its application programming interface (API) to obtain the NFC tagged objects' motion speed, motion frequency, and motion type while recognizing these tagged objects. Since NFC tags support straightforward augmentation to existed physical objects, such as modular building blocks and mechanical toys, OpenNFCSense offers an easy and safe method for rapid prototyping tangible user interfaces for designers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            }
          ],
          "personId": 61459
        }
      ]
    },
    {
      "id": 61511,
      "typeId": 11891,
      "title": "Planning Epidemic Interventions with EpiPolicy",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474794"
        },
        "Presentation": {
          "duration": "511",
          "title": "Planning Epidemic Interventions with EpiPolicy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xXfHnqZ9EVE"
        },
        "Preview": {
          "duration": 30,
          "title": "Planning Epidemic Interventions with EpiPolicy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=f7tJf7hhHeQ"
        },
        "Video Figure": {
          "duration": -1,
          "title": "Planning Epidemic Interventions with EpiPolicy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=YgbCgttxgDE"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Interactive Modeling",
        "Epidemic Simulation",
        "Policy Exploration"
      ],
      "sessionIds": [],
      "eventIds": [
        66230,
        66232
      ],
      "abstract": "We demonstrate EpiPolicy, a model-driven policy-making support tool for epidemic control. EpiPolicy allows a team of public-health officials, including epidemiologists and economists, to construct a compartmental disease model customized for their administrative region and its unique mobility and demographic characteristics. It allows users to specify various interventions, such as social distancing, school closure, vaccination, and disease-specific control measures, both in terms of their effect on disease spread and their economic cost. It also allows users to explore and analyze several possible schedules of these interventions, or policies. In this hands-on demonstration, attendees will be able to implement interventions and schedules of their choosing for a fictitious disease and region. They can then simulate different disease scenarios and examine the results of their policies. Through this interactive setup, we will illustrate to the participants the challenges of policy-making for epidemic control and how EpiPolicy’s abstractions and design principles support policy-makers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 60991
        },
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 61233
        },
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 61050
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Eco Health Alliance",
              "dsl": ""
            }
          ],
          "personId": 60935
        },
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 61198
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61042
        }
      ]
    },
    {
      "id": 61512,
      "typeId": 11891,
      "title": "X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474782"
        },
        "Presentation": {
          "duration": "471",
          "title": "X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uQZ-PleQ9bY"
        },
        "Preview": {
          "duration": 30,
          "title": "X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=t15ZPjxyZCg"
        },
        "Video Figure": {
          "duration": -1,
          "title": "X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WHsG7fisdqM"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Grasping",
        "Handheld Haptics",
        "Shape Display",
        "Shape Rendering",
        "Virtual Reality"
      ],
      "sessionIds": [],
      "eventIds": [
        66231
      ],
      "abstract": "X-Rings is a novel hand-mounted 360 degree shape display for Virtual Reality that renders objects in 3D and responds to user-applied touch and grasping force. Designed as a modular stack of motor-driven expandable rings (5.7-7.7 cm diameter), X-Rings renders radially-symmetric surfaces graspable by the user's whole hand. The device is strapped to the palm, allowing the fingers to freely make and break contact with the device. Capacitive sensors and motor current sensing provide estimates of finger touch states and gripping force. We demonstrate our device's rendering and sensing capabilities in a demo application that enables users to grasp and interact with a variety of different virtual objects.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Mechanical Engineering"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61171
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 60921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61179
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": "Microsoft Research"
            }
          ],
          "personId": 61181
        }
      ]
    },
    {
      "id": 61513,
      "typeId": 11891,
      "title": "Demonstrating Trusscillator: a System for Fabricating Human-Scale Human-Powered Oscillating Devices",
      "addons": {
        "Video Figure": {
          "duration": "287",
          "title": "Demonstrating Trusscillator: a System for Fabricating Human-Scale Human-Powered Oscillating Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=sPdrzyVUCOk"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66232,
        66233
      ],
      "abstract": "Trusscillator is an end-to-end system that allows non-engineers to create human-scale human-powered devices that perform oscillatory movements, such as playground equipment, workout devices, and interactive kinetic installations. While recent research has been focusing on generating mechanisms that produce specific movement-path, without considering the required energy for the motion (kinematic approach), Trusscillator supports users in designing mechanisms that recycle energy in the system in the form of oscillating mechanisms (dynamic approach), specifically with the help of coil-springs. The presented system features a novel set of tools tailored for designing the dynamic experience of the motion. These tools allow designers to focus on user experience-specific aspects, such as motion range, tempo, and effort while abstracting away the underlying technicalities of eigenfrequencies, spring constants, and energy. Since the forces involved in the resulting devices can be high, Trusscillator helps users to fabricate from steel by picking out appropriate steal springs, generating part lists, and producing stencils and welding jigs that help weld with precision. To validate our system, we designed, built, and tested a series of unique playground equipment featuring 2-4 degrees of movement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 60948
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61127
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61075
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60984
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61178
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61292
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "HCI lab"
            }
          ],
          "personId": 60927
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60955
        }
      ]
    },
    {
      "id": 61514,
      "typeId": 11891,
      "title": "Enhancing Model Assessment in Vision-based Interactive Machine Teaching through Real-time Saliency Map Visualization",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480194"
        },
        "Project Page": {
          "annotation": "Project Page",
          "hideBeforeConference": false,
          "hideFromPublicJson": false,
          "hideOnConferenceStart": false,
          "isAvailableForRegisteredMembersOnly": false,
          "title": "Project Page",
          "type": "custom",
          "url": "https://github.com/IIS-Lab/imt_vis_uist21"
        },
        "Video Figure": {
          "duration": "155",
          "title": "Enhancing Model Assessment in Vision-based Interactive Machine Teaching through Real-time Saliency Map Visualization",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GniNiu9WFCA"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Interactive Machine Teaching",
        "Saliency Map",
        "Visualization"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66232
      ],
      "abstract": "Interactive Machine Teaching systems allow users to create customized machine learning models through an iterative process of user-guided training and model assessment. They primarily offer confidence scores of each label or class as feedback for assessment by users. However, we observe that such feedback does not necessarily suffice for users to confirm the behavior of the model. In particular, confidence scores do not always offer the full understanding of what features in the data are used for learning, potentially leading to the creation of an incorrectly-trained model. In this demonstration paper, we present a vision-based interactive machine teaching interface with real-time saliency map visualization in the assessment phase. This visualization can offer feedback on which regions of each image frame the current model utilizes for classification, thus better guiding users to correct the corresponding concepts in the iterative teaching.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Interactive Intelligent Systems Lab."
            }
          ],
          "personId": 61457
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Interactive Intelligent Systems Lab."
            }
          ],
          "personId": 61449
        }
      ]
    },
    {
      "id": 61515,
      "typeId": 11891,
      "title": "UPLIGHT: A Novel Portable Game Device with Omnidirectional Projection Display",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480207"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Human computer interaction",
        "Spatial augmented reality",
        "Entertainment",
        "Game",
        "Omnidirectional display",
        "Tangible interaction",
        "Display",
        "Controller"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66232
      ],
      "abstract": "We hypothesized that the act of actively moving one's body to see the hidden parts of a sphere, cube, or any other structure with height and sides would be entertaining. In this paper, we propose a novel portable game device with omnidirectional display called ``UPLIGHT,'' which was created by combining the element of entertainment with the play style of a portable game device. We also describe the design of a prototype and a playable game application that we developed to achieve this interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ishikawa-ken",
              "city": "Nomi-shi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": "SatoLab"
            }
          ],
          "personId": 61461
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chofu",
              "institution": "The University of Electro-Communication",
              "dsl": ""
            }
          ],
          "personId": 66286
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Chofu-shi",
              "institution": "The University of Electro-Communications ",
              "dsl": ""
            }
          ],
          "personId": 66287
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ishikawa",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": "School of Knowledge Science"
            }
          ],
          "personId": 61470
        }
      ]
    },
    {
      "id": 61516,
      "typeId": 11891,
      "title": "SGToolkit: An Interactive Gesture Authoring Toolkit for Embodied Conversational Agents",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474789"
        },
        "Presentation": {
          "duration": "603",
          "title": "SGToolkit: An Interactive Gesture Authoring Toolkit for Embodied Conversational Agents",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qClSOtLiVlc"
        },
        "Preview": {
          "duration": 30,
          "title": "SGToolkit: An Interactive Gesture Authoring Toolkit for Embodied Conversational Agents",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xmFsT_e2BXw"
        },
        "Video Figure": {
          "duration": -1,
          "title": "SGToolkit: An Interactive Gesture Authoring Toolkit for Embodied Conversational Agents",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qsK3p1S_K48"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66230
      ],
      "abstract": "Non-verbal behavior is essential for embodied agents like social robots, virtual avatars, and digital humans. Existing behavior authoring approaches including keyframe animation and motion capturing are too expensive to use when there are numerous utterances requiring gestures. Automatic generation methods show promising results, but their output quality is not satisfactory yet, and it is hard to modify outputs as a gesture designer wants. We introduce a new gesture generation toolkit, named SGToolkit, which gives a higher quality output than automatic methods and is efficient than manual authoring. For the toolkit, we propose a neural generative model that synthesizes gestures from speech and accommodates fine-level pose controls and coarse-level style controls from users. The user study with 24 participants showed that the toolkit is favorable over manual authoring, and the generated gestures were also human-like and appropriate to input speech. The SGToolkit is platform agnostic, and the code is available at https://github.com/ai4r/SGToolkit.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute (ETRI)",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": " Korea Advanced Institute of Science and Technology (KAIST)",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 61000
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "HCI Lab, School of Computing"
            }
          ],
          "personId": 60952
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 61207
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "ETRI",
              "dsl": ""
            }
          ],
          "personId": 60962
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 61087
        }
      ]
    },
    {
      "id": 61517,
      "typeId": 11891,
      "title": "Fabricating Wooden Circuit Boards by Laser Beam Machining",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480191"
        },
        "Video Figure": {
          "duration": "216",
          "title": "Fabricating Wooden Circuit Boards by Laser Beam Machining",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JrUGSOWX4yQ"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Laser Beam Machining",
        "Digital Fabrication"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66232
      ],
      "abstract": "Laser cutting machines are commonly used in wood processing to cut and engrave wood. In this paper, we propose a method and workflow for producing various sensors and electrical circuits by partially carbonizing the wood surface with a laser cutting machine. Similar to wiring on a conventional printed circuit board (PCB), the carbonized part functions as a conductive electrical path. Several methods for creating small-scale graphene by using a raster-scanning laser beam have been proposed; however, raster-scanning requires a substantial amount of time to create a large circuit using carbon. This paper extends the method with a defocused vector-scanning CW laser beam and reduces the time and cost required for fabrication. The proposed method uses an affordable CW laser cutter to fabricate an electrical circuit including touch sensors, damage sensors, and load sensors on wood boards. The circuit can be easily connected to a common one-board microcontroller using metal screws and nails typically used in DIY woodworking.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Ochanomizu University",
              "dsl": ""
            }
          ],
          "personId": 61458
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo University of Technology",
              "dsl": ""
            }
          ],
          "personId": 61466
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Yahoo Japan Corporation",
              "dsl": ""
            }
          ],
          "personId": 61452
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 61478
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Ochanomizu University",
              "dsl": ""
            }
          ],
          "personId": 61428
        }
      ]
    },
    {
      "id": 61518,
      "typeId": 11891,
      "title": "Deep Augmented Performers: A New Ensemble Performance System by Fusion of Melody Morphing and Body Movements",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480201"
        },
        "Video Figure": {
          "duration": "168",
          "title": "Deep Augmented Performers: A New Ensemble Performance System by Fusion of Melody Morphing and Body Movements",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=P2G7MVJhIaw"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "Melody morphing method",
        "Electrical Muscle Stimulation (EMS)"
      ],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "The fusion of music-information processing and human physical functions will enable new musical experiences to be created. We developed an interactive music system called ``Deep Augmented Performers,'' which provides users with the experience of conducting a musical performance. This system converts music arranged using melody morphing into electrical muscle stimulation (EMS) to control the body movements of multiple performers. The melodies used in the system are divided into segments, and each segment has multiple variations of melodies. The user can interactively control the performers, thus the actual performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN",
              "dsl": ""
            }
          ],
          "personId": 61427
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN",
              "dsl": ""
            }
          ],
          "personId": 61453
        }
      ]
    },
    {
      "id": 61519,
      "typeId": 11891,
      "title": "Roadkill: Nesting Laser-Cut Objects for Fast Assembly",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474799"
        },
        "Presentation": {
          "duration": "416",
          "title": "Roadkill: Nesting Laser-Cut Objects for Fast Assembly",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nynWnvwA6Vc"
        },
        "Preview": {
          "duration": 30,
          "title": "Roadkill: Nesting Laser-Cut Objects for Fast Assembly",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=zvoX52dMYxs"
        },
        "Video Figure": {
          "duration": "289",
          "title": "Roadkill: Nesting Laser-Cut Objects for Fast Assembly",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=QCUlg61KAb0"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66230,
        66232
      ],
      "abstract": "We present Roadkill, a software tool that converts 3D models to 2D cutting plans for laser cutting—such that the resulting layouts allow for fast assembly. Roadkill achieves this by putting all relevant information into the cutting plan: (1) Thumbnails indicate which area of the model a set of parts belongs to. (2) Parts with exposed finger joints are easy to access, thereby suggesting to start assembly here. (3) Openings in the sheet act as jigs, affording assembly within the sheet. (4) Users continue assembly by inserting what has already been assembled into parts that are immediately adjacent or are pointed to by arrows. Roadkill maximizes the number of joints rendered in immediate adjacency by breaking down models into “subassemblies.” Within a subassembly, Roadkill holds the parts together using break-away tabs. (5) Users complete subassemblies according to their labels 1, 2, 3…, following 1 > 1 links to insert subassemblies into other subassemblies, until all parts come together. In our user study, Roadkill allowed participants to assemble layouts 2.4 times faster than layouts generated by a traditional pair-wise labeling of plates.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61138
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60942
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 61303
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60960
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "HCI lab"
            }
          ],
          "personId": 60927
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 61023
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 60955
        }
      ]
    },
    {
      "id": 61601,
      "typeId": 11892,
      "title": "Bendable Color ePaper Displays for Novel Wearable Applications and Mobile Visualization",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480213"
        },
        "Preview": {
          "duration": "30",
          "title": "Bendable Color ePaper Displays for Novel Wearable Applications and Mobile Visualization",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=hPsGRRORIsI"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66230,
        66232
      ],
      "abstract": "This paper presents a toolkit that allows to easily prototype with bendable color ePaper displays for designing and studying novel body-worn interfaces in mobile scenarios. We introduce a software and hardware platform that enables researchers for the first time to implement fully-functional wearable and UbiComp applications with interactive, curved color pixel displays. Further, we provide a set of visual and sensory-rich materials for customization and mounting options. To technically validate our approach and demonstrate its promising potential, we implemented eight real-world applications ranging from personal information and mobile data visualizations over active notifications to media controls. Finally, we report on first usage experiences and conclude with a research roadmap that outlines future applications and directions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Technische Universität Dresden",
              "dsl": "Interactive Media Lab Dresden"
            }
          ],
          "personId": 61556
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Technische Universität Dresden",
              "dsl": "Interactive Media Lab Dresden"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Technische Universität Dresden",
              "dsl": "Centre for Tactile Internet with Human-in-the-Loop (CeTI)"
            }
          ],
          "personId": 61590
        }
      ]
    },
    {
      "id": 61602,
      "typeId": 11892,
      "title": "FaceMe: An Augmented Reality Social Agent Game for Facilitating Children’s Learning about Emotional Expressions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480216"
        },
        "Preview": {
          "duration": "30",
          "title": "FaceMe: An Augmented Reality Social Agent Game for Facilitating Children’s Learning about Emotional Expressions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Ax6-eCOb0Fg"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66233
      ],
      "abstract": "Children with autism spectrum disorder (ASD) experience dysfunctional emotional development leading to negative effects on their social communication. Although interventions are effective in helping children with ASD improve their social skills over time, they have been found to lack the essential ability to engage children in a real social environment. In this paper, we present “FaceMe,” which is an augmented reality (AR) system that uses a virtual agent and a set of tangible toolkits to teach children with ASD about six basic emotions and improve their emotional and communication skills. On the basis of the pilot data, the results suggest that children, especially those with ASD, were willing to socialize with the virtual agent and understand more emotional states. It is hoped that FaceMe can be used as a tool to provide assistance to children with ASD, as well as a way for future interface system design to support emotional development in children.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 61576
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 61566
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 61567
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61545
        }
      ]
    },
    {
      "id": 61603,
      "typeId": 11892,
      "title": "Multi-Window Web Browser with History Tree Visualization for Virtual Reality Environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480221"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "This study introduces a multi-window web browser system to visualize all visited pages and their link structure in a virtual reality (VR) environment. Using VR space, it is possible to visualize many pages and their connections while maintaining their readability. To evaluate the usefulness of our system, we conducted a user study to compare our system to a conventional single-window browsing system. We then found that our system reduces the browsing operations and time for the task of comparing multiple web pages.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61530
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "TOkyo",
              "institution": "Shibaura tech",
              "dsl": "computer science and engineering"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "TOkyo",
              "institution": "Shibaura tech",
              "dsl": "computer science and engineering"
            }
          ],
          "personId": 61560
        }
      ]
    },
    {
      "id": 61604,
      "typeId": 11892,
      "title": "IntoTheVideos: Exploration of Dynamic 3D SpaceReconstruction From Single Sports Videos",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480215"
        },
        "Preview": {
          "duration": "30",
          "title": "IntoTheVideos: Exploration of Dynamic 3D SpaceReconstruction From Single Sports Videos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2AdIEZMQtcU"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66230,
        66231
      ],
      "abstract": "We present IntoTheVideos, a novel system that takes a sports video and reconstructs the 3D space (e.g., basketball court, football field) along with the highlighted event (e.g., making a basket, scoring a touchdown) to enable viewers to experience the sports entertainment with more freedom. Users can move around in the reconstructed 3D space and explore the event freely from different angles and distances as well as view the players in 3D. With this system, we hope to offer users a new option to enjoy sports entertainment beyond passively watching video footage potentially leading to more active fan engagement and participation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61533
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 60970
        }
      ]
    },
    {
      "id": 61605,
      "typeId": 11892,
      "title": "2.5D Simulated Keyframe Animation in Blender",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480222"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66233
      ],
      "abstract": "3D animation requires specialized skills and tends to limit creative expression in favor of physical feasibility, while 2D animation does the opposite. Another duality exists between simulated and keyframe animation. While simulations provide physical believability, keyframes give animators fine timing control. This project seeks to bridge the gap between these approaches to animation: leveraging the expressiveness of 2D animation, the robustness of 3D environment and camera movement, the physical feasibility of simulation, and the control of keyframing. To this end, we present a 2.5D animation interface that takes 2D drawn keyframes and 3D context (object, environment and camera movement) to generate simulated animations that adhere to the user-drawn keyframes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": ""
            }
          ],
          "personId": 61573
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Emeryville",
              "institution": "Pixar Animation Studios",
              "dsl": ""
            }
          ],
          "personId": 61564
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": ""
            }
          ],
          "personId": 61587
        }
      ]
    },
    {
      "id": 61606,
      "typeId": 11892,
      "title": "Towards Social Interaction between 1st and 2nd Person Perspectives on Bodily Play",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480211"
        },
        "Preview": {
          "duration": "30",
          "title": "Towards Social Interaction between 1st and 2nd Person Perspectives on Bodily Play",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ZuB5ltatGQI"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66232
      ],
      "abstract": "Bodily play, which is a productive social interaction for bonding social relationships, has positive impacts on self-efficacy, acute cognitive benefit, and emotion. However, most bodily play encourages players to enjoy their own experiences. There are limited researches on sharing players' perspectives to enhance players' empathy for understanding others. Thus, we propose an asymmetric two-person game in an immersive environment. This bodily play, which supports perspective-taking via the integration with the first- and second-perspectives, has a collaborative interface that allows users to share their physiological and emotional perspectives. Initial testing of the system shows that players can not only understand well the feeling and problems encountered by each other through sharing perspectives and information but also increases the social closeness of players and stimulates empathy after the interplay.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": "707"
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": "707"
            }
          ],
          "personId": 61582
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "No.1001, Daxue Rd., East Dist., Hsinchu City 300",
              "institution": "National Chiao Tung University",
              "dsl": "707 Lab"
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "No.1001, Daxue Rd., East Dist., Hsinchu City 300",
              "institution": "National Chiao Tung University",
              "dsl": "707 Lab"
            }
          ],
          "personId": 61537
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "No.1001, Daxue Rd., East Dist., Hsinchu City 300",
              "institution": "National Chiao Tung University",
              "dsl": "707 Lab"
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "No.1001, Daxue Rd., East Dist., Hsinchu City 300",
              "institution": "National Chiao Tung University",
              "dsl": "707 Lab"
            }
          ],
          "personId": 61571
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": "Computer Science"
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61575
        }
      ]
    },
    {
      "id": 61607,
      "typeId": 11892,
      "title": "Romadoro: Leveraging Nudge Techniques to Encourage Break-Taking",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480231"
        },
        "Preview": {
          "duration": "30",
          "title": "Romadoro: Leveraging Nudge Techniques to Encourage Break-Taking",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jBbesu0Iih8"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66230,
        66231
      ],
      "abstract": "Excessive screen-time has negative impacts on mental and physical well-being, and taking breaks is important to keeping creativity, interest, and productivity high. We developed \\textit{Romadoro}, a Chrome extension that uses the Pomodoro Technique and technology-mediated nudges to promote better break-taking practices. Nudges involve designing choices to predictably alter the behavior of users. To test the effectiveness of using technology mediated nudges together with the Pomodoro Technique on break-taking, we conducted a mixed design user study with 36 participants. The findings from our study indicate that nudge techniques have a significant impact on motivating users to take breaks. Our work demonstrates potential avenues for designing time-management apps that could be more beneficial to the users than the classic Pomodoro approach. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 61589
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Virtual Experiences Research Group"
            }
          ],
          "personId": 61523
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Department of Computer & Information Science & Engineering"
            }
          ],
          "personId": 61600
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 61581
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Dept of CISE"
            }
          ],
          "personId": 61593
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 61529
        }
      ]
    },
    {
      "id": 61608,
      "typeId": 11892,
      "title": "Animating Various Characters using Arm Gestures in Virtual Reality Environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480220"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66232
      ],
      "abstract": "In this study, we propose a method for efficiently animating various characters. The main concept is to build an animation-authoring system in a virtual reality (VR) environment and allow users to move anchors of a character model with their arms. With our system, users select two anchors, which are associated with two VR controllers. The users then directly specify the three-dimensional (3D) motions of the anchors by moving the controllers. To animate various characters with multiple anchors, users can repeat this specification process multiple times. To demonstrate the feasibility of our method, we show animations designed with our system, such as a walking fox, a walking spider, and a flapping hawk.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura institute of technology",
              "dsl": ""
            }
          ],
          "personId": 61578
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "TOkyo",
              "institution": "Shibaura tech",
              "dsl": "computer science and engineering"
            }
          ],
          "personId": 61560
        }
      ]
    },
    {
      "id": 61609,
      "typeId": 11892,
      "title": "A Language Acquisition Support System that Presents Differences and Distances from Model Speech",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480225"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "It is difficult for language learners to know whether they are speaking well and, if not, to know where their speech differs from that of native speakers and how much their speech differs from that of native speakers. Therefore, we propose a novel language learning system that solves these problems. The system uses self-supervised learning to determine whether the user's speech is good or not. The system also shows where the user's speech differs from the native speaker's speech by highlighting the corresponding places on the user's speech waveform. It also represents the learner's speech and the native speaker's speech as points on a two-dimensional coordinate to show the user how far apart they are. We expect that the learner's speech will gradually become better by repeatedly modifying the speech to eliminate these differences and bring the distance closer.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 61558
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 61561
        }
      ]
    },
    {
      "id": 61610,
      "typeId": 11892,
      "title": "Motion Improvisation: 3D Human Motion Synthesis with a Transformer",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480219"
        },
        "Preview": {
          "duration": "30",
          "title": "Motion Improvisation: 3D Human Motion Synthesis with a Transformer",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4fB7x-jBwVM"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66233
      ],
      "abstract": "The synthesis of complicated and realistic human motion is a challenging problem and is a significant task for game, film and animation industries. Many existing methods rely on complex and time-consuming keyframe-based methods that demand professional skills in animation software and motion capture hardware. On the other hand, casual users seek a playful experience to animate their favorite characters with a simple and easy-to-use tool. Recent work has explored building intuitive animation systems but suffers from inability to generate complex and expressive motions. To tackle this limitation, we present a keyframe-driven animation synthesis algorithm that is able to produce complex human motions with a few input keyframes, allowing the user to control the keyframes at will. Inspired by the success of attention-based techniques in natural language processing, our method completes body motions in a sequence-to-sequence manner and captures motion dependencies both spatially and temporally. We evaluate our method qualitatively and quantitatively on the LaFAN1 dataset, demonstrating improved accuracy compared with state of the art methods. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61599
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 60970
        }
      ]
    },
    {
      "id": 61611,
      "typeId": 11892,
      "title": "3DP-Ori: Bridging-Printing Based Origami Fabrication Method with Modifiable Reconfigurability",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480233"
        },
        "Preview": {
          "duration": "30",
          "title": "3DP-Ori: Bridging-Printing Based Origami Fabrication Method with Modifiable Reconfigurability",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vKPvZw0TqSE"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66233
      ],
      "abstract": "Origami is the art of making 2D/3D shapes by folding paper, HCI researchers have leveraged novel structure design and material techniques to create a wide range of origami-based shape-changing interface. Meanwhile, additive manufacturing which could easily fabricate complicated artifacts and augment the haptic sensation, has drawn more attention to the field. This paper presents 3D-Ori, a fabrication method of flexible origami construction with conventional FDM printers. To create and end-to-end pipeline, we developed a parametric design tool with dynamic folding simulation and feasibility estimation, which enable our software to output a printable semi-folded origami model for further easy manual deformation. Adopting and optimizing bridging-based printing, we leverage multiple geometric patterns with adjustable flexibility on creases, which further effect the haptic properties on 3D-Ori objects. We believe the 3D-Ori extends the design space of 3D printing beyond typically hard and fixed forms, and it will help designers and researchers to fabricate interactivities with various physical properties.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 61557
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61595
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61574
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 61536
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            },
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 61542
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61570
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61551
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 62138
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 61584
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61545
        }
      ]
    },
    {
      "id": 61612,
      "typeId": 11892,
      "title": "A Spatial Music Listening Experience in Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480218"
        },
        "Preview": {
          "duration": "30",
          "title": "A Spatial Music Listening Experience in Augmented Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=EZLmsE1zT5M"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66233
      ],
      "abstract": "Live music provides a more immersive and social experience that recorded music cannot replicate. In a live music setting, listeners perceive sounds differently based on their position with respect to the musicians and can enjoy the experience with others. To make recorded music a dynamic listening experience, we propose and implement an app that adds a spatial dimension to music using augmented reality to allow users to listen to a song as if it were played live. The app lets users place virtual instruments around a physical space and plays the instrument track for each instrument. Users can move around in the space and change the importance of various sound localization aspects to customize their experience. Finally, users can record and livestream to share their listening experience with others.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 61562
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 61544
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 61525
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 60970
        }
      ]
    },
    {
      "id": 61613,
      "typeId": 11892,
      "title": "Assisting with Voluntary Pinch Force Control by Using Electrical Muscle Stimulation and Active Bio-Acoustic Sensing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480214"
        },
        "Preview": {
          "duration": "30",
          "title": "Assisting with Voluntary Pinch Force Control by Using Electrical Muscle Stimulation and Active Bio-Acoustic Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=harW5TIzuB8"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "We present a novel wearable system for assisting with voluntary pinch force control that requires no devices on the fingers.\r\nWe use active bio-acoustic sensing to estimate voluntary pinch force with piezo elements attached to the back of the hand and electrical muscle stimulation to the forearm to control involuntary pinch force in a closed-loop system.\r\nWe conducted a user study with eight participants to investigate whether the system could assist with pinch force control under target forces of 3 N, 6 N, or 9 N.\r\nWhen they tried to get pinch force closer to each target force with our system, the medians of the absolute errors were 0.8 N, 1.0 N, and 2.8 N, respectively.\r\nWhen they tried to do it without our system, they were 0.9 N, 1.4 N, and 2.4 N, respectively.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 61535
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 61579
        }
      ]
    },
    {
      "id": 61614,
      "typeId": 11892,
      "title": "A Tool for Monitoring and Controlling Standalone Immersive HCI Experiments",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480217"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66230,
        66232
      ],
      "abstract": "We present an open source tool which provides commonly required functionality in HCI experimental research targeted at Immersive technologies. Using the tool, researchers can monitor and control a Unity scene from a controller panel running in the web-browser.\r\nResearchers can also use the tool to track the user status (e.g. see what the user sees) and trigger the execution of remote functions (e.g. alter the Unity Scene, the objects appearing, etc.). Moreover, we show a use case to exemplify how the tool is being used to support our research regarding Immersive Multisensory\r\nInteraction in a room-sized environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Universidad Carlos III de Madrid (UC3M)",
              "dsl": "DEI Interactive Systems"
            }
          ],
          "personId": 61524
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Leganés",
              "institution": "Universidad Carlos III de Madrid",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 61534
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "University Carlos III of Madrid",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 61541
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Leganes",
              "institution": "Universidad Carlos III de Madrid",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 61550
        }
      ]
    },
    {
      "id": 61615,
      "typeId": 11892,
      "title": "Flick Gesture Interaction in Augmented Reality: AR Carrom",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480229"
        },
        "Preview": {
          "duration": "30",
          "title": "Flick Gesture Interaction in Augmented Reality: AR Carrom",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=bV5dtrVg2Fc"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66233
      ],
      "abstract": "Gestural input can make augmented reality applications feel natural and intuitive. While gestures used as simple input controls are common, gestures that interact directly with the virtual objects are less so. These interaction gestures pose additional challenges since they require the application to make many interpretations about the hand in the camera's field of view such as depth, occlusion, size, and motion. In this work, we describe and propose a flick gesture control mechanic that estimates force and direction from a baseline pinch gesture. We demonstrate the gesture through an example implementation of an AR version of a game called Carrom in which we use our flick mechanic to dynamically interact with the virtual Carrom striker.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61559
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61588
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science"
            }
          ],
          "personId": 61543
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 60970
        }
      ]
    },
    {
      "id": 61616,
      "typeId": 11892,
      "title": "infOrigami: A Computer-aided Design Method for Introducing Traditional Perforated Boneless Lantern Craft to Everyday Interfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480228"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66232
      ],
      "abstract": "The traditional perforated boneless lantern craft is a three-dimensional (3D) lamp fabrication method in which paper sheets are folded without any skeleton, and often accompanied by hole-like patterns. It has been extensively applied throughout history using various means and is favored because of its low cost and environmental friendliness. However, the traditional craft requires a complicated manual production process and accumulated craft expertise, which limits its aesthetics and daily use-value. We propose a computer-aided design method to customize a 3D lamp with decorative patterns from paper pieces. The key idea is to establish an automatic workflow for producing 3D objects with visual information that is similar to the lantern tradition but without overloaded manual efforts. Our user tests and results demonstrate the potential for DIY everyday interfaces for creating personalized aesthetics, enhancing the visualization of information, and promoting multi-person handicraft activity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 61584
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61551
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 61522
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 61539
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 61580
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61548
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 62137
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 61549
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 61555
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 61557
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61545
        }
      ]
    },
    {
      "id": 61617,
      "typeId": 11892,
      "title": "ScenThread: Weaving Smell into Textiles",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480235"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "In this paper, we propose ScenThread, a controllable threadlike olfactory display that could be woven into textiles, creating a localized scent release on a flexible surface. ScenThread comprises a tubular PTFE scent reservoir, a lightweight and non-vibrating piezoelectric pumping system for delivering the scents, and a permeable silicone tubing to diffuse the aroma, as well as a heating module using conductive yarns for accelerating the scent release. ScenThread can be refilled and reused multiple times. We articulated the mechanical design of ScenThread and the pattern design of the heating module, smell intensity and multi-scent release. We conducted a preliminary study by testing the scent releasing performance through maximum smell distance and the duration time. We envision that ScenThread could integrate into a wide range of textile-based items in daily life naturally, seamlessly, and artistically.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai ",
              "institution": "Design School of Fashion and Art",
              "dsl": ""
            }
          ],
          "personId": 61547
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 61526
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 61585
        }
      ]
    },
    {
      "id": 61618,
      "typeId": 11892,
      "title": "Continuous Travel In Virtual Reality Using a 3D Portal",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480227"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66233
      ],
      "abstract": "In virtual reality (VR), continuous movement often leads to users experiencing vection, or cybersickness. To circumvent vection, users are typically given the choice to use teleportation. However, teleportation leads to less spatial awareness compared to continuous movement. One approach to combating cybersickness in continuous movement is to restrict the user’s field of view (FOV). This is typically done by occluding the user’s peripheral vision with a vignette. We developed a FOV restricting continuous locomotion system that does not occlude the user’s FOV and instead uses a 3D portal to display the continuous movement in a limited area in the user’s FOV. We found that this system reduces nausea and disorientation compared to continuous locomotion. However, our system did not significantly increase spatial awareness compared to teleportation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Cornell Tech"
            }
          ],
          "personId": 61569
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell University",
              "dsl": "Cornell Tech"
            }
          ],
          "personId": 61565
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": ""
            }
          ],
          "personId": 61552
        }
      ]
    },
    {
      "id": 61619,
      "typeId": 11892,
      "title": "Feeasy: An Interactive Crowd-Feedback System",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480224"
        },
        "Preview": {
          "duration": "30",
          "title": "Feeasy: An Interactive Crowd-Feedback System",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ndN1i6djgVc"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66233
      ],
      "abstract": "Established user-centered evaluation methods for interactive sys-tems are time-consuming and expensive. Crowd-feedback systemsprovide a quick and cheap solution to collect large amounts ofmeaningful feedback from users. However, existing crowd-feedbacksystems for evaluating interactive systems lack interactivity and aseamless integration into the developed artifact. This might havenegative effects on user engagement as well as feedback quality andquantity. In this work, we present \"Feeasy\", an interactive crowd-feedback system with five key design features that are motivated bya qualitative pilot study. Feeasy extends existing crowd-feedbacksystems for interactive designs by offering more interaction andcombining the feedback panel and the interactive system in oneuser interface. Thereby, it provides users a more seamless way tocontribute feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute of Information Systems and Marketing (IISM)"
            }
          ],
          "personId": 61583
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "DEUTSCHLAND",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute of Information Systems and Marketing (IISM)"
            }
          ],
          "personId": 61592
        }
      ]
    },
    {
      "id": 61620,
      "typeId": 11892,
      "title": "MathMap: Supporting Exploratory Problem Solving with Algebra ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480226"
        },
        "Preview": {
          "duration": "30",
          "title": "MathMap: Supporting Exploratory Problem Solving with Algebra",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vQ61mPn3Nio"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66230,
        66231
      ],
      "abstract": "Tools that support problem-solving in mathematics tend to focus on reaching a solution directly. In practice, it is common to go down paths that do not obviously lead to the solution. This part of the process should be reflected in the tools students use to help them better learn problem-solving strategies. MathMap is an application designed to help high-school students learn how to solve algebraic problems by encouraging them to use multiple strategies, maintain the history of previous attempts, and allows them to meaningfully compare methods with other students.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Creativity Lab"
            }
          ],
          "personId": 61563
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 61596
        }
      ]
    },
    {
      "id": 61621,
      "typeId": 11892,
      "title": "Rope X: Assistance and Guidance on Jumping Rope Frequency, based on Real-time, Heart Rate Feedback During Exercise",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480230"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66230,
        66232
      ],
      "abstract": "Jumping rope is a very efficient aerobic exercise, during which the exerciser can maintain an efficient heart rate range (heart rate value to 140 ~ 160bpm). However, at present, people are unable to recognize whether they are in this ideal aerobic state in a timely and accurate manner, and there is no convenient way of showing exercisers how to perform efficient jumping rope exercises. We developed an early prototype, Rope X, an exercise device that communicates the exerciser's heart rate status through a dynamic interactive interface and gives the exerciser a certain degree of guidance, as to the correct use of the jumping rope to improve the efficiency of the jumping rope exercise (aerobic exercise). ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61531
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "HangZhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61532
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hang Zhou",
              "institution": "ZheJiang University",
              "dsl": "ZheJiang University"
            }
          ],
          "personId": 61521
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61568
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61528
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "HangZhou",
              "institution": "ZheJiang University",
              "dsl": ""
            }
          ],
          "personId": 61546
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Ji'an",
              "institution": "Jinggangshan University",
              "dsl": "Jinggangshan University"
            }
          ],
          "personId": 61591
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61598
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "zhejiang ",
              "city": "ningbo city",
              "institution": "sino-german institute of design and communication zhejiang wanli university",
              "dsl": "college of science &technology ningbo university & zhejiang wanli university"
            }
          ],
          "personId": 61597
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61545
        }
      ]
    },
    {
      "id": 61622,
      "typeId": 11892,
      "title": "4D Doodling: Free Creation of Shape-Changing Decoration with A 3D Printing Pen",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480232"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66214,
        66232
      ],
      "abstract": "4D printing thermoplastic technology has become a common method for fabricating shape-changing interfaces, however, the fully automated 3D printing method also limits user's creative participation in the fabrication process. We present a 4D Doodling method which allow users to create deformable daily items in a free-style way using a 3D printing pen. In the doodling experiment, we summarized the experience of manual printing and provided a practical method to support the user's creation, which can be a way to increase humanity for 4D printing technology. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61545
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou City",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61540
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61572
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 61542
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61570
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61551
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 61536
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 61595
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College",
              "dsl": ""
            }
          ],
          "personId": 61584
        }
      ]
    },
    {
      "id": 61623,
      "typeId": 11892,
      "title": "Interactive Color Manipulation with Natural Language Modifiers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480212"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "Selecting colors is crucial for creating digital illustrations. However, achieving the desired colors, especially for inexperienced users, is difficult because the process involves unintuitive tuning of multiple parameters. In this paper, we propose an interactive color manipulation method allowing users to explore a color space using color modifiers based on natural language expressions such as \"lighter.\" With our method, users can modify colors using word-based modifier sliders. First, they select a modifier word, and the system then generates a modifier-based slider using a machine learning model. Additionally, our method allows users to edit multiple colors simultaneously while preserving the relationships among the original colors. We conducted a user study with amateur artists and novice users. The results indicate that our method can be inspiring, especially for novices, and can be particularly useful when combined with existing tools to support the creation and exploration of colors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 61577
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Strasbourg",
              "institution": "Université de Strasbourg",
              "dsl": ""
            }
          ],
          "personId": 61554
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 61538
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 61027
        }
      ]
    },
    {
      "id": 61624,
      "typeId": 11892,
      "title": "TTTV (Taste the TV) : Taste Presentation Display for “Licking the Screen” using a Rolling Transparent Sheet and a Mixture of Liquid Sprays",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480223"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66231
      ],
      "abstract": "Reproducing the taste of food and beverages as a media technology is an emerging problem of commercial value and entertainment utility. An already developed taste reproduction system uses electrolyte containing gels controlled by electric current to reproduce taste by ion-phoresis. In this study, an alternative taste reproduction mechanism was proposed that sprays a mixture of liquids on a rolling transparent sheet placed on a screen that displays the image of the food item under consideration for reproduction of its taste. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 61527
        }
      ]
    },
    {
      "id": 61625,
      "typeId": 11892,
      "title": "ViTT: Towards a Virtual Reality System that Supports the Learning of Ergonomic Patient Transfers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3480234"
        },
        "Preview": {
          "duration": "30",
          "title": "ViTT: Towards a Virtual Reality System that Supports the Learning of Ergonomic Patient Transfers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Q_MWUAD-F1s"
        }
      },
      "trackId": 11293,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66215,
        66233
      ],
      "abstract": "While patient transfers are part of nurses’ daily work, the manual transfer of patients can also pose a major risk to nurses’ health. The Kinaesthetics care conception may help nurses to conduct patient transfers more ergonomically. However, existing support to learn the concept is low. We introduce ViTT, a Virtual Reality system to promote the individual, self-directed learning of ergonomic patient transfers based on the Kinaesthetics care conception. The current implementation of ViTT supports a nurse in two phases: (i) instructions for a patient transfer, and (ii) training of the transfer with a virtual patient (based on a physics engine; implementation limited). In contrast to previous work, our approach provides an immersive experience that may allow for the ‘safe’ training of different transfer scenarios—e.g., patients with different impairments—and the study of different parameters that may influence nurses’ learning experience—e.g., the simulation of stress—in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": ""
            }
          ],
          "personId": 61594
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": ""
            }
          ],
          "personId": 61553
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": ""
            }
          ],
          "personId": 61586
        }
      ]
    },
    {
      "id": 66208,
      "typeId": 11753,
      "title": "Building Dusty Robotics",
      "addons": {},
      "trackId": 11331,
      "tags": [],
      "keywords": [
        "keynote",
        "robotics",
        "entrepreneurship"
      ],
      "sessionIds": [
        66211
      ],
      "eventIds": [],
      "abstract": "Construction is one of the largest industries on the planet, employing more than 10M workers in the US each year. Yet construction is also the second least-digitized industry; most of the work is still performed with manual labor, paper-based processes, and methods that haven't changed for millennia. Dusty Robotics believes in a future where robots and automation are standard tools employed by the construction workforce to build buildings more efficiently, safer, and at lower cost. In this talk I'll tell the story of how Dusty Robotics originated, our journey through the customer discovery process, and our vision for how robotics will change the face of construction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Dusty Robotics"
            }
          ],
          "personId": 66206
        }
      ]
    },
    {
      "id": 66209,
      "typeId": 11753,
      "title": "More Human HCI",
      "addons": {},
      "trackId": 11331,
      "tags": [],
      "keywords": [
        "keynote",
        "technology",
        "algorithmic fairness",
        "inclusion"
      ],
      "sessionIds": [
        66213
      ],
      "eventIds": [],
      "abstract": "Technology is interwoven into every aspect of our daily lives and work. It increases harmony when our smartphones, apps, etc. simplify our tasks, yet it increases harm, individually and collectively, when our privacy is violated, our discernment is compromised by disinformation, our opportunities are limited by algorithmic bias or our psychological or physical well-being is decreased by device distraction or social media addiction. These tech harms disproportionately affect some of us more so than others, especially womxn and people of color who are underrepresented within the tech industry. HCI is a common element in tech harmony and tech harm, given it is front and center within the contributing apps, devices and services. What role do we play as HCI researchers, practitioners and employers in mitigating tech harm? What role could we play? This interactive session explores user narratives and eye-opening tech industry events through the fabric of a sustainable disruption framework that we can adopt to not only deliver more human HCI but also greater tech harmony for us all.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "institution": "Thrivafy"
            }
          ],
          "personId": 66207
        }
      ]
    },
    {
      "id": 66210,
      "typeId": 11753,
      "title": "Ludic Design For Accessibility",
      "addons": {},
      "trackId": 11331,
      "tags": [],
      "keywords": [
        "keynote",
        "accessibility",
        "augmented reality"
      ],
      "sessionIds": [
        66212
      ],
      "eventIds": [],
      "abstract": "Technology solutions for accessibility have long been created using a narrow utilitarian lens, especially in the Global South, due to multi-dimensional challenges and resource constraints: an emphasis on purely functional outcomes supported by sterile cost-benefit analysis that ignores the fact that people with disability are people first with their own aspirations for leisure and enjoyment in addition to skills and employment. We propose an alternate design methodology called the Ludic Design for Accessibility (LDA) that puts play and playfulness at the center of all assistive technology design and use. We briefly touch upon the challenges of accessibility faced by the huge population of people with vision impairments in India and present our ongoing work with children and teachers in schools for the blind in India, in association with a non-profit, Vision Empower Trust. Though LDA is universally applicable, we highlight the factors that make it especially relevant in the context of accessibility in the Global South.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bangalore",
              "institution": "Microsoft Research India"
            }
          ],
          "personId": 66205
        }
      ]
    },
    {
      "id": 66217,
      "typeId": 11757,
      "title": "Opening Plenary",
      "addons": {},
      "trackId": 11332,
      "tags": [],
      "keywords": [
        "plenary",
        "opening"
      ],
      "sessionIds": [
        66211
      ],
      "eventIds": [],
      "abstract": "An introduction to the conference from the general and program chairs, and an announcement of the lasting impact award winners.",
      "authors": [
        {
          "affiliations": [],
          "personId": 61230
        }
      ]
    },
    {
      "id": 66218,
      "typeId": 11757,
      "title": "Closing Plenary",
      "addons": {},
      "trackId": 11332,
      "tags": [],
      "keywords": [
        "plenary",
        "closing"
      ],
      "sessionIds": [
        66213
      ],
      "eventIds": [],
      "abstract": "A brief wrap-up of a conference, announcement of awards, and introduction to UIST 2022.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 66584
        }
      ]
    },
    {
      "id": 66223,
      "typeId": 11757,
      "title": "Townhall",
      "addons": {},
      "trackId": 11332,
      "tags": [],
      "keywords": [
        "plenary",
        "townhall"
      ],
      "sessionIds": [
        66227
      ],
      "eventIds": [],
      "abstract": "An open-ended discussion about the UIST conference present and future, led by the chairs of UIST 2022.",
      "authors": [
        {
          "affiliations": [],
          "personId": 61067
        },
        {
          "affiliations": [],
          "personId": 61300
        }
      ]
    },
    {
      "id": 66225,
      "typeId": 11753,
      "title": "Lasting Impact Award: KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera",
      "addons": {},
      "trackId": 11333,
      "tags": [],
      "keywords": [
        "plenary",
        "award"
      ],
      "sessionIds": [
        66226
      ],
      "eventIds": [],
      "abstract": "The Lasting Impact Award recognizes UIST papers from 10 years prior that have had a substantial contribution to UIST and the HCI field. This year, the award is given to Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, and Andrew Fitzgibbon for their 2011 UIST paper \"KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera\".",
      "authors": [
        {
          "affiliations": [],
          "personId": 66662
        },
        {
          "affiliations": [],
          "personId": 66660
        },
        {
          "affiliations": [],
          "personId": 66661
        },
        {
          "affiliations": [],
          "personId": 66665
        },
        {
          "affiliations": [],
          "personId": 66666
        },
        {
          "affiliations": [],
          "personId": 66663
        },
        {
          "affiliations": [],
          "personId": 66664
        },
        {
          "affiliations": [],
          "personId": 66667
        },
        {
          "affiliations": [],
          "personId": 66668
        },
        {
          "affiliations": [],
          "personId": 66659
        }
      ]
    },
    {
      "id": 66452,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66235,
        66239,
        66264
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 61273
        }
      ]
    },
    {
      "id": 66453,
      "typeId": 11920,
      "title": "Tonopoly: A Monopoly-like Board Game with Toio",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66232
      ],
      "abstract": "We present Tonopoly, a monopoly-like board game using Toio robots. Tonopoly is a 2-4 player board game that ends in 10 rounds. Players need to collect stars to win the game. Players choose Toio robots as their characters and roll a dice to make their characters move. They can purchase items with money, and get or lose money when their characters step on certain squares. We provide three scenarios where players can get stars: purchase a star from a store, rob a star from other players, and get a star for free. Characters interact with other characters or physical objects in these scenarios. In addition, players can draw a card when they step on certain squares, after which they may get a chance to sabotage their opponents by 1) placing a mine on the track or 2) shooting other characters with cannons mounted on top of their characters. Tonopoly demonstrates the possibility of robot-robot and human-robot interactions in board games.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "city": "Taipei",
              "institution": "National Taiwan University"
            }
          ],
          "personId": 66429
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "city": "Taipei",
              "institution": "National Taiwan University"
            }
          ],
          "personId": 66431
        }
      ]
    },
    {
      "id": 66454,
      "typeId": 11920,
      "title": "Support Remote Social Connectedness with Small Robots",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "Social connections are known to have a long-term positive impact on human health and well-being. Strong social connections can help reduce loneliness and lead to an increased chance of longevity. Friends and family play an important role in people’s social life, as they provide a variety of support through shared activities and companionship. However, for people who are geographically separated from their family and friends, maintaining a high-quality social connection can be challenging. Taking the advantages of the tractable and interactivity of toio, we proposed a system named MoMoBot to support remote social communication and connection based on the toio robotic platform. MoMoBot serves as a communication medium between people at a distance, aiming to enrich the social interactions between distributed friends and family members by providing a variety of ambient awareness and social cues. In this proposal, we first provide an overview of related literature, then describe our proposed design of MoMoBot for remote social connections.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "USA",
              "city": "Minneapolis",
              "institution": "University of Minnesota"
            }
          ],
          "personId": 66451
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "city": "Minneapolis",
              "institution": "University of Minnesota"
            }
          ],
          "personId": 66422
        }
      ]
    },
    {
      "id": 66455,
      "typeId": 11920,
      "title": "BirdsEye: Breaking Out of the Twitter Echo Chamber With a Multi-Robot Interface",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66232
      ],
      "abstract": "Social media sites are an increasingly popular place for news and public discourse. However, the current algorithmic recommendation is creating echo chambers. By utilizing a projector and toio robots, we present a new way of browsing social media content that gives the user a birds’ eye view of all the different viewpoints regarding a topic and enables the user to directly interact with the algorithm. The user can cluster, browse, link, and regroup social media posts using intuitive interaction with the robots by moving, flipping, joining and tapping the robots. By directly interact with different viewpoints and the clustering algorithm, our application can help the user break out of the echo chamber and gain a comprehensive view on a topic.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Tongji University"
            }
          ],
          "personId": 66446
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Tongji University"
            }
          ],
          "personId": 66448
        }
      ]
    },
    {
      "id": 66456,
      "typeId": 11920,
      "title": "VIGOMON plus TOIO",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66232,
        66233
      ],
      "abstract": "Sedentary behavior, defined as “any waking behavior characterized by an energy expenditure ≤1.5 METs [metabolic equivalents] while in a sitting or reclining posture”. In the contemporary workplace, many office workers spend more than half of their entire workday seated. The 2011 American College of Sports Medicine Position Stand on Exercise Prescription recommends that, even for adults who meet physical activity and health guidelines, avoiding prolonged sitting should be a priority. To counteract, there is a growing need to investigate how to increase the office worker's physical activity and reduce prolonged sedentary behaviors in the office environment. Hereby, we present three concepts of vigorous monsters (VIGOMON) based on TOIO technic to help office workers reducing prolonged sedentary behaviors with physical activity promotion in the office environment. They are “Active Bean” “Lie-down Gym” and “Vitality Pop-up”. TOIO act as flexible robot helpers and playful controllers, used in combination with other objects in the scene, providing appropriate active interventions to help office workers keep health and well-being in an interactive way.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "city": "Eindhoven",
              "institution": "Technology University of Eindhoven"
            }
          ],
          "personId": 66423
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "city": "Eindhoven",
              "institution": "Technology University of Eindhoven"
            }
          ],
          "personId": 66421
        }
      ]
    },
    {
      "id": 66457,
      "typeId": 11917,
      "title": "Applying for Faculty Positions",
      "addons": {},
      "trackId": 11339,
      "tags": [],
      "keywords": [
        "ama",
        "ask-me-anything",
        "faculty applications",
        "social"
      ],
      "sessionIds": [],
      "eventIds": [
        66583
      ],
      "abstract": "In this AMA, I will answer any questions you may have about applying for HCI faculty positions. The main focus will be on the job market in North America (US, Canada) but we can also crowdsource answers about how job applications work in other countries from the participants of the AMA. I will start the AMA with a short talk in which I will walk you through the faculty job application process step by step, including what application materials are required and how to prepare for the on-campus interview. This AMA is particularly suitable for PhD students and postdocs who will be on the job market in the next 1-2 years, but it can also be helpful for students who are still early on the PhD track to help think about how to focus your time and energy in the next years.",
      "authors": [
        {
          "affiliations": [],
          "personId": 61017
        }
      ]
    },
    {
      "id": 66458,
      "typeId": 11920,
      "title": "LineUp: Projection-based AR Language Learning System",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66232,
        66233
      ],
      "abstract": "Learning a language by playing a sorting game. Supported by projection AR, each cube will be assigned a part of a sentence, users could move the cube to rearrange the order of the sentence. If the order is correct, the cube will be held together by the magnet and the pronunciation will be played. Otherwise, all the cubes are going to be mad(moving randomly). It’s for primary school children to study language at home by themself. Learning a language with a gamified method, like playing a board game, children will be motivated to learn in an interesting way.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "city": "Milan",
              "institution": "Politecnico di Milano"
            }
          ],
          "personId": 66432
        },
        {
          "affiliations": [
            {
              "country": "Korea",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology (KAIST)"
            }
          ],
          "personId": 66438
        }
      ]
    },
    {
      "id": 66459,
      "typeId": 11920,
      "title": "DOT: A Device for Object-Driven Telepresence",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "This proposal describes DOT, a telepresence platform that detects and integrates everyday objects. DOT will support synchronous and asynchronous manipulation of objects over distance, and is intended to be used as a platform for persistent co-presence. With our design, we specifically aim to enable pause in remote interactions. We focus on waiting because the time between responses has been identified as a key part of meta-communication, especially in relationship and trust development. As part of the project, we will showcase DOT through a variety of applications including remote collabora- tion and crafting, gameplay, and communication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "USA",
              "city": "Boulder",
              "institution": "ATLAS Institute, University of Colorado Boulder"
            }
          ],
          "personId": 66425
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "city": "Boulder",
              "institution": "ATLAS Institute, University of Colorado Boulder"
            }
          ],
          "personId": 66424
        }
      ]
    },
    {
      "id": 66460,
      "typeId": 11920,
      "title": "Global Art Robot Collective",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "Our system, Global Art Robot Collective or GARC, enables remote users to participate in the creation of physical public art. Participants remotely control the movements of small Toio robots that colour the ground as they travel. We provide a real-time view of the art and robots through a publicly available video livestream and user control of the robots through the livestream’s corresponding text chat. Users can use the text chat for conversation as well as sending text based command messages to direct robot movements. Additionally, people able to physically attend the space inhabited by the art can contribute by using additional on-site art tools that we provide.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "city": "Calgary",
              "institution": "University of Calgary"
            }
          ],
          "personId": 66435
        },
        {
          "affiliations": [],
          "personId": 61241
        }
      ]
    },
    {
      "id": 66461,
      "typeId": 11921,
      "title": "Connect to others with Who2chat!",
      "addons": {},
      "trackId": 11336,
      "tags": [],
      "keywords": [
        "social meetup",
        "social",
        "who2chat",
        "david karger"
      ],
      "sessionIds": [],
      "eventIds": [
        66603,
        66604,
        66605
      ],
      "abstract": "Who2chat is a system to help you meet new people at conferences. Who2chat helps you figure out who at the conference would be good to meet, then helps coordinate brief coffee-break-style meetings with those people. If you're a senior researcher, Who2chat helps you welcome new members to the community. You can find specific people to meet based on their research interests and help your junior colleague or students to find the right researchers to talk to. It can also help you meet old-timers you haven't met yet. If you're a newcomer, even a shy one, Who2chat can help you figure out who to talk to and help you approach them in a natural way. You can start by joining an ongoing conversation involving someone you already know. Or you can ask your friends or advisors to suggest people to talk to.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL"
            }
          ],
          "personId": 66442
        }
      ]
    },
    {
      "id": 66462,
      "typeId": 11921,
      "title": "Fabrication Research Lunch (FabLunch)",
      "addons": {},
      "trackId": 11336,
      "tags": [],
      "keywords": [
        "social meetup",
        "social",
        "fabrication",
        "networking",
        "lunch",
        "digital fabrication",
        "personal fabrication",
        "3D printing",
        "sig"
      ],
      "sessionIds": [],
      "eventIds": [
        66658
      ],
      "abstract": "Join us on our Ohyay meetup space and get to know fabrication researchers!",
      "authors": [
        {
          "affiliations": [],
          "personId": 61023
        },
        {
          "affiliations": [],
          "personId": 60982
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 66585
        }
      ]
    },
    {
      "id": 66464,
      "typeId": 11920,
      "title": "Swarm Fabrication: Reconfigurable 3D Printers and Drawing Plotters Made of Swarm Robots",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "We introduce Swarm Fabrication, a novel concept of creating on- demand, scalable, and reconfigurable fabrication machines made of swarm robots. We present ways to construct an element of fabrication machines, such as motors, elevator, table, feeder, and extruder, by leveraging toio robots and 3D printed attachments. By combining these elements, we demonstrate constructing a X-Y-Z plotter with multiple toio robots, which can be used for drawing plotters and 3D printers. We also show the possibility to extend our idea to more general-purpose fabrication machines, which include 3D printers, CNC machining, foam cutters, line drawing devices, pick and place machines, 3D scanning, etc. Through this, we draw a future vision, where the swarm robots can construct a scalable and reconfigurable fabrication machines on-demand, which can be deployed anywhere the user wishes. We believe this fabrication technique will become a means of interactive and highly flexible fabrication in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Tokyo",
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 66443
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "city": "Calgary",
              "institution": "University of Calgary"
            }
          ],
          "personId": 66450
        }
      ]
    },
    {
      "id": 66465,
      "typeId": 11920,
      "title": "TOIO for Braille Learning",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66232,
        66233
      ],
      "abstract": "We propose a system that will help visually impaired children to learn to read and write Braille with toio robots and braille cards. The cards represent letters of the alphabet with the raised dots textured (velvet, EVA, 3D printing, etc.), and a sticker for the robot’s identification. The robot will produce a sound when it finds each card/letter. The tasks can be individual or collaborative. Children will start by identifying sounds and letters with the cards. After that, they can build paths between cards to create words. The tasks with two robots can simulate mobility situations, important for spatial navigation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "city": "Lisbon",
              "institution": "Universidade de Lisboa"
            }
          ],
          "personId": 66427
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "city": "Lisbon",
              "institution": "Universidade de Lisboa"
            }
          ],
          "personId": 66426
        }
      ]
    },
    {
      "id": 66466,
      "typeId": 11918,
      "title": "UIST Social Hour",
      "addons": {},
      "trackId": 11338,
      "tags": [],
      "keywords": [
        "social",
        "social hour",
        "Ohyay",
        "networking",
        "chat"
      ],
      "sessionIds": [],
      "eventIds": [
        66499,
        66500,
        66501,
        66502,
        66503,
        66504,
        66505,
        66506,
        66507
      ],
      "abstract": "Join us on our Ohyay social space for UIST 2021 and get to know other attendees!",
      "authors": [
        {
          "affiliations": [],
          "personId": 60982
        },
        {
          "affiliations": [],
          "personId": 60919
        },
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Paris",
              "institution": "INRIA Paris Saclay"
            }
          ],
          "personId": 66441
        }
      ]
    },
    {
      "id": 66467,
      "typeId": 11920,
      "title": "StoryTime: Designing a Tangible Interface for Collaborative Story Creation to Support Remote Playful Intergenerational Interaction",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "This work presents a design idea which highlights meaningful interaction between grandparents and grandchildren via playfulness. Incorporating the toio robotic platform with a mobile application, we propose designing a tangible collaborative storytelling system for grandparents and grandchildren. The pair will first get a chance to video chat and set up their mats with Lego to create the story’s setting. Then, the grandparent and their grandchild will take turns building a story as they control the toio robots and record audio narratives.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "city": "Manitoba",
              "institution": "University of Manitoba"
            }
          ],
          "personId": 66444
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "city": "Manitoba",
              "institution": "University of Manitoba"
            }
          ],
          "personId": 66437
        }
      ]
    },
    {
      "id": 66468,
      "typeId": 11920,
      "title": "Inter-Reality Robot Interactions",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "Virtual reality experiences can often be lonely. While we may be immersed in amazing visuals, everything around us is virtual, and the touch with the real world is lost. What if there was a way to bring something real with us to this virtual world? We propose using the toio robots to connect the real world with the virtual to create inter-reality robot interactions. We define the term “inter- reality” as a connection between realities such as the real, virtual, or augmented. Leveraging on this idea, we propose using the toio robots to aid in inter-reality communication and bolster safety in virtual reality.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "USA",
              "city": "Virgina",
              "institution": "University of Virginia"
            }
          ],
          "personId": 66430
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "city": "Virgina",
              "institution": "University of Virginia"
            }
          ],
          "personId": 66428
        }
      ]
    },
    {
      "id": 66469,
      "typeId": 11920,
      "title": "Tangiers Toios: Robotic Blackjack Dealers",
      "addons": {},
      "trackId": 11335,
      "tags": [],
      "keywords": [
        "student innovation contest"
      ],
      "sessionIds": [],
      "eventIds": [
        66231,
        66233
      ],
      "abstract": "Games provide an interesting platform to test robot interaction capabilities for task automation. For our project, we will use toio robots to act as the dealer in blackjack. Implementation includes programming the robots and designing attachment mechanisms (e.g. Legos, 3D-printed parts) to accomplish required tasks such as drawing the top card off the deck, dealing cards to players, and flipping cards over when necessary. Robots will coordinate with each other and rely on user interaction via sound or gesture recognition (e.g. \"hit\", \"stand\"). Our goal is to create smooth and natural gameplay while highlighting the potential of toio robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "USA",
              "city": "Lakeland",
              "institution": "Florida Southern College"
            }
          ],
          "personId": 66447
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "city": "Lakeland",
              "institution": "Florida Southern College"
            }
          ],
          "personId": 66445
        }
      ]
    },
    {
      "id": 66550,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66236,
        66245,
        66278
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 61432
        }
      ]
    },
    {
      "id": 66551,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66236,
        66250,
        66265
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 61275
        }
      ]
    },
    {
      "id": 66552,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66252,
        66263,
        66279
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 60933
        }
      ]
    },
    {
      "id": 66553,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66241,
        66245,
        66281
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "institution": "U Hasselt"
            }
          ],
          "personId": 66541
        }
      ]
    },
    {
      "id": 66554,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66238,
        66262,
        66283
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "institution": "U Calgary"
            }
          ],
          "personId": 66538
        }
      ]
    },
    {
      "id": 66555,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66261,
        66265,
        66284
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "US",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL"
            }
          ],
          "personId": 66529
        }
      ]
    },
    {
      "id": 66556,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66237,
        66267,
        66278
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Peru",
              "institution": "Peruvian University of Applied Sciences (UPC)"
            }
          ],
          "personId": 66531
        }
      ]
    },
    {
      "id": 66557,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66247,
        66255,
        66275
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea",
              "institution": "KAIST"
            }
          ],
          "personId": 66532
        }
      ]
    },
    {
      "id": 66558,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66260,
        66274,
        66277
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea",
              "institution": "KAIST"
            }
          ],
          "personId": 66525
        }
      ]
    },
    {
      "id": 66559,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66242,
        66246,
        66272
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Paul Sabatier University"
            }
          ],
          "personId": 66526
        }
      ]
    },
    {
      "id": 66560,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66257,
        66258,
        66279
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "institution": "U Calgary"
            }
          ],
          "personId": 66527
        }
      ]
    },
    {
      "id": 66561,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66237,
        66276,
        66280
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "institution": "Bangladesh University of Engineering and Technology (BUET)"
            }
          ],
          "personId": 66528
        }
      ]
    },
    {
      "id": 66562,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66243,
        66259,
        66268
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 60926
        }
      ]
    },
    {
      "id": 66563,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66240,
        66277,
        66283
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "institution": "MSR India"
            }
          ],
          "personId": 66537
        }
      ]
    },
    {
      "id": 66564,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66235,
        66247,
        66271
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 61215
        }
      ]
    },
    {
      "id": 66565,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66243,
        66260,
        66273
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "Ulm"
            }
          ],
          "personId": 66535
        }
      ]
    },
    {
      "id": 66566,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66253,
        66256,
        66262
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 61224
        }
      ]
    },
    {
      "id": 66567,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66238,
        66246,
        66255
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 66440
        }
      ]
    },
    {
      "id": 66568,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66244,
        66249,
        66257
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "institution": "Xi'an Jiaotong-Liverpool University"
            }
          ],
          "personId": 66549
        }
      ]
    },
    {
      "id": 66569,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66249,
        66256,
        66273
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "institution": "McGill"
            }
          ],
          "personId": 66547
        }
      ]
    },
    {
      "id": 66570,
      "typeId": 11917,
      "title": "How to design your thesis statement (and use it in your job talk)",
      "addons": {},
      "trackId": 11339,
      "tags": [],
      "keywords": [
        "ama",
        "ask-me-anything",
        "thesis statement",
        "jobs",
        "social"
      ],
      "sessionIds": [],
      "eventIds": [
        66582
      ],
      "abstract": "If you are at the thesis-writing stage of your Ph.D., it’s very important to develop a crisp ‘thesis statement’: what do you know, as a result of conducting several years of research, and why is it important? The thesis statement includes both the research problem and your unique contribution. A great dissertation is not simply an account of the work you did, but rather an account of something new that you discovered or invented, as well as why it is important and how it will lead to fruitful future research. I will start with a short talk that describes a practical strategy for framing the different kinds of contributions you can make in HCI––empirical findings, methodology, design, technology, and theory––and how you can craft a successful thesis statement based on your particular type(s) of contributions. I will also talk briefly about how the thesis statement can help you when you apply for a job, both in crafting a research plan and describing your research. This AMA is particularly suitable for Ph.D. students who are currently writing their doctoral dissertations, but will be of interest to anyone who wants to ensure that their research papers and project proposals are well-framed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Paris",
              "institution": "INRIA Paris Saclay"
            }
          ],
          "personId": 66548
        }
      ]
    },
    {
      "id": 66571,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66250,
        66267,
        66274
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea",
              "institution": "KAIST"
            }
          ],
          "personId": 66533
        }
      ]
    },
    {
      "id": 66572,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66242,
        66252,
        66263
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 61073
        }
      ]
    },
    {
      "id": 66573,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66241,
        66251,
        66282
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 60982
        }
      ]
    },
    {
      "id": 66574,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66248,
        66272,
        66276
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "institution": "Hasselt"
            }
          ],
          "personId": 66524
        }
      ]
    },
    {
      "id": 66575,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66234,
        66259,
        66284
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "US",
              "institution": "Chicago"
            }
          ],
          "personId": 66518
        }
      ]
    },
    {
      "id": 66576,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66234,
        66254,
        66264
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "institution": "U Calgary"
            }
          ],
          "personId": 66519
        }
      ]
    },
    {
      "id": 66577,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66248,
        66266,
        66268
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 61246
        }
      ]
    },
    {
      "id": 66578,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66240,
        66258,
        66261
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "institution": "National Taiwan University"
            }
          ],
          "personId": 66517
        }
      ]
    },
    {
      "id": 66579,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66253,
        66266,
        66271
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 61175
        }
      ]
    },
    {
      "id": 66580,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66251,
        66270,
        66282
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "institution": "IIT Madras"
            }
          ],
          "personId": 66520
        }
      ]
    },
    {
      "id": 66581,
      "typeId": 11919,
      "title": "Lightning Talk",
      "addons": {},
      "trackId": 11337,
      "tags": [],
      "keywords": [
        "social",
        "lighting talk"
      ],
      "sessionIds": [
        66239,
        66244,
        66270
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "institution": "U Calgary"
            }
          ],
          "personId": 66521
        }
      ]
    },
    {
      "id": 66594,
      "typeId": 11921,
      "title": "UIST Haptics",
      "addons": {},
      "trackId": 11336,
      "tags": [],
      "keywords": [
        "social meetup",
        "social",
        "haptics sig"
      ],
      "sessionIds": [],
      "eventIds": [
        66647
      ],
      "abstract": "Haptic technologies are deepening the relationship between users and interactive devices. With haptics, not only can the user control their devices, the devices can directly transmit information to the user's body, in the form of vibrations, temperature, touch, pressure, forces and much more. Our community at UIST has been on the forefront of many haptic developments but there has never been a meeting specifically on this topic at the conference, so here we are: ready for the UIST haptics meeting!\n\nThis social event open to everyone who's interested in haptics (for accessibility, VR/AR, real-world, motor learning, communication, you name it!). We will be meeting in an informal setting and breaking out into smaller groups so we can talk to each other about everything XR! This event is designed to give you a chance to network with other like-minded haptic researchers but also to discuss big challenges in haptics: where are we going as a field, what haptic cues are missing, what's possible and impossible, and more!",
      "authors": [
        {
          "affiliations": [],
          "personId": 61224
        },
        {
          "affiliations": [],
          "personId": 60933
        }
      ]
    },
    {
      "id": 66595,
      "typeId": 11921,
      "title": "To be in person or not to be in person?: What should the UIST conference look like?",
      "addons": {},
      "trackId": 11336,
      "tags": [],
      "keywords": [
        "social meetup",
        "social",
        "diversity",
        "sustainability",
        "networking",
        "lunch",
        "sig"
      ],
      "sessionIds": [],
      "eventIds": [
        66610,
        66611
      ],
      "abstract": "Despite the many difficulties of the COVID-19 pandemic, it has given us an opportunity to reflect on conference priorities and witness a large-scale experiment of moving conferences to a virtual format. Last year we gathered data from the UIST conference, and it is clear participants favored in-person elements over their virtual counterparts. However, this data was complicated due to the COVID pandemic, as many struggled with care giving responsibilities and a general lack of social contact. We as a community could also have done a better job with the virtual experience as we continuously learn how to optimise it. The goal of this meetup is to begin a discussion about the community's priorities and to explore virtual and hybrid options for conferences beyond the Covid-19 pandemic - particularly in ways which improve diversity and sustainability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "UK",
              "institution": "University of Bath"
            }
          ],
          "personId": 66593
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Aachen",
              "institution": "RWTH"
            }
          ],
          "personId": 66592
        },
        {
          "affiliations": [
            {
              "country": "US",
              "institution": "UW"
            }
          ],
          "personId": 66589
        },
        {
          "affiliations": [
            {
              "country": "UK",
              "institution": "Lancaster University"
            }
          ],
          "personId": 66588
        }
      ]
    },
    {
      "id": 66596,
      "typeId": 11921,
      "title": "XR Social Meetup",
      "addons": {},
      "trackId": 11336,
      "tags": [],
      "keywords": [
        "social meetup",
        "social",
        "ar/vr",
        "mixed reality",
        "sig"
      ],
      "sessionIds": [],
      "eventIds": [
        66602
      ],
      "abstract": "The interactive exploration and design around Virtual and Mixed Reality (XR) are one of the largest parts of our UIST conference! As such, we are calling all XR-interested researchers to join us during UIST 2021 for a special social event for everyone who's interested in XR, be it AR, VR or just R! :) Similar to our CHI 2021 XR meet-up, we will be meeting in an informal setting and breaking out into smaller groups so we can talk to each other about a variety of topics in XR!",
      "authors": [
        {
          "affiliations": [],
          "personId": 61239
        },
        {
          "affiliations": [],
          "personId": 61040
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "University of Michigan"
            }
          ],
          "personId": 66584
        }
      ]
    },
    {
      "id": 66600,
      "typeId": 11922,
      "title": "HCI @ Facebook Reality Labs: Augmenting Human Capabilities for the XR Future",
      "addons": {},
      "trackId": 11340,
      "tags": [],
      "keywords": [
        "sponsor"
      ],
      "sessionIds": [
        66601
      ],
      "eventIds": [],
      "abstract": "HCI researchers and engineers at Facebook Reality Labs (FRL) are focused on solving the core interaction problems that will enable future all-day wearable Mixed Reality (XR) interfaces. In this session, you will hear from several FRL research scientists about the ground-breaking HCI research conducted in our lab, including innovative acoustic input sensing approaches, novel I/O wristbands and haptic gloves, 3D interaction techniques, adaptive and multimodal user interfaces, as well as a deep-dive on our Electromyography (EMG) input sensing research. The presentation will be followed by a short Q&A with other members of our team where we will discuss the numerous employment and collaboration opportunities in our lab.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Facebook Reality Labs"
            }
          ],
          "personId": 66599
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Facebook Reality Labs"
            }
          ],
          "personId": 66598
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Facebook Reality Labs"
            }
          ],
          "personId": 66597
        }
      ]
    },
    {
      "id": 66619,
      "typeId": 11923,
      "title": "Walking Through Virtual Doors: A Study on the Effects of Virtual Location Changes on Memory",
      "addons": {},
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "The spaces we inhabit can shape and influence the way in which we learn or reinforce information. Virtual reality (VR) is a technology that allows us to alter and create designed environments with great freedom over the visual, audio, and narrative elements. This freedom would benefit from further guidelines that detail approaches and implementations to best achieve desired information delivery goals. Previous work suggests that placing information between locations can aid recall of knowledge and events \\cite{pettijohn2016event}. In this research, we explore the efficacy of delivering information between VR locations, an approach that would be impractical in the real world, \r\nbut an applicable, novel strategy for VR experiences. We present two studies which suggest that VR may interfere with the incidental processing of multiple rooms and potential aid to recall as demonstrated in real world studies \\cite{pettijohn2016event} \\cite{smith1982enhancement}. These studies also suggest a cognitive overhead using VR that may interfere with individual memory strategies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "University of Plymouth",
              "dsl": "School of Engineering Computing and Mathematics"
            }
          ],
          "personId": 66618
        }
      ]
    },
    {
      "id": 66620,
      "typeId": 11923,
      "title": "Visual Design Reuse Through Style Recognition and Transfer",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3477591"
        }
      },
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "This work aims to transfer design attributes and styles within and across visual documents such as slides, graphic designs, and non-photorealistic renderings. Consistent style across elements is a hallmark of good graphic design. Many visual stylistic design patterns exist throughout visualizations, presentations, and interactive media experiences (games, visual novels). These patterns often exist in visual style guides, brand guides, and concept art. However, except for structured document layouts (e.g., HTML/CSS), design tools often do not enforce consistent style decisions or must be manually maintained.  Synchronizing style guides and designs require significant effort, discouraging exploration and the mixing of new ideas. This work introduces algorithms that recognize implicit patterns and structures in visual documents along with interfaces that let designers operate on these patterns, specifically, to view and apply design changes across pattern instances flexibly. The key benefits of visual redesign through implicit patterns are: (1) removing any dependence on upfront formal style declarations, (2) enabling extraction and distribution of implicit visual patterns, and (3) facilitating the exploration of novel visual design concepts through the mixing of styles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 66617
        }
      ]
    },
    {
      "id": 66621,
      "typeId": 11923,
      "title": "Get the GIST: An Interactive Toolkit to Support Generative Design through Metaheuristic Optimization",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3477584"
        }
      },
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "Generative design tools afford designers new ways of creating complex objects from 3D models to machine knittable textures. However, implementing the optimization algorithms that make generative design possible requires the rare combination of programming and domain expertise. I present the Generative Interactive Synthesis Toolkit (GIST) to simplify the implementation of generative design tools in novel domains. GIST factors common optimization algorithms into elements of an extensible library and structures optimization tasks around objectives and tactics specified by domain experts in a simple GUI. This moves the burden of domain expertise from programmers to domain experts. I demonstrate GIST in three unique domains: machine knitting, cookie recipes, and tactile maps for blind users. These show the versatility of GIST's structure. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 66613
        }
      ]
    },
    {
      "id": 66622,
      "typeId": 11923,
      "title": "From Illusions to Beyond-Real Interactions in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3477586"
        }
      },
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "Despite recent advances in technology, current virtual reality (VR) experiences have many limitations. When designing VR interactions, we can leverage the unique affordances of this virtual medium and our ability to programmatically control the renderings to not only overcome these limitations, but also to create new interactions that go beyond the replication of the real world. In my dissertation, I seek to answer the following research questions: How can we utilize the unique affordances that VR offers to overcome the current limitations of this technology? How can we go even further and design mixed reality interactions that leverage these affordances to extend our experiences in the real world? In my work, I approach movement-based VR interactions from a sensorimotor control perspective, carefully considering the plasticity and limits of human perception. To answer the first research question, I explore various visuo-haptic illusions to overcome the limitations of existing haptic devices. In my ongoing work, I am building tools that help researchers and practitioners design and evaluate novel and usable mixed reality interactions that have no real-world counterparts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 66614
        }
      ]
    },
    {
      "id": 66623,
      "typeId": 11923,
      "title": "Human-Scale Personal Fabrication",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3477588"
        }
      },
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "Building large structures from small elements, creating life-size animated creatures, or making contraptions that we can ride on have almost certainly been everyone’s childhood dreams. However, researchers and practitioners of personal fabrication have been mainly focusing on creating objects that fit into a human palm, also called “hand-size” objects. The reason behind this is not only because of the size limitation of consumer-grade fabrication machinery but also because of the very long printing time and high material costs of large-scale prototypes. To overcome these limitations, I combine 3D printed hubs and ready-made objects, such as plastic bottles, as well as welding steel rods into a certain type of node-link structures called “trusses”. However, the actual challenge behind my work is not only about achieving the size, but ensuring that the resulting large structures withstand the orders of magnitude larger forces than their hand-sized counterparts. Designing such structures requires substantial engineering know-how that users of personal fabrication equipment, such as makers, typically do not possess. By providing the lacking engineering know-how, my three end-to-end software systems TrussFab, TrussFormer, and Trusscillator enable non-experts to build such human-scale static, kinetic, and human-powered dynamic devices, such as pavilions, large-scale animatronic devices, and playground equipment. These systems achieve this by allowing users to focus on high-level design aspects, such as shape, animation, or riding experience, while abstracting away the underlying technicalities of forces, inertia, eigenfrequencies, etc. To help building the designs, the software generates the connector pieces and assembly instructions. With this body of work, I aim at democratizing engineering that enables individuals to design and fabricate large-scale objects and mechanisms that involve human-scale forces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction Lab"
            }
          ],
          "personId": 60948
        }
      ]
    },
    {
      "id": 66624,
      "typeId": 11923,
      "title": "Harnessing Disagreement to Create AI-Powered Systems That Reflect Our Values",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3477590"
        }
      },
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "How do we build artificial intelligence systems that reflect our values? Competing potential values we may want to choose from are, at their core, made up of disagreements between individual people. But while the raw datasets that most ML systems rely on are made up of individuals, today’s approaches to building AI typically abstract the individuals out of the pipeline. My thesis contributes a set of algorithms and interactive systems that re-imagine the pipeline for designing and evaluating AI systems, requiring that we deal with competing values in an informed and intentional way. I start from the insight that at each stage of the pipeline, we need to treat individuals as the key unit of operation, rather than the abstractions or aggregated pseudo-humans in use by today’s approaches. I instantiate this insight to address two problems that arise from today's AI pipeline. First, evaluation metrics produce actively misleading scores about the extent to which some people's values are being reflected. I introduce a mathematical transformation that more closely aligns metrics with the values and methods of user-facing performance measures. Second, the resulting AI systems either surreptitiously choose which values to listen to without input from users, or simply present several different outcomes to users without mechanisms to help them select an outcome grounded in their values. I propose a new interaction paradigm for deploying classifiers, asking users to compose a jury consisting of the individual people they'd like their classifiers' decisions to come from.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 66616
        }
      ]
    },
    {
      "id": 66625,
      "typeId": 11923,
      "title": "Taking Digital Product Design Coordination to the Next Level by Elastic Zooming and Linecept",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3477592"
        }
      },
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "Currently, we use a multitude of computing environments in our daily lives, and these devices are increasingly becoming an essential part of our everyday lives \\cite{santosa2013field, jokela2015diary}. For example, someone might start the day by reviewing notifications on a smartwatch, move to browsing the news and quickly responding to emails during breakfast on a smartphone, and then using a laptop and digital whiteboard during work hours to send emails and facilitate meetings. Sometimes, even multiple computing environments are used in parallel to fulfill a task --- Waeljas \\etal describe these are multi-channeled and cross-media service experiences \\cite{waeljas2010multichannel}. However, because existing software programs are seldom able to provide a seamless experience across multiple devices, users are forced to ``act as the bridge connecting their devices'' \\cite{dong2016understanding}. This bridging creates additional burdens on users, which have been found to increase cognitive load \\cite{dong2016understanding}. Additionally, even on the same screen, we use variable viewport sizes to run multiple software applications. These phenomena are more and more observable on handheld devices as well. In digital product design, the use of multiple devices and multiple software is even more augmented as a wide variety of stakeholders with different cultural backgrounds and tool preferences need to work together in order to efficiently deliver scalable and secure digital solutions to billions of customers. Until now, there has yet to be a uniform visualization technique that can serve as a main metaphor for user interfaces across a wide variety of screen sizes and shapes to access information. Such a uniform solution would allow a user to interact with displayed information in a fully consistent way from watch-sized to whiteboard-sized screens, and would also decrease the development time of digital products. As an additional benefit of a novel user interface type, it would allow the manifestation of new software productivity tools that would be significantly easier to use, which could yield to less time spent on work coordination, hence more time allowed to efficient work, and thus, leisure. This way, great societal benefits could be realized as well.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 66615
        }
      ]
    },
    {
      "id": 66626,
      "typeId": 11923,
      "title": "Systems to Democratize and Standardize Access to Web APIs",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3477587"
        }
      },
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "Today, many web sites offer third-party access to their data through web APIs. \r\nHowever, manually encoding URLs with arbitrary endpoints, parameters, authentication handshakes, and pagination, among other things, makes API use challenging and laborious for programmers, and untenable for novices. In addition, each API offers its own idiosyncratic data model, properties, and methods that a new user must learn, even when the sites manage the same common types of information as many others. In my research, I show how working with web APIs can be dramatically simplified by describing the APIs using a standardized, machine-readable ontology, and building systems that democratize and standardize access to these APIs. Specifically, I focus on: 1) systems to enable users to query and retrieve data through APIs without programming, 2) systems that standardize access to APIs and simplify the work for users—even non-programmers—to create interactive web applications that operate on data accessible through arbitrary APIs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 61020
        }
      ]
    },
    {
      "id": 66627,
      "typeId": 11923,
      "title": "Enabling Ubiquitous Personal Fabrication by Deconstructing Established Notions of Artifact Modeling",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3474349.3477589"
        }
      },
      "trackId": 11341,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        66657
      ],
      "abstract": "With the notion of personal fabrication, users are handed industry-level processes to design and manufacture arbitrary physical artifacts. While personal fabrication is a powerful opportunity, it is currently employed by hobbyists and enthusiasts. Consumers, accounting for a majority of the population, still employ workflows like shopping to acquire physical artifacts. The core of my thesis focuses on partially or fully omitting steps of modeling, by relying on outsourced design effort, remixing, and low-effort interactions. Through such deliberate omission of workflow steps, the required effort can be reduced. Instead of starting ''from scratch'', users may remix existing designs, tune parametric designs or merely retrieve their desired artifacts. This moves processes in personal fabrication towards shopping-like interactions, away from complex but powerful industrial CAD (computer-aided design) systems. Instead of relegating design processes to a disconnected workstation, users may conduct search, remix, and preview procedures in-situ, at the location of use for the future artifact. This may simplify the transfer of requirements from the physical environment. Low-effort–high-expressivity fabrication workflows may not be easy to achieve, but crucial for widespread dissemination of personal fabrication. The broader vision behind my focus on ''ubiquitous personal fabrication'' is one where any person can create highly personalized artifacts that suit their unique aesthetic and functional needs, without having to define and model every single detail of the artifact.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "Ulm University",
              "dsl": "Institute of Media Informatics"
            }
          ],
          "personId": 66612
        }
      ]
    },
    {
      "id": 66628,
      "typeId": 11916,
      "title": "The “Seen but Unnoticed” Vocabulary of Natural Touch: Revolutionizing Direct Interaction with Our Devices and One Another",
      "addons": {},
      "trackId": 11342,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        66228,
        66229
      ],
      "eventIds": [],
      "abstract": "This UIST Vision argues that “touch” input and interaction remains in its infancy when viewed in context of the seen but unnoticed vocabulary of natural human behaviors, activity, and environments that surround direct interaction with displays. Unlike status-quo touch interaction—a shadowplay of fingers on a single screen—I argue that our perspective of direct interaction should encompass the full rich context of individual use (whether via touch, sensors, or in combination with other modalities), as well as collaborative activity where people are engaged in local (co-located), remote (tele-present), and hybrid work. We can further view touch through the lens of the “Society of Devices,” where each person’s activities span many complementary, oft-distinct devices that offer the right task affordance (input modality, screen size, aspect ratio, or simply a distinct surface with dedicated purpose) at the right place and time. While many hints of this vision already exist (e.g. [1, 4, 13, 17, 48]), I speculate that a comprehensive program of research to systematically inventory, sense, and design interactions around such human behaviors and activities—and that fully embrace touch as a multi-modal, multi-sensor, multi-user, and multi-device construct—could revolutionize both individual and collaborative interaction with technology. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 61136
        }
      ]
    },
    {
      "id": 66646,
      "typeId": 11891,
      "title": "Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3472749.3474815"
        },
        "Presentation": {
          "duration": 549,
          "title": "Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=RSx_yFlM9Gg"
        },
        "Video Figure": {
          "duration": 0,
          "title": "Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces that Can Change their Appearance Depending on the Viewpoint",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=b4oTMnaxyuM"
        }
      },
      "trackId": 11291,
      "tags": [],
      "keywords": [
        "multi-material 3D printing",
        "optics",
        "lenticular lenses",
        "design tools"
      ],
      "sessionIds": [],
      "eventIds": [
        66214,
        66233
      ],
      "abstract": "In this paper, we present a method that makes 3D objects appear differently under different viewpoints. We accomplish this by 3D printing lenticular lenses across the curved surface of objects. By calculating the lens distribution and the corresponding surface color patterns, we can determine which appearance is shown to the user at each viewpoint.\r\n\r\nWe built a 3D editor that takes as input the 3D model, and the visual appearances, i.e. images, to show at different viewpoints. Our 3D editor then calculates the corresponding lens placements and underlying color pattern. On export, the user can use ray tracing to live preview the resulting appearance from each angle. The 3D model, color pattern, and lenses are then 3D printed in one pass on a multi-material 3D printer to create the final 3D object.\r\n\r\nTo determine the best fabrication parameters for 3D printing lenses, we printed lenses of different sizes and tested various post-processing techniques. To support a large number of different appearances, we compute the lens geometry that has the best trade-off between the number of viewpoints and the protrusion from the object geometry. Finally, we demonstrate our system in practice with a range of use cases for which we show the simulated and physical results side by side.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 61140
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 61229
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61246
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 60919
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Architecture"
            }
          ],
          "personId": 61269
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 61017
        }
      ]
    }
  ],
  "people": [
    {
      "id": 60905,
      "firstName": "Kyle",
      "lastName": "Chard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60906,
      "firstName": "Thomas",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60907,
      "firstName": "Zhouyu",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60908,
      "firstName": "Mukund",
      "lastName": "Raghothaman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60909,
      "firstName": "Ziwei",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60910,
      "firstName": "Xiyun",
      "lastName": "Hu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60911,
      "firstName": "Matthew",
      "lastName": "Kay",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60912,
      "firstName": "Nikhil",
      "lastName": "Jain",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60913,
      "firstName": "Jack",
      "lastName": "Forman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60914,
      "firstName": "Chang",
      "lastName": "Xiao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60915,
      "firstName": "Kristina",
      "lastName": "Höök",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60916,
      "firstName": "Pascal",
      "lastName": "Getreuer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60917,
      "firstName": "Nathaniel",
      "lastName": "Sands",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60918,
      "firstName": "Marie-Paule",
      "lastName": "Cani",
      "middleInitial": "R.",
      "affiliations": []
    },
    {
      "id": 60919,
      "firstName": "Michael",
      "lastName": "Wessely",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60920,
      "firstName": "Shih Chin",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60921,
      "firstName": "Eyal",
      "lastName": "Ofek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60922,
      "firstName": "Hamid",
      "lastName": "Ghaednia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60923,
      "firstName": "Ken",
      "lastName": "Nakagaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60924,
      "firstName": "Romain",
      "lastName": "Nith",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60925,
      "firstName": "Dimitri",
      "lastName": "Kanevsky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60926,
      "firstName": "Po-Yao (Cosmos)",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60927,
      "firstName": "Ran",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60928,
      "firstName": "Vishnu",
      "lastName": "Nair",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60929,
      "firstName": "Aniket",
      "lastName": "Kittur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60930,
      "firstName": "Tianyi",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60931,
      "firstName": "Karthik",
      "lastName": "Ramani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60932,
      "firstName": "Shumin",
      "lastName": "Zhai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60933,
      "firstName": "Jas",
      "lastName": "Brooks",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60934,
      "firstName": "Andrew",
      "lastName": "McNutt",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 60935,
      "firstName": "Whitney",
      "lastName": "Bagge",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60936,
      "firstName": "Albert",
      "lastName": "Tung",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60937,
      "firstName": "Khang",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60938,
      "firstName": "Difeng",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60939,
      "firstName": "Nobuyuki",
      "lastName": "Umetani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60940,
      "firstName": "William",
      "lastName": "Buxton",
      "middleInitial": "A.S.",
      "affiliations": []
    },
    {
      "id": 60941,
      "firstName": "Kaan",
      "lastName": "Akşit",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60942,
      "firstName": "Romeo",
      "lastName": "Sommerfeld",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60943,
      "firstName": "Changxi",
      "lastName": "Zheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60944,
      "firstName": "Philip",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60945,
      "firstName": "Ranjitha",
      "lastName": "Kumar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60946,
      "firstName": "Nisal Menuka",
      "lastName": "Gamage",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60947,
      "firstName": "Steven",
      "lastName": "Craig",
      "middleInitial": "R",
      "affiliations": []
    },
    {
      "id": 60948,
      "firstName": "Robert",
      "lastName": "Kovacs",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60949,
      "firstName": "Michael",
      "lastName": "Zinn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60950,
      "firstName": "Liang",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60951,
      "firstName": "Chun",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60952,
      "firstName": "Keunwoo",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60953,
      "firstName": "Fabian",
      "lastName": "Aust",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60954,
      "firstName": "malcolm",
      "lastName": "slaney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60955,
      "firstName": "Patrick",
      "lastName": "Baudisch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60956,
      "firstName": "Richard",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60957,
      "firstName": "Michael",
      "lastName": "Hughes",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 60958,
      "firstName": "Deniz",
      "lastName": "Arsan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60959,
      "firstName": "Xiaoguang",
      "lastName": "Han",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60960,
      "firstName": "Jonas",
      "lastName": "Noack",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60961,
      "firstName": "Teng",
      "lastName": "Han",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60962,
      "firstName": "Jaehong",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60963,
      "firstName": "Xuhai",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60964,
      "firstName": "Victor",
      "lastName": "Miller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60965,
      "firstName": "Yotam",
      "lastName": "Gingold",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60966,
      "firstName": "Yoshifumi",
      "lastName": "Kitamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60967,
      "firstName": "Emily",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60968,
      "firstName": "Jules",
      "lastName": "Françoise",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60969,
      "firstName": "Pascal",
      "lastName": "Chang",
      "middleInitial": "Chien-Hsi",
      "affiliations": []
    },
    {
      "id": 60970,
      "firstName": "Misha",
      "lastName": "Sra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60971,
      "firstName": "Yuepeng",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60972,
      "firstName": "Brad",
      "lastName": "Myers",
      "middleInitial": "A",
      "affiliations": []
    },
    {
      "id": 60973,
      "firstName": "Thad",
      "lastName": "Starner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60974,
      "firstName": "Radu-Daniel",
      "lastName": "Vatavu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60975,
      "firstName": "Tobias",
      "lastName": "Grosse-Puppendahl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60976,
      "firstName": "David",
      "lastName": "Sontag",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60977,
      "firstName": "David",
      "lastName": "Bethge",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60978,
      "firstName": "Hongbo",
      "lastName": "Fu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60979,
      "firstName": "Maakito",
      "lastName": "Inoue",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60980,
      "firstName": "Wenzhe",
      "lastName": "Cui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60981,
      "firstName": "Vidya",
      "lastName": "Setlur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60982,
      "firstName": "Mustafa Doga",
      "lastName": "Dogan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60983,
      "firstName": "David",
      "lastName": "Ledo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60984,
      "firstName": "Dominik",
      "lastName": "Meier",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60985,
      "firstName": "Auejin",
      "lastName": "Ham",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60986,
      "firstName": "Josephine",
      "lastName": "Koe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60987,
      "firstName": "Ryo",
      "lastName": "Suzuki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60988,
      "firstName": "Paul",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60989,
      "firstName": "Emmanouil",
      "lastName": "Potetsianakis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60990,
      "firstName": "David",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60991,
      "firstName": "Zain",
      "lastName": "Tariq",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60992,
      "firstName": "Blase",
      "lastName": "Ur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60993,
      "firstName": "Oriana",
      "lastName": "Riva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60994,
      "firstName": "Xin",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60995,
      "firstName": "Jiangtao",
      "lastName": "Gong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60996,
      "firstName": "Elaine",
      "lastName": "Huynh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60997,
      "firstName": "Isamu",
      "lastName": "Endo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60998,
      "firstName": "Siddhant",
      "lastName": "Singh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 60999,
      "firstName": "Sumit",
      "lastName": "Gulwani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61000,
      "firstName": "Youngwoo",
      "lastName": "Yoon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61001,
      "firstName": "Lewis",
      "lastName": "Chuang",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 61002,
      "firstName": "Jonathan",
      "lastName": "Mendelson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61003,
      "firstName": "Molly Jane",
      "lastName": "Nicholas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61004,
      "firstName": "Tanya",
      "lastName": "Jonker",
      "middleInitial": "R.",
      "affiliations": []
    },
    {
      "id": 61005,
      "firstName": "Karen",
      "lastName": "Modrei",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61006,
      "firstName": "Yu-Wen",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61007,
      "firstName": "Géry",
      "lastName": "Casiez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61008,
      "firstName": "Wei-Ju",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61009,
      "firstName": "Anusha",
      "lastName": "Withana",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61010,
      "firstName": "Stephen",
      "lastName": "DiVerdi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61011,
      "firstName": "Michael",
      "lastName": "Hagenow",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61012,
      "firstName": "Xuanzhong",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61013,
      "firstName": "Anthony",
      "lastName": "Tang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61014,
      "firstName": "Yuanzhi",
      "lastName": "Cao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61015,
      "firstName": "Xueshi",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61016,
      "firstName": "Bryan",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61017,
      "firstName": "Stefanie",
      "lastName": "Mueller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61018,
      "firstName": "Monica",
      "lastName": "Agrawal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61019,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61020,
      "firstName": "Tarfah",
      "lastName": "Alrashed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61021,
      "firstName": "Florian",
      "lastName": "Alt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61022,
      "firstName": "Yu-Hsin",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61023,
      "firstName": "Thijs",
      "lastName": "Roumen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61024,
      "firstName": "Mike",
      "lastName": "Chen",
      "middleInitial": "Y.",
      "affiliations": []
    },
    {
      "id": 61025,
      "firstName": "Divya",
      "lastName": "Gopinath",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61026,
      "firstName": "Steven",
      "lastName": "Acevedo Colon",
      "middleInitial": "Vidal",
      "affiliations": []
    },
    {
      "id": 61027,
      "firstName": "Takeo",
      "lastName": "Igarashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61028,
      "firstName": "Christian",
      "lastName": "Holz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61029,
      "firstName": "Raiza",
      "lastName": "Hanada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61030,
      "firstName": "Devon",
      "lastName": "McKeon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61031,
      "firstName": "Titus",
      "lastName": "Barik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61032,
      "firstName": "Cathy Mengying",
      "lastName": "Fang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61033,
      "firstName": "Robert",
      "lastName": "Jacob",
      "middleInitial": "J.K",
      "affiliations": []
    },
    {
      "id": 61034,
      "firstName": "Maozheng",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61035,
      "firstName": "Rubaiat Habib",
      "lastName": "Kazi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61036,
      "firstName": "Chris",
      "lastName": "Harrison",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61037,
      "firstName": "Rohit",
      "lastName": "Ramesh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61038,
      "firstName": "Zhongjin",
      "lastName": "Luo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61039,
      "firstName": "IV",
      "lastName": "Ramakrishnan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61040,
      "firstName": "Pedro",
      "lastName": "Lopes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61041,
      "firstName": "Ozgun",
      "lastName": "Kilic Afsar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61042,
      "firstName": "Dennis",
      "lastName": "Shasha",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61043,
      "firstName": "James",
      "lastName": "Landay",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 61044,
      "firstName": "Jürgen",
      "lastName": "Steimle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61045,
      "firstName": "Ken",
      "lastName": "Perlin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61046,
      "firstName": "Ingo",
      "lastName": "Apel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61047,
      "firstName": "Chenning",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61048,
      "firstName": "Xun",
      "lastName": "Qian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61049,
      "firstName": "Ken",
      "lastName": "Pfeuffer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61050,
      "firstName": "Mai",
      "lastName": "Le Xuan Anh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61051,
      "firstName": "Xiaochen",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61052,
      "firstName": "Tom",
      "lastName": "Yeh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61053,
      "firstName": "Omid",
      "lastName": "Abari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61054,
      "firstName": "Shan-Yuan",
      "lastName": "Teng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61055,
      "firstName": "Nikhita",
      "lastName": "Joshi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61056,
      "firstName": "Daniel",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61057,
      "firstName": "Cheng",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61058,
      "firstName": "Téo",
      "lastName": "Sanchez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61059,
      "firstName": "Daniel",
      "lastName": "Vogel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61060,
      "firstName": "Injoo",
      "lastName": "Moon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61061,
      "firstName": "Abdullatif",
      "lastName": "Dinc",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61062,
      "firstName": "Tovi",
      "lastName": "Grossman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61063,
      "firstName": "Deepana",
      "lastName": "Ishtaweera",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61064,
      "firstName": "Siling",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61065,
      "firstName": "Stephen",
      "lastName": "Chong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61066,
      "firstName": "James",
      "lastName": "Higgins",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 61067,
      "firstName": "Jacob",
      "lastName": "Wobbrock",
      "middleInitial": "O.",
      "affiliations": []
    },
    {
      "id": 61068,
      "firstName": "Takaaki",
      "lastName": "Kamigaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61069,
      "firstName": "Fanny",
      "lastName": "Chevalier",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61070,
      "firstName": "Steven",
      "lastName": "Horng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61071,
      "firstName": "Michael Xieyang",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61072,
      "firstName": "Michael",
      "lastName": "Gleicher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61073,
      "firstName": "Yujie",
      "lastName": "Tao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61074,
      "firstName": "Chandrakana",
      "lastName": "Nandi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61075,
      "firstName": "Lukas",
      "lastName": "Fritzsche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61076,
      "firstName": "Ryan",
      "lastName": "Nuqui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61077,
      "firstName": "Nathan",
      "lastName": "Frey",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61078,
      "firstName": "Joshua",
      "lastName": "Verdejo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61079,
      "firstName": "Heming",
      "lastName": "Zhu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61080,
      "firstName": "Chris",
      "lastName": "Parnin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61081,
      "firstName": "Karan",
      "lastName": "Ahuja",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61082,
      "firstName": "Ziyu",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61083,
      "firstName": "Takuro",
      "lastName": "Furumoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61084,
      "firstName": "Nivedita",
      "lastName": "Arora",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61085,
      "firstName": "Will",
      "lastName": "Brackenbury",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61086,
      "firstName": "Michal",
      "lastName": "Piovarči",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61087,
      "firstName": "Geehyuk",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61088,
      "firstName": "Peggy",
      "lastName": "Chi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61089,
      "firstName": "Jen-Hao",
      "lastName": "Cheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61090,
      "firstName": "Lydia",
      "lastName": "Chilton",
      "middleInitial": "B",
      "affiliations": []
    },
    {
      "id": 61091,
      "firstName": "Jan",
      "lastName": "Obernolte",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61092,
      "firstName": "Zhenyi",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61093,
      "firstName": "Sauvik",
      "lastName": "Das",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61094,
      "firstName": "Hrvoje",
      "lastName": "Benko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61095,
      "firstName": "Mayur",
      "lastName": "Naik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61096,
      "firstName": "Yukang",
      "lastName": "Yan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61097,
      "firstName": "Youngwook",
      "lastName": "Do",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61098,
      "firstName": "Yomna",
      "lastName": "Abdelrahman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61099,
      "firstName": "Sunjun",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61100,
      "firstName": "Akshay",
      "lastName": "Sharma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61101,
      "firstName": "Yuzhou",
      "lastName": "Zhuang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61102,
      "firstName": "Brandon",
      "lastName": "Feng",
      "middleInitial": "Yushan",
      "affiliations": []
    },
    {
      "id": 61103,
      "firstName": "Bruno",
      "lastName": "Fruchard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61104,
      "firstName": "Michael",
      "lastName": "Glueck",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61105,
      "firstName": "Paul",
      "lastName": "Asente",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61106,
      "firstName": "Daniela",
      "lastName": "Rodriguez",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 61107,
      "firstName": "Olivia",
      "lastName": "Seow",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61108,
      "firstName": "Wilmot",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61109,
      "firstName": "Jiahe",
      "lastName": "Lyu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61110,
      "firstName": "Seref",
      "lastName": "Güngör",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61111,
      "firstName": "Hollis",
      "lastName": "Lehv",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61112,
      "firstName": "Zhuojun",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61113,
      "firstName": "Fang-Ying",
      "lastName": "Liao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61114,
      "firstName": "Damien",
      "lastName": "Masson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61115,
      "firstName": "Conrad",
      "lastName": "Lempert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61116,
      "firstName": "Richard",
      "lastName": "Lyon",
      "middleInitial": "F",
      "affiliations": []
    },
    {
      "id": 61117,
      "firstName": "Klas",
      "lastName": "Hjort",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61118,
      "firstName": "Hila",
      "lastName": "Mor",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61119,
      "firstName": "Seungmoon",
      "lastName": "Choi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61120,
      "firstName": "Erik",
      "lastName": "Brendel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61121,
      "firstName": "Nischal",
      "lastName": "Shrestha",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61122,
      "firstName": "I-Chao",
      "lastName": "Shen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61123,
      "firstName": "Michel",
      "lastName": "Pahud",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61124,
      "firstName": "Kevin",
      "lastName": "Zhai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61125,
      "firstName": "Artem",
      "lastName": "Dementyev",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61126,
      "firstName": "Priyan",
      "lastName": "Vaithilingam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61127,
      "firstName": "Lukas",
      "lastName": "Rambold",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61128,
      "firstName": "Yung-Wen",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61129,
      "firstName": "Hai-Ning",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61130,
      "firstName": "Alexander",
      "lastName": "Jagaciak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61131,
      "firstName": "Prabal",
      "lastName": "Dutta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61132,
      "firstName": "Matthew",
      "lastName": "Conlen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61133,
      "firstName": "Elena",
      "lastName": "Glassman",
      "middleInitial": "L.",
      "affiliations": []
    },
    {
      "id": 61134,
      "firstName": "Chaeyong",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61135,
      "firstName": "Megan",
      "lastName": "Vo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61136,
      "firstName": "Ken",
      "lastName": "Hinckley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61137,
      "firstName": "Dong",
      "lastName": "Du",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61138,
      "firstName": "Muhammad",
      "lastName": "Abdullah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61139,
      "firstName": "Hijung Valentina",
      "lastName": "Shin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61140,
      "firstName": "Jiani",
      "lastName": "Zeng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61141,
      "firstName": "Yo-Seb",
      "lastName": "Jeon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61142,
      "firstName": "Michael",
      "lastName": "Sedlmair",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61143,
      "firstName": "Hiroshi",
      "lastName": "Ishii",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61144,
      "firstName": "Yan",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61145,
      "firstName": "Charles",
      "lastName": "Ramey",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61146,
      "firstName": "Bilge",
      "lastName": "Mutlu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61147,
      "firstName": "Katrina",
      "lastName": "Panovich",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61148,
      "firstName": "Donald",
      "lastName": "Degraen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61149,
      "firstName": "Emmanuel",
      "lastName": "Senft",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61150,
      "firstName": "Masahiro",
      "lastName": "Fujiwara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61151,
      "firstName": "Karan",
      "lastName": "Singh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61152,
      "firstName": "Jackson",
      "lastName": "Snowden",
      "middleInitial": "C",
      "affiliations": []
    },
    {
      "id": 61153,
      "firstName": "Bjoern",
      "lastName": "Hartmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61154,
      "firstName": "Luke",
      "lastName": "Murray",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61155,
      "firstName": "Margo",
      "lastName": "Seltzer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61156,
      "firstName": "Jeffrey",
      "lastName": "Heer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61157,
      "firstName": "Lea",
      "lastName": "Verou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61158,
      "firstName": "Jose",
      "lastName": "Echevarria",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61159,
      "firstName": "Vu",
      "lastName": "Le",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61160,
      "firstName": "Bernd",
      "lastName": "Bickel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61161,
      "firstName": "Bing-Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61162,
      "firstName": "Michael",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61163,
      "firstName": "Jason",
      "lastName": "Kace",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61164,
      "firstName": "Xinyong",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61165,
      "firstName": "Kazuyuki",
      "lastName": "Fujita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61166,
      "firstName": "Xue",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61167,
      "firstName": "Joseph Chee",
      "lastName": "Chang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61168,
      "firstName": "HANG",
      "lastName": "ZHAO",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61169,
      "firstName": "Xiaolei",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61170,
      "firstName": "Alexander",
      "lastName": "Achberger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61171,
      "firstName": "Eric",
      "lastName": "Gonzalez",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 61172,
      "firstName": "Daniel",
      "lastName": "Leithinger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61173,
      "firstName": "Xin",
      "lastName": "Yi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61174,
      "firstName": "Xiaoyi",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61175,
      "firstName": "Arata",
      "lastName": "Jingu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61176,
      "firstName": "Giuliana",
      "lastName": "Barrios Dell'Olio",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61177,
      "firstName": "Mingyuan",
      "lastName": "Zhong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61178,
      "firstName": "Jotaro",
      "lastName": "Shigeyama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61179,
      "firstName": "Mar",
      "lastName": "Gonzalez-Franco",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61180,
      "firstName": "Giles",
      "lastName": "Blaney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61181,
      "firstName": "Mike",
      "lastName": "Sinclair",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61182,
      "firstName": "Markus",
      "lastName": "Brand",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61183,
      "firstName": "Xiangshi",
      "lastName": "Ren",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61184,
      "firstName": "Yasutoshi",
      "lastName": "Makino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61185,
      "firstName": "Yun-Ting",
      "lastName": "Cheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61186,
      "firstName": "Antonio",
      "lastName": "Krüger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61187,
      "firstName": "Seraphina",
      "lastName": "Yong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61188,
      "firstName": "Cuong",
      "lastName": "Nguyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61189,
      "firstName": "Kresimir",
      "lastName": "Vidackovic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61190,
      "firstName": "Radiah",
      "lastName": "Rivu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61191,
      "firstName": "Aravind",
      "lastName": "Sagar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61192,
      "firstName": "Ravin",
      "lastName": "Balakrishnan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61193,
      "firstName": "Jay",
      "lastName": "Karp",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 61194,
      "firstName": "Seungjae",
      "lastName": "Oh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61195,
      "firstName": "Frederik",
      "lastName": "Smolders",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61196,
      "firstName": "Pengyu",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61197,
      "firstName": "Aakar",
      "lastName": "Gupta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61198,
      "firstName": "Azza",
      "lastName": "Abouzied",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61199,
      "firstName": "Bongshin",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61200,
      "firstName": "Jingyi",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61201,
      "firstName": "Mohamed",
      "lastName": "Kari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61202,
      "firstName": "Gang",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61203,
      "firstName": "Ohad",
      "lastName": "Fried",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61204,
      "firstName": "Reinhard",
      "lastName": "Schütte",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61205,
      "firstName": "Junsu",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61206,
      "firstName": "Robert",
      "lastName": "Radwin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61207,
      "firstName": "Minsu",
      "lastName": "Jang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61208,
      "firstName": "Paul",
      "lastName": "Streli",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61209,
      "firstName": "Alan",
      "lastName": "Tan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61210,
      "firstName": "Nicolai",
      "lastName": "Marquardt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61211,
      "firstName": "Kiyoshi",
      "lastName": "Kiyokawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61212,
      "firstName": "Jorge",
      "lastName": "Goncalves",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61213,
      "firstName": "Aaditya",
      "lastName": "Naik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61214,
      "firstName": "Martin",
      "lastName": "Weigel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61215,
      "firstName": "Jason",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61216,
      "firstName": "Thomas",
      "lastName": "Kosch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61217,
      "firstName": "Joshua",
      "lastName": "Sunshine",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61218,
      "firstName": "Hugo",
      "lastName": "Romat",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61219,
      "firstName": "Jiayue",
      "lastName": "Fan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61220,
      "firstName": "Zeyu",
      "lastName": "Yan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61221,
      "firstName": "Baptiste",
      "lastName": "Caramiaux",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61222,
      "firstName": "Ali",
      "lastName": "Mirzazadeh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61223,
      "firstName": "Lung-Pan",
      "lastName": "Cheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61224,
      "firstName": "Jasmine",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61225,
      "firstName": "Yang",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61226,
      "firstName": "Chunjong",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61227,
      "firstName": "Bedrich",
      "lastName": "Benes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61228,
      "firstName": "Yu-Wei",
      "lastName": "Chang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61229,
      "firstName": "Honghao",
      "lastName": "Deng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61230,
      "firstName": "Jeffrey",
      "lastName": "Nichols",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61231,
      "firstName": "Mohar",
      "lastName": "Kalra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61232,
      "firstName": "Laura-Bianca",
      "lastName": "Bilius",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61233,
      "firstName": "Miro",
      "lastName": "Mannino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61234,
      "firstName": "Yasmeen",
      "lastName": "Abdrabou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61235,
      "firstName": "Yifei",
      "lastName": "Cheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61236,
      "firstName": "Nathalie",
      "lastName": "Henry Riche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61237,
      "firstName": "Keru",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61238,
      "firstName": "Ruofei",
      "lastName": "Du",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61239,
      "firstName": "David",
      "lastName": "Lindlbauer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61240,
      "firstName": "Zhe",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61241,
      "firstName": "Jiannan",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61242,
      "firstName": "Yizheng",
      "lastName": "Gu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61243,
      "firstName": "Lisa",
      "lastName": "Elkin",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 61244,
      "firstName": "Pat",
      "lastName": "Hanrahan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61245,
      "firstName": "Farhani",
      "lastName": "Momotaz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61246,
      "firstName": "Yunyi",
      "lastName": "Zhu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61247,
      "firstName": "Wode",
      "lastName": "Ni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61248,
      "firstName": "Huaishu",
      "lastName": "Peng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61249,
      "firstName": "Frederik",
      "lastName": "Brudy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61250,
      "firstName": "Chengzhi",
      "lastName": "Shi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61251,
      "firstName": "Hsin-Yu",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61252,
      "firstName": "Siyu",
      "lastName": "Zha",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61253,
      "firstName": "Yuntao",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61254,
      "firstName": "Rundong",
      "lastName": "Tian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61255,
      "firstName": "Samuel",
      "lastName": "Silverman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61256,
      "firstName": "Arjun",
      "lastName": "Srinivasan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61257,
      "firstName": "Aaron",
      "lastName": "Elmore",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61258,
      "firstName": "Yan",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61259,
      "firstName": "Brian",
      "lastName": "Smith",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 61260,
      "firstName": "Fengming",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61261,
      "firstName": "Phoebe",
      "lastName": "Welch",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 61262,
      "firstName": "Jane",
      "lastName": "E",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 61263,
      "firstName": "Cong-He",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61264,
      "firstName": "Jessica",
      "lastName": "Tweneboah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61265,
      "firstName": "Varnika",
      "lastName": "Sinha",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61266,
      "firstName": "Hyunchul",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61267,
      "firstName": "Antony Albert Raj",
      "lastName": "Irudayaraj",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61268,
      "firstName": "Ruei-Che",
      "lastName": "Chang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61269,
      "firstName": "Axel",
      "lastName": "Kilian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61270,
      "firstName": "Sean",
      "lastName": "Follmer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61271,
      "firstName": "Mathieu",
      "lastName": "Nancel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61272,
      "firstName": "Mark",
      "lastName": "Parent",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61273,
      "firstName": "Ali",
      "lastName": "Zaidi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61274,
      "firstName": "Yuhui",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61275,
      "firstName": "Junyi",
      "lastName": "Zhu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61276,
      "firstName": "Syed Masum",
      "lastName": "Billah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61277,
      "firstName": "Franziska",
      "lastName": "Shelter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61278,
      "firstName": "Faizan",
      "lastName": "Jamil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61279,
      "firstName": "Jasper",
      "lastName": "Tran O'Leary",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61280,
      "firstName": "David",
      "lastName": "Karger",
      "middleInitial": "R",
      "affiliations": []
    },
    {
      "id": 61281,
      "firstName": "Xiaojun",
      "lastName": "Bi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61282,
      "firstName": "Ben",
      "lastName": "Lafreniere",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61283,
      "firstName": "Jun",
      "lastName": "Gong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61284,
      "firstName": "Sylvain",
      "lastName": "Malacria",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61285,
      "firstName": "Seung Hee",
      "lastName": "Jeong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61286,
      "firstName": "Stephanie",
      "lastName": "Santosa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61287,
      "firstName": "Ali",
      "lastName": "Shtarbanov",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61288,
      "firstName": "Germán",
      "lastName": "Leiva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61289,
      "firstName": "Irfan",
      "lastName": "Essa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61290,
      "firstName": "Joseph",
      "lastName": "Schwab",
      "middleInitial": "H",
      "affiliations": []
    },
    {
      "id": 61291,
      "firstName": "Clemens",
      "lastName": "Klokmose",
      "middleInitial": "Nylandsted",
      "affiliations": []
    },
    {
      "id": 61292,
      "firstName": "Shohei",
      "lastName": "Katakura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61293,
      "firstName": "Shao-Yu",
      "lastName": "Chu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61294,
      "firstName": "Yongsung",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61295,
      "firstName": "Eric",
      "lastName": "Paulos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61296,
      "firstName": "Daniel",
      "lastName": "Pohlandt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61297,
      "firstName": "Gregory",
      "lastName": "Abowd",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 61298,
      "firstName": "Ping-Yi",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61299,
      "firstName": "Nadya",
      "lastName": "Peek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61300,
      "firstName": "Maneesh",
      "lastName": "Agrawala",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61301,
      "firstName": "Takumi",
      "lastName": "Kasai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61302,
      "firstName": "Kazuki",
      "lastName": "Takashima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61303,
      "firstName": "Laurenz",
      "lastName": "Seidel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61304,
      "firstName": "Hiroyuki",
      "lastName": "Shinoda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61305,
      "firstName": "Teddy",
      "lastName": "Seyed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61306,
      "firstName": "Albrecht",
      "lastName": "Schmidt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61307,
      "firstName": "Jeffrey",
      "lastName": "Bigham",
      "middleInitial": "P",
      "affiliations": []
    },
    {
      "id": 61308,
      "firstName": "Cedric",
      "lastName": "Honnet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61309,
      "firstName": "Chih-An",
      "lastName": "Tsao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61310,
      "firstName": "Yi",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61311,
      "firstName": "Savvas",
      "lastName": "Petridis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61312,
      "firstName": "Rishav",
      "lastName": "Agarwal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61313,
      "firstName": "Vicky",
      "lastName": "Bilbily",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61314,
      "firstName": "Toby",
      "lastName": "Chong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61315,
      "firstName": "Daniel",
      "lastName": "Wigdor",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61316,
      "firstName": "Jingmei",
      "lastName": "Hu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61317,
      "firstName": "Mauricio",
      "lastName": "Sousa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61318,
      "firstName": "Jie",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61319,
      "firstName": "Jens Emil",
      "lastName": "Grønbæk",
      "middleInitial": "Sloth",
      "affiliations": []
    },
    {
      "id": 61320,
      "firstName": "Zhourong",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61417,
      "firstName": "Liuxin",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61418,
      "firstName": "Feng",
      "lastName": "Tian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61419,
      "firstName": "Qianying",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61421,
      "firstName": "Hiroyuki",
      "lastName": "Manabe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61422,
      "firstName": "Steven",
      "lastName": "Dow",
      "middleInitial": "P.",
      "affiliations": []
    },
    {
      "id": 61423,
      "firstName": "Wahyu",
      "lastName": "Hutama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61424,
      "firstName": "Yini",
      "lastName": "Qi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61425,
      "firstName": "Yuta",
      "lastName": "Sugiura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61426,
      "firstName": "Rei",
      "lastName": "Kawakami",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61427,
      "firstName": "Hiroya",
      "lastName": "Miura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61428,
      "firstName": "Itiro",
      "lastName": "Siio",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61429,
      "firstName": "Yicheng",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61431,
      "firstName": "Xinrui",
      "lastName": "Fang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61432,
      "firstName": "Martin",
      "lastName": "Nisser",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61433,
      "firstName": "Olivier",
      "lastName": "Bau",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61434,
      "firstName": "John",
      "lastName": "Stankovic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61435,
      "firstName": "Wenqiang",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61436,
      "firstName": "Michael",
      "lastName": "Terry",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61437,
      "firstName": "Alejandra",
      "lastName": "Molina",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61439,
      "firstName": "Yuhei",
      "lastName": "Imai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61440,
      "firstName": "Carrie",
      "lastName": "Cai",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 61441,
      "firstName": "Hannah",
      "lastName": "Huddleston",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61442,
      "firstName": "Stephen",
      "lastName": "MacNeil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61443,
      "firstName": "Aslan",
      "lastName": "B. Wong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61444,
      "firstName": "Qianru",
      "lastName": "Liao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61445,
      "firstName": "Yashaswini",
      "lastName": "Makaram",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61446,
      "firstName": "Leon",
      "lastName": "Cheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61447,
      "firstName": "Andrew",
      "lastName": "Wong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61448,
      "firstName": "Matthew",
      "lastName": "Donnelly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61449,
      "firstName": "Koji",
      "lastName": "Yatani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61450,
      "firstName": "Chengshuo",
      "lastName": "Xia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61451,
      "firstName": "Zijian",
      "lastName": "Ding",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61452,
      "firstName": "Kaori",
      "lastName": "Ikematsu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61453,
      "firstName": "Masatoshi",
      "lastName": "Hamanaka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61454,
      "firstName": "Kenneth",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61456,
      "firstName": "Shao-en",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61457,
      "firstName": "Zhongyi",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61458,
      "firstName": "Ayaka",
      "lastName": "Ishii",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61459,
      "firstName": "Rong-Hao",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61460,
      "firstName": "Scott",
      "lastName": "Jenson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61461,
      "firstName": "Toshikatsu",
      "lastName": "Nakamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61462,
      "firstName": "Lotta-Gili",
      "lastName": "Blumberg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61463,
      "firstName": "Aaron",
      "lastName": "Donsbach",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61464,
      "firstName": "Dishita",
      "lastName": "Turakhia",
      "middleInitial": "G",
      "affiliations": []
    },
    {
      "id": 61465,
      "firstName": "Ziheng",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61466,
      "firstName": "Kunihiro",
      "lastName": "Kato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61467,
      "firstName": "Keunwook",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61468,
      "firstName": "Mason",
      "lastName": "Hayes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61469,
      "firstName": "Hironori",
      "lastName": "Ishikawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61470,
      "firstName": "Toshiki",
      "lastName": "Sato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61471,
      "firstName": "Ellen",
      "lastName": "Jiang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61472,
      "firstName": "Karen",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61473,
      "firstName": "Ricardo",
      "lastName": "Gonzalez Penuela",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 61474,
      "firstName": "Edwin",
      "lastName": "Toh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61475,
      "firstName": "Daniel",
      "lastName": "Bevan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61477,
      "firstName": "KAISHUN",
      "lastName": "WU",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61478,
      "firstName": "Yoshihiro",
      "lastName": "Kawahara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61479,
      "firstName": "Kexin",
      "lastName": "Quan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61480,
      "firstName": "Hikari",
      "lastName": "Harashima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61481,
      "firstName": "Yoonji",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61520,
      "firstName": "Yong",
      "lastName": "Rui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61521,
      "firstName": "Yuyang",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61522,
      "firstName": "Jian",
      "lastName": "Fang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61523,
      "firstName": "Christopher",
      "lastName": "You",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61524,
      "firstName": "Alejandro",
      "lastName": "Rey",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61525,
      "firstName": "Mara",
      "lastName": "Downing",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61526,
      "firstName": "Hebo",
      "lastName": "Gong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61527,
      "firstName": "Homei",
      "lastName": "Miyashita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61528,
      "firstName": "Jiacheng",
      "lastName": "Cao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61529,
      "firstName": "Benjamin",
      "lastName": "Lok",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61530,
      "firstName": "Raku",
      "lastName": "Egawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61531,
      "firstName": "Yijun",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61532,
      "firstName": "Bing",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61533,
      "firstName": "Xiaotong",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61534,
      "firstName": "Andrea",
      "lastName": "Bellucci",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61535,
      "firstName": "Arinobu",
      "lastName": "Niijima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61536,
      "firstName": "Yitao",
      "lastName": "Fan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61537,
      "firstName": "Sai-Keung",
      "lastName": "Wong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61538,
      "firstName": "Tsukasa",
      "lastName": "Fukusato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61539,
      "firstName": "Jinpeng",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61540,
      "firstName": "Shuhong",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61541,
      "firstName": "Paloma",
      "lastName": "Diaz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61542,
      "firstName": "Deying",
      "lastName": "Pan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61543,
      "firstName": "Richa",
      "lastName": "Wadaskar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61544,
      "firstName": "Wei-Yee",
      "lastName": "Goh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61545,
      "firstName": "Guanyun",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61546,
      "firstName": "Yujing",
      "lastName": "Luo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61547,
      "firstName": "Yanan",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61548,
      "firstName": "Ziqi",
      "lastName": "Fang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61549,
      "firstName": "Cheng",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61550,
      "firstName": "Ignacio",
      "lastName": "Aedo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61551,
      "firstName": "Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61552,
      "firstName": "Harald",
      "lastName": "Haraldsson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61553,
      "firstName": "Daniel",
      "lastName": "Schweitzer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61554,
      "firstName": "Nolwenn",
      "lastName": "Maudet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61555,
      "firstName": "Fan",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61556,
      "firstName": "Konstantin",
      "lastName": "Klamka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61557,
      "firstName": "Lingyun",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61558,
      "firstName": "Kazuki",
      "lastName": "Kawamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61559,
      "firstName": "Justin",
      "lastName": "Nilsen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61560,
      "firstName": "Takashi",
      "lastName": "Ijiri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61561,
      "firstName": "Jun",
      "lastName": "Rekimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61562,
      "firstName": "Lawrence",
      "lastName": "Lim",
      "middleInitial": "K",
      "affiliations": []
    },
    {
      "id": 61563,
      "firstName": "Matthew",
      "lastName": "Beaudouin-Lafon",
      "middleInitial": "T",
      "affiliations": []
    },
    {
      "id": 61564,
      "firstName": "Nora",
      "lastName": "Willett",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 61565,
      "firstName": "Serge",
      "lastName": "Belongie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61566,
      "firstName": "Zixia",
      "lastName": "Zheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61567,
      "firstName": "Xiemin",
      "lastName": "Wei",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61568,
      "firstName": "Jing",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61569,
      "firstName": "Andrew",
      "lastName": "Atkins",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61570,
      "firstName": "Yue",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61571,
      "firstName": "Wei-Che",
      "lastName": "Chang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61572,
      "firstName": "Tianyi",
      "lastName": "Ye",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61573,
      "firstName": "Ilene",
      "lastName": "E",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61574,
      "firstName": "Mingming",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61575,
      "firstName": "Roy",
      "lastName": "Fan",
      "middleInitial": "Ping-Hao",
      "affiliations": []
    },
    {
      "id": 61576,
      "firstName": "Jiajia",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61577,
      "firstName": "Momoko",
      "lastName": "Sumida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61578,
      "firstName": "Tsubasa",
      "lastName": "Saito",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61579,
      "firstName": "Yuki",
      "lastName": "Kubo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61580,
      "firstName": "Jingchun",
      "lastName": "Geng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61581,
      "firstName": "Manas",
      "lastName": "Gupta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61582,
      "firstName": "Bo-Han",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61583,
      "firstName": "Saskia",
      "lastName": "Haug",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61584,
      "firstName": "Ye",
      "lastName": "Tao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61585,
      "firstName": "Zhitong",
      "lastName": "Cui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61586,
      "firstName": "Harald",
      "lastName": "Reiterer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61587,
      "firstName": "Adam",
      "lastName": "Finkelstein",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61588,
      "firstName": "Apurv",
      "lastName": "Varshney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61589,
      "firstName": "Katarina",
      "lastName": "Jurczyk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61590,
      "firstName": "Raimund",
      "lastName": "Dachselt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61591,
      "firstName": "Xinyu",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61592,
      "firstName": "Alexander",
      "lastName": "Maedche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61593,
      "firstName": "Lisa",
      "lastName": "Anthony",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61594,
      "firstName": "Maximilian",
      "lastName": "Dürr",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61595,
      "firstName": "Jiaji",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61596,
      "firstName": "Haijun",
      "lastName": "Xia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61597,
      "firstName": "Lei",
      "lastName": "Shi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61598,
      "firstName": "Cheng",
      "lastName": "Yao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61599,
      "firstName": "Yimeng",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 61600,
      "firstName": "Mahsan",
      "lastName": "Nourani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 62137,
      "firstName": "Cejun",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 62138,
      "firstName": "Junzhe",
      "lastName": "Ji",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66205,
      "firstName": "Manohar",
      "lastName": "Swaminathan",
      "affiliations": []
    },
    {
      "id": 66206,
      "firstName": "Tessa",
      "lastName": "Lau",
      "affiliations": []
    },
    {
      "id": 66207,
      "firstName": "Melody",
      "lastName": "Ivory",
      "affiliations": []
    },
    {
      "id": 66286,
      "firstName": "Tatsuya",
      "lastName": "Kato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66287,
      "firstName": "Ryusuke",
      "lastName": "Miyazaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66288,
      "firstName": "ZiQi",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66421,
      "firstName": "Xumeng",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 66422,
      "firstName": "Qiao",
      "lastName": "Jin",
      "affiliations": []
    },
    {
      "id": 66423,
      "firstName": "Xiang",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 66424,
      "firstName": "Ruhan",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 66425,
      "firstName": "Casey",
      "lastName": "Hunt",
      "affiliations": []
    },
    {
      "id": 66426,
      "firstName": "Lúcia",
      "lastName": "Abreu",
      "affiliations": []
    },
    {
      "id": 66427,
      "firstName": "Filipa",
      "lastName": "Rocha",
      "affiliations": []
    },
    {
      "id": 66428,
      "firstName": "Md Aashikur Rahman",
      "lastName": "Azim",
      "affiliations": []
    },
    {
      "id": 66429,
      "firstName": "Chi-Huan",
      "lastName": "Chiang",
      "affiliations": []
    },
    {
      "id": 66430,
      "firstName": "Adil",
      "lastName": "Rahman",
      "affiliations": []
    },
    {
      "id": 66431,
      "firstName": "Ching-Wen",
      "lastName": "Hung",
      "affiliations": []
    },
    {
      "id": 66432,
      "firstName": "Hongni",
      "lastName": "Ye",
      "affiliations": []
    },
    {
      "id": 66435,
      "firstName": "Terrance",
      "lastName": "Mok",
      "affiliations": []
    },
    {
      "id": 66437,
      "firstName": "Danika",
      "lastName": "Passler Bates",
      "affiliations": []
    },
    {
      "id": 66438,
      "firstName": "Yinzhu",
      "lastName": "Piao",
      "affiliations": []
    },
    {
      "id": 66440,
      "firstName": "Dhruv",
      "lastName": "Jain",
      "affiliations": []
    },
    {
      "id": 66441,
      "firstName": "Janin",
      "lastName": "Koch",
      "affiliations": []
    },
    {
      "id": 66442,
      "firstName": "Soya",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 66443,
      "firstName": "Hiroki",
      "lastName": "Kaimoto",
      "affiliations": []
    },
    {
      "id": 66444,
      "firstName": "Nabila",
      "lastName": "Chowdhury",
      "affiliations": []
    },
    {
      "id": 66445,
      "firstName": "Dylan",
      "lastName": "Schils",
      "affiliations": []
    },
    {
      "id": 66446,
      "firstName": "Shengchen",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 66447,
      "firstName": "Justin",
      "lastName": "Wit",
      "affiliations": []
    },
    {
      "id": 66448,
      "firstName": "Zixuan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 66450,
      "firstName": "Samin",
      "lastName": "Farajian",
      "affiliations": []
    },
    {
      "id": 66451,
      "firstName": "Ye",
      "lastName": "Yuan",
      "affiliations": []
    },
    {
      "id": 66478,
      "firstName": "Megan Kelly",
      "lastName": "Hofmann",
      "affiliations": []
    },
    {
      "id": 66479,
      "firstName": "Jeeeun",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 66480,
      "firstName": "Scott",
      "lastName": "Carter",
      "affiliations": []
    },
    {
      "id": 66482,
      "firstName": "Minsuk",
      "lastName": "Chang",
      "affiliations": []
    },
    {
      "id": 66483,
      "firstName": "Anhong",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 66484,
      "firstName": "Thomas",
      "lastName": "Langerak",
      "affiliations": []
    },
    {
      "id": 66485,
      "firstName": "Emily",
      "lastName": "Whiting",
      "affiliations": []
    },
    {
      "id": 66486,
      "firstName": "Xing-Dong",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 66489,
      "firstName": "Andrew",
      "lastName": "Head",
      "affiliations": []
    },
    {
      "id": 66490,
      "firstName": "Toby",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 66491,
      "firstName": "Amy",
      "lastName": "Pavel",
      "affiliations": []
    },
    {
      "id": 66492,
      "firstName": "Yang",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 66493,
      "firstName": "Andy",
      "lastName": "Wilson",
      "affiliations": []
    },
    {
      "id": 66494,
      "firstName": "Valkyrie",
      "lastName": "Savage",
      "affiliations": []
    },
    {
      "id": 66495,
      "firstName": "Ailie",
      "lastName": "Fraser",
      "affiliations": []
    },
    {
      "id": 66496,
      "firstName": "Sai",
      "lastName": "Swaminathan",
      "affiliations": []
    },
    {
      "id": 66498,
      "firstName": "Craig",
      "lastName": "Shultz",
      "affiliations": []
    },
    {
      "id": 66509,
      "firstName": "Amanda",
      "lastName": "Swearngin",
      "affiliations": []
    },
    {
      "id": 66510,
      "firstName": "James",
      "lastName": "Simpson",
      "affiliations": []
    },
    {
      "id": 66511,
      "firstName": "Yi-Chi",
      "lastName": "Liao",
      "affiliations": []
    },
    {
      "id": 66512,
      "firstName": "Yudai",
      "lastName": "Tanaka",
      "affiliations": []
    },
    {
      "id": 66514,
      "firstName": "Jaeyeon",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 66515,
      "firstName": "Prajish",
      "lastName": "Prasad",
      "affiliations": []
    },
    {
      "id": 66517,
      "firstName": "Vivian",
      "lastName": "Chan",
      "affiliations": []
    },
    {
      "id": 66518,
      "firstName": "Jun",
      "lastName": "Nishida",
      "affiliations": []
    },
    {
      "id": 66519,
      "firstName": "Neil",
      "lastName": "Chulpongsatorn",
      "affiliations": []
    },
    {
      "id": 66520,
      "firstName": "Vimal",
      "lastName": "Mollyn",
      "affiliations": []
    },
    {
      "id": 66521,
      "firstName": "Adnan",
      "lastName": "Karim",
      "affiliations": []
    },
    {
      "id": 66524,
      "firstName": "Tom",
      "lastName": "Veuskens",
      "affiliations": []
    },
    {
      "id": 66525,
      "firstName": "Donghyeon",
      "lastName": "Ko",
      "affiliations": []
    },
    {
      "id": 66526,
      "firstName": "Aziz",
      "lastName": "Niyazov",
      "affiliations": []
    },
    {
      "id": 66527,
      "firstName": "Shivesh",
      "lastName": "Jadon",
      "affiliations": []
    },
    {
      "id": 66528,
      "firstName": "Faria",
      "lastName": "Huq",
      "affiliations": []
    },
    {
      "id": 66529,
      "firstName": "Sloke",
      "lastName": "Shrestha",
      "affiliations": []
    },
    {
      "id": 66531,
      "firstName": "Gabriela Vega",
      "lastName": "Lopez",
      "affiliations": []
    },
    {
      "id": 66532,
      "firstName": "Mina",
      "lastName": "Huh",
      "affiliations": []
    },
    {
      "id": 66533,
      "firstName": "Jeongyeon",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 66535,
      "firstName": "Evgeny",
      "lastName": "Stemasov",
      "affiliations": []
    },
    {
      "id": 66537,
      "firstName": "Dhruv",
      "lastName": "Agarwal",
      "affiliations": []
    },
    {
      "id": 66538,
      "firstName": "Marcus",
      "lastName": "Friedel",
      "affiliations": []
    },
    {
      "id": 66541,
      "firstName": "Mannu",
      "lastName": "Lambrichts",
      "affiliations": []
    },
    {
      "id": 66547,
      "firstName": "Yaxuan",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 66548,
      "firstName": "Wendy",
      "lastName": "Mackay",
      "affiliations": []
    },
    {
      "id": 66549,
      "firstName": "Xiang",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 66584,
      "firstName": "Michael",
      "lastName": "Nebeling",
      "affiliations": []
    },
    {
      "id": 66585,
      "firstName": "Laura",
      "lastName": "Devendorf",
      "affiliations": []
    },
    {
      "id": 66588,
      "firstName": "Kelly",
      "lastName": "Widdicks",
      "affiliations": []
    },
    {
      "id": 66589,
      "firstName": "Eunice",
      "lastName": "Jun",
      "affiliations": []
    },
    {
      "id": 66592,
      "firstName": "Nur",
      "lastName": "Hamdan",
      "affiliations": []
    },
    {
      "id": 66593,
      "firstName": "Chris",
      "lastName": "Clarke",
      "affiliations": []
    },
    {
      "id": 66597,
      "firstName": "Ting",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 66598,
      "firstName": "Jason",
      "lastName": "Reisman",
      "affiliations": []
    },
    {
      "id": 66599,
      "firstName": "Eric",
      "lastName": "Whitmire",
      "affiliations": []
    },
    {
      "id": 66609,
      "firstName": "Liang",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 66612,
      "firstName": "Evgeny",
      "lastName": "Stemasov",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66613,
      "firstName": "Megan",
      "lastName": "Hofmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66614,
      "firstName": "Parastoo",
      "lastName": "Abtahi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66615,
      "firstName": "David",
      "lastName": "Kutas",
      "middleInitial": "T",
      "affiliations": []
    },
    {
      "id": 66616,
      "firstName": "Mitchell",
      "lastName": "Gordon",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 66617,
      "firstName": "Jeremy",
      "lastName": "Warner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66618,
      "firstName": "Paul",
      "lastName": "Watson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 66644,
      "firstName": "Jessalyn",
      "lastName": "Alvina",
      "affiliations": []
    },
    {
      "id": 66645,
      "firstName": "James",
      "lastName": "Eagan",
      "affiliations": []
    },
    {
      "id": 66659,
      "firstName": "Andrew",
      "lastName": "Davison",
      "affiliations": []
    },
    {
      "id": 66660,
      "firstName": "David",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 66661,
      "firstName": "Otmar",
      "lastName": "Hilliges",
      "affiliations": []
    },
    {
      "id": 66662,
      "firstName": "Shahram",
      "lastName": "Izadi",
      "affiliations": []
    },
    {
      "id": 66663,
      "firstName": "Pushmeet",
      "lastName": "Kohli",
      "affiliations": []
    },
    {
      "id": 66664,
      "firstName": "Jamie",
      "lastName": "Shotton",
      "affiliations": []
    },
    {
      "id": 66665,
      "firstName": "David",
      "lastName": "Molyneaux",
      "affiliations": []
    },
    {
      "id": 66666,
      "firstName": "Richard",
      "lastName": "Newcombe",
      "affiliations": []
    },
    {
      "id": 66667,
      "firstName": "Steve",
      "lastName": "Hodges",
      "affiliations": []
    },
    {
      "id": 66668,
      "firstName": "Dustin",
      "lastName": "Freeman",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 148,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": true,
    "publicationDate": "2021-10-14 18:49:23+00"
  }
}