{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10082,
    "shortName": "UIST",
    "year": 2022,
    "startDate": 1667001600000,
    "endDate": 1667347200000,
    "fullName": "ACM Symposium on User Interface Software and Technology",
    "url": "https://uist.acm.org/uist2022/",
    "location": "Bend, Oregon, USA",
    "timeZoneOffset": -420,
    "timeZoneName": "America/Los_Angeles",
    "logoUrl": "https://files.sigchi.org/conference/logo/10082/9242c35a-443b-18c6-fbf9-7a42a73924af.png",
    "accessibilityFaqUrl": "https://files.sigchi.org/conference/accessibility/10082/e656a89c-19cb-eb60-396d-c5bed8f78447.html",
    "name": "UIST 2022"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 72,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": true,
    "publicationDate": "2022-11-02 23:35:38+00"
  },
  "sponsors": [
    {
      "id": 10255,
      "name": "Meta Reality Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/e6edab36-5510-5961-1162-6f5d80e8b46e.png",
      "levelId": 10165,
      "url": "https://about.facebook.com/realitylabs/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10256,
      "name": "Adobe",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/dccf5de1-d18b-6123-348c-cb9272ce8e68.png",
      "levelId": 10167,
      "url": "https://adobe.com/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10257,
      "name": "Google (Wellness Sponsor)",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/fbed637d-47a3-dd76-fadb-a3c0af75d165.png",
      "levelId": 10168,
      "url": "https://google.com/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10258,
      "name": "Intel",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/b63ddb43-cb8c-bfb9-a9f6-f8be1a708154.png",
      "levelId": 10168,
      "url": "https://www.intel.com/",
      "order": 1,
      "extraPadding": 0
    },
    {
      "id": 10259,
      "name": "UCL",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/03a7ecf0-b9ff-431d-6526-9d7b95e0d66b.png",
      "levelId": 10168,
      "url": "https://www.ucl.ac.uk/london/",
      "order": 2,
      "extraPadding": 0
    },
    {
      "id": 10260,
      "name": "INTUITIVE",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/bc0074ad-6920-7f13-e647-50c1efde4258.png",
      "levelId": 10168,
      "url": "https://www.intuitive.com/en-us",
      "order": 3,
      "extraPadding": 0
    },
    {
      "id": 10261,
      "name": "Tableau",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/cef003e8-9010-e769-bdf7-5b19debbd988.png",
      "levelId": 10168,
      "url": "https://www.tableau.com/",
      "order": 4,
      "extraPadding": 0
    },
    {
      "id": 10265,
      "name": "Autodesk",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/db3dfae8-054c-35de-3df6-246994361701.png",
      "levelId": 10167,
      "url": "https://www.autodesk.com/",
      "order": 1,
      "extraPadding": 0
    },
    {
      "id": 10282,
      "name": "Toyota Research Institute",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10082/logo/e117df5b-6e69-8a04-cd1a-baccde2af93a.png",
      "levelId": 10168,
      "url": "https://www.tri.global/",
      "order": 5,
      "extraPadding": 0
    }
  ],
  "sponsorLevels": [
    {
      "id": 10160,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    },
    {
      "id": 10165,
      "name": "Platinum",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10166,
      "name": "Gold",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10167,
      "name": "Silver",
      "rank": 4,
      "isDefault": false
    },
    {
      "id": 10168,
      "name": "Bronze",
      "rank": 5,
      "isDefault": false
    }
  ],
  "floors": [
    {
      "id": 10143,
      "name": "Conference Venue",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10082/8808e797-8474-968f-76e9-ac784f999eb2.png",
      "roomIds": [
        10744,
        10753,
        10754,
        10760,
        10761,
        10755
      ]
    }
  ],
  "rooms": [
    {
      "id": 10744,
      "name": "Cascade A/J",
      "typeId": 12319,
      "setup": "Theatre"
    },
    {
      "id": 10753,
      "name": "Cascade E/F/G",
      "typeId": 12319,
      "setup": "Theatre"
    },
    {
      "id": 10754,
      "name": "Board Room",
      "typeId": 12319,
      "setup": "Special"
    },
    {
      "id": 10755,
      "name": "Conference Center Lobby",
      "typeId": 12319,
      "setup": "Special"
    },
    {
      "id": 10756,
      "name": "Exhibit Hall",
      "typeId": 12319,
      "setup": "Special"
    },
    {
      "id": 10757,
      "name": "Hotel Restaurant",
      "typeId": 12319,
      "setup": "Special"
    },
    {
      "id": 10758,
      "name": "Sunriver Resort",
      "typeId": 12319,
      "setup": "Special"
    },
    {
      "id": 10759,
      "name": "Hotel Parking Lot",
      "typeId": 12319,
      "setup": "Special"
    },
    {
      "id": 10760,
      "name": "Cascade C/D/B",
      "typeId": 12319,
      "setup": "Theatre"
    },
    {
      "id": 10761,
      "name": "Cascade C/D/B/A/J",
      "typeId": 12319,
      "setup": "Theatre"
    }
  ],
  "tracks": [
    {
      "id": 11862,
      "name": "UIST 2022 Demos",
      "typeId": 12315
    },
    {
      "id": 11863,
      "name": "UIST 2022 Visions",
      "typeId": 12339
    },
    {
      "id": 11864,
      "name": "UIST 2022 Posters",
      "typeId": 12320
    },
    {
      "id": 11865,
      "name": "UIST 2022 Papers",
      "typeId": 12319
    },
    {
      "id": 11866,
      "name": "UIST 2022 Doctoral Symposium",
      "typeId": 12316
    },
    {
      "id": 11870,
      "name": "UIST 2022 Student Innovation Contest",
      "typeId": 12364
    },
    {
      "id": 11892,
      "typeId": 12320
    },
    {
      "id": 11893,
      "typeId": 12389
    },
    {
      "id": 11945,
      "typeId": 12464
    }
  ],
  "contentTypes": [
    {
      "id": 12314,
      "name": "Course",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 12315,
      "name": "Demo",
      "color": "#006d2c",
      "duration": 120,
      "displayName": "Demos"
    },
    {
      "id": 12316,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 12317,
      "name": "Event",
      "color": "#ffc034",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 12318,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 12319,
      "name": "Paper",
      "color": "#0d42cc",
      "duration": 15,
      "displayName": "Papers"
    },
    {
      "id": 12320,
      "name": "Poster",
      "color": "#ff7a00",
      "duration": 5,
      "displayName": "Posters"
    },
    {
      "id": 12321,
      "name": "Work-in-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 12322,
      "name": "Workshop",
      "color": "#f60000",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 12323,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    },
    {
      "id": 12339,
      "name": "Vision",
      "color": "#32d923",
      "duration": 30
    },
    {
      "id": 12364,
      "name": "Student Innovation Contest",
      "color": "#969696",
      "duration": 120
    },
    {
      "id": 12389,
      "name": "ToCHI",
      "color": "#ff99ca",
      "duration": 15
    },
    {
      "id": 12464,
      "name": "AMA (Ask Me Anything)",
      "color": "#26e5f1",
      "duration": 60
    }
  ],
  "timeSlots": [
    {
      "id": 12472,
      "type": "SESSION",
      "startDate": 1667034000000,
      "endDate": 1667064600000
    },
    {
      "id": 12473,
      "type": "BREAK",
      "startDate": 1667068200000,
      "endDate": 1667077200000
    },
    {
      "id": 12474,
      "type": "SESSION",
      "startDate": 1667149200000,
      "endDate": 1667156400000
    },
    {
      "id": 12475,
      "type": "SESSION",
      "startDate": 1667156400000,
      "endDate": 1667167200000
    },
    {
      "id": 12476,
      "type": "SESSION",
      "startDate": 1667214000000,
      "endDate": 1667219400000
    },
    {
      "id": 12477,
      "type": "SESSION",
      "startDate": 1667226600000,
      "endDate": 1667232000000
    },
    {
      "id": 12478,
      "type": "SESSION",
      "startDate": 1667233800000,
      "endDate": 1667239200000
    },
    {
      "id": 12482,
      "type": "SESSION",
      "startDate": 1667293200000,
      "endDate": 1667298600000
    },
    {
      "id": 12483,
      "type": "SESSION",
      "startDate": 1667300400000,
      "endDate": 1667304000000
    },
    {
      "id": 12484,
      "type": "SESSION",
      "startDate": 1667313000000,
      "endDate": 1667318400000
    },
    {
      "id": 12488,
      "type": "BREAK",
      "startDate": 1667212200000,
      "endDate": 1667214000000
    },
    {
      "id": 12489,
      "type": "LUNCH",
      "startDate": 1667219400000,
      "endDate": 1667226600000
    },
    {
      "id": 12490,
      "type": "BREAK",
      "startDate": 1667232000000,
      "endDate": 1667233800000
    },
    {
      "id": 12491,
      "type": "SESSION",
      "startDate": 1667242800000,
      "endDate": 1667250000000
    },
    {
      "id": 12492,
      "type": "BREAK",
      "startDate": 1667298600000,
      "endDate": 1667300400000
    },
    {
      "id": 12493,
      "type": "SESSION",
      "startDate": 1667304000000,
      "endDate": 1667305800000
    },
    {
      "id": 12494,
      "type": "LUNCH",
      "startDate": 1667305800000,
      "endDate": 1667313000000
    },
    {
      "id": 12495,
      "type": "SESSION",
      "startDate": 1667379600000,
      "endDate": 1667385000000
    },
    {
      "id": 12496,
      "type": "BREAK",
      "startDate": 1667385000000,
      "endDate": 1667386800000
    },
    {
      "id": 12497,
      "type": "SESSION",
      "startDate": 1667386800000,
      "endDate": 1667390400000
    },
    {
      "id": 12498,
      "type": "SESSION",
      "startDate": 1667390400000,
      "endDate": 1667392200000
    },
    {
      "id": 12499,
      "type": "LUNCH",
      "startDate": 1667392200000,
      "endDate": 1667399400000
    },
    {
      "id": 12500,
      "type": "SESSION",
      "startDate": 1667399400000,
      "endDate": 1667404800000
    },
    {
      "id": 12501,
      "type": "BREAK",
      "startDate": 1667404800000,
      "endDate": 1667406600000
    },
    {
      "id": 12502,
      "type": "SESSION",
      "startDate": 1667406600000,
      "endDate": 1667412000000
    },
    {
      "id": 12503,
      "type": "SESSION",
      "startDate": 1667203200000,
      "endDate": 1667206800000
    },
    {
      "id": 12504,
      "type": "SESSION",
      "startDate": 1667206800000,
      "endDate": 1667212200000
    },
    {
      "id": 12505,
      "type": "SESSION",
      "startDate": 1667289600000,
      "endDate": 1667293200000
    },
    {
      "id": 12506,
      "type": "SESSION",
      "startDate": 1667376000000,
      "endDate": 1667379600000
    },
    {
      "id": 12847,
      "type": "BREAK",
      "startDate": 1667318400000,
      "endDate": 1667325600000
    },
    {
      "id": 12848,
      "type": "BREAK",
      "startDate": 1667325600000,
      "endDate": 1667332800000
    },
    {
      "id": 12849,
      "type": "BREAK",
      "startDate": 1667332800000,
      "endDate": 1667336400000
    }
  ],
  "sessions": [
    {
      "id": 85636,
      "name": "XR Interaction",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VkC54ZNO_HU"
        }
      },
      "isParallelPresentation": false,
      "importedId": "01de0229-d28f-4440-8275-a24d17bc51a9",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        89824
      ],
      "contentIds": [
        85018,
        84970,
        84993,
        85054,
        85035,
        85040
      ],
      "source": "SYS",
      "timeSlotId": 12476
    },
    {
      "id": 85637,
      "name": "Fabrication",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9VTmdjPFDNs"
        }
      },
      "isParallelPresentation": false,
      "importedId": "eb7cbea0-ddf1-4d0d-b3de-0203aaeb265d",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        84531
      ],
      "contentIds": [
        84979,
        84989,
        84969,
        85020,
        84974,
        85019
      ],
      "source": "SYS",
      "timeSlotId": 12476
    },
    {
      "id": 85638,
      "name": "Storytelling and Presentation",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VkC54ZNO_HU"
        }
      },
      "isParallelPresentation": false,
      "importedId": "8117c332-1c5b-4f6a-a4a9-271389deb2f9",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        89825
      ],
      "contentIds": [
        84976,
        85001,
        85021,
        85052,
        85010,
        85057
      ],
      "source": "SYS",
      "timeSlotId": 12477
    },
    {
      "id": 85639,
      "name": "Soft and Deformable Materials",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9VTmdjPFDNs"
        }
      },
      "isParallelPresentation": false,
      "importedId": "3d5706c1-c9b8-4827-9a00-474f8e59ad97",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        85140
      ],
      "contentIds": [
        85652,
        85056,
        84981,
        84972,
        85011,
        84978
      ],
      "source": "SYS",
      "timeSlotId": 12477
    },
    {
      "id": 86192,
      "name": "Novel Interactions",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VkC54ZNO_HU"
        }
      },
      "isParallelPresentation": false,
      "importedId": "1c03521d-0d8c-4298-9bda-eeea7172748d",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        89826
      ],
      "contentIds": [
        84987,
        85027,
        85017,
        85000,
        85043,
        84997
      ],
      "source": "SYS",
      "timeSlotId": 12478
    },
    {
      "id": 86193,
      "name": "Information and Visualization Interfaces",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=9VTmdjPFDNs"
        }
      },
      "isParallelPresentation": false,
      "importedId": "78856647-5b39-4da2-b580-4e94a4597c7a",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        84597
      ],
      "contentIds": [
        84983,
        85036,
        85042,
        84966,
        84988,
        84964
      ],
      "source": "SYS",
      "timeSlotId": 12478
    },
    {
      "id": 86194,
      "name": "XR Applications",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_9xrcPXkMMU"
        }
      },
      "isParallelPresentation": false,
      "importedId": "51b4e410-ea23-4202-bb77-0f326fcd1346",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        89827
      ],
      "contentIds": [
        85032,
        85029,
        85046,
        85033,
        84990,
        85058
      ],
      "source": "SYS",
      "timeSlotId": 12482
    },
    {
      "id": 86195,
      "name": "Accessibility",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2tvE5n_5SKo"
        }
      },
      "isParallelPresentation": false,
      "importedId": "015e0f3a-0570-4ac4-af4a-5e481f2c927c",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        84746
      ],
      "contentIds": [
        84996,
        85006,
        85008,
        84998,
        85037,
        85030
      ],
      "source": "SYS",
      "timeSlotId": 12482
    },
    {
      "id": 86197,
      "name": "XR Toolkits",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_9xrcPXkMMU"
        }
      },
      "isParallelPresentation": false,
      "importedId": "0fcced20-2e7d-457a-ad08-5d3d03a5f389",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        89828
      ],
      "contentIds": [
        85060,
        85028,
        85012,
        84991
      ],
      "source": "SYS",
      "timeSlotId": 12483
    },
    {
      "id": 86198,
      "name": "Mind and Body",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2tvE5n_5SKo"
        }
      },
      "isParallelPresentation": false,
      "importedId": "65862f85-30f2-4701-a7c8-e0546ca5a5e7",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        86378
      ],
      "contentIds": [
        84982,
        84963,
        84968,
        85644
      ],
      "source": "SYS",
      "timeSlotId": 12483
    },
    {
      "id": 86199,
      "name": "Beyond the Desktop",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_9xrcPXkMMU"
        }
      },
      "isParallelPresentation": false,
      "importedId": "2ece241b-ec07-49aa-b192-da28aac7f14a",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        89829
      ],
      "contentIds": [
        85055,
        84986,
        85038,
        84992,
        84995,
        85026
      ],
      "source": "SYS",
      "timeSlotId": 12484
    },
    {
      "id": 86200,
      "name": "Programming, Kits, and Libraries",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2tvE5n_5SKo"
        }
      },
      "isParallelPresentation": false,
      "importedId": "04edead6-ddea-4d39-969e-4cd006c58811",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        89830
      ],
      "contentIds": [
        84994,
        85024,
        85039,
        85051,
        85009,
        85031
      ],
      "source": "SYS",
      "timeSlotId": 12484
    },
    {
      "id": 86201,
      "name": "Material Interfaces and Displays",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lipwOvkvrsM"
        }
      },
      "isParallelPresentation": false,
      "importedId": "2011694f-6472-483f-bf92-c687d3208ced",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        84662
      ],
      "contentIds": [
        85014,
        85003,
        84975,
        84980,
        84971,
        85041
      ],
      "source": "SYS",
      "timeSlotId": 12495
    },
    {
      "id": 86202,
      "name": "Generative Design",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=txM-TuZawx8"
        }
      },
      "isParallelPresentation": false,
      "importedId": "d2cb4e50-d2a9-4371-8604-3c50702c93cc",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        85195
      ],
      "contentIds": [
        85015,
        85047,
        85004,
        85050,
        85025,
        85023
      ],
      "source": "SYS",
      "timeSlotId": 12495
    },
    {
      "id": 86203,
      "name": "XR Perception",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lipwOvkvrsM"
        }
      },
      "isParallelPresentation": false,
      "importedId": "c862605f-f502-4fcd-83c9-360cba6c4f26",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        84611
      ],
      "contentIds": [
        84985,
        85049,
        85034,
        85059
      ],
      "source": "SYS",
      "timeSlotId": 12497
    },
    {
      "id": 86204,
      "name": "3D Printing",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=txM-TuZawx8"
        }
      },
      "isParallelPresentation": false,
      "importedId": "5cdfb9e3-382f-4d36-985f-4bad74ed67c7",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        84912
      ],
      "contentIds": [
        85002,
        84984,
        85045,
        85022
      ],
      "source": "SYS",
      "timeSlotId": 12497
    },
    {
      "id": 86205,
      "name": "Modeling and Intent",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lipwOvkvrsM"
        }
      },
      "isParallelPresentation": false,
      "importedId": "0f2021db-e9d9-417c-8ea8-9592c9d3a3bd",
      "typeId": 12319,
      "roomId": 10760,
      "chairIds": [
        84725
      ],
      "contentIds": [
        85007,
        85044,
        84965,
        85013,
        85048,
        84977
      ],
      "source": "SYS",
      "timeSlotId": 12500
    },
    {
      "id": 86206,
      "name": "Search and Exploration",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=txM-TuZawx8"
        }
      },
      "isParallelPresentation": false,
      "importedId": "b11c99f9-d913-4efd-865b-978460e8335d",
      "typeId": 12319,
      "roomId": 10744,
      "chairIds": [
        84742
      ],
      "contentIds": [
        85053,
        85005,
        84999,
        84973,
        85016,
        84967
      ],
      "source": "SYS",
      "timeSlotId": 12500
    },
    {
      "id": 86210,
      "name": "Closing Keynote (Marissa Mayer) & Remarks",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ugIsvjJFHjQ"
        }
      },
      "isParallelPresentation": false,
      "importedId": "618605d4-11a5-4330-9038-d8bc2ca76775",
      "typeId": 12317,
      "roomId": 10761,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12502
    },
    {
      "id": 86420,
      "name": "Demos/SIC",
      "addons": {},
      "isParallelPresentation": true,
      "importedId": "59dc5cc4-7b7c-4a58-9a8a-b2570e51d0dd",
      "typeId": 12315,
      "roomId": 10756,
      "chairIds": [],
      "contentIds": [
        85451,
        85446,
        85499,
        85406,
        85458,
        85436,
        85440,
        85410,
        85456,
        85449,
        85522,
        85479,
        85424,
        85484,
        85412,
        85477,
        85438,
        85468,
        85525,
        85498,
        85405,
        85520,
        85511,
        85411,
        85509,
        85462,
        85515,
        85430,
        85464,
        85402,
        85455,
        85494,
        85453,
        85488,
        85482,
        85419,
        85416,
        85495,
        85437,
        85471,
        85425,
        85450,
        85439,
        85414,
        85461,
        85444,
        85489,
        85445,
        85435,
        85409,
        85447,
        85473,
        85467,
        85517,
        85475,
        85433,
        85503,
        85429,
        85408,
        85508,
        85407,
        85423,
        85510,
        85554,
        85557,
        85556,
        85551,
        85550,
        85553,
        85552,
        85555,
        85549,
        85548
      ],
      "source": "SYS",
      "timeSlotId": 12491
    },
    {
      "id": 86421,
      "name": "Doctoral Symposium (not public)",
      "isParallelPresentation": true,
      "importedId": "feae6106-4179-4d4b-91c3-485d476d8f9b",
      "typeId": 12316,
      "roomId": 10754,
      "chairIds": [],
      "contentIds": [
        85472,
        85466,
        85465,
        85460,
        85459,
        85470,
        85516,
        85505
      ],
      "source": "SYS",
      "timeSlotId": 12472
    },
    {
      "id": 86422,
      "name": "Coffee Break & Poster Session A",
      "addons": {},
      "isParallelPresentation": true,
      "importedId": "e6ef884d-3314-43b9-9483-b92db5488cda",
      "typeId": 12320,
      "roomId": 10753,
      "chairIds": [],
      "contentIds": [
        85514,
        85512,
        85497,
        85513,
        85500,
        85413,
        85404,
        85478,
        85504,
        85457,
        85434,
        85506,
        85491,
        85502,
        85442,
        85496,
        85490,
        85501,
        85474,
        85403,
        85476,
        85426,
        85443,
        85519,
        85427,
        85431,
        85523
      ],
      "source": "SYS",
      "timeSlotId": 12488
    },
    {
      "id": 86423,
      "name": "Coffee Break & Poster Session A",
      "addons": {},
      "isParallelPresentation": true,
      "importedId": "a418959d-fd56-4a4e-873f-2d3598e6557c",
      "typeId": 12320,
      "roomId": 10753,
      "chairIds": [],
      "contentIds": [
        85514,
        85512,
        85497,
        85513,
        85500,
        85413,
        85404,
        85478,
        85504,
        85457,
        85434,
        85506,
        85491,
        85502,
        85442,
        85496,
        85490,
        85501,
        85474,
        85403,
        85476,
        85426,
        85443,
        85519,
        85427,
        85431,
        85523
      ],
      "source": "SYS",
      "timeSlotId": 12501
    },
    {
      "id": 86424,
      "name": "Coffee Break & Poster Session B",
      "addons": {},
      "isParallelPresentation": true,
      "importedId": "621a627f-1346-42b6-9447-f004b3cb7ed9",
      "typeId": 12320,
      "roomId": 10753,
      "chairIds": [],
      "contentIds": [
        85524,
        85481,
        85483,
        85518,
        85469,
        85432,
        85492,
        85487,
        85422,
        85507,
        85415,
        85420,
        85486,
        85441,
        85480,
        85452,
        85417,
        85521,
        85463,
        85454,
        85421,
        85448,
        85418,
        85493,
        85428,
        85401,
        86427
      ],
      "source": "SYS",
      "timeSlotId": 12490
    },
    {
      "id": 86425,
      "name": "Coffee Break & Poster Session B",
      "addons": {},
      "isParallelPresentation": true,
      "importedId": "3a32cc65-1238-4afe-97b3-0ba06cab1bcc",
      "typeId": 12320,
      "roomId": 10753,
      "chairIds": [],
      "contentIds": [
        85524,
        85481,
        85483,
        85518,
        85469,
        85432,
        85492,
        85487,
        85422,
        85507,
        85415,
        85420,
        85486,
        85441,
        85480,
        85452,
        85417,
        85521,
        85463,
        85454,
        85421,
        85448,
        85418,
        85493,
        85428,
        85401,
        85485
      ],
      "source": "SYS",
      "timeSlotId": 12496
    },
    {
      "id": 86426,
      "name": "Coffee Break & Poster Session C",
      "addons": {},
      "isParallelPresentation": true,
      "importedId": "24c07e0a-b504-406b-a7b4-4e74fe18485c",
      "typeId": 12320,
      "roomId": 10753,
      "chairIds": [],
      "contentIds": [
        86331,
        86329,
        86340,
        86339,
        86337,
        86335,
        86328,
        86327,
        86326,
        86325,
        86322,
        86324,
        86323,
        86334,
        86333,
        86336,
        86343,
        86342,
        86345,
        86344,
        86347,
        86346,
        89933,
        89934,
        89914,
        89915,
        89913,
        89916,
        89918,
        89917,
        90850,
        90852,
        90851
      ],
      "source": "SYS",
      "timeSlotId": 12492
    },
    {
      "id": 90637,
      "name": "Vision: Barbara Tversky",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_9xrcPXkMMU"
        },
        "vision": {
          "type": "customFile",
          "url": "https://uist.acm.org/uist2022/assets/files/uist22-vision-Barbara-Tversky.pdf"
        }
      },
      "isParallelPresentation": false,
      "importedId": "67520928-f0ee-4da3-a966-6d6eec32634e",
      "typeId": 12339,
      "roomId": 10760,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12493
    },
    {
      "id": 90638,
      "name": "Vision: Jaime Teevan",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lipwOvkvrsM"
        },
        "vision": {
          "type": "customFile",
          "url": "https://uist.acm.org/uist2022/assets/files/uist22-vision-Jaime-Teevan.pdf"
        }
      },
      "isParallelPresentation": false,
      "importedId": "65eba97a-eeb7-4d06-8113-01b57d314d69",
      "typeId": 12339,
      "roomId": 10760,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12498
    },
    {
      "id": 90639,
      "name": "Opening Remarks & Keynote (Ted Chiang)",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VkC54ZNO_HU"
        }
      },
      "isParallelPresentation": false,
      "importedId": "374bd18b-bb8c-461a-b150-8d19e9798793",
      "typeId": 12317,
      "roomId": 10761,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12504
    }
  ],
  "events": [
    {
      "id": 85633,
      "name": "Welcome Reception",
      "isParallelPresentation": false,
      "importedId": "1203d531-088d-40cf-ad28-11b597f958bc",
      "typeId": 12317,
      "roomId": 10760,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667156400000,
      "endDate": 1667167200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 85634,
      "name": "Continental Breakfast",
      "isParallelPresentation": false,
      "importedId": "7f1fbbfb-2371-4636-bac0-58c658285b93",
      "typeId": 12317,
      "roomId": 10755,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667203200000,
      "endDate": 1667206800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86207,
      "name": "Conference Banquet in Sunriver",
      "isParallelPresentation": false,
      "importedId": "36bb772b-ab9a-476e-925b-dcc0df0a096b",
      "typeId": 12317,
      "roomId": 10758,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667325600000,
      "endDate": 1667332800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86208,
      "name": "Buses Return from Sunriver",
      "isParallelPresentation": false,
      "importedId": "29c1b2d2-1d36-451d-a798-25041acfb355",
      "typeId": 12317,
      "roomId": 10758,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667332800000,
      "endDate": 1667336400000,
      "description": "Buses depart Sunriver every 30 minutes from 8:00pm-9:00pm. ~20 min. bus ride (please mask up!)",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86209,
      "name": "Buses Depart for Sunriver",
      "isParallelPresentation": false,
      "importedId": "d5a8dfb7-da30-4f7e-b009-50e8a381e05e",
      "typeId": 12317,
      "roomId": 10759,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667318400000,
      "endDate": 1667325600000,
      "description": "Buses depart hotel every 30 minutes from 4:00pm-5:30pm. ~20 min. bus ride (please mask up!)",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86211,
      "name": "Registration Opens",
      "isParallelPresentation": false,
      "importedId": "73b1bab8-2497-47eb-b7c2-792e9fe5eb80",
      "typeId": 12317,
      "roomId": 10755,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667203200000,
      "endDate": 1667206800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86213,
      "name": "Registration Opens",
      "isParallelPresentation": false,
      "importedId": "b02bc389-b611-4f15-be61-ddecf3e69ebf",
      "typeId": 12317,
      "roomId": 10755,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667289600000,
      "endDate": 1667293200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86214,
      "name": "Registration Opens",
      "isParallelPresentation": false,
      "importedId": "ba7c82e8-98e1-4d3a-b3c0-2952766e4c66",
      "typeId": 12317,
      "roomId": 10755,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667376000000,
      "endDate": 1667379600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86215,
      "name": "Continental Breakfast",
      "isParallelPresentation": false,
      "importedId": "dcfbe920-e3c0-433a-b2de-fd5e68fca431",
      "typeId": 12317,
      "roomId": 10755,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667289600000,
      "endDate": 1667293200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86216,
      "name": "Continental Breakfast",
      "isParallelPresentation": false,
      "importedId": "262676fd-73b5-4be6-a2e6-403f20ae7d3b",
      "typeId": 12317,
      "roomId": 10755,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667376000000,
      "endDate": 1667379600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86217,
      "name": "Registration Opens",
      "isParallelPresentation": false,
      "importedId": "c8704d73-070f-43bb-9429-ac22596faa69",
      "typeId": 12317,
      "roomId": 10755,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667149200000,
      "endDate": 1667156400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 86219,
      "name": "Doctoral Symposium Dinner",
      "isParallelPresentation": false,
      "importedId": "4954d623-225b-4c4c-9fb4-23eecb0849b9",
      "typeId": 12317,
      "roomId": 10757,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667068200000,
      "endDate": 1667077200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 90640,
      "name": "AMA Session by Lining Yao on \"Doing antidisciplinary research\"",
      "isParallelPresentation": false,
      "importedId": "6b802312-288c-4db2-abad-8b74437b2855",
      "typeId": 12317,
      "roomId": 10744,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667309400000,
      "endDate": 1667313000000,
      "presenterIds": [
        90642
      ],
      "source": "SYS"
    },
    {
      "id": 90645,
      "name": "AMA Session by Pedro Lopes on \"Being a new faculty/starting a new lab\"",
      "isParallelPresentation": false,
      "importedId": "ba7c2243-fadb-41ec-85d0-4631687ab1ac",
      "typeId": 12317,
      "roomId": 10744,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1667395800000,
      "endDate": 1667399400000,
      "presenterIds": [
        84662
      ],
      "source": "SYS"
    }
  ],
  "contents": [
    {
      "id": 84963,
      "typeId": 12319,
      "title": "ELAXO : Rendering Versatile Resistive Force Feedback for Fingers Grasping and Twisting",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937642097848411"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545677"
        },
        "Preview": {
          "duration": "30",
          "title": "ELAXO : Rendering Versatile Resistive Force Feedback for Fingers Grasping and Twisting",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=TlyaZPgD390"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2537",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86198
      ],
      "eventIds": [],
      "abstract": "Haptic feedback not only enhances immersion in virtual reality (VR) but also delivers experts’ haptic sensation tips in VR training, e.g., properly clamping a tenon and mortise joint or tightening a screw in the assembly of VR factory training, which could even improve the training performance. However, various and complicated manipulation is in different scenarios. Although haptic feedback of virtual objects’ shape, stiffness or resistive force in pressing or grasping is achieved by previous research, rotational resistive force when twisting or turning virtual objects is seldom discussed or explored, especially for a wearable device. Therefore, we propose a wearable device, ELAXO, to integrate continuous resistive force and continuous rotational resistive force with or without resilience in grasping and twisting, respectively. ELAXO is an exoskeleton with rings, mechanical brakes and elastic bands. The brakes achieve shape rendering and switch between with and without resilience modes for the resistive force. The detachable and rotatable rings and elastic bands render continuous resistive force in grasping and twisting. We conducted a just noticeable difference (JND) study to understand users’ distinguishability in the four conditions, resistive force and rotational resistive force with and without resilience, separately. A VR study was then performed to verify that the versatile resistive force feedback from ELAXO enhances the VR experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei City",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 84918
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 84633
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 84844
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 84859
        }
      ]
    },
    {
      "id": 84964,
      "typeId": 12319,
      "title": "Diffscriber: Describing Visual Design Changes to Support Mixed-Ability Collaborative Presentation Authoring",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937645331660911"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545637"
        },
        "Preview": {
          "duration": "30",
          "title": "Diffscriber: Describing Visual Design Changes to Support Mixed-Ability Collaborative Presentation Authoring",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=zzgvQLxm5-U"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1562",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86193
      ],
      "eventIds": [],
      "abstract": "Visual slide-based presentations are ubiquitous, yet slide authoring tools are largely inaccessible to people who are blind or visually impaired (BVI). When authoring presentations, the 9 BVI presenters in our formative study usually work with sighted collaborators to produce visual slides based on the text content they produce. While BVI presenters valued collaborators’ visual design skills, the collaborators often felt they could not fully review and provide feedback on the visual changes that were made. We present Diffscriber, a system that identifies and describes changes to a slide’s content, layout, and style for presentation authoring. Using our system, BVI presentation authors can efficiently review changes to their presentation by navigating either a summary of high-level changes or individual slide elements. To learn more about changes of interest, presenters can use a generated change hierarchy to navigate to lower-level change details and element styles. BVI presenters using Diffscriber were able to identify slide design changes and provide feedback more easily as compared to using only the slides alone. More broadly, Diffscriber illustrates how advances in detecting and describing visual differences can improve mixed-ability collaboration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": " Human-Computer Interaction Institute"
            }
          ],
          "personId": 84655
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84842
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84952
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas, Austin",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84742
        }
      ]
    },
    {
      "id": 84965,
      "typeId": 12319,
      "title": "INTENT: Interactive Tensor Transformation Synthesis",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937648464810084"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545653"
        },
        "Preview": {
          "duration": "30",
          "title": "INTENT: Interactive Tensor Transformation Synthesis",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=qlyYHM8qxew"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-7305",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86205
      ],
      "eventIds": [],
      "abstract": "There is a growing interest in adopting Deep Learning (DL) given its superior performance in many domains. However, modern DL frameworks such as TensorFlow often come with a steep learning curve. In this work, we propose INTENT, an interactive system that infers user intent and generates corresponding TensorFlow code on behalf of users. INTENT helps users understand and validate the semantics of generated code by rendering individual tensor transformation steps with intermediate results and element-wise data provenance. Users can further guide INTENT by marking certain TensorFlow operators as desired or undesired, or directly manipulating the generated code. A within-subjects user study with 18 participants shows that users can finish programming tasks in TensorFlow more successfully with only half the time, compared with a variant of INTENT that has no interaction or visualization support.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 84808
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84690
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 84897
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84628
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 84533
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84941
        }
      ]
    },
    {
      "id": 84966,
      "typeId": 12319,
      "title": "MuscleRehab: Improving Unsupervised Physical Rehabilitation by Monitoring and Visualizing Muscle Engagement ",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937651962855444"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545705"
        },
        "Preview": {
          "duration": "30",
          "title": "MuscleRehab: Improving Unsupervised Physical Rehabilitation by Monitoring and Visualizing Muscle Engagement",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=F3qrHOW3MRI"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8756",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86193
      ],
      "eventIds": [],
      "abstract": "Unsupervised physical rehabilitation traditionally has used motion tracking to determine correct exercise execution. However, motion tracking is not representative of the assessment of physical therapists, which focus on muscle engagement. In this paper, we investigate if monitoring and visualizing muscle engagement during unsupervised physical rehabilitation improves the execution accuracy of therapeutic exercises by showing users whether they target the right muscle groups. To accomplish this, we use wearable electrical impedance tomography (EIT) to monitor the muscle engagement and visualize the current state on a virtual muscle-skeleton avatar. We use additional optical motion tracking to also monitor the user's movement. We run a user study with 10 participants that compares exercise execution while seeing muscle + motion data vs. motion data only, and also present the recorded data to a group of physical therapists for post-rehabilitation analysis. The results indicate that monitoring and visualizing muscle engagement can improve both the therapeutic exercise accuracy for users during rehabilitation, and post-rehabilitation evaluation for physical therapists.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 84696
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84565
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84532
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts General Hospital",
              "dsl": ""
            }
          ],
          "personId": 84542
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts General Hospital ",
              "dsl": ""
            }
          ],
          "personId": 84934
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 84797
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 84967,
      "typeId": 12319,
      "title": "Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries.",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937655003717664"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545672"
        },
        "Preview": {
          "duration": "29",
          "title": "Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries.",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wRSBdha1I9E"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6580",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86206
      ],
      "eventIds": [],
      "abstract": "We propose a text editor to help users plan, structure and reflect on their writing process. It provides continuously updated paragraph-wise summaries as margin annotations, using automatic text summarization. Summary levels range from full text, to selected (central) sentences, down to a collection of keywords. To understand how users interact with this system during writing, we conducted two user studies (N=4 and N=8) in which people wrote analytic essays about a given topic and article. As a key finding, the summaries gave users an external perspective on their writing and helped them to revise the content and scope of their drafted paragraphs. People further used the tool to quickly gain an overview of the text and developed strategies to integrate insights from the automated summaries. More broadly, this work explores and highlights the value of designing AI tools for writers, with Natural Language Processing (NLP) capabilities that go beyond direct text generation and correction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84916
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84750
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84855
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84910
        }
      ]
    },
    {
      "id": 84968,
      "typeId": 12319,
      "title": "Muscle Synergies Learning with Electrical Muscle Stimulation for Playing the Piano",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937658107494410"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545666"
        },
        "Preview": {
          "duration": "31",
          "title": "Muscle Synergies Learning with Electrical Muscle Stimulation for Playing the Piano",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7h2JzBmoiGk"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8640",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86198
      ],
      "eventIds": [],
      "abstract": "When playing scales on the piano, playing all notes evenly is a basic technique to improve the quality of music. However, it is difficult for beginners to do this because they need to achieve appropriate muscle synergies of the forearm and shoulder muscles, i.e., pressing keys as well as sliding their hands sideways. In this paper, we propose a system using electrical muscle stimulation (EMS) to teach beginners how to improve their muscle synergies while playing scales. We focus on ``thumb-under'' method and assist with it by applying EMS to the deltoid muscle. We conducted a user study to investigate whether our EMS-based system can help beginners learn new muscle synergies in playing ascending scales. We divided the participants into two groups: an experimental group that practiced with EMS and a control group that practiced without EMS. The results showed that practicing with EMS was more effective in improving the evenness of scales than without EMS and that the muscle synergies changed after practicing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 84866
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 84688
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokosuka-shi Kanagawa",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 84693
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 84811
        }
      ]
    },
    {
      "id": 84969,
      "typeId": 12319,
      "title": "AirLogic: Embedding Pneumatic Computation and I/O in 3D Models to Fabricate Electronics-Free Interactive Objects",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937660577939567"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545642"
        },
        "Preview": {
          "duration": "30",
          "title": "AirLogic: Embedding Pneumatic Computation and I/O in 3D Models to Fabricate Electronics-Free Interactive Objects",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vGndUaMWDR0"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6980",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85637
      ],
      "eventIds": [],
      "abstract": "Researchers have developed various tools and techniques towards the vision of on-demand fabrication of custom, interactive devices. Recent work has 3D-printed artefacts like speakers, electromagnetic actuators, and hydraulic robots. However, these are non-trivial to instantiate as they require post-fabrication mechanical-- or electronic assembly. We introduce AirLogic: a technique to create electronics-free, interactive objects by embedding pneumatic input, logic processing, and output widgets in 3D-printable models. AirLogic devices can perform basic computation on user inputs and create visible, audible, or haptic feedback; yet they do not require electronic circuits, physical assembly, or resetting between uses. Our library of 13 exemplar widgets can embed \\al-style computational capabilities in existing 3D models. We evaluate our widgets' performance---quantifying the loss of airflow (1) in each widget type, (2) based on printing orientation, and (3) from internal object geometry. Finally, we present five applications that illustrate AirLogic's potential.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": ""
            }
          ],
          "personId": 84912
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "NetCompany",
              "dsl": ""
            }
          ],
          "personId": 84896
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University ",
              "dsl": "Department of Information Technology "
            }
          ],
          "personId": 84593
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Hasselt",
              "institution": "Flanders Make - Expertise Centre for Digital Media",
              "dsl": "Hasselt University "
            }
          ],
          "personId": 84731
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84666
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "University of Birmingham",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": ""
            }
          ],
          "personId": 84840
        }
      ]
    },
    {
      "id": 84970,
      "typeId": 12319,
      "title": "PassengXR: A Low Cost Platform for Any-Car, Multi-User, Motion-Based Passenger XR Experiences",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937664076009492"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545657"
        },
        "Preview": {
          "duration": "30",
          "title": "PassengXR: A Low Cost Platform for Any-Car, Multi-User, Motion-Based Passenger XR Experiences",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=F_qSL2M9N6s"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-9214",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85636
      ],
      "eventIds": [],
      "abstract": "We present PassengXR, an open-source toolkit for creating passenger eXtended Reality (XR) experiences in Unity. XR allows travellers to move beyond the physical limitations of in-vehicle displays, rendering immersive virtual content based on - or ignoring - vehicle motion. There are considerable technical challenges to using headsets in moving environments: maintaining the forward bearing of IMU-based headsets; conflicts between optical and inertial tracking of inside-out headsets; obtaining vehicle telemetry; and the high cost of design given the necessity of testing in-car. As a consequence, existing vehicular XR research typically relies on controlled, simple routes to compensate. PassengXR is a cost-effective open-source in-car passenger XR solution. We provide a reference set of COTS hardware that enables the broadcasting of vehicle telemetry to multiple headsets. Our software toolkit then provides support to correct vehicle-headset alignment, and then create a variety of passenger XR experiences, including: vehicle-locked content; motion- and location-based content; and co-located multi-passenger applications. PassengXR also supports the recording and playback of vehicle telemetry, assisting offline design  without resorting to costly in-car testing. Through an evaluation-by-demonstration, we show how our platform can assist practitioners in producing novel, multi-user passenger XR experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 84758
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 84921
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 84935
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "glasgow",
              "city": "glasgow",
              "institution": "University of Glasgow",
              "dsl": "Computing Science"
            }
          ],
          "personId": 84951
        }
      ]
    },
    {
      "id": 84971,
      "typeId": 12319,
      "title": "MagneShape: A Non-electrical Pin-Based Shape-Changing Display ",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937666798112870"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545645"
        },
        "Preview": {
          "duration": "30",
          "title": "MagneShape: A Non-electrical Pin-Based Shape-Changing Display",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=c4ewnWFlQAU"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6345",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86201
      ],
      "eventIds": [],
      "abstract": "Pin-based shape-changing displays can present dynamic shape changes by actuating a number of pins. However, the use of many linear actuators to achieve this makes the electrical structure and mechanical construction of the display complicated. We propose a simple pin-based shape-changing display that outputs shape and motions without any electronic elements. Our display consists of magnetic pins in a pin housing, with a magnetic sheet underneath it. The magnetic sheet has a specific magnetic pattern on its surface, and each magnetic pin has a magnet at its lower end. The repulsive force generated between the magnetic sheet and the magnetic pin levitates the pin vertically, and the height of the pin-top varies depending on the magnetic pattern. \r\nThis paper introduces the basic structure of the display and compares several fabrication methods for the magnetic pins, to highlight the applicability of this method. We have also demonstrated some applications and discussed future possibilities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Atsugi",
              "institution": "Nippon Telegraph and Telephone Corporation",
              "dsl": "NTT Communication Science Laboratories"
            }
          ],
          "personId": 84886
        }
      ]
    },
    {
      "id": 84972,
      "typeId": 12319,
      "title": "Sketch-Based Design of Foundation Paper Pieceable Quilts",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937669893488700"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545643"
        },
        "Preview": {
          "duration": "30",
          "title": "Sketch-Based Design of Foundation Paper Pieceable Quilts",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=AVRlYyi4n2A"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-9296",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85639
      ],
      "eventIds": [],
      "abstract": "Foundation paper piecing is a widely used quilt-making technique in which fabric pieces are sewn onto a paper guide to facilitate construction. But, designing paper pieceable quilt patterns is challenging because the sewing process imposes constraints on both the geometry and sewing order of the fabric pieces. Based on a formative study with expert quilt designers, we develop a novel sketch-based tool for designing such quilt patterns. Our tool lets designers sketch a partial design as a set of edges, which may intersect but do not have to form closed polygons, and our tool automatically completes it into a fully paper pieceable pattern. We contribute a new sketch-completion algorithm that extends the input sketched edges into a planar mesh composed of closed polygonal faces representing fabric pieces, determines a paper pieceable sewing order for the faces, and breaks complicated sketches into independently paper pieceable sections when necessary. A partial input design often admits multiple visually different completions. Thus, our tool lets designers specify completion heuristics, which are based on current quilt design practices, to control the appearance of the completed quilt. Initial user evaluations with novice and expert quilt designers suggest that our tool fits within current design workflows and greatly facilitates designing foundation paper pieceable quilts by allowing users to focus on the visual design rather than tedious constraint checks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84695
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": ""
            }
          ],
          "personId": 84815
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 84942
        }
      ]
    },
    {
      "id": 84973,
      "typeId": 12319,
      "title": "Scrapbook: Screenshot-Based Bookmarks for Effective Digital Resource Curation across Applications",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937672951156768"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545678"
        },
        "Preview": {
          "duration": "30",
          "title": "Scrapbook: Screenshot-Based Bookmarks for Effective Digital Resource Curation across Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7SWa1TpInaQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4209",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86206
      ],
      "eventIds": [],
      "abstract": "Modern knowledge workers typically need to use multiple resources, such as documents, web pages, and applications, at the same time. This complexity in their computing environments forces workers to restore various resources in the course of their work. \r\nHowever, conventional curation methods like bookmarks, recent document histories, and file systems place limitations on effective retrieval. Such features typically work only for resources of one type within one application, ignoring the interdependency between resources needed for a single task. In addition, text-based handles do not provide rich cues for users to recognize their associated resources. Hence, the need to locate and reopen relevant resources can significantly hinder knowledge workers' productivity. \r\nTo address these issues, we designed and developed Scrapbook, a novel application for digital resource curation across applications that uses screenshot-based bookmarks. Scrapbook extracts and stores all the metadata (URL, file location, and application name) of windows visible in a captured screenshot to facilitate restoring them later. A week-long field study indicated that screenshot-based bookmarks helped participants curate digital resources. Additionally, participants reported that multimodal---visual and textual---data helped them recall past computer activities and reconstruct working contexts efficiently.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84902
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 84885
        }
      ]
    },
    {
      "id": 84974,
      "typeId": 12319,
      "title": "iWood: Makeable Vibration Sensor for Interactive Plywood ",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937676008800366"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545640"
        },
        "Preview": {
          "duration": "29",
          "title": "iWood: Makeable Vibration Sensor for Interactive Plywood",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=w5p8WkBxU98"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4726",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85637
      ],
      "eventIds": [],
      "abstract": "iWood is interactive plywood that can sense vibration based on triboelectric effect. As a material, iWood survives common woodworking operations, such as sawing, screwing, and nailing and can be used to create furniture and artifacts. Things created using iWood inherit its sensing capability and can detect a variety of user input and activities based on their unique vibration patterns. Through a series of experiments and machine simulations, we carefully chose the size of the sensor electrodes, the type of triboelectric materials, and the bonding method of the sensor layers to optimize the sensitivity and fabrication complexity. The sensing performance of iWood was evaluated with 4 gestures and 12 daily activities carried out on a table, nightstand, and cutting board, all created using iWood. Our result suggested over 90% accuracies for activity and gesture recognition.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84760
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 84608
        }
      ]
    },
    {
      "id": 84975,
      "typeId": 12319,
      "title": "Kinergy: Creating 3D Printable Motion using Embedded Kinetic Energy",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937678990942289"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545636"
        },
        "Preview": {
          "duration": "30",
          "title": "Kinergy: Creating 3D Printable Motion using Embedded Kinetic Energy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=keeWX802S1E"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2827",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86201
      ],
      "eventIds": [],
      "abstract": "We present Kinergy—an interactive design tool for creating self-propelled motion by harnessing the energy stored in 3D printable springs. To produce controllable output motions, we introduce 3D printable kinetic units, a set of parameterizable designs that encapsulate 3D printable springs, compliant locks, and transmission mechanisms for three non-periodic motions—instant translation, instant rotation, continuous translation—and four periodic motions—continuous rotation, reciprocation, oscillation, intermittent rotation. Kinergy allows the user to create motion-enabled 3D models by embedding kinetic units, customize output motion characteristics by parameterizing embedded springs and kinematic elements, control energy by operating the specialized lock, and preview the resulting motion in an interactive environment. We demonstrate the potential of our techniques via example applications from spring-loaded cars to kinetic sculptures and close with a discussion of key challenges such as geometric constraints.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 84746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 84779
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84881
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 84816
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 84817
        }
      ]
    },
    {
      "id": 84976,
      "typeId": 12319,
      "title": "CodeToon: Story Ideation, Auto Comic Generation, and Structure Mapping for Code-Driven Storytelling",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937681780162590"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545617"
        },
        "Preview": {
          "duration": "30",
          "title": "CodeToon: Story Ideation, Auto Comic Generation, and Structure Mapping for Code-Driven Storytelling",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=S0O-3dbmlYQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1211",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85638
      ],
      "eventIds": [],
      "abstract": "Recent work demonstrated how we can design and use coding strips, a form of comic strips with corresponding code, to enhance the teaching and learning in programming. However, creating coding strips is a creative, time-consuming process. Creators have to generate stories from code (code -> story) and design comics from stories (story -> comic). We contribute CodeToon, a comic authoring tool that facilitates this code-driven storytelling process with two mechanisms: (1) story ideation from code using metaphor and (2) automatic comic generation from the story. We conducted a two-part user study that evaluates the tool and participants’ generated comics to test whether CodeToon facilitates the authoring process and helps generate quality comics. Our results show that CodeToon helps users create accurate, informative, and useful coding strips in a significantly shorter time. Overall, this work contributes methods and design guidelines for code-driven storytelling and opens up new opportunities for using art to support computer science education.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 84825
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 84564
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            }
          ],
          "personId": 84748
        }
      ]
    },
    {
      "id": 84977,
      "typeId": 12319,
      "title": "Photographic Lighting Design with Photographer-in-the-Loop Bayesian Optimization",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937685882179705"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545690"
        },
        "Preview": {
          "duration": "30",
          "title": "Photographic Lighting Design with Photographer-in-the-Loop Bayesian Optimization",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=VM_JvPeTHbk"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1573",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86205
      ],
      "eventIds": [],
      "abstract": "It is important for photographers to have the best possible lighting configuration at the time of shooting; otherwise, they need post-processing on images, which may cause artifacts and deterioration. Thus, photographers often struggle to find the best possible lighting configuration by manipulating lighting devices, including light sources and modifiers, in a trial-and-error manner. In this paper, we propose a novel computational framework to support photographers. This framework assumes that every lighting device is programmable; that is, its adjustable parameters (e.g., orientation, intensity, and color temperature) can be set using a program. Using our framework, photographers do not need to learn how the parameter values affect the resulting lighting, and even do not need to determine the strategy of the trial-and-error process; instead, photographers need only concentrate on evaluating which lighting configuration is more desirable among options suggested by the system. The framework is enabled by our novel photographer-in-the-loop Bayesian optimization, which is sample-efficient (i.e., the number of required evaluation steps is small) and which can also be guided by providing a rough painting of the desired lighting configuration if any. We demonstrate how the framework works in both simulated virtual environments and a physical environment, suggesting that it could find pleasing lighting configurations quickly in around 10 iterations. Our user study suggests that the framework enables the photographer to concentrate on the look of captured images rather than the parameters, compared with the traditional manual lighting workflow.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Digital Nature Group"
            }
          ],
          "personId": 84572
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 84713
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 84814
        }
      ]
    },
    {
      "id": 84978,
      "typeId": 12319,
      "title": "Computational Design of Active Kinesthetic Garments",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937688784642048"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545674"
        },
        "Preview": {
          "duration": "30",
          "title": "Computational Design of Active Kinesthetic Garments",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7QKA9_VPRrc"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3511",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85639
      ],
      "eventIds": [],
      "abstract": "Garments with the ability to provide kinesthetic force-feedback on-demand can augment human capabilities in a non-obtrusive way, enabling numerous applications in VR haptics, motion assistance, and robotic control. However, designing such garments is a complex, and often manual task, particularly when the goal is to resist multiple motions with a single design. In this work, we propose a computational pipeline for designing connecting structures between active components---one of the central challenges in this context. We focus on electrostatic (ES) clutches that are compliant in their passive state while strongly resisting elongation when activated. Our method automatically computes optimized connecting structures that efficiently resist a range of pre-defined body motions on demand. We propose a novel dual-objective optimization approach to simultaneously maximize the resistance to motion when clutches are active, while minimizing resistance when inactive. We demonstrate our method on a set of problems involving different body sites and a range of motions. We further fabricate and evaluate a subset of our automatically created designs against manually created baselines using mechanical testing and in a VR pointing study. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Advanced Interactive Technologies Lab"
            }
          ],
          "personId": 84624
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Zurich",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "CRL"
            }
          ],
          "personId": 84641
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH",
              "dsl": ""
            }
          ],
          "personId": 84607
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Zurich",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "CRL"
            }
          ],
          "personId": 84536
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Advanced Interactive Technologies Lab"
            }
          ],
          "personId": 84711
        }
      ]
    },
    {
      "id": 84979,
      "typeId": 12319,
      "title": "Flaticulation: Laser Cutting Joints with Articulated Angles",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937691095715870"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545695"
        },
        "Preview": {
          "duration": "30",
          "title": "Flaticulation: Laser Cutting Joints with Articulated Angles",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Lv5tjHJSRvw"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4967",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85637
      ],
      "eventIds": [],
      "abstract": "We present Flaticulation, a method to laser cut joints that clutch two cut-in-place flat boards at designated articulated angles. We discover special T-patterns added on the shared edge of two pieces allowing them to be clutched at a bending angle. We analyze the structure and propose a parametric model regarding the T-pattern under laser cutting to predict the joint articulated angle. We validate our proposed model by measuring real prototypes and conducting stress-strain analysis to understand their structural strength. Finally, we provide a user interface for our example applications, including fast assembling unfolded 3D polygonal models and adding detent mechanisms for functional objects such as a mouse and reconfigurable objects such as a headphone.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84834
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84848
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84856
        }
      ]
    },
    {
      "id": 84980,
      "typeId": 12319,
      "title": "HapTag: A Compact Actuator for Rendering Push-Button Tactility on Soft Surfaces",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937693406756874"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545644"
        },
        "Preview": {
          "duration": "30",
          "title": "HapTag: A Compact Actuator for Rendering Push-Button Tactility on Soft Surfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=p0fEfbc0ho8"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2029",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86201
      ],
      "eventIds": [],
      "abstract": "As touch interactions become ubiquitous in the field of human computer interactions, it is critical to enrich haptic feedback to improve efficiency, accuracy and immersive experiences. This paper presents HapTag, a thin and flexible actuator to support integration of push button tactile renderings to daily soft surfaces. Specifically, HapTag works under the principle of hydraulically amplified electroactive actuator (HASEL) while being optimized by embedding a pressure sensing layer, and being activated with dedicated voltage appliance in response to users' input actions, resulting in fast response time, controllable and expressive push-button tactile rendering capabilities. HapTag is in compact formfactor, and can be attached, integrated, or embedded on various soft surfaces like cloth, leather and rubber. Three common push button tactile patterns were adopted and implemented with HapTag. We validated the feasibility and expressiveness of HapTag by demonstrating a series of innovative applications under different circumstances.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 84609
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xi'an",
              "institution": "Xi'an jiaotong university",
              "dsl": ""
            }
          ],
          "personId": 84721
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": ""
            }
          ],
          "personId": 84898
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "University of Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 84724
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology ",
              "dsl": ""
            }
          ],
          "personId": 84868
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "University of Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 84826
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Chinese Academy of Sciences",
              "dsl": "Institute of Software"
            }
          ],
          "personId": 84823
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 84590
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "School of mechanical engineering",
              "dsl": "Xi'an Jiaotong University"
            }
          ],
          "personId": 84552
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": ""
            }
          ],
          "personId": 84813
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 84589
        }
      ]
    },
    {
      "id": 84981,
      "typeId": 12319,
      "title": "Prototyping Soft Devices with Interactive Bioplastics",
      "award": "BEST_PAPER",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937696724455488"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545623"
        },
        "Preview": {
          "duration": "30",
          "title": "Prototyping Soft Devices with Interactive Bioplastics",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=m05e7F_aufo"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3911",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85639
      ],
      "eventIds": [],
      "abstract": "Designers and makers are increasingly interested in leveraging bio-based and bio-degradable 'do-it-yourself' (DIY) materials for sustainable prototyping.\r\nTheir self-produced bioplastics possess compelling properties such as self-adhesion but have so far not been functionalized to create soft interactive devices, due to a lack of DIY techniques for the fabrication of functional electronic circuits and sensors.\r\nIn this paper, we contribute a DIY approach for creating Interactive Bioplastics that is accessible to a wide audience, making use of easy-to-obtain bio-based raw materials and familiar tools. We present three types of conductive bioplastic materials and their formulation: sheets, pastes and foams.\r\nOur materials enable additive and subtractive fabrication of soft circuits and sensors.\r\nFurthermore, we demonstrate how these materials can substitute conventional prototyping materials, be combined with off-the-shelf electronics, and be fed into a sustainable material `life-cycle' including disassembly, re-use, and re-melting of materials. A formal characterization of our conductors highlights that they are even on-par with commercially available carbon-based conductive pastes. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": ""
            }
          ],
          "personId": 84955
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Pôle Universitaire Léonard de Vinici, De Vinci Innovation Center",
              "dsl": ""
            }
          ],
          "personId": 84670
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84700
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "De Vinci Research Center",
              "dsl": "Pôle Universitaire Léonard de Vinci"
            }
          ],
          "personId": 84759
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 84664
        }
      ]
    },
    {
      "id": 84982,
      "typeId": 12319,
      "title": "SleepGuru: Personalized Sleep Planning System for Real-life Actionability and Negotiability",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937700172189696"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545709"
        },
        "Preview": {
          "duration": "29",
          "title": "SleepGuru: Personalized Sleep Planning System for Real-life Actionability and Negotiability",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=oV5Ka-zxfbI"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-5020",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86198
      ],
      "eventIds": [],
      "abstract": "Widely-accepted sleep guidelines advise regular bedtimes and sleep hygiene. An individual's adherence is often viewed as a matter of self-regulation and anti-procrastination. We pose a question from a different perspective: What if it comes to a matter of one's social or professional duty that mandates irregular daily life, making it incompatible with the premise of standard guidelines? We propose SleepGuru, an individually actionable sleep planning system featuring one's real-life compatibility and extended forecast. Adopting theories on sleep physiology, SleepGuru builds a personalized predictor on the progression of the user's sleep pressure over a course of upcoming schedules and past activities sourced from her online calendar and wearable fitness tracker. Then, SleepGuru service provides individually actionable multi-day sleep schedules which respect the user's inevitable real-life irregularities while regulating her week-long sleep pressure. We elaborate on the underlying physiological principles and mathematical models, followed by 3-stage study and deployment. \r\nWe develop a mobile user interface providing individual predictions and adjustability backed by cloud-side optimization. We deploy SleepGuru in-the-wild to 20 users for 8 weeks, where we found positive effects of SleepGuru in sleep quality, compliance rate, sleep efficiency, alertness, long-term followability, and so on.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84911
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84852
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84904
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84901
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 84770
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84909
        }
      ]
    },
    {
      "id": 84983,
      "typeId": 12319,
      "title": "Scholastic: Graphical Human-AI Collaboration for Inductive and Interpretive Text Analysis",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937703636684861"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545681"
        },
        "Preview": {
          "duration": "30",
          "title": "Scholastic: Graphical Human-AI Collaboration for Inductive and Interpretive Text Analysis",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vqOtS-AeLbE"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6474",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86193
      ],
      "eventIds": [],
      "abstract": "Interpretive scholars generate knowledge from text corpora by manually sampling documents, applying codes, and refining and collating codes into categories until meaningful themes emerge. Given a large corpus, machine learning could help scale this data sampling and analysis, but prior research shows that experts are generally concerned about algorithms potentially disrupting or driving interpretive scholarship. We take a human-centered design approach to addressing concerns around machine-assisted interpretive research to build Scholastic, which incorporates a machine-in-the-loop clustering algorithm to scaffold interpretive text analysis. As a scholar applies codes to documents and refines them, the resulting coding schema serves as structured metadata which constrains hierarchical document and word clusters inferred from the corpus. Interactive visualizations of these clusters can help scholars strategically sample documents further toward insights. Scholastic demonstrates how human-centered algorithm design and visualizations employing familiar metaphors can support inductive and interpretive research methodologies through interactive topic modeling and document clustering.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 84867
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Applied Mathematics"
            }
          ],
          "personId": 84957
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Information Science"
            }
          ],
          "personId": 84837
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "CU Boulder",
              "dsl": "Department of Information Science"
            }
          ],
          "personId": 84586
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Information Science"
            }
          ],
          "personId": 84534
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina-Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84913
        }
      ]
    },
    {
      "id": 84984,
      "typeId": 12319,
      "title": "interiqr: Unobtrusive Edible Tags using Food 3D Printing",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937707545763891"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545669"
        },
        "Preview": {
          "duration": "30",
          "title": "interiqr: Unobtrusive Edible Tags using Food 3D Printing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Re4_AB8CLUU"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1180",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86204
      ],
      "eventIds": [],
      "abstract": "We present interiqr, a method that utilizes the infill parameter in the 3D printing process to embed information inside the food that is difficult to recognize with the human eye. Our key idea is to utilize the air space or secondary materials to generate a specific pattern inside the food without changing the model geometry. As a result, our method exploits the patterns that appear as hidden edible tags to store the data and simultaneously adds them to a 3D printing pipeline. Our contribution also includes the framework that connects the user with a data-embedding interface through the food 3D printing process, and the decoding system allows the user to decode the information inside the 3D printed food through backlight illumination and a simple image processing technique. Finally, we evaluate the usability of our method under different settings and demonstrate our method through the example application scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 84926
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 84818
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 84681
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 84663
        }
      ]
    },
    {
      "id": 84985,
      "typeId": 12319,
      "title": "Exploring Sensory Conflict Effect Due to Upright Redirection While Using VR in Reclining & Lying Positions",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937710674706483"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545692"
        },
        "Preview": {
          "duration": "30",
          "title": "Exploring Sensory Conflict Effect Due to Upright Redirection While Using VR in Reclining & Lying Positions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=bKAz3PO0gRQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2037",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86203
      ],
      "eventIds": [],
      "abstract": "When users use Virtual Reality (VR) in nontraditional postures, such as while reclining or lying in relaxed positions, their views lean upwards and need to be corrected, to make sure they see upright contents and perceive the interactions as if they were standing. Such upright redirection is expected to cause visual-vestibular-proprioceptive conflict, affecting users' internal perceptions (e.g., body ownership, presence, simulator sickness) and external perceptions (e.g., egocentric space perception) in VR. Different body reclining angles may affect vestibular sensitivity and lead to the dynamic weighting of multi-sensory signals in the sensory integration. In the paper, we investigated the impact of upright redirection on users' perceptions, with users' physical bodies tilted at various angles backward and views upright redirected accordingly. The results showed that upright redirection led to simulator sickness, confused self-awareness, weak upright illusion, and increased space perception deviations to various extents when users are at different reclining positions, and the situations were the worst at the 45-degree conditions. Based on these results, we designed some illusion-based and sensory-based methods, that were shown effective in reducing the impact of sensory conflict through preliminary evaluations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software",
              "dsl": "Chinese Academy of Sciences"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "College of Computer Science and Technology",
              "dsl": "University of Chinese Academy of Sciences"
            }
          ],
          "personId": 84562
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Capital Normal University",
              "dsl": ""
            }
          ],
          "personId": 84620
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing University of Technology",
              "dsl": ""
            }
          ],
          "personId": 84629
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 84589
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "College of Artificial Intelligence",
              "dsl": "Nanjing University of Information Science and Technology"
            }
          ],
          "personId": 84883
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of software, Chinese Academy of Sciences",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "School of Artificial Intelligence",
              "dsl": "University of Chinese Academy of Sciences"
            }
          ],
          "personId": 84590
        }
      ]
    },
    {
      "id": 84986,
      "typeId": 12319,
      "title": "DiscoBand: Multiview Depth-Sensing Smartwatch Strap for Hand, Arm and Environment Tracking",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937714155995166"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545634"
        },
        "Preview": {
          "duration": "30",
          "title": "DiscoBand: Multiview Depth-Sensing Smartwatch Strap for Hand, Arm and Environment Tracking",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=mt0IU8SrOa8"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6639",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86199
      ],
      "eventIds": [],
      "abstract": "Real-time tracking of a user’s hands, arms and environment is valuable in a wide variety of HCI applications, from context awareness to virtual reality. Rather than rely on fixed and external tracking infrastructure, the most flexible and consumer-friendly approaches are mobile, self-contained, and compatible with popular device form factors (e.g., smartwatches). In this vein, we contribute DiscoBand, a thin sensing strap not exceeding 1 cm in thickness. Sensors operating so close to the skin inherently face issues with occlusion. To overcome this, our strap uses eight distributed depth sensors imaging the hand from different viewpoints, creating a sparse 3D point cloud. An additional eight depth sensors image outwards from the band to track the user’s body and surroundings. In addition to evaluating arm and hand pose tracking, we also describe a series of supplemental applications powered by our band's data, including held object recognition and environment mapping. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Human-Computer Interaction Institute, Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 84944
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84659
        }
      ]
    },
    {
      "id": 84987,
      "typeId": 12319,
      "title": "Flexel: A Modular Floor Interface for Room-Scale Tactile Sensing",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937717511438498"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545699"
        },
        "Preview": {
          "duration": "30",
          "title": "Flexel: A Modular Floor Interface for Room-Scale Tactile Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=XzdyRwFK7kE"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4339",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86192
      ],
      "eventIds": [],
      "abstract": "  Human environments are physically supported by floors, which prevents people and furniture from falling against gravitational pull. \r\n  Since our body motions continuously generate vibrations and loads that propagate to the ground, measurement of these expressive signals leads to unobtrusive activity sensing. \r\n  In this study, we present Flexel, a modular floor interface for room-scale tactile sensing. By paving a room with floor interfaces, our system can immediately begin to infer touch positions, track user locations, recognize foot gestures, and detect object locations.\r\n  Through a series of exploratory studies, we figured out the preferable hardware design that adheres to construction conventions, as well as the optimal sensor density that mediates the trade-off between costs and performance. \r\n  In addition, we summarize a design guideline that is generalizable to other floor interfaces. \r\n  Moreover, we demonstrate example applications for room-scale tactile sensing enabled by Flexel systems. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84829
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84682
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84775
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84869
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84684
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84707
        }
      ]
    },
    {
      "id": 84988,
      "typeId": 12319,
      "title": "Fuse: In-Situ Sensemaking Support in the Browser",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937722729144402"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545693"
        },
        "Preview": {
          "duration": "31",
          "title": "Fuse: In-Situ Sensemaking Support in the Browser",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7A0FLhMmujM"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-9629",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86193
      ],
      "eventIds": [],
      "abstract": "People spend a significant amount of time trying to make sense of the internet, collecting content from a variety of sources and organizing it to make decisions and achieve their goals. While humans are able to fluidly iterate on collecting and organizing information in their minds, existing tools and approaches introduce significant friction into the process. We introduce Fuse, a browser extension that externalizes users’ working memory by combining low-cost collection with lightweight organization of content in a compact card-based sidebar that is always available. Fuse helps users simultaneously extract key web content and structure it in a lightweight and visual way. We discuss how these affordances help users externalize more of their mental model into the system (e.g., saving, annotating, and structuring items) and support fast reviewing and resumption of task contexts. Our 22-month public deployment and follow-up interviews provide longitudinal insights into the structuring behaviors of real-world users conducting information foraging tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84785
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Semantic Scholar",
              "dsl": "AI2"
            }
          ],
          "personId": 84784
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Adelphi",
              "institution": "US Army",
              "dsl": "Army Research Labs"
            }
          ],
          "personId": 84887
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 84671
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 84766
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 84610
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 84554
        }
      ]
    },
    {
      "id": 84989,
      "typeId": 12319,
      "title": "Mixels: Fabricating Interfaces using Programmable Magnetic Pixels",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937726168465458"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545698"
        },
        "Preview": {
          "duration": "30",
          "title": "Mixels: Fabricating Interfaces using Programmable Magnetic Pixels",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JKuCthryzhA"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-9903",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85637
      ],
      "eventIds": [],
      "abstract": "In this paper, we present Mixels, programmable magnetic pixels that can be rapidly fabricated using an electromagnetic printhead mounted on an off-the-shelve 3-axis CNC machine. The ability to program magnetic material pixel-wise with varying magnetic force enables Mixels to create new tangible, tactile, and haptic interfaces. To facilitate the creation of interactive objects with Mixels, we provide a user interface that lets users specify the high-level magnetic behavior and that then computes the underlying magnetic pixel assignments and fabrication instructions to program the magnetic surface. Our custom hardware add-on based on an electromagnetic printhead and hall effect sensor clips onto a standard 3-axis CNC machine and can both write and read magnetic pixel values from magnetic material. Our evaluation shows that our system can reliably program and read magnetic pixels of various strengths, that we can predict the behavior of two interacting magnetic surfaces before programming them, that our electromagnet is strong enough to create pixels that utilize the maximum magnetic strength of the material being programmed, and that this material remains magnetized when removed from the magnetic plotter.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84698
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84882
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84953
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84592
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84861
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84611
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 84990,
      "typeId": 12319,
      "title": "Mimic: In-Situ Recording and Re-Use of Demonstrations to Support Robot Teleoperation",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937729502945310"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545639"
        },
        "Preview": {
          "duration": "31",
          "title": "Mimic: In-Situ Recording and Re-Use of Demonstrations to Support Robot Teleoperation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Q6sNQHgkKuw"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4614",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86194
      ],
      "eventIds": [],
      "abstract": "Remote teleoperation is an important robot control method when they cannot operate fully autonomously. Yet, teleoperation presents challenges to effective and full robot utilization: controls are cumbersome, inefficient, and the teleoperator needs to actively attend to the robot and its environment. Inspired by end-user programming, we propose a new interaction paradigm to support robot teleoperation for combinations of repetitive and complex movements. We introduce Mimic, a system that allows teleoperators to demonstrate and save robot trajectories as templates, and re-use them to execute the same action in new situations. Templates can be re-used through (1) macros—parametrized templates assigned to and activated by buttons on the controller, and (2) programs—sequences of parametrized templates that operate autonomously. A user study in a simulated environment showed that after initial set up time, participants completed manipulation tasks faster and more easily compared to traditional direct control.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84756
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84754
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 84894
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 84632
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84678
        }
      ]
    },
    {
      "id": 84991,
      "typeId": 12319,
      "title": "RemoteLab: Virtual Reality Remote study Tool Kit",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937732111794317"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545679"
        },
        "Preview": {
          "duration": "30",
          "title": "RemoteLab: Virtual Reality Remote study Tool Kit",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=i_JaTKGJfzA"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3372",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86197
      ],
      "eventIds": [],
      "abstract": "User studies play a critical role in human subject research, including human-computer interaction. Virtual reality (VR) researchers tend to conduct user studies in-person at their laboratory, where participants experiment with novel equipment to complete tasks in a simulated environment, which is often new to many. However, due to social distancing requirements in recent years, VR research has been disrupted by preventing participants from attending in-person laboratory studies. On the other hand, affordable head-mounted displays are becoming common, enabling access to VR experiences and interactions outside traditional research settings. Recent research has shown that unsupervised remote user studies can yield reliable results, however, the setup of experiment software designed for remote studies can be technically complex and convoluted. We present a novel open-source Unity toolkit, RemoteLab, designed to facilitate the preparation of remote experiments by providing a set of tools that synchronize experiment state across multiple computers, record and collect data from various multimedia sources, and replay the accumulated data for analysis. This toolkit facilitates VR researchers to conduct remote experiments when in-person experiments are not feasible or increase the sampling variety of a target population and reach participants that otherwise would not be able to attend in-person.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84794
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84838
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Microsoft Corp.",
              "dsl": "Microsoft Research"
            }
          ],
          "personId": 84841
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84539
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 84541
        }
      ]
    },
    {
      "id": 84992,
      "typeId": 12319,
      "title": "EtherPose: Continuous Hand Pose Tracking with Wrist-Worn Antenna Impedance Characteristic Sensing",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937734724866098"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545665"
        },
        "Preview": {
          "duration": "31",
          "title": "EtherPose: Continuous Hand Pose Tracking with Wrist-Worn Antenna Impedance Characteristic Sensing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=n7eqGbqmnwc"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6001",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86199
      ],
      "eventIds": [],
      "abstract": "EtherPose is a continuous hand pose tracking system employing two wrist-worn antennas, from which we measure the real-time dielectric loading resulting from different hand geometries (i.e., poses). Unlike worn camera-based methods, our RF approach is more robust to occlusion from clothing and avoids capturing potentially sensitive imagery. Through a series of simulations and empirical studies, we designed a proof-of-concept, worn implementation built around compact vector network analyzers. Sensor data is then interpreted by a machine learning backend, which outputs a fully-posed 3D hand. In a user study, we show how our system can track hand pose with a mean Euclidean joint error of 11.6 mm, even when covered in fabric. We also studied 2DOF wrist angle and micro-gesture tracking. In the future, our approach could be miniaturized and extended to include more and different types of antennas, operating at different self resonances. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84753
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84659
        }
      ]
    },
    {
      "id": 84993,
      "typeId": 12319,
      "title": "DEEP: 3D Gaze Pointing in Virtual Reality Leveraging Eyelid Movement",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937738327765062"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545673"
        },
        "Preview": {
          "duration": "30",
          "title": "DEEP: 3D Gaze Pointing in Virtual Reality Leveraging Eyelid Movement",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lTz8SN7s7SY"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8421",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85636
      ],
      "eventIds": [],
      "abstract": "Gaze-based target suffers from low input precision and target occlusion. In this paper, we explored to leverage the continuous eyelid movement to support high-efficient and occlusion-robust dwell-based gaze pointing in virtual reality. We first conducted two user studies to examine the users' eyelid movement pattern both in unintentional and intentional conditions. The results proved the feasibility of leveraging intentional eyelid movement that was distinguishable with natural movements for input. We also tested the participants' dwelling pattern for targets with different sizes and locations. Based on these results, we propose DEEP, a novel technique that enables the users to see through occlusions by controlling the aperture angle of their eyelids and dwell to select the targets with the help of a probabilistic input prediction model. Evaluation results showed that DEEP with dynamic depth and location selection incorporation significantly outperformed its static variants, as well as a naive dwelling baseline technique. Even for 100% occluded targets, it could achieve an average selection speed of 2.5s with an error rate of 2.3%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Institute for Network Sciences and Cyberspace"
            }
          ],
          "personId": 84790
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 84719
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": ""
            }
          ],
          "personId": 84715
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications",
              "dsl": ""
            }
          ],
          "personId": 84528
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 84584
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 84647
        }
      ]
    },
    {
      "id": 84994,
      "typeId": 12319,
      "title": "Using Annotations for Sensemaking About Code",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937740961779722"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545667"
        },
        "Preview": {
          "duration": "30",
          "title": "Using Annotations for Sensemaking About Code",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0BTIZwbU-Vk"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6487",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86200
      ],
      "eventIds": [],
      "abstract": "Developers spend significant amounts of time finding, relating, navigating, and, more broadly, making sense of code. \r\nWhile sensemaking, developers must keep track of many pieces of information including the objectives of their task, the code locations of interest, their questions and hypotheses about the behavior of the code, and more. \r\nDespite this process being such an integral aspect of software development, there is little tooling support for externalizing and keeping track of developers' information, which led us to develop Catseye -- an annotation tool for lightweight notetaking about code. \r\nCatseye has advantages over traditional methods of externalizing code-related information, such as commenting, in that the annotations retain the original context of the code while not actually modifying the underlying source code, and can support richer interactions such as lightweight versioning, following-up on the annotation content, and can be used as navigational aids. \r\nIn our investigation of developers' notetaking processes using Catseye, we found developers were able to successfully use annotations to support their code sensemaking when completing a debugging task, with participants in a small study who used Catseye fixing more bugs, on average, than the baseline.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84830
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84598
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 84739
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Hunter College",
              "dsl": ""
            }
          ],
          "personId": 84802
        }
      ]
    },
    {
      "id": 84995,
      "typeId": 12319,
      "title": "DigituSync: A Dual-User Passive Exoskeleton Glove That Adaptively Shares Hand Gestures",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937743818117120"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545630"
        },
        "Preview": {
          "duration": "31",
          "title": "DigituSync: A Dual-User Passive Exoskeleton Glove That Adaptively Shares Hand Gestures",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=XpOKhXOXnt8"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2604",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86199
      ],
      "eventIds": [],
      "abstract": "We engineered DigituSync, a passive-exoskeleton that physically links two hands together, enabling two users to adaptively transmit finger movements in real-time. It uses multiple four-bar linkages to transfer both motion and force, while still preserving congruent haptic feedback. Moreover, we implemented a variable-length linkage that allows adjusting the force transmission ratio between the two users and regulates the amount of intervention, which enables users to customize their learning experience. DigituSync's benefits emerge from its passive design: unlike existing haptic devices (motor-based exoskeletons or electrical muscle stimulation), DigituSync has virtually no latency and does not require batteries/electronics to transmit or adjust movements, making it useful and safe to deploy in many settings, such as between students and teachers in a classroom. We validated DigituSync by means of technical evaluations and a user study, demonstrating that it instantly transfers finger motions and forces with the ability of adaptive force transmission, which allowed participants to feel more control over their own movements and to feel the teacher’s intervention was more responsive. We also conducted two exploratory sessions with a music teacher and deaf-blind users, which allowed us to gather experiential insights from the teacher’s side and explore DigituSync in applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84535
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84922
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84544
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84662
        }
      ]
    },
    {
      "id": 84996,
      "typeId": 12319,
      "title": "Seeing our Blind Spots: Smart Glasses-based Simulation to Increase Design Students Awareness of Visual Impairment",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937747446169720"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545687"
        },
        "Preview": {
          "duration": "30",
          "title": "Seeing our Blind Spots: Smart Glasses-based Simulation to Increase Design Students Awareness of Visual Impairment",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=XqZATw2iBVk"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1512",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86195
      ],
      "eventIds": [],
      "abstract": "As the population ages, many will acquire visual impairments. To improve design for these users, it is essential to build awareness of their perspective during everyday routines, especially for design students. \r\n    Although several visual impairment simulation toolkits exist in both academia and as commercial products, analog, and static visual impairment simulation tools do not simulate effects concerning the user's eye movements. Meanwhile, VR and video see-through-based AR simulation methods are constrained by smaller fields of view when compared with the natural human visual field and also suffer from vergence-accommodation conflict (VAC) which correlates with visual fatigue, headache, and dizziness.\r\n    In this paper, we enable an on-the-go, VAC-free, visually impaired experience by leveraging our optical see-through glasses. The FOV of our glasses is approximately 160 degrees for horizontal and 140 degrees for vertical, and participants can experience both losses of central vision and loss of peripheral vision at different severities.\r\n    Our evaluation (n =14) indicates that the glasses can significantly and effectively reduce visual acuity and visual field without causing typical motion sickness symptoms such as headaches and or visual fatigue. Questionnaires and qualitative feedback also showed how the glasses helped to increase participants’ awareness of visual impairment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 84548
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": "KMD"
            }
          ],
          "personId": 84579
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Keio Graduate School of Media Design"
            }
          ],
          "personId": 84744
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 84555
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmiths University of London",
              "dsl": "Computing"
            }
          ],
          "personId": 84604
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84710
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Graduate School of Media Design"
            }
          ],
          "personId": 84854
        }
      ]
    },
    {
      "id": 84997,
      "typeId": 12319,
      "title": "WaddleWalls: Room-scale Interactive Partitioning System using a Swarm of Robotic Partitions",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937750109556776"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545615"
        },
        "Preview": {
          "duration": "30",
          "title": "WaddleWalls: Room-scale Interactive Partitioning System using a Swarm of Robotic Partitions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=saWchCXZvq4"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2609",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86192
      ],
      "eventIds": [],
      "abstract": "We propose WaddleWalls, a room-scale interactive partitioning system using a swarm of robotic partitions that allows occupants to interactively reconfigure workspace partitions to satisfy their privacy and interaction needs. The system can automatically arrange the partitions' layout designed by the user on demand. The user specifies the target partition's position, orientation, and height using the controller's 3D manipulations. In this work, we discuss the design consideration of the interactive partition system and implement WaddleWalls' proof-of-concept prototype assembled with off-the-shelf materials. We demonstrate the functionalities of WaddleWalls through several application scenarios in an open-planned office environment. We also conduct an initial user evaluation that compares WaddleWalls with conventional wheeled partitions, finding that WaddleWalls allows effective workspace partitioning and mitigates the physical and temporal efforts needed to fulfill ad hoc social and privacy requirements. Finally, we clarify the feasibility, potential, and future challenges of WaddleWalls through an interview with experts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Research Institute of Electrical Communication, Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 84771
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 84948
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Research Institute of Electrical Communication, Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 84833
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Miyagi",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 84780
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 84594
        }
      ]
    },
    {
      "id": 84998,
      "typeId": 12319,
      "title": "Interactive Public Displays and Wheelchair Users: Between Direct, Personal and Indirect, Assisted Interaction",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937752806510704"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545662"
        },
        "Preview": {
          "duration": "31",
          "title": "Interactive Public Displays and Wheelchair Users: Between Direct, Personal and Indirect, Assisted Interaction",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KdYAcCc-MLQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6369",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86195
      ],
      "eventIds": [],
      "abstract": "We examine accessible interactions for wheelchair users and public displays with three studies. In a first study, we conduct a Systematic Literature Review, from which we report very few scientific papers on this topic and a preponderant focus on touch input. In a second study, we conduct a Systematic Video Review using YouTube as a data source, and unveil accessibility challenges for public displays and several input modalities alternative to direct touch. In a third study, we conduct semi-structured interviews with eleven wheelchair users to understand their experience interacting with public displays and to collect their preferences for more accessible input modalities. Based on our findings, we propose the \"assisted interaction\" phase to extend Vogel and Balakrishnan's four-phase interaction model with public displays, and the \"ability\" dimension for cross-device interaction design to support, via users' personal mobile devices, independent use of interactive public displays.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 84600
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 84803
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava ",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 84864
        }
      ]
    },
    {
      "id": 84999,
      "typeId": 12319,
      "title": "Summarizing Sets of Related ML-Driven Recommendations for Improving File Management in Cloud Storage",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937756174536764"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545704"
        },
        "Preview": {
          "duration": "30",
          "title": "Summarizing Sets of Related ML-Driven Recommendations for Improving File Management in Cloud Storage",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=eHmtPWwNu1Q"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-5279",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86206
      ],
      "eventIds": [],
      "abstract": "Personal cloud storage systems increasingly offer recommendations to help users retrieve or manage files of interest. For example, Google Drive's Quick Access predicts and surfaces files likely to be accessed. However, when multiple, related recommendations are made, interfaces typically present recommended files and any accompanying explanations individually, burdening users. To improve the usability of ML-driven personal information management systems, we propose a new method for summarizing related file-management recommendations. We generate succinct summaries of groups of related files being recommended. Summaries reference the files' shared characteristics. Through a within-subjects online study in which participants received recommendations for groups of files in their own Google Drive, we compare our summaries to baselines like visualizing a decision tree model or simply listing the files in a group. Compared to the baselines, participants expressed greater understanding and confidence in accepting recommendations when shown our novel recommendation summaries.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84705
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84527
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84889
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84621
        }
      ]
    },
    {
      "id": 85000,
      "typeId": 12319,
      "title": "Gesture-aware Interactive Machine Teaching with In-situ Object Annotations",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937758963728444"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545648"
        },
        "Preview": {
          "duration": "30",
          "title": "Gesture-aware Interactive Machine Teaching with In-situ Object Annotations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8Ps6hxlnyq4"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4586",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86192
      ],
      "eventIds": [],
      "abstract": "Interactive Machine Teaching (IMT) systems allow non-experts to easily create Machine Learning (ML) models. However, existing vision-based IMT systems either ignore annotations on the objects of interest or require users to annotate in a post-hoc manner. Without the annotations on objects, the model may misinterpret the objects using unrelated features. Post-hoc annotations cause additional workload, which diminishes the usability of the overall model building process. In this paper, we develop LookHere, which integrates in-situ object annotations into vision-based IMT. LookHere exploits users' deictic gestures to segment the objects of interest in real time. This segmentation information can be additionally used for training. To achieve the reliable performance of this object segmentation, we utilize our custom dataset called HuTics, including 2040 front-facing images of deictic gestures toward various objects by 170 people. The quantitative results of our user study showed that participants were 16.3 times faster in creating a model with our system compared to a standard IMT system with a post-hoc annotation process while demonstrating comparable accuracies. Additionally, models created by our system showed a significant accuracy improvement ($\\Delta mIoU=0.466$) in segmenting the objects of interest compared to those without annotations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Interactive Intelligent Systems Lab."
            }
          ],
          "personId": 84791
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84603
        }
      ]
    },
    {
      "id": 85001,
      "typeId": 12319,
      "title": "Record Once, Post Everywhere: Automatic Shortening of Audio Stories for Social Media",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937762050752532"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545680"
        },
        "Preview": {
          "duration": "30",
          "title": "Record Once, Post Everywhere: Automatic Shortening of Audio Stories for Social Media",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=QwMJlSkhb9E"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4621",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85638
      ],
      "eventIds": [],
      "abstract": "Following the prevalence of short-form video, short-form voice content has emerged on social media platforms like Twitter and Facebook. A challenge that creators face is hard constraints on the content length. If the initial recording is not short enough, they need to re-record or edit their content. Both are time-consuming, and the latter, if supported, can have a learning curve. Moreover, creators need to manually create multiple versions to publish content on platforms with different length constraints. To simplify this process, we present ROPE (Record Once, Post Everywhere). Creators can record voice content once, and our system will automatically shorten it to all length limits by removing parts of the recording for each target. We formulate this as a combinatorial optimization problem and propose a novel algorithm that automatically selects optimal sentence combinations from the original content to comply with each length constraint. Creators can customize the algorithmically shortened content by specifying sentences to include or exclude. Our system can also use the user-specified constraints to recompute and provides a new version. We conducted a user study comparing ROPE with a sentence-based manual editing baseline. The results show that ROPE can generate high-quality edits, alleviating the cognitive loads of creators for shortening content. While our system and user study address short-form voice content specifically, we believe that the same concept can also be applied to other media such as video with narration and dialog.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84640
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 84646
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 84568
        }
      ]
    },
    {
      "id": 85002,
      "typeId": 12319,
      "title": "Automated Filament Inking for Multi-color FFF 3D Printing",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937765687201824"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545654"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2323",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86204
      ],
      "eventIds": [],
      "abstract": "We propose a novel system for low-cost multi-color Fused Filament Fabrication (FFF) 3D printing, allowing for the creation of customizable colored filament using a pre-processing approach. We developed an open-source device to automatically ink filament using permanent markers. Our device can be built using 3D printed parts and off-the-shelf electronics. An accompanying web-based interface allows users to view GCODE toolpaths for a multi-color print and quickly generate filament color profiles. Taking a pre-processing approach makes this system compatible with the majority of desktop 3D printers on the market, as the processed filament behaves no differently from conventional filaments. Furthermore, inked filaments can be produced economically, reducing the need for excessive purchasing of material to expand color options. We demonstrate the efficacy of our system by fabricating monochromatic objects, objects with gradient colors, objects with bi-directional properties, as well as multi-color objects with up to four colors in a single print.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Visual Computing Lab"
            }
          ],
          "personId": 84872
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84810
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84716
        }
      ]
    },
    {
      "id": 85003,
      "typeId": 12319,
      "title": "spaceR: Knitting Ready-Made, Tactile, and Highly Responsive Spacer-Fabric Force Sensors for Continuous Input",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937772725239861"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545694"
        },
        "Preview": {
          "duration": "30",
          "title": "spaceR: Knitting Ready-Made, Tactile, and Highly Responsive Spacer-Fabric Force Sensors for Continuous Input",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=BcyZY1jIR1U"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1759",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86201
      ],
      "eventIds": [],
      "abstract": "With spaceR, we present both design and implementation of a resistive force-sensor based on a spacer fabric knit. Due to its softness and elasticity, our sensor provides an appealing haptic experience. It enables continuous input with high precision due to its innate haptic feedback and can be manufactured ready-made on a regular two-bed weft knitting machine, without requiring further post-processing steps. For our multi-component knit, we add resistive yarn to the filler material, in order to achieve a highly sensitive and responsive pressure sensing textile. Sensor resistance drops by ~90% when actuated with moderate finger pressure of 2 N, making the sensor accessible also for straightforward readout electronics. We discuss related manufacturing parameters and their effect on shape and electrical characteristics and explore design opportunities to harness visual and tactile affordances. Finally, we demonstrate several application scenarios by implementing diverse spaceR variations, including analog rocker- and four-way directional buttons, and show the possibility of mode-switching by tracking temporal data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Media Interaction Lab"
            }
          ],
          "personId": 84960
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Media Interaction Lab"
            }
          ],
          "personId": 84743
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bozen-Bolzano",
              "dsl": "Media Interaction Lab"
            }
          ],
          "personId": 84765
        }
      ]
    },
    {
      "id": 85004,
      "typeId": 12319,
      "title": "GANzilla: User-Driven Direction Discovery in Generative Adversarial Networks",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937776735002624"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545638"
        },
        "Preview": {
          "duration": "30",
          "title": "GANzilla: User-Driven Direction Discovery in Generative Adversarial Networks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cAjLsiOU-BY"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2600",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86202
      ],
      "eventIds": [],
      "abstract": "Generative Adversarial Network (GAN) is being widely adopted in numerous application areas, such as data preprocessing, image editing, and creativity support. However, GAN's 'black box' nature prevents non-expert users from controlling what data a model generates, spawning a plethora of prior work that focused on algorithm-driven approaches to automatically extract editing directions to control GAN. Complementarily, we propose a GANzilla---a user-driven tool that empowers a user with the classic scatter/gather technique to iteratively discover directions to meet their editing intents. In a work session with 12 participants, GANzilla users were able to discover directions that (i) edited images to match provided examples (closed-ended tasks) and that (ii) met a high-level goal, e.g., making the face happier, while showing diversity across individuals (open-ended tasks).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California Los Angeles",
              "dsl": "Electrical and Computer Engineering, Human Computer Interface Group"
            }
          ],
          "personId": 84891
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 84616
        }
      ]
    },
    {
      "id": 85005,
      "typeId": 12319,
      "title": "Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature ",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937781310996530"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545660"
        },
        "Preview": {
          "duration": "30",
          "title": "Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7t2G9tNwgTw"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6408",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86206
      ],
      "eventIds": [],
      "abstract": "Reviewing the literature to understand relevant threads of past work is a critical part of research and vehicle for learning. However, as the scientific literature grows the challenges for users to find and make sense of the many different threads of research grow as well. Previous work has helped scholars to find and group papers with citation information or textual similarity using standalone tools or overview visualizations. Instead, in this work we explore a tool integrated into users' reading process that helps them with leveraging authors' existing summarization of threads, typically in introduction or related work sections, in order to situate their own work's contributions. To explore this we developed a prototype that supports efficient extraction and organization of threads along with supporting evidence as scientists read research articles. The system then recommends further relevant articles based on user-created threads. We evaluate the system in a lab study and find that it helps scientists to follow and curate research threads without breaking out of their flow of reading, collect relevant papers and clips, and discover interesting new articles to further grow threads.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for AI",
              "dsl": "Semantic Scholar"
            }
          ],
          "personId": 84784
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84939
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 84554
        }
      ]
    },
    {
      "id": 85006,
      "typeId": 12319,
      "title": "CrossA11y: Identifying Video Accessibility Issues via Cross-modal Grounding",
      "award": "BEST_PAPER",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937784301531297"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545703"
        },
        "Preview": {
          "duration": "30",
          "title": "CrossA11y: Identifying Video Accessibility Issues via Cross-modal Grounding",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WhL0FktjlT0"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2602",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86195
      ],
      "eventIds": [],
      "abstract": "Authors make their videos visually accessible by adding audio descriptions (AD), and auditorily accessible by adding closed captions (CC). \r\nHowever, creating AD and CC is challenging and tedious, especially for non-professional describers and captioners, due to the difficulty of identifying accessibility problems in videos.\r\nA video author will have to watch the video through and manually check for inaccessible information frame-by-frame, for both visual and auditory modalities. \r\nIn this paper, we present CrossA11y, a system that helps authors efficiently detect and address visual and auditory accessibility issues in videos. \r\nUsing cross-modal grounding analysis, CrossA11y automatically measures accessibility of visual and audio segments in a video by checking for modality asymmetries.\r\nCrossA11y then displays these segments and surfaces visual and audio accessibility issues in a unified interface, making it intuitive to locate, review, script AD/CC in-place, and preview the described and captioned video immediately. \r\nWe demonstrate the effectiveness of CrossA11y through a lab study with 11 participants, comparing to existing baseline.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 84781
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 84606
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 84832
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 84616
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas, Austin",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84742
        }
      ]
    },
    {
      "id": 85007,
      "typeId": 12319,
      "title": "Personalized Game Difficulty Prediction Using Factorization Machines",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937788084785152"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545624"
        },
        "Preview": {
          "duration": "31",
          "title": "Personalized Game Difficulty Prediction Using Factorization Machines",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=QIFBY8wliuU"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4072",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86205
      ],
      "eventIds": [],
      "abstract": "The accurate and personalized estimation of task difficulty provides many opportunities for optimizing user experience. However, user diversity makes such difficulty estimation hard, in that empirical measurements from some user sample do not necessarily generalize to others.\r\n\r\nIn this paper, we contribute a new approach for personalized difficulty estimation of game levels, borrowing methods from content recommendation. Using factorization machines (FM) on a large dataset from a commercial puzzle game, we are able to predict difficulty as the number of attempts a player requires to pass future game levels, based on observed attempt counts from earlier levels and levels played by others. In addition to performance and scalability, FMs offer the benefit that the learned latent variable model can be used to study the characteristics of both players and game levels that contribute to difficulty. We compare the approach to a simple non-personalized baseline and a personalized prediction using Random Forests. Our results suggest that FMs are a promising tool enabling game designers to both optimize player experience and learn more about their players and the game.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "IT University of Copenhagen",
              "dsl": "Digital Design"
            }
          ],
          "personId": 84807
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84907
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "IT University of Copenhagen",
              "dsl": "Digital Design"
            }
          ],
          "personId": 84828
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 84877
        }
      ]
    },
    {
      "id": 85008,
      "typeId": 12319,
      "title": "Grid-Coding: An Accessible, Efficient, and Structured Coding Paradigm for Blind and Low-Vision Programmers",
      "award": "BEST_PAPER",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937794564980756"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545620"
        },
        "Preview": {
          "duration": "30",
          "title": "Grid-Coding: An Accessible, Efficient, and Structured Coding Paradigm for Blind and Low-Vision Programmers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=LWJhi1mxuf0"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8555",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86195
      ],
      "eventIds": [],
      "abstract": "Sighted programmers often rely on visual cues (e.g., syntax coloring, keyword highlighting, code formatting) to perform common coding activities in text-based languages (e.g., Python). Unfortunately, blind and low-vision (BLV) programmers hardly benefit from these visual cues because they interact with computers via assistive technologies (e.g., screen readers), which fail to communicate visual semantics meaningfully. Prior work on making text-based programming languages and environments accessible mostly focused on code navigation and, to some extent, code debugging, but not much toward code editing, which is an essential coding activity.\r\n\r\nWe present Grid-Coding to fill this gap. Grid-Coding renders source code in a structured 2D grid, where each row, column, and cell have consistent, meaningful semantics. Its design is grounded on prior work and refined by 28 BLV programmers through online participatory sessions for 2 months. We implemented the Grid-Coding prototype as a  spreadsheet-like web application for Python and evaluated it with a study with 12 BLV programmers. This study revealed that, compared to a text editor (i.e.,  the go-to editor for  BLV programmers), our prototype enabled BLV programmers to navigate source code quickly, find the context of a statement easily, detect syntax errors in existing code effectively,  and write new code with fewer syntax errors. The study also revealed how BLV programmers adopted Grid-Coding and demonstrated novel interaction patterns conducive to increased programming productivity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 84560
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Bangladesh University of Engineering and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 84831
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park ",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 84906
        }
      ]
    },
    {
      "id": 85009,
      "typeId": 12319,
      "title": "Concept-Labeled Examples for Library Comparison",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937791062745158"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545647"
        },
        "Preview": {
          "duration": "31",
          "title": "Concept-Labeled Examples for Library Comparison",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=IAwxNB9qHbg"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-7741",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86200
      ],
      "eventIds": [],
      "abstract": "Programmers often rely on online resources—such as code examples, documentation, blogs, and Q&A forums—to compare similar libraries and select the one most suitable for their own tasks and contexts. However, this comparison task is often done in an ad-hoc manner, which may result in suboptimal choices. Inspired by Analogical Learning and Variation Theory, we hypothesize that rendering many concept-annotated code examples from different libraries side-by-side can help programmers (1) develop a more comprehensive understanding of the libraries' similarities and distinctions and (2) make more robust, appropriate library selections. We designed a novel interactive interface, ParaLib, and used it as a technical probe to explore to what extent many side-by-side concept-annotated examples can facilitate the library comparison and selection process. A within-subjects user study with 20 programmers shows that, when using ParaLib, participants made more consistent, suitable library selections and provided more comprehensive summaries of libraries' similarities and differences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "IACS"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "IACS"
            }
          ],
          "personId": 84946
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84650
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 84767
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84941
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "SEAS"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "SEAS"
            }
          ],
          "personId": 84749
        }
      ]
    },
    {
      "id": 85010,
      "typeId": 12319,
      "title": "RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937798780268594"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545702"
        },
        "Preview": {
          "duration": "29",
          "title": "RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=lRSai-XRYyk"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-7467",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85638
      ],
      "eventIds": [],
      "abstract": "We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter’s perspective to demonstrate the effectiveness of our system.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84540
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84611
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 84657
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computer Science "
            }
          ],
          "personId": 84601
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84927
        }
      ]
    },
    {
      "id": 85011,
      "typeId": 12319,
      "title": "Project Primrose: Reflective Light-Diffuser Modules for Non-Emissive Flexible Display Systems",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937801775009924"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545625"
        },
        "Preview": {
          "duration": "30",
          "title": "Project Primrose: Reflective Light-Diffuser Modules for Non-Emissive Flexible Display Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=3ZTB79o7DqM"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6895",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85639
      ],
      "eventIds": [],
      "abstract": "Recent advances in smart materials have enabled displays to move beyond planar surfaces into the fabric of everyday life. We propose reflective light-diffuser modules for non-emissive flexible display systems. Our system leverages reflective-backed polymer-dispersed liquid crystal (PDLC), an electroactive material commonly used in smart window applications. This low-power non-emissive material can be cut to any shape, and dynamically diffuses light. We present the design & fabrication of two exemplar artifacts, a canvas and a handbag, that use the reflective light-diffuser modules. We also describe our content authoring pipeline and interaction modalities. We hope this work inspires future designers of flexible displays.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 84683
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 84943
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Systems Incorporated",
              "dsl": ""
            }
          ],
          "personId": 84950
        }
      ]
    },
    {
      "id": 85012,
      "typeId": 12319,
      "title": "MechARspace: An Authoring System Enabling Bidirectional Binding of AR with Toys in Real-time",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937805344342196"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545668"
        },
        "Preview": {
          "duration": "31",
          "title": "MechARspace: An Authoring System Enabling Bidirectional Binding of AR with Toys in Real-time",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=LfiE2KYyBZo"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2451",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86197
      ],
      "eventIds": [],
      "abstract": "Augmented Reality (AR), which blends physical and virtual worlds, presents the possibility of enhancing traditional toy design. By leveraging bidirectional virtual-physical interactions between humans and the designed artifact, such AR-enhanced toys can provide more playful and interactive experiences for traditional toys. However, designers are constrained by the complexity and technical difficulties of the current AR content creation processes. We propose MechARspace, an immersive authoring system that supports users to create toy-AR interactions through direct manipulation and visual programming. Based on the elicitation study, we propose a bidirectional interaction model which maps both ways: from the toy inputs to reactions of AR content, and also from the AR content to the toy reactions. This model guides the design of our system which includes a plug-and-play hardware toolkit and an in-situ authoring interface. We present multiple use cases enabled by MechARspace to validate this interaction model. Finally, we evaluate our system with a two-session user study where users first recreated a set of predefined toy-AR interactions and then implemented their own AR-enhanced toy designs.\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 84778
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 84741
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 84557
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 84959
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 84668
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 84605
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 84558
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering - C Design Lab"
            }
          ],
          "personId": 84627
        }
      ]
    },
    {
      "id": 85013,
      "typeId": 12319,
      "title": "Breathing Life Into Biomechanical User Models",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018937952111444019"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545689"
        },
        "Preview": {
          "duration": "30",
          "title": "Breathing Life Into Biomechanical User Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KAEgtrUUsw0"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1886",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86205
      ],
      "eventIds": [],
      "abstract": "Forward biomechanical simulation in HCI holds great promise as a tool for evaluation, design, and engineering of user interfaces. Although reinforcement learning (RL) has been used to simulate biomechanics in interaction, prior work has relied on unrealistic assumptions about the control problem involved, which limits the plausibility of emerging policies. These assumptions include direct torque actuation as opposed to muscle-based control; direct, privileged access to the external environment, instead of imperfect sensory observations; and lack of interaction with physical input devices. In this paper, we present a new approach for learning muscle-actuated control policies based on perceptual feedback in interaction tasks with physical input devices. This allows modelling of more realistic interaction tasks with cognitively plausible visuomotor control. We show that our simulated user model successfully learns a variety of tasks representing different interaction methods, and that the model exhibits characteristic movement regularities observed in studies of pointing. We provide an open-source implementation which can be extended with further biomechanical models, perception models, and interactive environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84836
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": ""
            }
          ],
          "personId": 84835
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Serious Games"
            }
          ],
          "personId": 84798
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Serious Games"
            }
          ],
          "personId": 84704
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": ""
            }
          ],
          "personId": 84631
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "University of Birmingham",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 84737
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 84877
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": ""
            }
          ],
          "personId": 84787
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 84956
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 84929
        }
      ]
    },
    {
      "id": 85014,
      "typeId": 12319,
      "title": "Reconfigurable Elastic Metamaterials",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938026379984998"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545649"
        },
        "Preview": {
          "duration": "30",
          "title": "Reconfigurable Elastic Metamaterials",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8H926LtVMWc"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1520",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86201
      ],
      "eventIds": [],
      "abstract": "We present a novel design for materials that are reconfigurable by end-users. Conceptually, we propose decomposing such reconfigurable materials into (1) a generic, complex material consisting of engineered microstructures (known as metamaterials) designed to be purchased and (2) a simple configuration geometry that can be fabricated by end-users to fit their individual use cases. Specifically, in this paper we investigate reconfiguring our material’s elasticity, such that it can cover existing objects and thereby augment their material properties. Users can configure their materials by generating the configuration geometry using our interactive editor, 3D printing it using commonly available filaments (e. g., PLA), and pressing it onto the generic material for local coupling. We characterize the mechanical properties of our reconfigurable elastic metamaterial and showcase the material’s applicability as, e.g., augmentation for haptic props in virtual reality, a reconfigurable shoe sole for different activities, or a battleship-like ball game.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 84893
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 84824
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon",
              "dsl": ""
            }
          ],
          "personId": 84546
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 84623
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 84595
        }
      ]
    },
    {
      "id": 85015,
      "typeId": 12319,
      "title": "OPAL: Multimodal Image Generation for News Illustrations",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938029651537951"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545621"
        },
        "Preview": {
          "duration": "31",
          "title": "OPAL: Multimodal Image Generation for News Illustrations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=FimeghwQZyQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-7228",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86202
      ],
      "eventIds": [],
      "abstract": "Advances in multimodal AI have presented people with powerful ways to create images from text. Recent work has shown that text-to-image generations are able to represent a broad range of subjects and artistic styles. However, finding the right visual language for text prompts is difficult. In this paper, we address this challenge with Opal, a system that produces text-to-image generations for news illustration. Given an article, Opal guides users through a structured search for visual concepts and provides a pipeline allowing users to generate illustrations based on an article's tone, keywords, and related artistic styles. Our evaluation shows that Opal efficiently generates diverse sets of news illustrations, visual assets, and concept ideas. Users with Opal generated two times more usable results than users without. We discuss how structured exploration can help users better understand the capabilities of human AI co-creative systems. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 84958
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 84757
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 84718
        }
      ]
    },
    {
      "id": 85016,
      "typeId": 12319,
      "title": "FeedLens: Polymorphic Lenses for Personalizing Exploratory Search over Knowledge Graphs",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938032885342248"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545631"
        },
        "Preview": {
          "duration": "30",
          "title": "FeedLens: Polymorphic Lenses for Personalizing Exploratory Search over Knowledge Graphs",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Wxgazv_PrbQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8956",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86206
      ],
      "eventIds": [],
      "abstract": "The vast scale and open-ended nature of knowledge graphs (KGs) make exploratory search over them cognitively demanding for users. We introduce a new technique, polymorphic lenses, that improves exploratory search over a KG by obtaining new leverage from the existing preference models that KG-based systems maintain for recommending content. The approach is based on a simple but powerful observation: in a KG, preference models can be re-targeted to recommend not only entities of a single base entity type (e.g., papers in the scientific literature KG, products in an e-commerce KG), but also all other types (e.g., authors, conferences, institutions; sellers, buyers). We implement our technique in a novel system, FeedLens, which is built over Semantic Scholar, a production system for navigating the scientific literature KG. FeedLens reuses the existing preference models on Semantic Scholar---people's curated research feeds---as lenses for exploratory search. Semantic Scholar users can curate multiple feeds/lenses for different topics of interest, e.g., one for human-centered AI and another for document embeddings. Although these lenses are defined in terms of papers, FeedLens re-purposes them to also guide search over authors, institutions, venues, etc. Our system design is based on feedback from intended users via two pilot surveys (n=17 and n=13, respectively). We compare FeedLens and Semantic Scholar via a third (within-subjects) user study (n=15) and find that FeedLens increases user engagement while reducing the cognitive effort required to complete a short literature review task. Our qualitative results also highlight people's preference for this more effective exploratory search experience enabled by FeedLens.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 84694
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for Artificial Intelligence",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evanston",
              "institution": "Northwestern University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84924
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for AI",
              "dsl": "Semantic Scholar"
            }
          ],
          "personId": 84914
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for AI",
              "dsl": ""
            }
          ],
          "personId": 84622
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for Artificial Intelligence",
              "dsl": "Semantic Scholar"
            }
          ],
          "personId": 84735
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 84782
        }
      ]
    },
    {
      "id": 85017,
      "typeId": 12319,
      "title": "NFCStack: Identifiable Physical Building Blocks that Support Concurrent Construction and Frictionless Interaction",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938035720691803"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545658"
        },
        "Preview": {
          "duration": "31",
          "title": "NFCStack: Identifiable Physical Building Blocks that Support Concurrent Construction and Frictionless Interaction",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=dUO1TIO1O90"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8558",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86192
      ],
      "eventIds": [],
      "abstract": "In this paper, we propose NFCStack, which is a physical building block system that supports stacking and frictionless interaction and is based on near-field communication (NFC). This system consists of a portable station that can support and resolve the order of three types of passive identifiable stackable: bricks, boxes, and adapters. The bricks support stable and sturdy physical construction, whereas the boxes support frictionless tangible interactions. The adapters provide an interface between the aforementioned two types of stackable and convert the top of a stack into a terminal for detecting interactions between NFC-tagged objects. In contrast to existing systems based on NFC or radio-frequency identification technologies, NFCStack is portable, supports simultaneous interactions, and resolves stacking and interaction events responsively, even when objects are not strictly aligned. Evaluation results indicate that the proposed system effectively supports 12 layers of rich-ID stacking with the three types of building block, even if every box is stacked with a 6-mm offset. The results also indicate possible generalized applications of the proposed system, including 2.5-dimensional construction. The interaction styles are described using several educational application examples, and the design implications of this research are explained.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84672
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84614
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84812
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84760
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 84793
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84773
        }
      ]
    },
    {
      "id": 85018,
      "typeId": 12319,
      "title": "Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938038644113418"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545626"
        },
        "Preview": {
          "duration": "30",
          "title": "Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=pN7QlnHTW3A"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1525",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85636
      ],
      "eventIds": [],
      "abstract": "This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to “affect” each other through physical actuation and digital computation. In the exist- ing AR sketching, the relationship between virtual and physical worlds is only one-directional — while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketch- ing interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84549
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Delhi",
              "city": "New Delhi",
              "institution": "IIIT-Delhi",
              "dsl": "Weave Lab"
            }
          ],
          "personId": 84638
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84761
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84919
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary ",
              "institution": "University of Calgary ",
              "dsl": ""
            }
          ],
          "personId": 84806
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84851
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84543
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84611
        }
      ]
    },
    {
      "id": 85019,
      "typeId": 12319,
      "title": "Fibercuit: Prototyping High-Resolution Flexible and Kirigami Circuits with a Fiber Laser Engraver",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938041949245570"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545652"
        },
        "Preview": {
          "duration": "30",
          "title": "Fibercuit: Prototyping High-Resolution Flexible and Kirigami Circuits with a Fiber Laser Engraver",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Ix4Atyy4waY"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-9772",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85637
      ],
      "eventIds": [],
      "abstract": "Prototyping compact devices with unique form factors often requires the PCB manufacturing process to be outsourced, which can be expensive and time-consuming.\r\nIn this paper, we present Fibercuit, a set of rapid prototyping techniques to fabricate high-resolution, flexible circuits on-demand using a fiber laser engraver. \r\nWe showcase techniques that can laser cut copper-based composites to form fine-pitch conductive traces, laser fold copper substrates that can form kirigami structures, and laser solder surface-mount electrical components using off-the-shelf soldering pastes. \r\nCombined with our software pipeline, an end user can design and fabricate flexible circuits which are dual-layer and three-dimensional, thereby exhibiting a wide range of form factors. \r\nWe demonstrate Fibercuit by showcasing a set of examples, including a custom dice, flex cables, custom end-stop switches, electromagnetic coils, LED earrings and a circuit in the form of kirigami crane.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": ""
            }
          ],
          "personId": 84574
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 84613
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84602
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84881
        }
      ]
    },
    {
      "id": 85020,
      "typeId": 12319,
      "title": "HingeCore: Laser-Cut Foamcore for Fast Assembly",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938044511965184"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545618"
        },
        "Preview": {
          "duration": "30",
          "title": "HingeCore: Laser-Cut Foamcore for Fast Assembly",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=O3mjEZYlEzQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2581",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85637
      ],
      "eventIds": [],
      "abstract": "We present HingeCore, a novel type of laser-cut 3D structure made from sandwich materials, such as foamcore. The key design element behind HingeCore is what we call a finger hinge, which we produce by laser-cutting foamcore “half-way”. The primary benefit of finger hinges is that they allow for very fast assembly, as they allow models to be assembled by folding and because folded hinges stay put at the intended angle, based on the friction between fingers alone, which eliminates the need for glue or tabs. Finger hinges are also highly robust, with some 5mm foamcore models withstanding 62kg. We present HingeCoreMaker, a stand-alone software tool that automatically converts 3D models to HingeCore layouts, as well as an integration into a 3D modeling tool for laser cutting (Kyub [7]). We have used Hinge-CoreMaker to fabricate design objects, including speakers, lamps, and a life-size bust, as well as structural objects, such as functional furniture. In our user study, participants assembled HingeCore layouts 2.9x faster than layouts generated using the state-of-the-art for plate-based assembly (Roadkill [1]).\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84751
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84570
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84875
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84669
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84588
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 84679
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84677
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 84949
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84692
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84714
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84772
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84617
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84774
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84561
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84736
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84936
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84529
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84649
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84581
        }
      ]
    },
    {
      "id": 85021,
      "typeId": 12319,
      "title": "OmniScribe: Authoring Immersive Audio Descriptions for 360° Videos",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938047850614935"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545613"
        },
        "Preview": {
          "duration": "30",
          "title": "OmniScribe: Authoring Immersive Audio Descriptions for 360Â° Videos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=kdc-Qqml5jw"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-5850",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85638
      ],
      "eventIds": [],
      "abstract": "Blind people typically access videos via audio descriptions (AD) crafted by sighted describers who comprehend, select, and describe crucial visual content in the videos. 360° video is an emerging storytelling medium that enables immersive experiences that people may not possibly reach in everyday life. However, the omnidirectional nature of 360° videos makes it challenging for describers to perceive the holistic visual content and interpret spatial information that is essential to create immersive ADs for blind people. Through a formative study with a professional describer, we identified key challenges in describing 360° videos and iteratively designed OmniScribe, a system that supports the authoring of immersive ADs for 360° videos. OmniScribe uses AI-generated content-awareness overlays for describers to better grasp 360° video content. Furthermore, OmniScribe enables describers to author spatial AD and immersive labels for blind users to consume the videos immersively with our mobile prototype. In a study with 11 professional and novice describers, we demonstrated the value of OmniScribe in the authoring workflow; and a study with 8 blind participants revealed the promise of immersive AD over standard AD for 360° videos. Finally, we discuss the implications of promoting 360° video accessibility.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84895
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University ",
              "dsl": ""
            }
          ],
          "personId": 84745
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "National Taiwan University"
            }
          ],
          "personId": 84776
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei ",
              "institution": "National Taiwan University ",
              "dsl": ""
            }
          ],
          "personId": 84702
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Computer Science & Information Engineering"
            }
          ],
          "personId": 84734
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipie",
              "institution": "Audio Description Development Assoication",
              "dsl": ""
            }
          ],
          "personId": 84801
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84773
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 84597
        }
      ]
    },
    {
      "id": 85022,
      "typeId": 12319,
      "title": "ShrinkCells: Localized and Sequential Shape-Changing Actuation of 3D-Printed Objects via Selective Heating",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938051143151668"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545670"
        },
        "Preview": {
          "duration": "30",
          "title": "ShrinkCells: Localized and Sequential Shape-Changing Actuation of 3D-Printed Objects via Selective Heating",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=DplBQ_g5Uio"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3397",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86204
      ],
      "eventIds": [],
      "abstract": "The unique behaviors of thermoplastic polymers enable shape-changing interfaces made of 3D printed objects that do not require complex electronics integration. While existing techniques greatly rely on external heat applied globally on a 3D printed object to initiate all at once the shape-changing behavior (e.g., hot water, heat gun, oven), independent control of multiple parts of the object becomes nearly impossible.\r\nWe introduce ShrinkCells, a set of shape-changing actuators that rely on localized heat to shrink or bend. This is achieved by combining the properties of two materials --- conductive PLA is used to generate localized heat that selectively triggers the shrinking of a Shape Memory Polymer.\r\nThe unique benefit of ShrinkCells is their capability of triggering simultaneous or sequential shape transformations for different geometries using a single power supply. The result is 3D printed rigid structures that actuate in sequence, avoiding self-collisions when unfolding.\r\nWe contribute to the body of literature on 4D fabrication by a systematic investigation of selective heating with two different materials, the design and evaluation of the ShrinkCells shape-changing primitives, and applications demonstrating the usage of these actuators.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 84880
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST Industrial Design",
              "dsl": ""
            }
          ],
          "personId": 84699
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 84531
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 84673
        }
      ]
    },
    {
      "id": 85023,
      "typeId": 12319,
      "title": "Chatbots Facilitating Consensus-building in Asynchronous Co-Design",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938055240982609"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545671"
        },
        "Preview": {
          "duration": "30",
          "title": "Chatbots Facilitating Consensus-building in Asynchronous Co-Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=IiSALxEyelM"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6381",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86202
      ],
      "eventIds": [],
      "abstract": "Consensus-building is an essential process for the success of co-design projects. To build consensus, stakeholders need to discuss conflicting needs and viewpoints, converge their ideas toward shared interests, and grow their willingness to commit to group decisions. However, managing group discussions is challenging in large co-design projects with multiple stakeholders. In this paper, we investigate the interaction design of a chatbot that can mediate consensus-building conversationally. By interacting with individual stakeholders, the chatbot collects ideas for satisfying conflicting needs and engages stakeholders to consider others' viewpoints, without having stakeholders directly interact with each other. Results from an empirical study in an educational setting (N = 12) suggest that the approach can increase stakeholders' commitment to group decisions and maintain the effect even on the group decisions that conflict with personal interests. We conclude that chatbots can facilitate consensus-building in small-to-medium-sized projects, but more work is needed to scale up to larger projects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Department of Electrical Engineering"
            }
          ],
          "personId": 84878
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken ",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            }
          ],
          "personId": 84591
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 84722
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 84929
        }
      ]
    },
    {
      "id": 85024,
      "typeId": 12319,
      "title": "Notational Programming for Notebook Environments: A Case Study with Quantum Circuits",
      "award": "HONORABLE_MENTION",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938058516729906"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545619"
        },
        "Preview": {
          "duration": "30",
          "title": "Notational Programming for Notebook Environments: A Case Study with Quantum Circuits",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wpbOyBqLORU"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3318",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86200
      ],
      "eventIds": [],
      "abstract": "We articulate a vision for computer programming that includes pen-based computing, a paradigm we term notational programming. Notational programming blurs contexts: certain typewritten variables can be referenced in handwritten notation and vice-versa. To illustrate this paradigm, we developed an extension, Notate, to computational notebooks which allows users to open drawing canvases within lines of code. As a case study, we explore quantum programming and designed a notation, Qaw, that extends quantum circuit notation with abstraction features, such as variable-sized wire bundles and recursion. Results from a usability study with novices suggest that users find our core interaction of implicit cross-context references intuitive, but suggests further improvements to debugging infrastructure, interface design, and recognition rates. Throughout, we discuss questions raised by the notational paradigm, including a shift from 'recognition' of notations to 'reconfiguration' of practices and values around programming, and from 'sketching' to writing and drawing, or what we call 'notating.'",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Department of Information Science"
            }
          ],
          "personId": 84642
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84800
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84789
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84656
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 84764
        }
      ]
    },
    {
      "id": 85025,
      "typeId": 12319,
      "title": "BO as Assistant: Using Bayesian Optimization for Asynchronously Generating Design Suggestions",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938063591837767"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545664"
        },
        "Preview": {
          "duration": "30",
          "title": "BO as Assistant: Using Bayesian Optimization for Asynchronously Generating Design Suggestions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=XYxl4dwMUUM"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1012",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86202
      ],
      "eventIds": [],
      "abstract": "Many design tasks involve parameter adjustment, and designers often struggle to find desirable parameter value combinations by manipulating sliders back and forth. For such a multi-dimensional search problem, Bayesian optimization (BO) is a promising technique because of its intelligent sampling strategy; in each iteration, BO samples the most effective points considering both exploration (i.e., prioritizing unexplored regions) and exploitation (i.e., prioritizing promising regions), enabling efficient searches. However, existing BO-based design frameworks take the initiative in the design process and thus are not flexible enough for designers to freely explore the design space using their domain knowledge. In this paper, we propose a novel design framework, BO as Assistant, which enables designers to take the initiative in the design process while also benefiting from BO's sampling strategy. The designer can manipulate sliders as usual; the system monitors the slider manipulation to automatically estimate the design goal on the fly and then asynchronously provides unexplored-yet-promising suggestions using BO's sampling strategy. The designer can choose to use the suggestions at any time. This framework uses a novel technique to automatically extract the necessary information to run BO by observing slider manipulation without requesting additional inputs. Our framework is domain-agnostic, demonstrated by applying it to photo color enhancement, 3D shape design for personal fabrication, and procedural material design in computer graphics.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 84713
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 84783
        }
      ]
    },
    {
      "id": 85026,
      "typeId": 12319,
      "title": "TipTrap: A Co-located Direct Manipulation Technique for Acoustically Levitated Content",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938066884378624"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545675"
        },
        "Preview": {
          "duration": "30",
          "title": "TipTrap: A Co-located Direct Manipulation Technique for Acoustically Levitated Content",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=pTS3wIFDfdw"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6666",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86199
      ],
      "eventIds": [],
      "abstract": "Acoustic levitation has emerged as a promising approach for mid-air displays, by using multiple levitated particles as 3D voxels, cloth and thread props, or high-speed tracer particles, under the promise of creating 3D displays that users can see, hear and feel with their bare eyes, ears and hands. However, interaction with this mid-air content always occurred at a distance, since external objects in the display volume (e.g. user's hands) can disturb the acoustic fields and make the particles fall. This paper proposes TipTrap, a co-located direct manipulation technique for acoustically levitated particles. TipTrap leverages the reflection of ultrasound on the users' skin and employs a closed-loop system to create functional acoustic traps 2.1 mm below the fingertips, and addresses its 3 basic stages: selection, manipulation and deselection. We use Finite-Differences Time Domain (FDTD) simulations to explain the principles enabling TipTrap, and explore how finger reflections and user strategies influence the quality of the traps (e.g. approaching direction, orientation and tracking errors), and use these results to design our technique. We then implement the technique, characterizing its performance with a robotic hand setup and finish with an exploration of the ability of TipTrap to manipulate different types of levitated content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": ""
            }
          ],
          "personId": 84940
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Pamplona",
              "institution": "Universidad Pública de Navarra",
              "dsl": ""
            }
          ],
          "personId": 84580
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science/ MSD "
            }
          ],
          "personId": 84665
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Navarre",
              "city": "Pamplona",
              "institution": "Universidad Publica de Navarra",
              "dsl": "UpnaLab"
            }
          ],
          "personId": 84892
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College of London",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84651
        }
      ]
    },
    {
      "id": 85027,
      "typeId": 12319,
      "title": "ForceSight: Non-Contact Force Sensing with Laser Speckle Imaging",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938069677776989"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545622"
        },
        "Preview": {
          "duration": "30",
          "title": "ForceSight: Non-Contact Force Sensing with Laser Speckle Imaging",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=c7U0BBB5QWU"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2189",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86192
      ],
      "eventIds": [],
      "abstract": "Force sensing has been a key enabling technology for a wide range of interfaces such as digitally enhanced body and world surfaces for touch interactions. Additionally, force often contains rich contextual information about user activities and can be used to enhance machine perception for improved user and environment awareness. To sense force, conventional approaches rely on contact sensors made of pressure-sensitive materials such as piezo films/discs or force-sensitive resistors. We present ForceSight, a non-contact force sensing approach using laser speckle imaging. Our key observation is that object surfaces deform in the presence of force. This deformation, though very minute, manifests as observable and discernible laser speckle shifts, which we leverage to sense the applied force. This non-contact force-sensing capability opens up new opportunities for rich interactions and can be used to power user-/environment-aware interfaces. We first built and verified the model of laser speckle shift with surface deformations. To investigate the feasibility of our approach, we conducted studies on metal, plastic, wood, along with a wide variety of materials. Additionally, we included supplementary tests to fully tease out the performance of our approach. Finally, we demonstrated the applicability of ForceSight with several demonstrative example applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84583
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84538
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84729
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84846
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84612
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84685
        }
      ]
    },
    {
      "id": 85028,
      "typeId": 12319,
      "title": "RealityLens: A User Interface for Blending Customized Physical World View into Virtual Reality",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938072613781595"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545686"
        },
        "Preview": {
          "duration": "30",
          "title": "RealityLens: A User Interface for Blending Customized Physical World View into Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=RGGkWFHLUxM"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4005",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86197
      ],
      "eventIds": [],
      "abstract": "Research has enabled virtual reality (VR) users to interact with the physical world by blending the physical world view into the virtual environment. However, current solutions are designed for specific use cases and hence are not capable of covering users' varying needs for accessing information about the physical world. This work presents RealityLens, a user interface that allows users to peep into the physical world in VR with the reality lenses they deployed for their needs. For this purpose, we first conducted a preliminary study with experienced VR users to identify users' needs for interacting with the physical world, which led to a set of features for customizing the scale, placement, and activation method of a reality lens. We evaluated the design in a user study (n=12) and collected the feedback of participants engaged in two VR applications while encountering a range of interventions from the physical world. The results show that users' VR presence tends to be better preserved when interacting with the physical world with the support of the RealityLens interface.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84706
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84773
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 84755
        }
      ]
    },
    {
      "id": 85029,
      "typeId": 12319,
      "title": "ARDW: An Augmented Reality Workbench for Printed Circuit Board Debugging",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938075524644874"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545684"
        },
        "Preview": {
          "duration": "31",
          "title": "ARDW: An Augmented Reality Workbench for Printed Circuit Board Debugging",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WtxfCVtfkRE"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8209",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86194
      ],
      "eventIds": [],
      "abstract": "Debugging printed circuit boards (PCBs) can be a time-consuming process, requiring frequent context switching between PCB design files (schematic and layout) and the physical PCB. To assist electrical engineers in debugging PCBs, we present ARDW, an augmented reality workbench consisting of a monitor interface featuring PCB design files, a projector-augmented workspace for PCBs, tracked test probes for selection and measurement, and a connected test instrument. The system supports common debugging workflows for augmented visualization on the physical PCB as well as augmented interaction with the tracked probes. We quantitatively and qualitatively evaluate the system with 10 electrical engineers from industry and academia, finding that ARDW speeds up board navigation and provides engineers with greater confidence in debugging. We discuss practical design considerations and paths for improvement to future systems.\r\nA video demo of the system may be accessed here: https://youtu.be/RbENbf5WIfc.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 84726
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 84873
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Human-Computer Interaction + Design",
              "dsl": ""
            }
          ],
          "personId": 84545
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84850
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84577
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 84871
        }
      ]
    },
    {
      "id": 85030,
      "typeId": 12319,
      "title": "TangibleGrid: Tangible Web Layout Design for Blind Users",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938079207227422"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545627"
        },
        "Preview": {
          "duration": "30",
          "title": "TangibleGrid: Tangible Web Layout Design for Blind Users",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=BKxexDW1zbQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1812",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86195
      ],
      "eventIds": [],
      "abstract": "We present TangibleGrid, a novel device that allows blind users to understand and design the layout of a web page with real-time tangible feedback. We conducted semi-structured interviews and a series of co-design sessions with blind users to elicit insights that guided the design of TangibleGrid. Our final prototype contains shape-changing brackets representing the web elements and a baseboard representing the web page canvas. Blind users can design a web page layout through creating and editing web elements by snapping or adjusting tangible brackets on top of the baseboard. The baseboard senses the brackets' type, size, and location, verbalizes the information, and renders the web page on the client browser. Through a formative user study, we found that blind users could understand a web page layout through TangibleGrid. They were also able to design a new web layout from scratch without the help of sighted people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84945
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "College of Information Studies"
            }
          ],
          "personId": 84932
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland, College Park",
              "dsl": ""
            }
          ],
          "personId": 84596
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84881
        }
      ]
    },
    {
      "id": 85031,
      "typeId": 12319,
      "title": "FLEX-SDK: An Open-Source Software Development Kit for Creating Social Robots",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938082495582330"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545707"
        },
        "Preview": {
          "duration": "29",
          "title": "FLEX-SDK: An Open-Source Software Development Kit for Creating Social Robots",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=H19xNKsx6uo"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6308",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86200
      ],
      "eventIds": [],
      "abstract": "We present FLEX-SDK: an open-source software development kit that allows creating a social robot from two simple tablet screens. FLEX-SDK involves tools for designing the robot face and its facial expressions, creating screens for input/output interactions, controlling the robot through a Wizard-of-Oz interface, and scripting autonomous interactions through a simple text-based programming interface. We demonstrate how this system can be used to replicate an interaction study and we present nine case studies involving controlled experiments, observational studies, participatory design sessions, and outreach activities in which our tools were used by researchers and participants to create and interact with social robots. We discuss common observations and lessons learned from these case studies. Our work demonstrates the potential of FLEX-SDK to lower the barrier to entry for Human-Robot Interaction research. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84847
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 84703
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 84653
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Human-Centered Design and Engineering"
            }
          ],
          "personId": 84709
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 84894
        }
      ]
    },
    {
      "id": 85032,
      "typeId": 12319,
      "title": "ReCapture: AR-Guided Time-lapse Photography",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938085934891130"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545641"
        },
        "Preview": {
          "duration": "30",
          "title": "ReCapture: AR-Guided Time-lapse Photography",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=3FnbhWRSAGM"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-2624",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86194
      ],
      "eventIds": [],
      "abstract": "We present ReCapture, a system that leverages AR-based guidance to help users capture time-lapse data with hand-held mobile devices. ReCapture works by repeatedly guiding users back to the precise location of previously captured images so they can record time-lapse videos one frame at a time without leaving their camera in the scene. Building on previous work in computational re-photography, we combine three different guidance modes to enable parallel hand-held time-lapse capture in general settings. We demonstrate the versatility of our system on a wide variety of subjects and scenes captured over a year of development and regular use, and explore different visualizations of unstructured hand-held time-lapse data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 84849
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Graphics Lab"
            }
          ],
          "personId": 84639
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 84553
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84566
        }
      ]
    },
    {
      "id": 85033,
      "typeId": 12319,
      "title": "Prolonging VR Haptic Experiences by Harvesting Kinetic Energy from the User",
      "award": "HONORABLE_MENTION",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938088967376957"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545635"
        },
        "Preview": {
          "duration": "30",
          "title": "Prolonging VR Haptic Experiences by Harvesting Kinetic Energy from the User",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=TcZh5PFuX9s"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4096",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86194
      ],
      "eventIds": [],
      "abstract": "We propose a new technical approach to implement untethered VR haptic devices that contain no battery, yet can render on-demand haptic feedback. The key is that via our approach, a haptic device charges itself by harvesting the user’s kinetic energy (i.e., movement)—even without the user needing to realize this. This is achieved by integrating the energy-harvesting with the virtual experience, in a responsive manner. Whenever our batteryless haptic device is about to lose power, it switches to harvesting mode (by engaging its clutch to a generator) and, simultaneously, the VR headset renders an alternative version of the current experience that depicts resistive forces (e.g., rowing a boat in VR). As a result, the user feels realistic haptics that corresponds to what they should be feeling in VR, while unknowingly charging the device via their movements. Once the haptic device’s supercapacitors are charged, they wake up its microcontroller to communicate with the VR headset. The VR experience can now use the recently harvested power for on-demand haptics, including vibration, electrical or mechanical force-feedback; this process can be repeated, ad infinitum. We instantiated a version of our concept by implementing an exoskeleton (with vibration, electrical & mechanical force-feedback) that harvests the user’s arm movements. We validated it via a user study, in which participants, even without knowing the device was harvesting, rated its’ VR experience as more realistic & engaging than with a baseline VR setup. Finally, we believe our approach enables haptics for prolonged uses, especially useful in untethered VR setups, since devices capable of haptic feedback are traditionally only reserved for situations with ample power. Instead, with our approach, a user who engages in hours-long VR and grew accustomed to finding a battery-dead haptic device that no longer works, will simply resurrect the haptic device with their movement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84675
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84857
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84662
        }
      ]
    },
    {
      "id": 85034,
      "typeId": 12319,
      "title": "Look over there! Investigating Saliency Modulation for Visual Guidance with Augmented Reality Glasses",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938091932762182"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545633"
        },
        "Preview": {
          "duration": "30",
          "title": "Look over there! Investigating Saliency Modulation for Visual Guidance with Augmented Reality Glasses",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=92j6ncPG_D8"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-7361",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86203
      ],
      "eventIds": [],
      "abstract": "Augmented Reality has traditionally been used to display digital overlays in real environments. Many AR applications such as remote collaboration, picking tasks, or navigation require highlighting physical objects for selection or guidance. These highlights use graphical cues such as outlines and arrows. Whilst effective, they greatly contribute to visual clutter, possibly occlude scene elements, and can be problematic for long-term use. Substituting those overlays, we explore saliency modulation to accentuate objects in the real environment to guide the user’s gaze. Instead of manipulating video streams, like done in perception and cognition research, we investigate saliency modulation of the real world using optical-see-through head-mounted displays. This is a new challenge, since we do not have full control over the view of the real environment. In this work we provide our specific solution to this challenge, including built prototypes and their evaluation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 84843
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 84805
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Otago Business School"
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Otago Business School"
            }
          ],
          "personId": 84636
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Computer Science"
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84928
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 84863
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Otago",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Department of Information Science"
            },
            {
              "country": "New Zealand",
              "state": "Otago",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Department of Information Science"
            }
          ],
          "personId": 84537
        }
      ]
    },
    {
      "id": 85035,
      "typeId": 12319,
      "title": "Integrating Real-World Distractions into Virtual Reality",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938095468544010"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545682"
        },
        "Preview": {
          "duration": "32",
          "title": "Integrating Real-World Distractions into Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=G9zn40zGyTg"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4373",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85636
      ],
      "eventIds": [],
      "abstract": "With the proliferation of consumer-level virtual reality (VR) devices, users started experiencing VR in less controlled environments, such as in social gatherings and public areas. While the current VR hardware provides an increasingly immersive experience, it ignores stimuli originating from the physical surroundings that distract users from the VR experience. To block distractions from the outside world, many users wear noise-canceling headphones. However, this is insufficient to block loud or transient sounds (e.g., drilling or hammering) and, especially, multi-modal distractions (e.g., air drafts, temperature shifts from an A/C, construction vibrations, or food smells). To tackle this, we explore a new concept, where we directly integrate the distracting stimuli from the user’s physical surroundings into their virtual reality experience to enhance presence. Using our approach, an otherwise distracting wind gust can be directly mapped to the sway of trees in a VR experience that already contains trees. Using our novel approach, we demonstrate how to integrate a range of distractive stimuli into the VR experience, such as haptics (temperature, vibrations, touch), sounds, and smells. To validate our approach, we conducted three user studies and a technical evaluation. First, to validate our key principle, we conducted a controlled study where participants were exposed to distractions while playing a VR game. We found that our approach improved users’ sense of presence, compared to wearing noise-canceling headphones. From these results, we engineered a sensing module that detects a set of simple distractive signals (e.g., sounds, winds, and temperature shifts). We validated our hardware in a technical evaluation and in an out-of-lab study where participants played VR games in an uncontrolled environment. Moreover, to gather the perspective of VR content creators that might one day utilize a system inspired by our findings, we invited game designers to use our approach and collected their feedback and VR designs. Finally, we present design considerations for mapping distracting external stimuli and discuss ethical considerations of integrating real-world stimuli into virtual reality.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84689
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84662
        }
      ]
    },
    {
      "id": 85036,
      "typeId": 12319,
      "title": "Wikxhibit: Using HTML and Wikidata to Author Applications that Link Data Across the Web",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938099075661824"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545706"
        },
        "Preview": {
          "duration": "31",
          "title": "Wikxhibit: Using HTML and Wikidata to Author Applications that Link Data Across the Web",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=CtWq0cOLekQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8973",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86193
      ],
      "eventIds": [],
      "abstract": "Wikidata is a companion to Wikipedia that captures a substantial part of the information about most Wikipedia entities in a machine-readable structured form. In addition to directly representing information from Wikipedia itself, Wikidata also cross-references how additional information about these entities can be accessed through APIs on hundreds of other websites. \r\n\r\nThis trove of valuable information has become a source of numerous domain-specific information presentations on the web, such as art galleries or directories of actors. Developers have created a number of such tools that present Wikidata data, sometimes combined with data accessed through Wikidata's cross-referenced web APIs. However, the creation of these presentations requires significant programming effort and is often impossible for non-programmers.\r\n\r\nIn this work, we empower users, even non-programmers, to create presentations of Wikidata and other sources of data on the web, using only HTML with no additional programming. We present Wikxhibit, a JavaScript library for creating HTML-based data presentations of Wikidata and the other data APIs it cross-references. Wikxhibit allows a user to author plain HTML that, with the addition of a few new attributes, is able to dynamically fetch and display any Wikidata data or its cross-referenced Web APIs. Wikxhibit's JavaScript library uses Wikidata as the bridge to connect all the cross-referenced web APIs, allowing users to aggregate data from multiple Web APIs at once, seamlessly connecting object to object, without even realizing that they are pulling data from multiple websites. We integrate Wikxhibit with Mavo, an HTML language extension for describing web applications declaratively, to empower plain-HTML authors to create presentations of Wikidata.\r\nOur evaluation shows that users, even non-programmers, can create presentations of Wikidata and other sources of web data using Wikxhibit in just 5 minutes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 84648
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 84769
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 84915
        }
      ]
    },
    {
      "id": 85037,
      "typeId": 12319,
      "title": "PSST: Enabling Blind or Visually Impaired Developers to Author Sonifications of Streaming Sensor Data",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938102091370536"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545700"
        },
        "Preview": {
          "duration": "30",
          "title": "PSST: Enabling Blind or Visually Impaired Developers to Author Sonifications of Streaming Sensor Data",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=UDIl9krawxg"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-9149",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86195
      ],
      "eventIds": [],
      "abstract": "We present the first toolkit that equips blind and visually impaired (BVI) developers with the tools to create accessible data displays. Called PSST (Physical computing Streaming Sensor data Toolkit), it enables BVI developers to understand the data generated by sensors from a mouse to a micro:bit physical computing platform. By assuming visual abilities, earlier efforts to make physical computing accessible fail to address the need for BVI developers to access sensor data. PSST enables BVI developers to understand real-time, real-world sensor data by providing control over what should be displayed, as well as when to display and how to display sensor data. PSST supports filtering based on raw or calculated values, highlighting, and transformation of data. Output formats include tonal sonification, nonspeech audio files, speech, and SVGs for laser cutting. We validate PSST through a series of demonstrations and a user study with BVI developers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 84763
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 84827
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridgeshire",
              "city": "Cambridge",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 84658
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 84820
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 84547
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 84575
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 84788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 84635
        }
      ]
    },
    {
      "id": 85038,
      "typeId": 12319,
      "title": "DeltaPen: A Device with Integrated High-Precision Translation and Rotation Sensing on Passive Surfaces",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938105161596938"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545655"
        },
        "Preview": {
          "duration": "30",
          "title": "DeltaPen: A Device with Integrated High-Precision Translation and Rotation Sensing on Passive Surfaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=5fd0B5Blvvo"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4134",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86199
      ],
      "eventIds": [],
      "abstract": "We present DeltaPen, a pen device that operates on passive surfaces without the need for external tracking systems or active sensing surfaces. DeltaPen integrates two adjacent lens-less optical flow sensors at its tip, from which it reconstructs accurate directional motion as well as yaw rotation. DeltaPen also supports tilt interaction using a built-in inertial sensor. A pressure sensor and high-fidelity haptic actuator complements our pen device while retaining a compact form factor that supports mobile use on uninstrumented surfaces. We present a processing pipeline that reliably extracts fine-grained pen translations and rotations from the two optical flow sensors. To asses the accuracy of our translation and angle estimation pipeline, we conducted a technical evaluation in which we compared our approach with ground-truth measurements of participants' pen movements during typical pen interactions. We conclude with several example applications that leverage our device's capabilities. Taken together, we demonstrate novel input dimensions with DeltaPen that have so far only existed in systems that require active sensing surfaces or external tracking.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84860
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84626
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84652
        }
      ]
    },
    {
      "id": 85039,
      "typeId": 12319,
      "title": "SemanticOn: Specifying Content-Based Semantic Conditions for Web Automation Programs",
      "award": "HONORABLE_MENTION",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938108676423752"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545691"
        },
        "Preview": {
          "duration": "31",
          "title": "SemanticOn: Specifying Content-Based Semantic Conditions for Web Automation Programs",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=OmJVaji-GJI"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-1940",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86200
      ],
      "eventIds": [],
      "abstract": "Data scientists, researchers, and clerks often create web automation programs to perform repetitive yet essential tasks, such as data scraping and data entry. However, existing web automation systems lack mechanisms for defining conditional behaviors where the system can intelligently filter candidate content based on semantic filters (e.g., extract texts based on key ideas or images based on entity relationships). We introduce SemanticOn, a system that enables users to specify, refine, and incorporate visual and textual semantic conditions in web automation programs via two methods: natural language description via prompts or information highlighting. Users can coordinate with SemanticOn to refine the conditions as the program continuously executes or reclaim manual control to repair errors. In a user study, participants completed a series of conditional web automation tasks. They reported that SemanticOn helped them effectively express and refine their semantic intent by utilizing visual and textual conditions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84908
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 84930
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 84674
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 84533
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84754
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84678
        }
      ]
    },
    {
      "id": 85040,
      "typeId": 12319,
      "title": "Optimizing the Timing of Intelligent Suggestion in Virtual Reality",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938112275124375"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545632"
        },
        "Preview": {
          "duration": "29",
          "title": "Optimizing the Timing of Intelligent Suggestion in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Q4sFc-WJx4g"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4817",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85636
      ],
      "eventIds": [],
      "abstract": "Intelligent suggestion techniques can enable low-friction selection-based input within virtual or augmented reality (VR/AR) systems. Such techniques leverage probability estimates from a target prediction model to provide users with an easy-to-use method to select the most probable target in an environment. For example, a system could highlight the predicted target and enable a user to select it with a simple click. However, as the probability estimates can be made at any time, it is unclear when an intelligent suggestion should be presented. Earlier suggestions could save a user time and effort but be less accurate. Later suggestions, on the other hand, could be more accurate but save less time and effort. This paper thus proposes a computational framework that can be used to determine the optimal timing of intelligent suggestions based on user-centric costs and benefits. A series of studies demonstrated the value of the framework for minimizing task completion time and maximizing suggestion usage and showed that it was both theoretically and empirically effective at determining the optimal timing for intelligent suggestions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 84563
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84559
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84680
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84720
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84625
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84819
        }
      ]
    },
    {
      "id": 85041,
      "typeId": 12319,
      "title": "MetamorphX: An Ungrounded 3-DoF Moment Display that Changes its Physical Properties through Rotational Impedance Control",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938114934321192"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545650"
        },
        "Preview": {
          "duration": "30",
          "title": "MetamorphX: An Ungrounded 3-DoF Moment Display that Changes its Physical Properties through Rotational Impedance Control",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jqxv95YCnhs"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-9946",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86201
      ],
      "eventIds": [],
      "abstract": "Humans can estimate the properties of wielded objects (e.g., inertia and viscosity) using the force applied to the hand. \r\nWe focused on this mechanism and aimed to represent the properties of wielded objects by dynamically changing the force applied to the hand.\r\nWe propose MetamorphX, which uses control moment gyroscopes (CMGs) to generate ungrounded, 3-degrees of freedom moment feedback.\r\nThe high-response moments obtained CMGs allow the inertia and viscosity of motion to be set to the desired values via impedance control.\r\nA technical evaluation indicated that our device can generate a moment with a 60-ms delay. \r\nThe inertia and viscosity of motion were varied by 0.01 kgm^2 and 0.1 Ns, respectively.\r\nAdditionally, we demonstrated that our device can dynamically change the inertia and viscosity of motion through virtual reality applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84727
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84933
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "the University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84917
        }
      ]
    },
    {
      "id": 85042,
      "typeId": 12319,
      "title": "Wigglite: Low-cost Information Collection and Triage",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938117509615697"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545661"
        },
        "Preview": {
          "duration": "31",
          "title": "Wigglite: Low-cost Information Collection and Triage",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=EzrYm_wzIuc"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-5862",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86193
      ],
      "eventIds": [],
      "abstract": "Consumers conducting comparison shopping, researchers making sense of competitive space, and developers looking for code snippets online all face the challenge of capturing the information they find for later use without interrupting their current flow. In addition, during many learning and exploration tasks, people need to externalize their mental context, such as estimating how urgent a topic is to follow up on, or rating a piece of evidence as a \"pro\" or \"con,\" which helps scaffold subsequent deeper exploration. However, current approaches incur a high cost, often requiring users to select, copy, context switch, paste, and annotate information in a separate document without offering specific affordances that capture their mental context. In this work, we explore a new interaction technique called \"wiggling,\" which can be used to fluidly collect, organize, and rate information during early sensemaking stages with a single gesture. Wiggling involves rapid back-and-forth movements of a pointer or up-and-down scrolling on a smartphone, which can indicate the information to be collected and its valence, using a single, light-weight gesture that does not interfere with other interactions that are already available. Through implementation and user evaluation, we found that wiggling helped participants accurately collect information and encode their mental context with a 58% reduction in operational cost while being 24% faster compared to a common baseline.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84686
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84785
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84939
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Semantic Scholar",
              "dsl": "AI2"
            }
          ],
          "personId": 84784
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 84554
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 84598
        }
      ]
    },
    {
      "id": 85043,
      "typeId": 12319,
      "title": "Integrating Living Organisms in Devices to Implement Care-based Interactions",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018938120877650020"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545629"
        },
        "Preview": {
          "duration": "30",
          "title": "Integrating Living Organisms in Devices to Implement Care-based Interactions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=2G_86J-Yq1c"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-5587",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86192
      ],
      "eventIds": [],
      "abstract": "Researchers have been exploring how incorporating care-based interactions can change the user’s attitude & relationship towards an interactive device. This is typically achieved through virtual care where users care for digital entities. In this paper, we explore this concept further by investigating how physical care for a living organism, embedded as a functional component of an interactive device, also changes user-device relationships. Living organisms differ as they require an environment conducive to life, which in our concept, the user is responsible for providing by caring for the organism (e.g., feeding it). We instantiated our concept by engineering a smartwatch that includes a slime mold that physically conducts power to a heart rate sensor inside the device, acting as a living wire. In this smartwatch, the availability of heart-rate sensing depends on the health of the slime mold—with the user’s care, the slime mold becomes conductive and enables the sensor; conversely, without care, the slime mold dries and disables the sensor (resuming care resuscitates the slime mold). To explore how our living device was perceived by users, we conducted a study where participants wore our slime mold-integrated smartwatch for 9-14 days. We found that participants felt a sense of responsibility, developed a reciprocal relationship, and experienced the organism’s growth as a source of affect. Finally, to allow engineers and designers to expand on our work, we abstract our findings into a set of technical and design recommendations when engineering an interactive device that incorporates this type of care-based relationship. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84858
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84662
        }
      ]
    },
    {
      "id": 85044,
      "typeId": 12319,
      "title": "Bayesian Hierarchical Pointing Models",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939230325583953"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545708"
        },
        "Preview": {
          "duration": "30",
          "title": "Bayesian Hierarchical Pointing Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jl2tUs5TtZk"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3564",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86205
      ],
      "eventIds": [],
      "abstract": "Bayesian hierarchical models are probabilistic models that have hierarchical structures and use Bayesian methods for inferences. In this paper, we extend Fitts' law to be a Bayesian hierarchical pointing model and compare it with the typical pooled pointing models (i.e., treating all observations as the same pool), and the individual pointing models (i.e., building an individual model for each user separately). The Bayesian hierarchical pointing models outperform pooled and individual pointing models in predicting the distribution \\hl{and the mean of pointing movement time, especially when the training data are sparse.} Our investigation also shows that \\hl{both noninformative and weakly informative priors are adequate for modeling pointing actions,} although the weakly informative prior performs slightly better than the noninformative prior when the training data size is small. Overall, we conclude that the expected advantages of Bayesian hierarchical models hold for the pointing tasks. Bayesian hierarchical modeling should be adopted a more principled and effective approach of building pointing models than the current common practices in HCI which use pooled or individual models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84786
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Plainview",
              "institution": "Plainview Old Bethpage John F. Kennedy High School",
              "dsl": ""
            }
          ],
          "personId": 84576
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 84578
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84920
        }
      ]
    },
    {
      "id": 85045,
      "typeId": 12319,
      "title": "SenSequins: Smart Textile Using 3D Printed Conductive Sequins",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939234192724159"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545688"
        },
        "Preview": {
          "duration": "30",
          "title": "SenSequins: Smart Textile Using 3D Printed Conductive Sequins",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PYzkH8WHJHI"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3686",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86204
      ],
      "eventIds": [],
      "abstract": "In this research, we used traditional sequin embroidery as the basis and a 3D printer to expand the design space of sequin materials and structures, by developing a new 2.5D smart conductive sequin textile with multiple sensing and interactions as well as providing users with a customizing system for automated design and manufacturing.\r\n\r\nThrough 3D printing, we have developed a variety of 3D sequins. We used each sequin as an individual design unit to realize various circuit designs and sensing functions by adjusting the design primitives such as conductivity, shape, and arrangement.\r\nWe also designed applications such as motion sensing of body movements, and posture detection of the ankle. In addition, we surveyed user requirements through user testing to optimize the design space.\r\n\r\nThis paper describes the design space, design software, automation, application, and user study of various smart sequin textiles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Keio University ",
              "dsl": ""
            }
          ],
          "personId": 84961
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 84676
        }
      ]
    },
    {
      "id": 85046,
      "typeId": 12319,
      "title": "Detecting Input Recognition Errors and User Errors Using Gaze Dynamics in Virtual Reality",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939237418139708"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545628"
        },
        "Preview": {
          "duration": "30",
          "title": "Detecting Input Recognition Errors and User Errors Using Gaze Dynamics in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=FjP9TZSOgZA"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-7403",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86194
      ],
      "eventIds": [],
      "abstract": "Gesture-based recognition systems are susceptible to input recognition errors and user errors, both of which negatively affect user experiences and can be frustrating to correct. Prior work has suggested that user gaze patterns following an input event could be used to detect input recognition errors and subsequently improve interaction. However, to be useful, error detection systems would need to detect various types of high-cost errors. Furthermore, to build a reliable detection model for errors, gaze behaviour following these errors must be manifested consistently across different tasks. Using data analysis and machine learning models, this research examined gaze dynamics following input events in virtual reality (VR). Across three distinct point-and-select tasks, we found differences in user gaze patterns following three input events: correctly recognized input actions, input recognition errors, and user errors. These differences were consistent across tasks, selection versus deselection actions, and naturally occurring versus experimentally injected input recognition errors. A multi-class deep neural network successfully discriminated between these three input events using only gaze dynamics, achieving an AUC-ROC-OVR score of 0.78. Together, these results demonstrate the utility of gaze in detecting interaction errors and have implications for the design of intelligent systems that can assist with adaptive error recovery.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Reality Labs Research, Meta",
              "dsl": ""
            }
          ],
          "personId": 84587
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84680
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 84923
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84678
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs: Research",
              "dsl": ""
            }
          ],
          "personId": 84625
        }
      ]
    },
    {
      "id": 85047,
      "typeId": 12319,
      "title": "Social Simulacra: Creating Populated Prototypes for Social Computing Systems",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939240907808939"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545616"
        },
        "Preview": {
          "duration": "30",
          "title": "Social Simulacra: Creating Populated Prototypes for Social Computing Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Xv8wwtDhj_I"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8734",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86202
      ],
      "eventIds": [],
      "abstract": "Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer's description of a community’s design---goal, rules, and member personas---and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of \"what if?\" scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models' training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 84938
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 84630
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 84644
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 84723
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 84645
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84876
        }
      ]
    },
    {
      "id": 85048,
      "typeId": 12319,
      "title": "DualVoice: Speech Interaction that Discriminates between Normal and Whispered Voice Input",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939245261508621"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545685"
        },
        "Preview": {
          "duration": "30",
          "title": "DualVoice: Speech Interaction that Discriminates between Normal and Whispered Voice Input",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=5On80Jk-2_Y"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-9948",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86205
      ],
      "eventIds": [],
      "abstract": "Interactions based on automatic speech recognition (ASR) have become widely used, with speech input being increasingly utilized to create documents. However, as there is no easy way to distinguish between commands being issued and text required to be input in speech, misrecognitions are difficult to identify and correct, meaning that documents need to be manually edited and corrected. The input of symbols and commands is also challenging because these may be misrecognized as text letters. To address these problems, this study proposes a speech interaction method called DualVoice, by which commands can be input in a whispered voice and letters in a normal voice. The proposed method does not require any specialized hardware other than a regular microphone, enabling a complete hands-free interaction. The method can be used in a wide range of situations where speech recognition is already available, ranging from text input to mobile/wearable computing. Two neural networks were designed in this study, one for discriminating normal speech from whispered speech, and the second for recognizing whisper speech. A prototype of a text input system was then developed to show how normal and whispered voice can be used in speech text input. Other potential applications using DualVoice are also discussed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 84550
        }
      ]
    },
    {
      "id": 85049,
      "typeId": 12319,
      "title": "Color-to-Depth Mappings as Depth Cues in Virtual Reality",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939248348495992"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545646"
        },
        "Preview": {
          "duration": "29",
          "title": "Color-to-Depth Mappings as Depth Cues in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=XcML2TyBeYU"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-7411",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86203
      ],
      "eventIds": [],
      "abstract": "Despite significant improvements to Virtual Reality (VR) technologies, most VR displays are fixed focus and depth perception is still a key issue that limits the user experience and the interaction performance. To supplement humans' inherent depth cues (e.g., retinal blur, motion parallax), we investigate users' perceptual mappings of distance to virtual objects' appearance to generate visual cues aimed to enhance depth perception. As a first step, we explore color-to-depth mappings for virtual objects so that their appearance differs in saturation and value to reflect their distance. Through a series of controlled experiments, we elicit and analyze users' strategies of mapping a virtual object's hue, saturation, value and a combination of saturation and value to its depth. Based on the collected data, we implement a computational model that generates color-to-depth mappings fulfilling adjustable requirements on confusion probability, number of depth levels, and consistent saturation/value changing tendency. We demonstrate the effectiveness of color-to-depth mappings in a 3D sketching task, showing that compared to single-colored targets and strokes, with our mappings, the users were more confident in the accuracy without extra cognitive load and reduced the perceived depth error by 60.8%. We also implement four VR applications and demonstrate how our color cues can benefit the user experience and interaction performance in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 84730
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 84962
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 84697
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 84822
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 84884
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 84725
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 84809
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 84647
        }
      ]
    },
    {
      "id": 85050,
      "typeId": 12319,
      "title": "We-toon: A Communication Support System between Writers and Artists in Collaborative Webtoon Sketch Revision",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939252140167218"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545612"
        },
        "Preview": {
          "duration": "30",
          "title": "We-toon: A Communication Support System between Writers and Artists in Collaborative Webtoon Sketch Revision",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=sXCcZv6vQJw"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-7013",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86202
      ],
      "eventIds": [],
      "abstract": "We present a communication support system, namely \\textit{We-toon}, that can bridge the webtoon writers and artists during sketch revision (i.e., character design and draft revision). In the highly iterative design process between the webtoon writers and artists, writers often have difficulties in precisely articulating their feedback on sketches owing to their lack of drawing proficiency. This drawback makes the writers rely on textual descriptions and reference images found using search engines, leading to indirect and inefficient communications. Inspired by a formative study, we designed \\textit{We-toon} to help writers revise webtoon sketches and effectively communicate with artists. Through a GAN-based image synthesis and manipulation, \\textit{We-toon} can interactively generate diverse reference images and synthesize them locally on any user-provided image. Our user study with 24 professional webtoon authors demonstrated that \\textit{We-toon} outperforms the traditional methods in terms of communication effectiveness and the writers' satisfaction level related to the revised image. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seongnam, Gyeonggi",
              "institution": "Webtoon AI, NAVER WEBTOON Corp.",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul National University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 84732
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul National University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 84879
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul National University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 84762
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "SEOUL",
              "institution": "NAVER WEBTOON",
              "dsl": "WEBTOON AI"
            }
          ],
          "personId": 84925
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "SeongNam",
              "institution": "Naver Webtoon Ltd",
              "dsl": "Webtoon AI"
            }
          ],
          "personId": 84752
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Yongin-si",
              "institution": "Hankuk University of Foreign Studies",
              "dsl": "Division of Biomedical Engineering"
            }
          ],
          "personId": 84733
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Suwon",
              "institution": "Sungkyunkwan University",
              "dsl": ""
            }
          ],
          "personId": 84615
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul National University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 84888
        }
      ]
    },
    {
      "id": 85051,
      "typeId": 12319,
      "title": "Exploring the Learnability of Program Synthesizers by Novice Programmers",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939255281696768"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545659"
        },
        "Preview": {
          "duration": "30",
          "title": "Exploring the Learnability of Program Synthesizers by Novice Programmers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KcBa3l_J3pY"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4024",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86200
      ],
      "eventIds": [],
      "abstract": "Modern program synthesizers are increasingly delivering on their promise of lightening the burden of programming by automatically generating code, but little research has addressed how we can make such systems learnable to all. In this work, we ask: What aspects of program synthesizers contribute to and detract from their learnability by novice programmers? We conducted a thematic analysis of 22 observations of novice programmers, during which novices worked with existing program synthesizers, then participated in semi-structured interviews. Our findings shed light on how their specific points in the synthesizer design space affect these tools' learnability by novice programmers, including the type of specification the synthesizer requires, the method of invoking synthesis and receiving feedback, and the size of the specification. We also describe common misconceptions about what constitutes meaningful progress and useful specifications for the synthesizers, as well as participants' common behaviors and strategies for using these tools. From this analysis, we offer a set of design opportunities to inform the design of future program synthesizers that strive to be learnable by novice programmers. This work serves as a first step toward understanding how we can make program synthesizers more learnable by novices, which opens up the possibility of using program synthesizers in educational settings as well as developer tooling oriented toward novice programmers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Electrical Engineering and Computer Sciences"
            }
          ],
          "personId": 84799
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Electrical Engineering and Computer Sciences"
            }
          ],
          "personId": 84551
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Electrical Engineering and Computer Sciences"
            }
          ],
          "personId": 84571
        }
      ]
    },
    {
      "id": 85052,
      "typeId": 12319,
      "title": "Synthesis-Assisted Video Prototyping From a Document",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939258595197039"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545676"
        },
        "Preview": {
          "duration": "30",
          "title": "Synthesis-Assisted Video Prototyping From a Document",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=rzpeQwOilPk"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3057",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85638
      ],
      "eventIds": [],
      "abstract": "Video productions commonly start with a script, especially for talking head videos that feature a speaker narrating to the camera. When the source materials come from a written document -- such as a web tutorial, it takes iterations to refine content from a text article to a spoken dialogue, while considering visual compositions in each scene. We propose Doc2Video, a video prototyping approach that converts a document to interactive scripting with a preview of synthetic talking head videos. Our pipeline decomposes a source document into a series of scenes, each automatically creating a synthesized video of a virtual instructor. Designed for a specific domain -- programming cookbooks, we apply visual elements from the source document, such as a keyword, a code snippet or a screenshot, in suitable layouts. Users edit narration sentences, break or combine sections, and modify visuals to prototype a video in our Editing UI. We evaluated our pipeline with public programming cookbooks. Feedback from professional creators shows that our method provided a reasonable starting point to engage them in interactive scripting for a narrated instructional video.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 84712
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 84661
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 84569
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 84599
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 84747
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Google",
              "dsl": "Google Research"
            }
          ],
          "personId": 84931
        }
      ]
    },
    {
      "id": 85053,
      "typeId": 12319,
      "title": "InterWeave: Presenting Search Suggestions in Context Scaffolds Information Search and Synthesis",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939261623484477"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545696"
        },
        "Preview": {
          "duration": "30",
          "title": "InterWeave: Presenting Search Suggestions in Context Scaffolds Information Search and Synthesis",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=FShRt1SLrPc"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8229",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86206
      ],
      "eventIds": [],
      "abstract": "Web search is increasingly used to satisfy complex, exploratory information goals. Exploring and synthesizing information into knowledge can be slow and cognitively demanding due to a disconnect between search tools and sense-making workspaces. Our work explores how we might integrate contextual query suggestions within a person's sensemaking environment. We developed InterWeave a prototype that leverages a human wizard to generate contextual search guidance and to place the suggestions within the emergent structure of a searchers’ notes. To investigate how weaving suggestions into the sensemaking workspace affects a user's search and sensemaking behavior, we ran a between-subjects study (n=34) where we compare InterWeave’s in context placement with a conventional list of query suggestions. InterWeave’s approach not only promoted active searching, information gathering and knowledge discovery, but also helped participants keep track of new suggestions and connect newly discovered information to existing knowledge, in comparison to presenting suggestions as a separate list. These results point to directions for future work to interweave contextual and natural search guidance into everyday work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California",
              "dsl": "Design Lab"
            }
          ],
          "personId": 84954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla ",
              "institution": "University of California, San Diego ",
              "dsl": "UCSD Design Lab "
            }
          ],
          "personId": 84870
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": "Design Lab"
            }
          ],
          "personId": 84582
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": "Dept of Cognitive Science"
            }
          ],
          "personId": 84862
        }
      ]
    },
    {
      "id": 85054,
      "typeId": 12319,
      "title": "RIDS: Implicit Detection of a Selection Gesture Using Hand Motion Dynamics During Freehand Pointing in Virtual Reality",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939265142501446"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545701"
        },
        "Preview": {
          "duration": "31",
          "title": "RIDS: Implicit Detection of a Selection Gesture Using Hand Motion Dynamics During Freehand Pointing in Virtual Reality",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=rQ5_O2VYWZ0"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3299",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85636
      ],
      "eventIds": [],
      "abstract": "Freehand interactions with augmented and virtual reality are grow- ing in popularity, but they lack reliability and robustness. Implicit behavior from users, such as hand or gaze movements, might pro- vide additional signals to improve the reliability of input. In this paper, the primary goal is to improve the detection of a selection gesture in VR during point-and-click interaction. Thus, we propose and investigate the use of information contained within the hand motion dynamics that precede a selection gesture. We built two models that classified if a user is likely to perform a selection gesture at the current moment in time. We collected data during a pointing-and-selection task from 15 participants and trained two models with different architectures, i.e., a logistic regression classifier was trained using predefined hand motion features and a temporal convolutional network (TCN) classifier was trained using raw hand motion data. Leave-one-subject-out cross-validation PR- AUCs of 0.36 and 0.90 were obtained for each model respectively, demonstrating that the models performed well above chance (=0.13). The TCN model was found to improve the precision of a noisy selection gesture by 11.2% without sacrificing recall performance. An initial analysis of the generalizability of the models demonstrated above-chance performance, suggesting that this approach could be scaled to other interaction tasks in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84556
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84680
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84819
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84795
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84720
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc.",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84625
        }
      ]
    },
    {
      "id": 85055,
      "typeId": 12319,
      "title": "Phrase-Gesture Typing on Smartphones",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939268556668968"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545683"
        },
        "Preview": {
          "duration": "31",
          "title": "Phrase-Gesture Typing on Smartphones",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_-Sry4yVvLo"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6445",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86199
      ],
      "eventIds": [],
      "abstract": "We study phrase-gesture typing, a gesture typing method that allows users to type short phrases by swiping through all the letters of the words in a phrase using a single, continuous gesture. Unlike word-gesture typing, where text needs to be entered word by word, phrase-gesture typing enters text phrase by phrase. To demonstrate the usability of phrase-gesture typing, we implemented a prototype called PhraseSwipe. Our system is composed of a frontend interface designed specifically for typing through phrases and a backend phrase-level gesture decoder developed based on a transformer-based neural language model. Our decoder was trained using five million phrases of varying lengths of up to five words, chosen randomly from the Yelp Review Dataset. Through a user study with 12 participants, we demonstrate that participants could type using PhraseSwipe at an average speed of 34.5 WPM with a Word Error Rate of 1.1%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84660
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hubei",
              "city": "Wuhan",
              "institution": "Huazhong University of Science and Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 84585
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84920
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 84608
        }
      ]
    },
    {
      "id": 85056,
      "typeId": 12319,
      "title": "X-Bridges: Designing Tunable Bridges to Enrich 3D Printed Objects' Deformation and Stiffness",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939271983415336"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545710"
        },
        "Preview": {
          "duration": "30",
          "title": "X-Bridges: Designing Tunable Bridges to Enrich 3D Printed Objects' Deformation and Stiffness",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wGDVaKOAGvY"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8505",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85639
      ],
      "eventIds": [],
      "abstract": "Bridges are unique structures appeared in fused deposition modeling (FDM) that make rigid prints flexible but not fully explored. This paper presents X-Bridges, an end-to-end workflow that allows novice users to design tunable bridges that can enrich 3D printed objects' deformable and physical properties. Specifically, we firstly provide a series of deformation primitives (e.g. bend, twist, coil, compress and stretch) with three versions of stiffness (loose, elastic, stable) based on parametrized bridging experiments. Embedding the printing parameters, a design tool is developed to modify the imported 3D model, evaluate optimized printing parameters for bridges, preview shape-changing process, and generate the G-code file for 3D printing. Finally, we demonstrate the design space of X-Bridges through a set of applications that enable foldable, resilient, and interactive shape-changing objects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 84717
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 84804
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 84821
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 84637
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 84792
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "International Design Institute"
            }
          ],
          "personId": 84708
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 84865
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 84839
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 84618
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 84874
        }
      ]
    },
    {
      "id": 85057,
      "typeId": 12319,
      "title": "Augmented Chironomia for Presenting Data to Remote Audiences",
      "award": "HONORABLE_MENTION",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939274734878741"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545614"
        },
        "Preview": {
          "duration": "30",
          "title": "Augmented Chironomia for Presenting Data to Remote Audiences",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=W0l5cTuindE"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-6449",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85638
      ],
      "eventIds": [],
      "abstract": "To facilitate engaging and nuanced conversations around data, we contribute a touchless approach to interacting directly with visualization in remote presentations. We combine dynamic charts overlaid on a presenter's webcam feed with continuous bimanual hand tracking, demonstrating interactions that highlight and manipulate chart elements appearing in the foreground. These interactions are simultaneously functional and deictic, and some allow for the addition of \"rhetorical flourish\", or expressive movement used when speaking about quantities, categories, and time intervals. We evaluated our approach in two studies with professionals who routinely deliver and attend presentations about data. The first study considered the presenter perspective, where 12 participants delivered presentations to a remote audience using a presentation environment incorporating our approach. The second study considered the audience experience of 17 participants who attended presentations supported by our environment. Finally, we reflect on observations from these studies and discuss related implications for engaging remote audiences in conversations about data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 84687
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Surrey",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Art and Technology"
            }
          ],
          "personId": 84900
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 84619
        }
      ]
    },
    {
      "id": 85058,
      "typeId": 12319,
      "title": "ARnnotate: An Augmented Reality Interface for Collecting Custom Dataset of 3D Hand-Object Interaction Pose Estimation",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939277985456289"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545663"
        },
        "Preview": {
          "duration": "30",
          "title": "ARnnotate: An Augmented Reality Interface for Collecting Custom Dataset of 3D Hand-Object Interaction Pose Estimation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=XL28ETVs20w"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-4669",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86194
      ],
      "eventIds": [],
      "abstract": "Vision-based 3D pose estimation has substantial potential in hand-object interaction applications and requires user-specified datasets to achieve robust performance. We propose ARnnotate, an Augmented Reality (AR) interface enabling end-users to create custom data using a hand-tracking-capable AR device. Unlike other dataset collection strategies, ARnnotate first guides a user to manipulate a virtual bounding box and records its poses and the user's hand joint positions as the labels. By leveraging the spatial awareness of AR, the user manipulates the corresponding physical object while following the in-situ AR animation of the bounding box and hand model, while ARnnotate captures the user's first-person view as the images of the dataset. A 12-participant user study was conducted, and the results proved the system's usability in terms of the spatial accuracy of the labels, the satisfactory performance of the deep neural networks trained with the data collected by ARnnotate, and the users' subjective feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 84668
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette ",
              "institution": "Purdue University",
              "dsl": "School of Electrical and Computer Engineering"
            }
          ],
          "personId": 84890
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette ",
              "institution": "Purdue University ",
              "dsl": "School of Mechanical Engineering "
            }
          ],
          "personId": 84530
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 84557
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 84558
        }
      ]
    },
    {
      "id": 85059,
      "typeId": 12319,
      "title": "VRhook: A Data Collection Tool for VR Motion Sickness Research",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939281659678851"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545656"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-3581",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86203
      ],
      "eventIds": [],
      "abstract": "Despite the increasing popularity of VR games, one factor hindering the industry's rapid growth is motion sickness experienced by the users. Symptoms such as fatigue and nausea severely hamper the user experience. Machine Learning methods could be used to automatically detect motion sickness in VR experiences, but generating the extensive labeled dataset needed is a challenging task. It needs either very time consuming manual labeling by human experts or modification of proprietary VR application source codes for label capturing. To overcome these challenges, we developed a novel data collection tool, VRhook, which can collect data from any VR game without needing access to its source code. This is achieved by dynamic hooking, where we can inject custom code into a game's run-time memory to record each video frame and its associated transformation matrices. Using this, we can automatically extract various useful labels such as rotation, speed, and acceleration. In addition, VRhook can blend a customized screen overlay on top of game contents to collect self-reported comfort scores. In this paper, we describe the technical development of VRhook, demonstrate its utility with an example, and describe directions for future research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            }
          ],
          "personId": 84947
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            }
          ],
          "personId": 84573
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Auckland",
              "city": "Auckland",
              "institution": "Auckland Bio engineering Institute, University Of Auckland ",
              "dsl": "Augmented Human Lab"
            }
          ],
          "personId": 84654
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Aucklad",
              "institution": "University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            }
          ],
          "personId": 84796
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Mawson Lakes",
              "institution": "University of South Australia",
              "dsl": "ITMS"
            }
          ],
          "personId": 84728
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 84738
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Facebook",
              "dsl": ""
            }
          ],
          "personId": 84845
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Facebook",
              "dsl": ""
            }
          ],
          "personId": 84701
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Department of Information Systems and Analytics, National University of Singapore",
              "dsl": "Augmented Human Lab"
            }
          ],
          "personId": 84899
        }
      ]
    },
    {
      "id": 85060,
      "typeId": 12319,
      "title": "AUIT – the Adaptive User Interfaces Toolkit for Designing XR Applications",
      "addons": {
        "discord": {
          "type": "url",
          "url": "https://discord.com/channels/734723053036109904/1018939284474052659"
        },
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526113.3545651"
        },
        "Preview": {
          "duration": "30",
          "title": "AUIT â€“ the Adaptive User Interfaces Toolkit for Designing XR Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=AN4buBaJZFQ"
        }
      },
      "isBreak": false,
      "importedId": "uist22a-8196",
      "source": "PCS",
      "trackId": 11865,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86197
      ],
      "eventIds": [],
      "abstract": "Adaptive user interfaces can improve experiences in Extended Reality (XR) applications by adapting interface elements according to the user's context. Although extensive work explores different adaptation policies, XR creators often struggle with their implementation, which involves laborious manual scripting. The few available tools are underdeveloped for realistic XR settings where it is often necessary to consider conflicting aspects that affect an adaptation. We fill this gap by presenting AUIT, a toolkit that facilitates the design of optimization-based adaptation policies. AUIT allows creators to flexibly combine policies that address common objectives in XR applications, such as element reachability, visibility, and consistency. Instead of using rules or scripts, specifying adaptation policies via adaptation objectives simplifies the design process and enables creative exploration of adaptations. After creators decide which adaptation objectives to use, a multi-objective solver finds appropriate adaptations in real-time. A study showed that AUIT allowed creators of XR applications to quickly and easily create high-quality adaptations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84691
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84740
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": "Computational Interaction Lab"
            }
          ],
          "personId": 84903
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84667
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Institute of Visual Computing and Human-Centered Technology"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84567
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 84929
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84634
        }
      ]
    },
    {
      "id": 85401,
      "typeId": 12320,
      "title": "ASTREL: Prototyping Shape-changing Interface with Variable Stiffness Soft Robotics Module",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558733"
        },
        "Poster": {
          "hashSum": "LGDvCJ6CkBDEmncqRW8eCN9e+x/4jCZS/kokBBtOYGQ=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85401/poster/84ddc590-3106-8d33-4132-f5b8165cd901.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85401/poster/60445839-355b-4cbe-c542-4621d9bb24ae.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3077",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Prototyping a shape-changing interface is challenging because it requires knowledge of both electronics and mechanical engineering. In this study, we introduced a prototyping platform using a soft pneumatic artificial muscle(PAMs) and modular 3D printed reinforcement. To facilitate a wide variety of applications we propose six types of reinforcement modules capable of either shape deformation and/or variable stiffness. Users can create an approximate prototype using lego-built modules with magnetic connectors. A modeling toolkit can then be used to recreate and customize the prototype structure. After 3D printing, the shape-changing interface can be assembled by threading the PAMs through holes in the reinforcement. We envision that this prototyping platform can be useful in shape-changing interface exploration, where researchers can create working prototypes easily, rapidly, and at a low cost.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Meguro",
              "institution": "Tokyo Institute of Technology",
              "dsl": "Koike Lab"
            }
          ],
          "personId": 85223
        }
      ]
    },
    {
      "id": 85402,
      "typeId": 12315,
      "title": "Demonstration of Mixels: Fabricating Interfaces using Programmable Magnetic Pixels",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1106",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Tangible Interfaces",
        "Digital Fabrication"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In this demonstration, we present Mixels, programmable magnetic pixels that can be rapidly fabricated using an electromagnetic printhead mounted on an off-the-shelve 3D printer. The ability to program a magnetic material pixel-wise with varying magnetic forces enables Mixels to create new tangible, tactile, and haptic interfaces. To facilitate the creation of interactive objects with Mixels, we provide a user interface that lets users specify the high-level magnetic behavior and that then computes the underlying magnetic pixel assignments and fabrication instructions to program the magnetic surface. Our custom hardware add-on based on an electromagnetic printhead clips onto a standard 3D printer and can both write and read magnetic pixel values from magnetic material. Finally, we highlight the importance of fundamental magnet parameters such as coercivity and remanence in designing a magnetic plotter, and show how leveraging 2D plotting enables a number of interactive applications. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84698
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84882
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84953
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84592
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84861
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84611
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 85403,
      "typeId": 12320,
      "title": "Little Garden: An augmented reality game for older adults to promote body movement",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558731"
        },
        "Poster": {
          "hashSum": "92nuWcjiFPoY0yjYyLSeax5KspcFdfGiFrjipExlfs8=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85403/poster/80a532ba-619a-502d-0997-7392d87bf513.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85403/poster/46baf48d-87bd-ded7-d646-fac9526c278e.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-6101",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Physical activity is one of the most effective ways to help older adults stay healthy, but traditional training methods for older adults use single tasks and are boring, often making it difficult for the elderly to achieve good exercise results. In contrast to existing digital games, games based on augmented reality technology have the potential to promote physical activity in the elderly. This paper presents Little Garden, an interactive augmented-reality game designed for older adults. It uses projective augmented reality technology, physical card manipulation, virtual social scenarios to increase user engagement and motor initiation. The pilot data show that the game system promotes physical engagement and provides a good user experience. We believe that augmented reality technology provides a new approach to interface design for age-appropriate user-interface experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 85370
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 85151
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 85294
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            }
          ],
          "personId": 85334
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Qingdao",
              "institution": "Qingdao University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 85163
        }
      ]
    },
    {
      "id": 85404,
      "typeId": 12320,
      "title": "Puppeteer: Manipulating Human Avatar Actions with Intuitive Hand Gestures and Upper-Body Postures",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558689"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-5098",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "We present Puppeteer, an input prototype system that allows players directly control their avatars through intuitive hand gestures and upper-body postures.\r\nWe selected 17 avatar actions discovered in the pilot study and conducted a gesture elicitation study to invite 12 participants to design best representing hand gestures and upper-body postures for each action.\r\nThen we implemented a prototype system using the MediaPipe framework to detect keypoints and a self-trained model to recognize 17 hand gestures and 17 upper-body postures.\r\nFinally, three applications demonstrate the interactions enabled by Puppeteer.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Graduate Institute of Networking and Multimedia"
            }
          ],
          "personId": 85205
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer science and engineering"
            }
          ],
          "personId": 84895
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Graduate Institute of Networking and Multimedia"
            }
          ],
          "personId": 85345
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 85272
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 84755
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84773
        }
      ]
    },
    {
      "id": 85405,
      "typeId": 12315,
      "title": "Demonstrating iWood: Makeable Vibration Sensor For interactive Plywood ",
      "isBreak": false,
      "importedId": "uist22d-1103",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "iWood is interactive plywood that can sense vibration based on triboelectric effect. As a material, iWood can be used to create furniture and artifacts. Things created using iWood inherit its sensing capability and can detect a variety of user input and activities based on their unique vibration patterns. iWood is makeable, meaning that its sensor survives common woodworking operations, such as sawing, screwing, and nailing. We developed iWood using a layer of triboelectric material sandwiched between two layers of electrodes, each attached to a plywood substrate. Unlike the existing sensors based on triboelectric effect, the electrodes of iWood were designed to stagger with each other to avoid short-circuiting caused by fabrication operations. Through a series of experiments and machine simulations, we carefully chose the size of the electrodes, the type of triboelectric materials, and the bonding method of the sensor layers to optimize the sensitivity and fabrication complexity. We evaluated the sensing performance of our interactive plywood across 4 gestures and 12 daily activities carried out on a table, nightstand, and cutting board, all created using iWood. Our study yielded over 90% accuracies for activity and gesture recognition. We conclude by presenting several fabrication and usage scenarios to demonstrate the unique capability enabled by our interactive plywood.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84760
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 84608
        }
      ]
    },
    {
      "id": 85406,
      "typeId": 12315,
      "title": "Augmented Chironomia for Presenting Data to Remote Audiences",
      "isBreak": false,
      "importedId": "uist22d-1070",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "n/a: fast-track paper demo (not in proceedings)"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "To facilitate engaging and nuanced conversations around data, we contribute a touchless approach to interacting directly with visualization in remote presentations. We combine dynamic charts overlaid on a presenter's webcam feed with continuous bimanual hand tracking, demonstrating interactions that highlight and manipulate chart elements appearing in the foreground. These interactions are simultaneously functional and deictic, and some allow for the addition of \"rhetorical flourish\", or expressive movement used when speaking about quantities, categories, and time intervals.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 84687
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Surrey",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Art and Technology"
            }
          ],
          "personId": 84900
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 84619
        }
      ]
    },
    {
      "id": 85407,
      "typeId": 12315,
      "title": "Touchibo: Multimodal Texture-Changing Robotic Platform for Shared Human Experiences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558643"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1071",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "soft robot",
        "pneumatic",
        "texture",
        "children, human-robot interaction",
        "storytelling",
        "multimodal",
        "inclusion",
        "haptics",
        "emotions",
        "platform",
        "olfaction",
        "conversational dynamics"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Touchibo is a modular robotic platform for enriching interpersonal communication in human-robot group activities, suitable for children with mixed visual abilities. Touchibo incorporates several modalities, including dynamic textures, scent, audio, and light. Two prototypes are demonstrated for supporting storytelling activities and mediating group conversations between children with and without visual impairment. Our goal is to provide an inclusive platform for children to interact with each other, perceive their emotions, and become more aware of how they impact others.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Mechanical and Aerospace Engineering"
            }
          ],
          "personId": 85277
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon ",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 85281
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 85127
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 85347
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Universidade de Lisboa",
              "dsl": ""
            }
          ],
          "personId": 85190
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "INESC-ID, IST"
            }
          ],
          "personId": 85240
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 85363
        }
      ]
    },
    {
      "id": 85408,
      "typeId": 12315,
      "title": "Thermoformable Shell for Repeatable Thermoforming",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558632"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1079",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "3D printing",
        "Thermoforming",
        "Metamaterial",
        "Self-repair"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We propose a thermoformable shell called TF-Shell that allows repeatable thermoforming. Due to the low thermal conductivity of typical printing materials like polylactic acid (PLA), thermoforming 3D printed objects is largely limited. Through embedding TF-Shell, users can thermoform target parts in diverse ways. Moreover, the deformed structures can be restored by reheating. In this demo, we introduce the TF-Shell and demonstrate four thermoforming behaviors with the TF-Shell embedded figure. With our approach, we\r\nenvision bringing the value of hands-on craft to digital fabrication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "The Department of Industrial Design"
            }
          ],
          "personId": 85229
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology",
              "dsl": "Dept. of Industrial Design"
            }
          ],
          "personId": 85144
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 85293
        }
      ]
    },
    {
      "id": 85409,
      "typeId": 12315,
      "title": "Monitoring Muscle Engagement via Electrical Impedance Tomography for Unsupervised Physical Rehabilitation",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1076",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Physical Rehabilitation",
        "Health Sensing",
        "EIT",
        "Muscle Engagement"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In this demo, we present MuscleRehab, a virtual reality rehabilitation system that tracks user's motion via optical motion tracking and user's muscle engagement via electrical impedance tomography (EIT) and visualizes the data on a virtual muscle-skeleton avatar. By deploying MuscleRehab, We investigate if monitoring and visualizing muscle engagement during unsupervised physical rehabilitation improves the execution accuracy of therapeutic exercises by showing users whether they target the right muscle groups. The results indicate that monitoring and visualizing muscle engagement can improve both the therapeutic exercise accuracy for users during rehabilitation, and post-rehabilitation evaluation for physical therapists. We introduce each element of MuscleRehab system, and demonstrate our custom wearable EIT devices with phantom and on-body setups.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 84696
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84565
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84532
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts General Hospital",
              "dsl": ""
            }
          ],
          "personId": 84542
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts General Hospital ",
              "dsl": ""
            }
          ],
          "personId": 84934
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 84797
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 85410,
      "typeId": 12315,
      "title": "ConfusionLens: Dynamic and Interactive Visualization for Performance Analysis of Multiclass Image Classifiers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558631"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1077",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "User Interface",
        "Image Classification",
        "Confusion Matrices",
        "Performance Analysis",
        "Interactive Visualization"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Building higher-quality image classification models requires better performance analysis (PA) methods to help understand their behaviors. We propose ConfusionLens, a dynamic and interactive visualization interface that augments a conventional confusion matrix with focus+context visualization. This interface makes it possible to adaptively provide relevant information for different kinds of PA tasks. Specifically, it allows users to seamlessly switch table layouts among three views (overall view, class-level view, and between-class view) while observing all of the instance images in a single screen. This paper presents a ConfusionLens prototype that supports hundreds of instances and its several extensions to further support practical PA tasks, such as activation map visualization and instance sorting/filtering. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 85307
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Miyagi",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 84780
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 84948
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 84594
        }
      ]
    },
    {
      "id": 85411,
      "typeId": 12315,
      "title": "Demonstration of AUIT – the Adaptive User Interfaces Toolkit for Designing XR Applications",
      "isBreak": false,
      "importedId": "uist22d-1110",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Adaptive user interfaces can improve experiences in Cross Reality (XR) applications by changing interface elements according to the user's context. Although extensive work explores different adaptation policies, XR creators often struggle with their implementation, which involves laborious manual scripting. The few available tools are underdeveloped for realistic XR settings where is often necessary to consider conflicting aspects that affect an adaptation. We fill this gap by presenting AUIT, a toolkit that facilitates the design of optimization-based adaptation policies. AUIT allows creators to flexibly combine policies that address common objectives in XR applications, such as element reachability, visibility, and consistency. In contrast to using rules or scripts, specifying adaptation policies via adaptation objectives simplifies the design process and enables creative exploration of adaptations. After creators decide which objectives to use, a multi-objective solver finds appropriate adaptations in real-time. A study showed that AUIT allowed creators of XR applications to quickly and easily create high-quality adaptations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84691
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84740
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": "Computational Interaction Lab"
            }
          ],
          "personId": 84903
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84667
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Institute of Visual Computing and Human-Centered Technology"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84567
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 84929
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84634
        }
      ]
    },
    {
      "id": 85412,
      "typeId": 12315,
      "title": "Demonstrating NFCStack: Identifiable Physical Building Blocks that Support Concurrent Construction and Frictionless Interaction",
      "isBreak": false,
      "importedId": "uist22d-1074",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "NFC, building blocks, stacking, tangible interaction"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In this paper, we propose NFCStack, which is a physical building block system that supports stacking and frictionless interaction and is based on near-field communication (NFC). This system consists of a portable station that can support and resolve the order of three types of passive identifiable stackable: bricks, boxes, and adapters. The bricks support stable and sturdy physical construction, whereas the boxes support frictionless tangible interactions. The adapters provide an interface between the aforementioned two types of stackable and convert the top of a stack into a terminal for detecting interactions between NFC-tagged objects. In contrast to existing systems based on NFC or radio-frequency identification technologies, NFCStack is portable, supports simultaneous interactions, and resolves stacking and interaction events responsively, even when objects are not strictly aligned. Evaluation results indicate that the proposed system effectively supports 12 layers of rich-ID stacking with the three types of building block, even if every box is stacked with a 6-mm offset. The results also indicate possible generalized applications of the proposed system, including 2.5-dimensional construction. The interaction styles are described using several educational application examples, and the design implications of this research are explained.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84672
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            }
          ],
          "personId": 84793
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84812
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84614
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84760
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84773
        }
      ]
    },
    {
      "id": 85413,
      "typeId": 12320,
      "title": "Exploring Virtual Object Translation in Head-Mounted Augmented Reality for Upper Limb Motor Rehabilitation with Motor Performance and Eye Movement Characteristics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558734"
        },
        "Poster": {
          "hashSum": "ufSqwtrqxWVZ3Euer6Bf/hah85vKU/zSnQtld1kjDpY=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85413/poster/aaeef599-ea15-590b-a6bc-33c0a22a6b20.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85413/poster/8869e425-5d56-3e83-230d-6cfe26b97009.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-4159",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Head-mounted augmented reality (AR) technology is currently employed in upper limb motor rehabilitation, and the degrees of freedom (DoF) of virtual object translation modes become critical for rehabilitation tasks in AR settings. Since motor performance is the primary focus of motor rehabilitation, this study assessed it across different translation modes (1DoF and 3DoF) via task efficiency and accuracy analysis. In addition, eye movement characteristics were used to further illustrate motor performance. This research revealed 1DoF and 3DoF modes showing their own benefits in upper limb motor rehabilitation tasks. Finally, this study recommended selecting or integrating these two translation modes for future rehabilitation task design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "Design School"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Liverpool",
              "institution": "University of Liverpool",
              "dsl": "Department of Civil Engineering and Industrial Design"
            }
          ],
          "personId": 85164
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "Design School"
            }
          ],
          "personId": 85169
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 85209
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 85295
        }
      ]
    },
    {
      "id": 85414,
      "typeId": 12315,
      "title": "InfraredTags Demo: Invisible AR Markers and Barcodes Using Infrared Imaging and 3D Printing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558660"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1072",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "unobtrusive tags",
        "barcodes",
        "3D printing",
        "infrared imaging",
        "computer vision",
        "augmented reality",
        "tracking"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We showcase InfraredTags, which are 2D markers and barcodes imperceptible to the naked eye that can be 3D printed as part of objects, and detected rapidly by low-cost near-infrared cameras. We achieve this by printing objects from an infrared-transmitting filament, which infrared cameras can see through, and by having air gaps inside for the tag's bits, which appear at a different intensity in the infrared image.\r\n\r\nWe built a user interface that facilitates the integration of common tags (QR codes, ArUco markers) with the object geometry to make them 3D printable as InfraredTags. We also developed a low-cost infrared imaging module that augments existing mobile devices and decodes tags using our image processing pipeline. We demonstrate how our method enables various applications, such as object tracking and embedding metadata for augmented reality and tangible interactions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85231
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85351
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85342
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84819
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 85415,
      "typeId": 12320,
      "title": "Sharing Heartbeat: Toward conducting heartrate and speech rhythm through tactile presentation of pseudo-heartbeats",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558704"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3226",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": " Currently, the ongoing COVID-19 pandemic makes physical contact, such as handshakes, difficult. However, physical contact is effective in strengthening the bonds between people.\r\n In this study, we aim to compensate for the physical contact lost during the COVID-19 pandemic by presenting a pseudo-heartbeat through a speaker to reproduce entrainment and the synchronized state of heartbeats induced by physiological synchronization.\r\n We evaluated the effects of the device in terms of speech rhythm and heart rate. \r\nThe experimental results showed that a presentation of 80 BPM significantly reduced the difference in heart rate between the two participants, bringing them closer to a synchronized heart rate state.\r\nThe heart rates of participants were significantly lower when 45 BPM and 80 BPM were presented than when no stimulus was given. \r\nFurthermore, when 45 BPM was presented, the silent periods between conversations were significantly more extended than when no stimulus was given. \r\nThis result indicates that this device can intentionally create the entrainment phenomenon and a synchronized heart rate state, thereby producing the same effect of physical contact communication without contact.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 85271
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 85387
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "College of Engineering Systems,University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 85191
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "College of Policy and Planning Sciences"
            }
          ],
          "personId": 85129
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Engineering, Information and Systems"
            }
          ],
          "personId": 85202
        }
      ]
    },
    {
      "id": 85416,
      "typeId": 12315,
      "title": "Explorations of Wrist Haptic Feedback for AR/VR Interactions with Tasbi",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558658"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1073",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "haptics",
        "wearables",
        "multisensory",
        "bracelet",
        "virtual reality"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Most widespread haptic feedback devices for augmented and virtual reality (AR/VR) fall into one of two categories: simple hand-held controllers with a single vibration actuator, or complex glove systems with several embedded actuators. In this work, we explore haptic feedback on the wrist for interacting with virtual objects. We use Tasbi, a compact bracelet device capable of rendering complex multisensory squeeze and vibrotactile feedback. Leveraging Tasbi's haptic rendering, and using standard visual and audio rendering of a head mounted display, we present several interactions that tightly integrate sensory substitutive haptics with visual and audio cues. Interactions include push/pull buttons, rotary knobs, textures, rigid body weight and inertia, and several custom bimanual manipulations such as shooting an arrow from a bow. These demonstrations suggest that wrist-based haptic feedback substantially improves virtual hand-based interactions in AR/VR compared to no haptic feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 85133
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84819
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta Inc",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 85257
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": ""
            }
          ],
          "personId": 85323
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook, Inc.",
              "dsl": ""
            }
          ],
          "personId": 85393
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 85200
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 85139
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 85254
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 84720
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 85146
        }
      ]
    },
    {
      "id": 85417,
      "typeId": 12320,
      "title": "Fringer: A Finger-Worn Passive Device Enabling Computer Vision Based Force Sensing Using Moiré Fringes",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558706"
        },
        "Poster": {
          "hashSum": "s3+F2wzwSUVW/Swo/b8u8DaD0V+Q+0EkT1r1bb4EUsQ=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85417/poster/6a506702-d562-3faa-5202-41a1e142bcae.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85417/poster/ef30a27f-df24-a31a-fa5a-6480d54c0c6e.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3583",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Despite the importance of utilizing forces when interacting with objects, sensing force interactions without active force sensors is challenging. We introduce Fringer, a finger sleeve that physically visualizes the force to allow a camera to estimate the force without using any active sensors. The sleeve has stripe-pattern slits, a sliding paper with stripe pattern, and a compliant layer that converts force into sliding paper movements. The patterns of the slit and the paper have different frequencies to create Moiré fringes, which can magnify the small displacement caused by the compliant layer compression for  webcams to easily capture such displacement. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville ",
              "institution": "University of Virginia",
              "dsl": ""
            }
          ],
          "personId": 85189
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85238
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85204
        }
      ]
    },
    {
      "id": 85418,
      "typeId": 12320,
      "title": "FullPull : A Stretchable UI to Input Pulling Strength on Touch Surfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558775"
        },
        "Poster": {
          "hashSum": "wRrfKpX9EjqE9yI39vM+68o8O9tA5L5JXqBXtJ6xGKU=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85418/poster/097c0855-5df8-301e-81f0-f23627a072cb.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85418/poster/f758d903-0096-ecfc-3531-4b5745cc8c98.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-5241",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Touch surfaces are used as input interfaces for many devices such as smartphones, tablets, and smartwatches. However, the flexibility of the input surface is low, and the possible input operations are limited to planar ones such as touch and swipe. In contrast, in the field of HCI, there has been much research on increasing the number of input interactions by attaching augmented devices with various physical characteristics to the touch surface. However, most of these interactions are limited to operations where pressure is applied to the input surface. In this study, we propose FullPull, which consists of a rubber tube filled with conductive ink and a suction cup to attach the rubber tube to the surface. FullPull allows users to input pulling depth and strength on the touch surface.  We implemented a prototype FullPull device which can be attached to an existing capacitive touch surface and can be pulled by a user. We then evaluated the accuracy of tensile strength estimation of the implemented device. The results showed that the outflow current value when stretched could be classified into four tensile strength levels.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Aoyama Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 85278
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Aoyama Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 85328
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 85266
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Miyagi",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 84780
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sagamihara",
              "institution": "Aoyama Gakuin University",
              "dsl": "College of Science and Engineering"
            }
          ],
          "personId": 85353
        }
      ]
    },
    {
      "id": 85419,
      "typeId": 12315,
      "title": "Expert Goggles: Detecting and Annotating Visualizations using a Machine Learning Classifier",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558627"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1059",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "just-in-time learning",
        "context-aware learning",
        "data literacy learning"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Data visualizations are an increasingly common way to communicate information online. However, not everyone has the data literacy skills necessary to interpret complex visualizations effectively. We present Expert Goggles, a Chrome browser extension that provides just-in-time guidance to help non-experts interpret the visualizations they encounter on the web. Expert Goggles uses a machine learning classifier to automatically determine the visualization type and uses this context to deliver relevant learning materials. We discuss how this approach to automatically detect the context and provide just-in-time support might transform everyday experiences into informal learning opportunities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Temple University",
              "dsl": "Computer and Information Sciences"
            }
          ],
          "personId": 85142
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Temple University",
              "dsl": "Computer and Information Sciences"
            }
          ],
          "personId": 85629
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Temple University",
              "dsl": "Computer and Information Sciences"
            }
          ],
          "personId": 85395
        }
      ]
    },
    {
      "id": 85420,
      "typeId": 12320,
      "title": "Thumble: One-Handed 3D Object Manipulation Using a Thimble-Shaped Wearable Device in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558703"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-5240",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Conventional controllers or hand-tracking interactions in VR cause hand fatigue while manipulating 3D objects because repetitive wrist rotation and hand movements are often required. As a solution to this inconvenience, we propose Thumble, a novel wearable input device worn on the thumb for modifying the orientation of 3D objects. Thumble can rotate the 3D objects depending on the orientation of the thumb and using the thumb pad as an input surface on which the index finger rubs to control the direction and degree of rotations. Therefore, it requires minimal motion of the wrist and the arm. Through the informal user study, we collected the subjective feedback of users and found that Thumble has less hand movement than a conventional VR controller.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 85242
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Graduate School of Culture Technology"
            }
          ],
          "personId": 85368
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 85152
        }
      ]
    },
    {
      "id": 85421,
      "typeId": 12320,
      "title": "AIx speed: Playback Speed Optimization using Listening Comprehension of Speech Recognition Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558727"
        },
        "Poster": {
          "hashSum": "0p8eSUMpEXXRpY7cS/4gvfMXgSV9gqyveW8twZe0qwg=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85421/poster/cfadc801-5317-e036-024c-f04498a5cb19.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85421/poster/37d53b9c-b719-998a-b631-685e19913ccc.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3859",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "In recent years, more and more time has been spent watching videos for online seminars, lectures, and entertainment. In order to improve time efficiency, people often adjust the playback speed to a speed that suits them best. However, it is troublesome to adjust the optimal speed for each video and even more challenging to change and adjust the speed for each speaker within a single video. Therefore, we propose \"AIx speed,\" a system that maximizes the playback speed within the range where the speech recognition model can recognize and flexibly adjusts the playback speed for the entire video. This system makes it possible to set a flexible playback speed that balances playback time and content comprehension, compared to fixing the playback speed for the entire video.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 85216
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 84550
        }
      ]
    },
    {
      "id": 85422,
      "typeId": 12320,
      "title": "Methods of Gently Notifying Pedestrians of Approaching Objects when Listening to Music",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558728"
        },
        "Poster": {
          "hashSum": "t75zkI5CfaWWHCuUDppjg09JKL4pfvtC0rsXwF7DZ1I=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85422/poster/ce5f06b6-f024-3635-3aee-06935c74d494.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85422/poster/d499dc0a-e489-779c-672e-a6202559d206.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-9350",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Many people now listen to music with earphones while walking, and are less likely to notice approaching people, cars, etc. Many methods of detecting approaching objects and notifying pedestrians have been proposed, but few have focused on low urgency situations or music listeners, and many notification methods are unpleasant. Therefore, in this work, we propose methods of gently notifying pedestrians listening to music of approaching objects using environmental sound. We conducted experiments in a virtual environment to assess directional perception accuracy and comfort. Our results show the proposed method allows participants to detect the direction of approaching objects as accurately as explicit notification methods, with less discomfort.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Aichi",
              "city": "Nagoya",
              "institution": "Nagoya University",
              "dsl": ""
            }
          ],
          "personId": 85198
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Aichi",
              "city": "Nagoya",
              "institution": "Nagoya University",
              "dsl": "Institute of Innovation for Future Society"
            },
            {
              "country": "Japan",
              "state": "Aichi",
              "city": "Nagoya",
              "institution": "Tier IV, Inc.",
              "dsl": ""
            }
          ],
          "personId": 85149
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Aichi",
              "city": "Nagoya",
              "institution": "Nagoya University",
              "dsl": "Graduate School of Informatics"
            }
          ],
          "personId": 85299
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Aichi",
              "city": "Nagoya",
              "institution": "Meijo University",
              "dsl": "Faculty of Urban Science"
            }
          ],
          "personId": 85148
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Aichi",
              "city": "Nagoya",
              "institution": "Nagoya University",
              "dsl": "Institute of Innovation for Future Society"
            },
            {
              "country": "Japan",
              "state": "Aichi",
              "city": "Nagoya",
              "institution": "Tier IV, Inc.",
              "dsl": ""
            }
          ],
          "personId": 85175
        }
      ]
    },
    {
      "id": 85423,
      "typeId": 12315,
      "title": "Using a Dual-Camera Smartphone to Recognize Imperceptible 2D Barcodes Embedded in Videos",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558672"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1067",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Screen-camera Communication",
        "Ubiquitous Computing"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Invisible screen-camera communication is promising in that it does not interfere with the video viewing experience. In the imperceptible color vibration method, which displays two colors of the same luminance alternately at high speed for each pixel, embedded information is decoded by taking the difference between distant frames on the time axis. Therefore, the interframe differences of the original video contents affect the decoding performance. In this study, we propose a decoding method which utilizes simultaneously captured images using a dual-camera smartphone with different exposure times. This allows taking the color difference between the frames that are close to each other on the time axis. The feasibility of this approach is demonstrated through several application examples.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 85168
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 85154
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Bunkyo, Japan",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 85305
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Fukuoka",
              "institution": "Kyushu University",
              "dsl": "Graduate School of Information Science and Electrical Engineering"
            }
          ],
          "personId": 85337
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 85315
        }
      ]
    },
    {
      "id": 85424,
      "typeId": 12315,
      "title": "Demonstrating HingeCore: Laser-Cut Foamcore for Fast Assembly",
      "isBreak": false,
      "importedId": "uist22d-1068",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We demonstrate HingeCore, a novel type of laser-cut 3D structure made from sandwich materials, such as foamcore. The key design element behind HingeCore is what we call a finger hinge, which we produce by laser-cutting foamcore “half-way”. The primary benefit of finger hinges is that they allow for very fast assembly, as they allow models to be assembled by folding and because folded hinges stay put at the intended angle, based on the friction between fingers alone, which eliminates the need for glue or tabs. Finger hinges are also highly robust, with some 5mm foamcore models withstanding 62kg. We present HingeCoreMaker, a stand-alone software tool that automatically converts 3D models to HingeCore layouts, as well as an integration into a 3D modeling tool for laser cutting (Kyub [7]). We have used HingeCoreMaker to fabricate design objects, including speakers, lamps, and a life-size bust, as well as structural objects, such as functional furniture. In our user study, participants assembled HingeCore layouts 2.9x faster than layouts generated using the state-of-the-art for plate-based assembly (Roadkill [1]).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84751
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84570
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 84949
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84774
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84561
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84736
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84936
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 84649
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 84581
        }
      ]
    },
    {
      "id": 85425,
      "typeId": 12315,
      "title": "GANzilla: User-Driven Direction Discovery in Generative Adversarial Networks",
      "isBreak": false,
      "importedId": "uist22d-1101",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Generative Adversarial Network (GAN) is being widely adopted in numerous application areas, such as data preprocessing, image editing, and creativity support. However, GAN's 'black box' nature prevents non-expert users from controlling what data a model generates, spawning a plethora of prior work that focused on algorithm-driven approaches to automatically extract editing directions to control GAN. Complementarily, we propose a GANzilla---a user-driven tool that empowers a user with the classic scatter/gather technique to iteratively discover directions to meet their editing intents. In a work session with 12 participants, GANzilla users were able to discover directions that (i) edited images to match provided examples (closed-ended tasks) and that (ii) met a high-level goal, e.g., making the face happier, while showing diversity across individuals (open-ended tasks).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California Los Angeles",
              "dsl": "Electrical and Computer Engineering, Human Computer Interface Group"
            }
          ],
          "personId": 84891
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 84616
        }
      ]
    },
    {
      "id": 85426,
      "typeId": 12320,
      "title": "Bodyweight Exercise based Exergame to Induce High Intensity Interval Training",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558692"
        },
        "Poster": {
          "hashSum": "qH6uQJz4fLv+Qs5LhWVLgzxhxvEi7j0stQBYZUpFnug=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85426/poster/68f62e80-0cd6-7fdb-6300-a52d77c4abf2.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85426/poster/5baac8ab-af6f-c9dc-bffd-b826a2570de1.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-9751",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Exergames have been proposed as an attractive way of making exercise fun; however, most of them do not reach the recommended intensity. Although HCI research has explored how exergame can be designed to follow High Intensity Interval Training (HIIT) that is effective exercise consisting of intermittent vigorous activity and short rest or low-intensity exercise, there are limited studies on designing bodyweight exercise (BWE) based exergame to follow HIIT. In this paper, we propose BWE based exergame to encourage users to maintain high intensity exercise. Our initial study (n=10) showed that the exergame had a significant effect on enjoyment, while the ratio of incorrect posture (ex., squat) also increased due to participants’ concentration on the exergame, which imply future design implications of BWE based exergames.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Samsung Research",
              "dsl": ""
            }
          ],
          "personId": 85340
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Samsung Electronics",
              "dsl": ""
            }
          ],
          "personId": 85258
        }
      ]
    },
    {
      "id": 85427,
      "typeId": 12320,
      "title": "TrackItPipe: A Fabrication Pipeline To Incorporate Location and Rotation Tracking Into 3D Printed Objects",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558719"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85427/poster/4b22648d-39b1-1082-b606-81b13f0a72b1.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85427/poster/1a41bd9f-688b-4c54-d252-23f3cb2fe501.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-5516",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "The increasing convergence of the digital and physical world creates a growing urgency to integrate 3D printed physical tangibles with virtual environments. A precise position and rotation tracking are essential to integrate such physical objects with a virtual environment. However, available 3D models commonly do not provide tracking support on their composition, which requires modifications by CAD experts. This poses a challenge for users with no prior CAD experience. This work presents TrackItPipe, a fabrication pipeline supporting users by semi-automatically adding different tracking capabilities for 3D printed tangibles tailored to the requirements of the environment. TrackItPipe integrates modifications to the model structure and produces the respective tangibles for 3D printing. In a preliminary study with CAD experts and beginners, we demonstrate the feasibility of TrackItPipe to equip 3D models with tracking capabilities rapidly. We conclude that TrackItPipe lowers the barrier for CAD beginners to create trackable fabricated tangibles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 85170
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": ""
            }
          ],
          "personId": 85226
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": "Saarland Informatics Campus"
            }
          ],
          "personId": 85239
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 85136
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University of Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 85300
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 85187
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 85371
        }
      ]
    },
    {
      "id": 85428,
      "typeId": 12320,
      "title": "DIY Graphics Tab: A Cost-Effective Alternative to Graphics Tablet for Educators",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558718"
        },
        "Poster": {
          "hashSum": "Legk04jklVxxIiH5wzl4QJ2rEaG9oRgBHJAhJBNmbXc=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85428/poster/e27115f4-f307-b01a-2750-c552212ad517.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85428/poster/e967ea67-7410-c5d8-23d3-e98be5ab33c6.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2889",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Recording lectures is a normal task for online educators, and a graphics tablet is a great tool for that. However, it is very expensive for many instructors. In this paper, we propose an alternative called \"DIY Graphics Tab\" that functions largely in the same way as a graphic tab, but requires only a pen, paper, and laptop's webcam. Our system takes images of writings on a paper with a webcam and outputs the contents. The task is not straightforward since there are obstacles, such as hand occlusion, paper movements, lighting, and perspective distortion due to the viewing angle. A pipeline is used that applies segmentation and post-processing for generating appropriate output from input frames. We also conducted user experience evaluations from the teachers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Computer Science"
            },
            {
              "country": "Bangladesh",
              "state": "Dhaka",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science and Technology ",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 85156
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 85358
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 85141
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 85297
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 85263
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Computer Science"
            },
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 85366
        }
      ]
    },
    {
      "id": 85429,
      "typeId": 12315,
      "title": "SpinOcchietto: A Wearable Skin-Slip Haptic Device for Rendering Width and Motion of Objects Gripped Between the Fingertips",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558651"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1063",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Haptics",
        "skin-slip",
        "wearable",
        "Virtual Reality",
        "Interaction Technique"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Various haptic feedback techniques have been explored to enable users to interact with their virtual surroundings using their hands. However, investigation on interactions with virtual objects slipping against the skin using skin-slip haptic feedback is still at its early stages. Prior skin-slip virtual reality (VR) haptic display implementations involved bulky actuation mechanisms and were not suitable for multi-finger and bimanual interactions. As a solution to this limitation, we present SpinOcchietto, a wearable skin-slip haptic feedback device using spinning discs for rendering the width and movement of virtual objects gripped between the fingertips. SpinOcchietto was developed to miniaturize and simplify SpinOcchio, a 6-DoF handheld skin-slip haptic display. With its smaller, lighter, and wearable form factor, SpinOcchietto enables users with a wide range of hand sizes to interact with virtual objects with their thumb and index fingers while freeing the rest of the hand. Users can perceive the speed of virtual objects slipping against the fingertips and can use varying grip strengths to grab and release the objects. Three demo applications were developed to showcase the different types of virtual object interactions enabled by the prototype.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 85152
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 84673
        }
      ]
    },
    {
      "id": 85430,
      "typeId": 12315,
      "title": "Demonstration of Lenticular Objects: 3D Printed Objects with Lenticular Lens Surfaces That Can Change their Appearance Depending on the Viewpoint",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558628"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1064",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "multi-material 3D printing",
        "optics",
        "lenticular lenses",
        "design tools"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We present Lenticular Objects, which are 3D objects that appear differently from different viewpoints. We accomplish this by 3D printing lenticular lenses across the curved surface of objects and computing underlying surface color patterns, which enables to generate different appearances to the user at each viewpoint. \r\nIn addition, we present the Lenticular Objects 3D editor, which takes as input the 3D model and multiple surface textures, i.e. images, that are visible at multiple viewpoints. Our tool calculates the lens placements distribution on the surface of the objects and underlying color pattern. On export, the user can use ray tracing to live preview the resulting appearance from each angle. The 3D model, color pattern, and lenses are then 3D printed in one pass on a multi-material 3D printer to create the final 3D object. We demonstrate our system in practice with a range of use cases that benefits from appearing differently under different viewpoints.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 85130
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 85250
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85280
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85140
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Independent Researcher",
              "dsl": ""
            }
          ],
          "personId": 85312
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 85431,
      "typeId": 12320,
      "title": "Rapid Prototyping Dynamic Robotic Fibers for Tunable Movement",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558696"
        },
        "Poster": {
          "hashSum": "TYw+MUWzJJWIZoZn3C3fCCklfG2Gz8QM8Y50Aj1IaOo=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85431/poster/00a2b99d-1c37-b89c-59ac-e59ca802eede.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85431/poster/53d9812f-6ab2-fe65-6e61-d51e2b143823.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-6568",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Liquid crystal elastomers (LCEs) are promising shape-changing actuators for soft robotics in human–computer interaction (HCI). Current LCE manufacturing processes, such as fiber-drawing, extrusion, and 3D printing, face limitations on form-giving and accessibility. We introduce a novel rapid-prototyping approach for thermo-responsive LCE fiber actuators based on vacuum molding extrusion. Our contributions are threefold, a) a vacuum fiber molding (VFM) machine, b) LCE actuators with customizable fiber shapes c) open-source hackability of the machine. We build and test the VFM machine to generate shape-changing movements from four fiber actuators (pincer, curl, ribbon, and hook), and we look at how these new morphologies bridge towards soft robotic device integration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "TU Eindhoven",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 85251
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "TU Eindhoven",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 85182
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "TU Eindhoven",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 85285
        }
      ]
    },
    {
      "id": 85432,
      "typeId": 12320,
      "title": "Echofluid: An Interface for Remote Choreography Learning and Co-creation Using Machine Learning Techniques",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558708"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-6005",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Born from physical activities, dance carries beyond mere body movement. Choreographers interact with audiences’ perceptions through the kinaesthetics, creativity, and expressivity of whole-body performance, inviting them to construct experience, emotion, culture, and meaning together. Computational choreography support can bring endless possibilities into this one of the most experiential and creative artistic forms. While various interactive and motion technologies have been developed and adopted to support creative choreographic processes, little work has been done in exploring incorporating machine learning in a choreographic system, and few remote dance teaching systems in particular have been suggested. In this exploratory work, we proposed Echofluid-a novel AI-based choreographic learning and support system that allows student dancers to compose their own AI models for learning, evaluation, exploration, and creation. In this poster, we present the design, development and ongoing validation process of Echofluid, and discuss the possibilities of applying machine learning in collaborative art and dance as well as the opportunities of augmenting interactive experiences between the performers and audiences with emerging technologies.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 85150
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Center for Human Computer Interaction"
            }
          ],
          "personId": 85391
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 85283
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 85137
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 85321
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 85177
        }
      ]
    },
    {
      "id": 85433,
      "typeId": 12315,
      "title": "Shadowed Speech: an Audio Feedback System which Slows Down Speech Rate",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558640"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1007",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Computer-Enhanced Interaction",
        "Delayed Auditory Feedback"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In oral communication, it is important to speak at a speed appropriate for the situation. However, we need a lot of training in order to control our speech rate as intended. This paper proposes a speech rate control system which enables the user to speak at a pace closer to the target rate using Delayed Auditory Feedback (DAF). We implement a prototype and confirm that the proposed system can slow down the speech rate when the user speaks too fast without giving any instructions to the speaker on how to respond to the audio feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chiyoda-ku",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 85165
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chiyoda-ku",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 85335
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chiyoda-ku",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 85286
        }
      ]
    },
    {
      "id": 85434,
      "typeId": 12320,
      "title": "HomeView: Automatically Building Smart Home Digital Twins With Augmented Reality Headsets",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558709"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85434/poster/439d51c5-db90-56f0-65e0-12f678ec10a7.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85434/poster/12977c22-1f16-8dbc-18ee-48df1881c8b0.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-8423",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Digital twins have demonstrated great capabilities in the industrial setting, but the cost of building them prohibits their usage in home environments.\r\nWe present HomeView, a system that automatically builds and maintains a digital twin using data from Augmented Reality (AR) headsets and Internet of Things (IoT) devices. We evaluated the system in a simulator and found it performs better than the baseline algorithm. The user feedback on programming IoT devices also suggests that contextual information rendered by HomeView is preferable to text descriptions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": ""
            }
          ],
          "personId": 85174
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 85211
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 85275
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 85308
        }
      ]
    },
    {
      "id": 85435,
      "typeId": 12315,
      "title": "M&M: Molding and Melting Method Using a Replica Diffraction Grating Film and a Laser for Decorating Chocolate with Structural Color",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558642"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1005",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "human-chocolate interaction",
        "structural color",
        "digital fablication"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Chocolate is a great food loved around the world.\r\nMethods to decorate chocolate with patterns drawn in structural colors have been developed; however, the methods which requires precision molds with nanoscale processing takes a lot of cost and time.\r\nIn this paper, I propose a new method to decorate chocolate with structural color using a laser engraving machine and a replica diffraction grating film.\r\nThe proposed method is composed of two simple steps: 1) molding chocolate on a replica diffraction grating film and 2) melting chocolate with structural color with a laser to draw a design.\r\nThe proposed method allows creation of chocolates decorated with structural color with only a simple manufacturing process and inexpensive equipment without special precision molds.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chiyoda-ku",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 85335
        }
      ]
    },
    {
      "id": 85436,
      "typeId": 12315,
      "title": " CircuitAssist: Automatically Dispensing Electronic Components to Facilitate Circuit Building ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558671"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1006",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "personal fabrication",
        "circuit assembly",
        "actuation",
        "education"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "When learning how to build circuits, one of the challenges novice makers face is how to identify the components needed for the circuit. Many makerspaces are stocked with a variety of electronic components that look visually similar or have similar names. Thus, novice makers may pick the wrong component, which creates a non-functional circuit although the wires are correctly connected. To address this issue, we present CircuitAssist, an actuated electronics component shelf connected to a tutorial system that dispenses electronic components for the maker in the order that they occur in the tutorial. The shelf contains dispensers for each component type with a custom release mechanism actuated by a servo motor that dispenses one component at a time. Makers can work with CircuitAssist by either following one of the provided tutorials or by directly selecting a component they need from the user interface. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Belmont",
              "institution": "Carlmont High School",
              "dsl": ""
            }
          ],
          "personId": 85289
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Hoboken",
              "institution": "Stevens Institute of Technology ",
              "dsl": "Electrical and Computer Engineering/Schafer School of Engineering"
            }
          ],
          "personId": 85260
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 85437,
      "typeId": 12315,
      "title": "Flaticulation: Laser Cutting Joints with Articulated Angles",
      "isBreak": false,
      "importedId": "uist22d-1003",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Fabrication"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We present Flaticulation, a method to laser cut joints that clutch two cut-in-place flat boards at designated articulated angles. We discover special T-patterns added on the shared edge of two pieces allowing them to be clutched at a bending angle. We analyze the structure and propose a parametric model regarding the T-pattern under laser cutting to predict the joint articulated angle. We validate our proposed model by measuring real prototypes and conducting stress-strain analysis to understand their structural strength. Finally, we provide a user interface for our example applications, including fast assembling unfolded 3D polygonal models and adding detent mechanisms for functional objects such as a mouse and reconfigurable objects such as a headphone.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84834
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84848
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84856
        }
      ]
    },
    {
      "id": 85438,
      "typeId": 12315,
      "title": "Demonstrating Verso: Extending Computational Notebooks for Exploratory Digital Fabrication",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558644"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1092",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Makers from increasingly diverse backgrounds use digital fabrication machines to explore novel design spaces. However, software tools for fabrication are designed primarily for replication-based tasks; programming machines for bespoke applications while accounting for physical contingencies remains challenging. To better support exploratory fabrication, we present Verso, a proof-of-concept extension to computational notebooks.\r\nVerso affords graphical control via modules that result in continuous feedback, letting makers fluidly view and iterate on their workflows. Modules also provide controlled synchronization between code and external digital and physical processes while preserving a clear flow of data. Toolpath stylesheets (TSS) translate machine instructions into task-specific visualizations that can be projected into the machine's physical work space. To demonstrate our approach, we synthesize three design goals and propose three example exploratory workflows for subtractive manufacturing, materials science, and biology.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85330
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85296
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85369
        }
      ]
    },
    {
      "id": 85439,
      "typeId": 12315,
      "title": "Hands-On: Using Gestures to Control Descriptions of a Virtual Environment for People with Visual Impairments",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558669"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1090",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Virtual Reality",
        "Visual Impairments",
        "Accessibility",
        "Audio Feedback",
        "Haptic Feedback",
        "Perception",
        "Hand Gestures"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Virtual reality (VR) uses three main senses to relay information: sight, sound, and touch. People with visual impairments (PVI) rely primarily on auditory and haptic feedback to receive information in VR. While researchers have explored several approaches to make navigation and perception of objects more accessible in VR, none of them offer a natural way to request descriptions of objects, nor control of the flow of auditory information. In this demonstration, we present a haptic glove that PVI can use to request object descriptions in VR with their hands through familiar hand gestures. We contribute designs for a set of hand gestures that allow PVI to interactively get descriptions of the VR environment. We plan to conduct an user study where we will have PVI interact with a VR environment using these gestures to request audio descriptions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech, Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 85179
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 85192
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Community College of Baltimore County",
              "dsl": ""
            }
          ],
          "personId": 85241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 85185
        }
      ]
    },
    {
      "id": 85440,
      "typeId": 12315,
      "title": "CodeToon: Story Ideation, Auto Comic Generation, and Structure Mapping for Code-Driven Storytelling",
      "isBreak": false,
      "importedId": "uist22d-1012",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "code-driven storytelling",
        "comics",
        "coding strip",
        "authoring tool"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Recent work demonstrated how we can design and use coding strips, a form of comic strips with corresponding code, to enhance the teaching and learning in programming. However, creating coding strips is a creative, time-consuming process. Creators have to generate stories from code (code -> story) and design comics from stories (story -> comic). We contribute CodeToon, a comic authoring tool that facilitates this code-driven storytelling process with two mechanisms: (1) story ideation from code using metaphor and (2) automatic comic generation from the story. We conducted a two-part user study that evaluates the tool and participants’ generated comics to test whether CodeToon facilitates the authoring process and helps generate quality comics. Our results show that CodeToon helps users create accurate, informative, and useful coding strips in a significantly shorter time. Overall, this work contributes methods and design guidelines for code-driven storytelling and opens up new opportunities for using art to support computer science education.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "HCI Lab"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Design Lab"
            }
          ],
          "personId": 84825
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 84564
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            }
          ],
          "personId": 84748
        }
      ]
    },
    {
      "id": 85441,
      "typeId": 12320,
      "title": "RemoconHanger: Making Head Rotation in Remote Person using the Hanger Reflex",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558700"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85441/poster/3b890274-7320-34a8-46cd-d3066e11bd72.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85441/poster/0127d9d2-6dfd-360a-892e-571b1c5f7f51.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-9045",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "For remote collaboration, it is essential to intuitively grasp the situation and spatial location. However, the difficulty in grasping information about the remote user's orientation can hinder remote communication. For example, if a remote user turns his or her head to the right to operate a device on the right, and this sensation cannot be shared, the image sent by the remote user suddenly appears to flow laterally, and it will lose the positional relationship like Figure 1 (left). Therefore, we propose a device using the \"hanger reflex\" to experience the sensation of head rotation intuitively. The \"hanger reflex\" is a phenomenon in which the head turns unconsciously when a wire hanger is placed on the head. It has been verified that the sensation of turning is produced by the distribution of pressure exerted by a device worn on the head. This research aims to construct a mechanism to verify its effectiveness for telecommunication that can unconsciously experience the remote user's rotation sensation using the hanger reflex phenomenon. An inertial measurement unit(IMU) grasps the remote user's rotation information like Figure 1 (right).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Graduate School of Interdisciplinary Information Studies",
              "dsl": "The University of Tokyo"
            }
          ],
          "personId": 85157
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 85362
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 84550
        }
      ]
    },
    {
      "id": 85442,
      "typeId": 12320,
      "title": "HapticLever: Kinematic Force Feedback using a 3D Pantograph",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558736"
        },
        "Poster": {
          "hashSum": "mRyaEiMTKpJespkPfroxGxi+VtaAGK0W5871j5czfsg=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85442/poster/13f336da-7405-d3e7-8fd6-c78fdbd144e7.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85442/poster/d568bbea-3eb2-a1f9-c237-0551c2556a8b.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-4338",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "HapticLever is a new kinematic approach for VR haptics which uses a 3D pantograph to stiffly render large-scale surfaces using small-scale proxies.\r\nThe HapticLever approach does not consume power to render forces, but rather puts a mechanical constraint on the end effector using a small-scale proxy surface.\r\nThe HapticLever approach provides stiff force feedback when the user interacts with a static virtual surface, but allows the user to move their arm freely when moving through free virtual space.\r\nWe present the problem space, the related work, and the HapticLever design approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 85378
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 85627
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84611
        }
      ]
    },
    {
      "id": 85443,
      "typeId": 12320,
      "title": "Early Usability Evaluation Of A Relational Agent For The COVID-19 Pandemic",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558702"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85443/poster/7aa5ee71-7b1e-dbd4-eec1-3fd20ef51949.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85443/poster/9c20c97b-ee63-c1c7-1211-7bef848cd5bc.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-7607",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Relational agents (RAs) have shown effectiveness in various health interventions with and without healthcare professionals (HCPs) and hospital facilities. RAs have not been widely researched in COVID-19 context, although they can give health interventions during the pandemic. Addressing this gap,  this work presents an early usability evaluation of a prototypical RA, which is iteratively designed and developed in collaboration with infected patients (n=21) and two groups of HCPs (n=19, n=16) to aid COVID-19 patients at various stages about four main tasks: testing guidance, support during self-isolation, handling emergency situations, and promoting post-infection mental well-being. The prototype obtained an average score of 58.82 on the system usability scale (SUS) after being evaluated by 98 people. This result implies that the suggested design still needs to be improved for greater usability and adoption.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "School of Computing and Informatics"
            }
          ],
          "personId": 85172
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "School of Computing and Informatics"
            }
          ],
          "personId": 85253
        }
      ]
    },
    {
      "id": 85444,
      "typeId": 12315,
      "title": "Interactive 3D Zoetrope with a Strobing Flashlight",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558663"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1013",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "animation",
        "3D printing",
        "zoetrope",
        "fabrication"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We propose a 3D printed zoetrope mounted on a bike wheel where users can watch the 3D figures come to life in front of their eyes. Each frame of our animation is a 9 by 16 cm 3D fabricated diorama containing a small scene. A strobed flashlight synced with the spinning of the wheel shows the viewer each frame at just the right time, creating the illusion of 3D motion. The viewer can hold and shine the flashlight into the scene, illuminating each frame from their own point of view. Our zoetrope is modular and can have different 16 frame animations substituted in and out for fast prototyping of many cinematography, fabrication, and strobe lighting techniques. Our interactive truly 3D movie experience will push the zoetrope format to tell more complex stories and better engage viewers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85400
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85628
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 85360
        }
      ]
    },
    {
      "id": 85445,
      "typeId": 12315,
      "title": "Knitted Force Sensors",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558656"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1098",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "knitting",
        "spacer fabric",
        "pressure sensor",
        "tubular knit",
        "strain sensor",
        "resistive sensor",
        "textile interface",
        "e-textiles"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In this demo, we present two types of knitted resistive force sensors for both pressure and strain sensing. They can be manufactured ready-made on a two-bed weft knitting machine, without requiring further post-processing steps. Due to their softness, elasticity, stretch-ability, and breath-ability our sensors provide an appealing haptic experience. We show their working principle, discuss their advantages and limitations, and elaborate on different areas of application. They are presented as standalone demonstrators, accompanied by exemplary applications for an easier understanding of their haptic and sensing capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Media Interaction Lab"
            }
          ],
          "personId": 85309
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Media Interaction Lab"
            }
          ],
          "personId": 85398
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Media Interaction Lab"
            }
          ],
          "personId": 85354
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Media Interaction Lab"
            }
          ],
          "personId": 84960
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": "Media Interaction Lab"
            }
          ],
          "personId": 84743
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bozen-Bolzano",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 84765
        }
      ]
    },
    {
      "id": 85446,
      "typeId": 12315,
      "title": "A bonding technique for electric circuit prototyping using conductive transfer foil and soldering iron",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558670"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1097",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "circuit prototyping",
        "bonding",
        "transfer foil",
        "3D printer"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Several electric circuit prototyping techniques have been proposed.\r\nWhile most focus on creating the conductive traces, we focus on the bonding technique needed for this kind of circuit.\r\nOur technique is an extension of existing work in that we use the traces themselves as the bonding material for the components. \r\nThe bonding process is not soldering but yields joints of adequate connectivity.\r\nA hot soldering iron is used to activate the traces and bond the component to the circuit.\r\nSimple circuits are created on MDF, paper, and acrylic board to show the feasibility of the technique.\r\nIt is also confirmed that the resistance of the contact points is sufficiently low.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "東京都",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": "School"
            }
          ],
          "personId": 85397
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 85138
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 85252
        }
      ]
    },
    {
      "id": 85447,
      "typeId": 12315,
      "title": "Music Scope Pad",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558647"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1095",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "This paper describes a novel video selecting interface that enables us to select videos without having to click a mouse or touch a screen. Existing video players enable us to see and hear only one video at a time, and thus we have to play pieces individually to select the one we want to hear from numerous new videos such as music videos, which involves a large number of mouse and screen-touch operations. The main advantage of our video selecting interface is that it detects natural movements, such as head or hand movements when users are listening to sounds and they can focus on a particular sound source that they want to hear. By moving their head left or right, users can hear the source from a frontal position as the tablet detects changes in the direction they are facing. By putting their hand behind their ear, users can focus on a particular sound source.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN",
              "dsl": ""
            }
          ],
          "personId": 85155
        }
      ]
    },
    {
      "id": 85448,
      "typeId": 12320,
      "title": "Transtiff: A Stick Interface with Various Stiffness by Artificial Muscle Mechanism",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558777"
        },
        "Poster": {
          "hashSum": "VJCStCl/QVyMUNdxIEC5aDhGnK49Xsrqezzo+72tUhY=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85448/poster/d4299bed-0893-892e-e2a3-a0cfaaa1b87c.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85448/poster/8ed92934-44d8-3ddc-ee45-4da10037b861.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3483",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "We manipulate stick objects, such as chopsticks and pens in our daily life. The senses of the human hand are extremely sensitive and can acquire detailed information. By perceiving changes in the feel of the finger, we perceive the characteristic of the object when we manipulate sticks, such as pens and brushes. Therefore, we can extend the tactile experience of touching something by controlling a stick's grasping sensation. In this study, we propose Transtiff which has a joint that changes its stiffness dynamically to a stick object that generally cannot bend. We use a piston mechanism that uses a small motor to compress the liquid in a flexible tube to control the stiffness of the joint.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa, Kawasaki",
              "institution": "Aoyama Gakuin University",
              "dsl": "Aoyama Gakuin University, Itoh Lab"
            }
          ],
          "personId": 85180
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Aoyama Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 85328
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sagamihara",
              "institution": "Aoyama Gakuin University",
              "dsl": "College of Science and Engineering"
            }
          ],
          "personId": 85353
        }
      ]
    },
    {
      "id": 85449,
      "typeId": 12315,
      "title": "Demo : Prototyping Soft Devices with Interactive Bioplastics",
      "isBreak": false,
      "importedId": "uist22d-1113",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "bioplastics",
        "biomaterials",
        "do-it-yourself",
        "DIY",
        "sustainability"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "This demo accompanies the paper Prototyping Soft Devices with Interactive Bioplastics and showcases the paper's contributions. During this demo, participants will see and touch the three types of conductive bioplastic materials presented in the paper, their non-conductive forms, and other variations. A live fabrication of one of our sensors (e.g., conductive bioplastic foam pressure sensor) will be performed. The participants will be able to touch it and test it right away. Finally, to illustrate our material's ‘life-cycle’, which includes disassembly, reuse, and remelting, some of the samples will be remelted and reused to create bioplastic artworks (e.g., colorful sheets for decorative design).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 84955
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris La Défense",
              "institution": "Léonard de Vinci Pôle Universitaire, Research Center",
              "dsl": ""
            }
          ],
          "personId": 84670
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85255
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris La Défense",
              "institution": " Research Center",
              "dsl": "Léonard de Vinici Pôle Universitaire"
            }
          ],
          "personId": 84759
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 84664
        }
      ]
    },
    {
      "id": 85450,
      "typeId": 12315,
      "title": "Gesture-aware Interactive Machine Teaching with In-situ Object Annotations",
      "isBreak": false,
      "importedId": "uist22d-1081",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Interactive Machine Teaching (IMT) systems allow non-experts to easily create Machine Learning (ML) models. However, existing vision-based IMT systems either ignore annotations on the objects of interest or require users to annotate in a post-hoc manner. Without the annotations on objects, the model may focus on learning features unrelated to the objects. Post-hoc annotations cause additional workload, which diminishes the usability of the overall model building process. In this paper, we develop LookHere, which integrates in-situ object annotations into vision-based IMT. LookHere exploits users' deictic gestures to segment the objects of interest in real time. This segmentation information can be additionally used for training. To achieve the reliable performance of this object segmentation, we utilize our custom dataset called HuTics, including 2040 front-facing images of deictic gestures toward various objects by 170 people. The quantitative results of our user study showed that participants were 16.3 times faster in creating a model with our system compared to a standard IMT system with a post-hoc annotation process while demonstrating comparable accuracies. Additionally, models created by our system showed a significant accuracy improvement (\\Delta mIoU=0.466) in segmenting the objects of interest compared to those without annotations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Interactive Intelligent Systems Lab."
            }
          ],
          "personId": 84791
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84603
        }
      ]
    },
    {
      "id": 85451,
      "typeId": 12315,
      "title": "A Triangular Actuating Device Stand that Dynamically Adjusts Mobile Screen's Position",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558637"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1082",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "mobile device stand",
        "shape-changing interface",
        "ergonomics"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "This demo presents a triangular actuating device stand that can automatically adjust the height and tilt angle of a mounted mobile device (e.g., smartphone) to adapt to the user's varying interaction needs (e.g., touch, browsing, and viewing). We employ a unique mechanism to deform the stand's triangular shape with two extendable reel actuators, which enables us to reposition the mobile screen mounted on the hypotenuse side. Each actuator is managed by the mobile device and controls the height and base of the stand's triangular shape, respectively. To demonstrate the potential of our new actuating device stand, we present two types of interaction scenarios: manual device repositioning based on the user's postures or gestures captured by the device's front camera and automatic device repositioning that adapt to the on-screen contents the user will interact with (i.e., touch-based menus, web browsers, illustrator, and video viewers).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 85221
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 84948
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Miyagi",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 84780
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 84594
        }
      ]
    },
    {
      "id": 85452,
      "typeId": 12320,
      "title": "Involuntary Exhalation Control by Facial Vibration",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558693"
        },
        "Poster": {
          "hashSum": "68DVoGHSpF0UoOfmj5HAIxoFXmYaZyz3h5tyR6H4Gxs=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85452/poster/fca149ba-fb98-b3d7-0007-db7ecb836124.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85452/poster/453b91f3-d1fa-0712-cff1-3de469bec988.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2149",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Breathing affects physical and mental health as well as skills in playing sports and musical instruments.\r\nPrevious studies proposed various methods using sensory stimuli to assist users with controlling their breathing voluntarily by paying attention to it.\r\nHowever, focusing on breathing is difficult when they play sports or instruments because there are many factors to focus on.\r\nTherefore, we propose a wearable system that can control the user's exhalation involuntarily by facial vibration; pushing air from the cheeks independent of the user’s voluntary breathing.\r\nOur system can control the exhaled air velocity and duration by changing the frequency, amplitude, and duration of the facial vibration.\r\nWe consider our system will help novices acquire advanced skills in playing wind instruments like circular breathing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokosuka",
              "institution": "NTT Human Informatics Laboratories, NTT Corporation",
              "dsl": ""
            }
          ],
          "personId": 85143
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 84866
        }
      ]
    },
    {
      "id": 85453,
      "typeId": 12315,
      "title": "Designing a Hairy Haptic Display using 3D Printed Hairs and Perforated Plates",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558655"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1087",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Digital fabrication",
        "3D printing",
        "haptic device",
        "hairy display"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Haptic displays that can convey various material sensations and physical properties on conventional 2D displays or in virtual reality (VR) environments, have been widely explored in the field of human-computer interaction (HCI). We introduce a fabrication technique of haptic apparatus using 3D printed hairs, which can stimulate the users' sensory perception with hair-like bristles mimicking furry animals. Design parameters that determine 3D printed hair's properties such as length, density, and direction, can affect the stiffness and roughness of the contact area between the hair tip and the user's sensory receptors on their skin, thus changing stimulation patterns. To further explore the expressivity of this apparatus, we present a haptic display built with controlling mechanisms. The device is constructed by threading many 3D printed hairs through a perforated plate, manipulating the length and direction of hairs via the connected inner actuator. We present the design specifications including printing parameters, assembly, and electronics through a demonstration of prototypes, and future works.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 85217
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 84531
        }
      ]
    },
    {
      "id": 85454,
      "typeId": 12320,
      "title": "TaskScape: Fostering Holistic View on To-do List With Tracking Plan and Emotion",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558720"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-4209",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Despite advancements with intelligence and connectivity in the workspace, productivity tools, such as to-do list applications, still, measure workers' performance by a binary state---completed, yet completed, and thus the number of tasks completed. Such quantitative measurements can often overlook human values and individual well-being.  While concepts such as positive computing and digital well-being are on the rise in the HCI community, few systems have been proposed to effectively integrate holistic considerations for mental and emotional well-being into productivity tools. In this work, we depart from the classic task list management tool and explore the construction of well-being-centered to-do list software. We propose a task management system–TaskScape—, which allow users to have awareness on the following two aspects: (1) how they plan and complete tasks and (2) how they feel towards their work.  With the proposed system, we will investigate if having holistic view on their tasks can facilitate reflection on what they work on, how they stick to their plans, and how their tasks portfolio support their emotional well-being, nudging users to reflect upon their work, planning performance, and their emotional values towards their work.  In this poster, we share the design, development, and ongoing validation progress of TaskScape, which is aimed to nudge workers to holistically view work productivity, reminding users that work is more than just work but life.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 85150
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 84885
        }
      ]
    },
    {
      "id": 85455,
      "typeId": 12315,
      "title": "Demonstration of TangibleGrid: a Tangible Web Layout Design Tool for Blind Users",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1088",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Accessible web design, tactile feedback, tangible user interface, visual impairment, accessibility"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In this demonstration, We showcase TangibleGrid, a novel device that allows blind users to understand and design the web page layout with real-time tangible and audio feedback. Our device contains shape-changing brackets representing the web elements and a baseboard representing the web page canvas. Blind users can understand web page layout by feeling the brackets' type, size, and location on the baseboard. They can also design a web page layout through creating and editing web elements by snapping or adjusting tangible brackets on top of the baseboard. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84945
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "College of Information Studies"
            }
          ],
          "personId": 84932
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland, College Park",
              "dsl": ""
            }
          ],
          "personId": 84596
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84881
        }
      ]
    },
    {
      "id": 85456,
      "typeId": 12315,
      "title": "DATALEV: Acoustophoretic Data Physicalisation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558638"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1086",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Data Physicalisation",
        "Acoustic Levitation",
        "Physical Assembly"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Here, we demonstrate DataLev, a data physicalisation platform with a physical assembly pipeline that allows us to computationally assemble 3D physical charts using acoustically levitated contents. DataLev consists of several enhancement props that allow us to incorporate high-resolution projection, different 3D printed artifacts and multi-modal interaction. DataLev supports reconfigurable and dynamic physicalisations that we animate and illustrate for different chart types. Our work opens up new opportunities for data storytelling using acoustic levitation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85302
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85382
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84651
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85303
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85316
        }
      ]
    },
    {
      "id": 85457,
      "typeId": 12320,
      "title": " MoonBuddy: A Voice-based Augmented Reality User Interface That Supports Astronauts During Extravehicular Activities ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558690"
        },
        "Poster": {
          "hashSum": "8uaqRzCPFl6gmc4zJz2SNPq58QG1fIN/ViMIYvr9We4=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85457/poster/083cff9c-4da4-f511-f114-11b4f3391d1c.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85457/poster/4620f11f-a8af-211a-bffc-cdda1549a2c1.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-5379",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "As NASA pursues Artemis missions to the moon and beyond, it is essential to equip astronauts with the appropriate human-autonomy enabling technology necessary for the elevated demands of lunar surface exploration and extreme terrestrial access. We present MoonBuddy, an application built for the Microsoft HoloLens 2 that utilizes Augmented Reality (AR) and voice-based interaction to assist astronauts in communication, navigation, and documentation on future lunar extravehicular activities (EVAs), with the goal of reducing cognitive load and increasing effective task completion. User testing results for MoonBuddy under simulated lunar conditions have been positive overall with participants indicating that the application was easy to use and helpful in completing the\r\nrequired tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 85346
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 85256
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 85132
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 85245
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 85167
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 85236
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 85282
        }
      ]
    },
    {
      "id": 85458,
      "typeId": 12315,
      "title": "Calligraphy Z: A Fabricatable Pen Plotter for Handwritten Strokes with Z-Axis Pen Pressure",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558657"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1084",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Calligraphy",
        "3D Printer",
        "Digital Fabrication",
        "Computer Numerical Control"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In the current age, the use of desktop publishing software and printing presses makes it possible to produce various expressions. \r\nOn the other hand, it is difficult to perfectly replicate the ink grazing and subtle pressure fluctuations that occur when using a writing implement to output characters on a printer.\r\nIn this study, we reproduce such incidental brushstrokes by using a writing implement to output text layout created on software.\r\nTo replicate slight variations in strokes, we developed Calligraphy Z, a system that consists of a writing device and an application.\r\nThe writing device controls the vertical position of the writing tool in addition to the writing position, thus producing handwritten-like character output, and an application that generates G-code for the device operation from user input.\r\nWith the application, users can select their favorite fonts, input words, and adjust the layout to operate the writing device using several types of extended font data with writing pressure data prepared in advance.\r\nAfter developing our system, we compared the strokes of several writing implements to select the most suitable one for Calligraphy Z. \r\nWe also conducted evaluations of the identification of characters output by Calligraphy Z and those output by a printing machine. We found participants in the evaluation experiment perceive the features of handwritten characters, such as ink blotting and fine blurring of strokes, in the characters by our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Hino",
              "institution": "Tokyo Metropolitan University",
              "dsl": "Graduate School of Systems Design"
            }
          ],
          "personId": 85288
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Hino",
              "institution": "Tokyo Metropolitan University",
              "dsl": "Graduate School of Systems Design"
            }
          ],
          "personId": 85234
        }
      ]
    },
    {
      "id": 85459,
      "typeId": 12316,
      "durationOverride": 450,
      "title": "Environmental physical intelligence: Seamlessly deploying sensors and actuators to our everyday life",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558525"
        }
      },
      "isBreak": false,
      "importedId": "uist22e-1032",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86421
      ],
      "eventIds": [],
      "abstract": "Weiser has predicted the third generation of computing would result in individuals interacting with many computing devices and ultimately can “weave themselves into the fabric of everyday life until they are indistinguishable from it”. However, how to achieve this seamlessness and what associated interaction should be developed are still under investigation. On the other hand, the material composition, structures and operating logic of a variety of physical objects existing in everyday life determine how we interact with them. The intelligence of the built environment does not only rely on the encoded computational abilities within the “brain” (like the controllers of home appliances), but also the physical intelligence encoded in their “body” (e.g., materials, mechanical structures). In my research, I work on creating computational materials with different encoded material properties (e.g., conductivity, transparency, water-solubility, self-assembly, etc.) that can be seamlessly integrated into our living environment to enrich different modalities of information communication.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Interactive Computing",
              "dsl": "Georgia Institute of Technology"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Interactive Computing",
              "dsl": "Georgia Institute of Technology"
            }
          ],
          "personId": 85343
        }
      ]
    },
    {
      "id": 85460,
      "typeId": 12316,
      "durationOverride": 450,
      "title": "Empowering domain experts to author valid statistical analyses ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558530"
        }
      },
      "isBreak": false,
      "importedId": "uist22e-1035",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86421
      ],
      "eventIds": [],
      "abstract": " Reliable statistical analyses are critical for making scientific discoveries, guiding policy, and informing decisions. To author reliable statistical analyses, integrating knowledge about the domain, data, statistics, and programming is necessary. However, this is an unrealistic expectation for many analysts who may possess domain expertise but lack statistical or programming expertise, including many researchers, policy makers, and other data scientists. How can our statistical software help these analysts? To address this need, we first probed into the cognitive and operational processes involved in authoring statistical analyses and developed the theory of hypothesis formalization. Authoring statistical analyses is a dual-search process that requires grappling with assumptions about conceptual relationships and iterating on statistical model implementations. This led to our key insight: statistical software needs to help analysts translate what they know about their domain and data into statistical modeling programs. To do so, statistical software must provide programming interfaces and interaction models that allow statistical non-experts to express their analysis goals accurately and reflect on their domain knowledge and data. Thus far, we have developed two such systems that embody this insight: Tea and Tisane. Ongoing work on rTisane explores new semantics for more accurately eliciting analysis intent and conceptual knowledge. Additionally, we are planning a summative evaluation of rTisane to assess our hypothesis that this new way of authoring statistical analyses makes domain experts more aware of their implicit assumptions, able to author and understand nuanced statistical models that answer their research questions, and avoid previous analysis mistakes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85296
        }
      ]
    },
    {
      "id": 85461,
      "typeId": 12315,
      "title": "Integrating Living Organisms in Devices to Implement Care-based Interactions",
      "isBreak": false,
      "importedId": "uist22d-1027",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Researchers have been exploring how incorporating care-based interactions can change the user’s attitude & relationship towards an interactive device. This is typically achieved through virtual care where users care for digital entities. In this paper, we explore this concept further by investigating how physical care for a living organism, embedded as a functional component of an interactive device, also changes user-device relationships. Living organisms differ as they require an environment conducive to life, which in our concept, the user is responsible for providing by caring for the organism (e.g., feeding it). We instantiated our concept by engineering a smartwatch that includes a slime mold that physically conducts power to a heart rate sensor inside the device, acting as a living wire. In this smartwatch, the availability of heart-rate sensing depends on the health of the slime mold—with the user’s care, the slime mold becomes conductive and enables the sensor; conversely, without care, the slime mold dries and disables the sensor (resuming care resuscitates the slime mold). To explore how our living device was perceived by users, we conducted a study where participants wore our slime mold-integrated smartwatch for 9-14 days. We found that participants felt a sense of responsibility, developed a reciprocal relationship, and experienced the organism’s growth as a source of affect. Finally, to allow engineers and designers to expand on our work, we abstract our findings into a set of technical and design recommendations when engineering an interactive device that incorporates this type of care-based relationship.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84858
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84662
        }
      ]
    },
    {
      "id": 85462,
      "typeId": 12315,
      "title": "Demonstration of Geppetteau: Enabling haptic perceptions of virtual fluids in various vessel profiles using a string-driven haptic interface",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558611"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1028",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Ungrounded haptic feedback",
        "string-driven actuation",
        "virtual reality",
        "fluid dynamics"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Liquids sloshing around in vessels produce unique unmistakable tactile sensations of handling fluids in daily life, laboratory environments, and industrial contexts. \r\nProviding nuanced congruent tactile sensations would enrich interactions of handling fluids in virtual reality (VR). To this end, we introduce Geppetteau, a novel string-driven weight-shifting mechanism capable of providing a continuous spectrum of perceivable tactile sensations of handling virtual liquids in VR vessels. Geppetteau's weight-shifting actuation system can be housed in 3D-printable shells, adapting to varying vessel shapes and sizes. A variety of different fluid behaviors can be felt using our haptic interface.\r\nIn this work, Geppetteau assumes the shape of conical, spherical, cylindrical, and cuboid flasks, widening the range of augmentable shapes beyond the state-of-the-art of existing mechanical systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Honors College"
            }
          ],
          "personId": 85396
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Electrical, Computer and Energy Engineering"
            }
          ],
          "personId": 85248
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Arts Media and Engineering"
            }
          ],
          "personId": 85389
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing and Augmented Intelligence"
            }
          ],
          "personId": 85222
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing and Augmented Intelligence"
            }
          ],
          "personId": 85313
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "Ira A. Fulton Schools of Engineering"
            }
          ],
          "personId": 85325
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Arts, Media and Engineering"
            }
          ],
          "personId": 85208
        }
      ]
    },
    {
      "id": 85463,
      "typeId": 12320,
      "title": "RCSketch: Sketch, Build, and Control Your Dream Vehicles",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558688"
        },
        "Poster": {
          "hashSum": "8F+o6OD5JgR8wZ+mno49IP3GQbvd3UHCcWsb5R5LHKg=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85463/poster/a9d70644-3817-f106-50ae-43562703ed50.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85463/poster/1d5a1f0b-51de-7f73-dd40-daf0ff8ba693.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3273",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "We present RCSketch, a system that lets children sketch their dream vehicles in 3D, build moving structures of those vehicles, and control them from multiple viewpoints. As a proof of concept, we implemented our system and designed five vehicles  that could perform a wide variety of realistic movements.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 85193
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 85329
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 85348
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 85349
        }
      ]
    },
    {
      "id": 85464,
      "typeId": 12315,
      "title": "Demonstration of MetamorphX: An Ungrounded Moment Display that Changes its Physical Properties",
      "isBreak": false,
      "importedId": "uist22d-1025",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We will demonstrate MetamorphX, an ungrounded hand-held 3 DoF moment display. We have found that by providing moment feedback to the rotational motion of a human, we can represent various inertia and viscosity of grasped objects. \r\nWe combined this device with high-quality VR hand interaction tools in the demonstration. As a result, you can grasp various virtual objects at different positions and feel changes in the moment of inertia and viscous resistance when you shake them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84727
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84933
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "the University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84917
        }
      ]
    },
    {
      "id": 85465,
      "typeId": 12316,
      "durationOverride": 450,
      "title": "Designing Tools for Autodidactic Learning of Skills",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558526"
        }
      },
      "isBreak": false,
      "importedId": "uist22e-1033",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86421
      ],
      "eventIds": [],
      "abstract": "In the last decade, HCI researchers have designed and engineered several systems to lower the entry barrier for beginners and support novices in learning hands-on creative skills, such as motor skills, fabrication, circuit prototyping, and design. \r\n\r\nIn my research, I contribute to this body of work by designing tools that enable learning by oneself, also known as autodidactism. My research lies at the intersection of system design, learning sciences, and technologies that support physical skill-learning. Through my research projects, I propose to re-imagine the design of systems for skill-learning through the lens of learner-centric theories and frameworks. \r\n\r\nI present three sets of research projects - (1) adaptive learning of motor skills, (2) game-based learning for fabrication skills, and (3) reflection-based learning of maker skills. Through these projects, I demonstrate how we can leverage existing theories, frameworks, and approaches from the learning sciences to design autodidactic systems for skill- learning.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT CSAIL"
            }
          ],
          "personId": 85385
        }
      ]
    },
    {
      "id": 85466,
      "typeId": 12316,
      "durationOverride": 450,
      "title": "Design and Fabricate Personal Health Sensing Devices",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558528"
        }
      },
      "isBreak": false,
      "importedId": "uist22e-1034",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86421
      ],
      "eventIds": [],
      "abstract": "With the development of low-cost electronics, rapid prototyping techniques, as well as widely available mobile devices (e.g. mobile phones, smart watches), users are able to develop their own basic interactive functional applications, either on top of existing device platforms, or as stand-alone devices. However, the boundary for creating personal health sensing devices, both function prototyping and fabrication -wise, are still high. In this paper, I present my works on designing and fabricating personal health sensing devices with rapid function prototyping techniques and novel sensing technologies. Through these projects and ongoing future research, I am working towards my vision that everyone can design and fabricate highly-customized health sensing devices based on their body form and desired functions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84905
        }
      ]
    },
    {
      "id": 85467,
      "typeId": 12315,
      "title": "Point Cloud Capture and Editing for AR Environmental Design",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558636"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1034",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "AR environmental design",
        "point cloud",
        "capture and editing"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We present a tablet-based system for AR environmental design using point clouds. It integrates point cloud capture and editing in a single AR workflow to help users quickly prototype design ideas in their spatial context. We hypothesize that point clouds are well suited for prototyping, as they can be captured rapidly and then edited immediately on the capturing device in situ. Our system supports a variety of point cloud editing operations in AR, including selection, transformation, hole filling, drawing, and animation. This enables a wide range of design applications for objects, interior environments, buildings, and landscapes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": ""
            }
          ],
          "personId": 85160
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 85188
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe",
              "dsl": ""
            }
          ],
          "personId": 85377
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": ""
            }
          ],
          "personId": 85322
        }
      ]
    },
    {
      "id": 85468,
      "typeId": 12315,
      "title": "Demonstrating a Fabricatable Bioreactor Toolkit for Small-Scale Biochemical Automation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558661"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1035",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Bioreactors",
        "Fermentation",
        "Small-Scale Brewing",
        "Fabricatable toolkit",
        "Small-Scale Automation"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Biological and chemical engineering creates novel materials through custom workflows. Supporting such materials development through systems research such as toolkits and software is increasingly of interest to HCI. Bioreactors are widely used systems which can grow materials, converting feedstock into valuable products through fermentation. However, integrated bioreactors are difficult to design and program. We present a modular toolkit for developing custom bioreactors. Our toolkit contains custom hardware and software for adding chemicals, monitoring the mixture, and refining outputs. We demonstrate our bioreactor toolkit with a beer brewing application, an automated process which involves several biochemical reactions that are comparable to other synthetic biology processes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Human-Centered Design and Engineering"
            }
          ],
          "personId": 85225
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85369
        }
      ]
    },
    {
      "id": 85469,
      "typeId": 12320,
      "title": "Self-Supervised Approach for Few-shot Hand Gesture Recognition",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558707"
        },
        "Poster": {
          "hashSum": "WNzZjcciW/UbLp8lqqWM8kyrjbJOVoTWBTYFzmt/NUw=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85469/poster/2af703a0-395b-5069-d270-059136fcd46f.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85469/poster/9b23730d-6e05-e59c-c8ee-117cb7cd29a3.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2854",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Data-driven machine learning approaches have become increasingly used in human-computer interaction (HCI) tasks. However, compared with traditional machine learning tasks, for which large datasets are available and maintained, each HCI project needs to collect new datasets because HCI systems usually propose new sensing or use cases. Such datasets tend to be lacking in amount and lead to low performance or place a burden on participants in user studies. In this paper, taking hand gesture recognition using wrist-worn devices as a typical HCI task, I propose a self-supervised approach that achieves high performance with little burden on the user. The experimental results showed that hand gesture recognition was achieved with a very small number of labeled training samples (five samples with 95% accuracy for 5 gestures and 10 samples with 95% accuracy for 10 gestures). The results support the story that when the user wants to design 5 new gestures, he/she can activate the feature in less than 2 minutes. I discuss the potential of this self-supervised framework for the HCI community.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo",
              "institution": "The University of Tokyo",
              "dsl": "The University of Tokyo"
            }
          ],
          "personId": 85390
        }
      ]
    },
    {
      "id": 85470,
      "typeId": 12316,
      "durationOverride": 450,
      "title": "Exploiting and Guiding User Interaction in Interactive Machine Teaching",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558529"
        }
      },
      "isBreak": false,
      "importedId": "uist22e-1026",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86421
      ],
      "eventIds": [],
      "abstract": "Humans are talented with the ability to perform diverse interactions in the teaching process. However, when humans want to teach AI, existing interactive systems only allow humans to perform repetitive labeling, causing an unsatisfactory teaching experience. My Ph.D. research studies Interactive Machine Teaching (IMT), an emerging field of HCI research that aims to enhance humans' teaching experience in the AI creation process. My research builds IMT systems that exploit and guide user interaction and shows that such in-depth integration of human interaction can benefit both AI models and user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Interactive Intelligent Systems Lab."
            }
          ],
          "personId": 84791
        }
      ]
    },
    {
      "id": 85471,
      "typeId": 12315,
      "title": "ForceSight: Non-Contact Force Sensing with Laser Speckle Imaging",
      "isBreak": false,
      "importedId": "uist22d-1032",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Force sensing has been a key enabling technology for a wide range of interfaces such as digitally enhanced surfaces for on-body and on-world touch interactions. Additionally, force often contains rich contextual information about user activities and can be used to enhance machine perception for an improved user and environment awareness. To sense force, conventional approaches rely on contact sensors made of pressure-sensitive materials such as piezo films/discs or force-sensitive resistors. We present ForceSight, a non-contact force sensing approach using laser speckle imaging. Our key observation is that object surfaces deform in the presence of force. This deformation, though very minute, manifests as observable and discernable laser speckle shifts, which we leverage to sense the applied force. This non-contact force-sensing capability opens up new opportunities for rich interactions and can be used to power user-/environment-aware interfaces. We first built and verified the model of laser speckle shift with surface deformations. To investigate the feasibility of our approach, we conducted studies on metal, plastic, wood, along with a wide variety of materials. Additionally, we included supplementary tests to fully tease out the performance of our approach. Finally, we demonstrated the applicability of ForceSight with several demonstrative example applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84583
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84538
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84729
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84846
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84612
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 84685
        }
      ]
    },
    {
      "id": 85472,
      "typeId": 12316,
      "durationOverride": 450,
      "title": "Artistic User Expressions in AI-powered Creativity Support Tools",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558531"
        }
      },
      "isBreak": false,
      "importedId": "uist22e-1027",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86421
      ],
      "eventIds": [],
      "abstract": "Novel AI algorithms introduce a new generation of AI-powered Creativity Support Tools (AI-CSTs). These tools can inspire and surprise users with algorithmic outputs that the users could not expect. However, users can struggle to align their intentions with unexpected algorithmic behaviors. My dissertation research studies how user expressions in art-making AI-CSTs need to be designed. With an interview study with 14 artists and a literature survey on 111 existing CSTs, I first isolate three requirements: 1) allow users to express under-constrained intentions, 2) enable the tool and the user to co-learn the user expressions and the algorithmic behaviors, and 3) allow easy and expressive iteration. Based on these requirements, I introduce two tools, 1) Artinter, which learns how the users express their visual art concepts within their communication process for art commissions, and 2) TaleBrush, which facilitates the under-constrained and iterative expression of use intents through sketching-based story generation. My research provides guidelines for designing user expression interactions for AI-CSTs while demonstrating how they can suggest new designs of AI-CSTs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 85367
        }
      ]
    },
    {
      "id": 85473,
      "typeId": 12315,
      "title": "Photographic Lighting Design with Photographer-in-the-Loop Bayesian Optimization",
      "isBreak": false,
      "importedId": "uist22d-1030",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "lighting design",
        "human-in-the-loop optimization",
        "Bayesian optimization",
        "photography"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "It is important for photographers to have the best possible lighting configuration at the time of shooting; otherwise, they need post-processing on images, which may cause artifacts and deterioration. Thus, photographers often struggle to find the best possible lighting configuration by manipulating lighting devices, including light sources and modifiers, in a trial-and-error manner. In this paper, we propose a novel computational framework to support photographers. This framework assumes that every lighting device is programmable; that is, its adjustable parameters (e.g., orientation, intensity, and color temperature) can be set using a program. Using our framework, photographers do not need to learn how the parameter values affect the resulting lighting, and even do not need to determine the strategy of the trial-and-error process; instead, photographers need only concentrate on evaluating which lighting configuration is more desirable among options suggested by the system. The framework is enabled by our novel photographer-in-the-loop Bayesian optimization, which is sample-efficient (i.e., the number of required evaluation steps is small) and which can also be guided by providing a rough painting of the desired lighting configuration if any. We demonstrate how the framework works in both simulated virtual environments and a physical environment, suggesting that it could find pleasing lighting configurations quickly in around 10 iterations. Our user study suggests that the framework enables the photographer to concentrate on the look of captured images rather than the parameters, compared with the traditional manual lighting workflow.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Digital Nature Group"
            }
          ],
          "personId": 84572
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 84713
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 84814
        }
      ]
    },
    {
      "id": 85474,
      "typeId": 12320,
      "title": "Towards Semantically AwareWord Cloud Shape Generation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558724"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85474/poster/e85bb552-3eaf-5514-0656-37db16d2bca7.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85474/poster/774bcd0b-70f7-4fbb-75bc-73cfafa91e03.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-9547",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Word clouds are a data visualization technique that showcases a subset of words from a body of text in a cluster form, where a word’s font size encodes some measure of its relative importance—typically frequency—in the text. This technique is primarily used to help viewers glean the most pertinent information from long text documents and to compare and contrast different pieces of text. Despite their popularity, previous research has shown that word cloud designs are often not optimally suited for analytical tasks such as summarization or topic understanding. We propose a solution for generating more effective visualization technique that shapes the word cloud to reflect the key topic(s) of the text. Our method automates the processes of manual image selection and masking required from current word cloud tools to generate shaped word clouds, better allowing for quick summarization. We showcase two approaches using classical and state-of-the-art methods. Upon successfully generating semantically shaped word clouds using both methods, we performed preliminary evaluations with 5 participants. We found that although most participants preferred shaped word clouds over regular ones, the shape can be distracting and detrimental to information extraction if it is not directly relevant to the text or contains graphical imperfections. Our work has implications on future semantically-aware word cloud generation tools as well as efforts to balance visual appeal of word clouds with their effectiveness in textual comprehension.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Human Centered Design & Engineering"
            }
          ],
          "personId": 85388
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85379
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 85290
        }
      ]
    },
    {
      "id": 85475,
      "typeId": 12315,
      "title": "SenSequins: Smart Textile Using 3D Printed Conductive Sequins",
      "isBreak": false,
      "importedId": "uist22d-1018",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Smart Textiles",
        "Conductive Material",
        "Wearable device",
        "Sensing Interface",
        "Digital Fabrication"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In this research, we used traditional sequin embroidery as the basis and a 3D printer to expand the design space of sequin materials and structures, by developing a new 2.5D smart conductive sequin textile with multiple sensing and interactions as well as providing users with a customizing system for automated design and manufacturing.\r\nThrough 3D printing, we have developed a variety of 3D sequins. We used each sequin as an individual design unit to realize various circuit designs and sensing functions by adjusting the design primitives such as conductivity, shape, and arrangement.\r\nWe also designed applications such as motion sensing of body movements, and posture detection of the ankle. In addition, we surveyed user requirements through user testing to optimize the design space.\r\nThis paper describes the design space, design software, automation, application, and user study of various smart sequin textiles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 84961
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 84676
        }
      ]
    },
    {
      "id": 85476,
      "typeId": 12320,
      "title": "\"Inconsistent Performance\": Understanding Concerns of Real-World Users on Smart Mobile Health Applications Through Analyzing App Reviews",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558698"
        },
        "Poster": {
          "hashSum": "AG+LCUEi0WXEQ9/aVdCqAxDUakrhx9380TZLE1yV0jI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85476/poster/ddf6167e-82ce-2c6c-98da-5f63611e0558.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85476/poster/2123b956-5e88-bc38-c88b-bc93b78e4292.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-8553",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "While smart mobile health apps that adapt to users' progressive individual needs are proliferating, many of them struggle to fulfill their promises due to an inferior user experience. Understanding the concerns of real-world users related to those apps, and their smart components in particular, could help advance the app design to attract and retain users. In this paper, we target this issue through a preliminary thematic analysis of 120 user reviews of six smart health apps. We found that accuracy, customizability, and convenience of data input are primary concerns raised in real-world user reviews. Many concerns on the smart components are related to the trust issue of the users towards the apps. However, several important aspects such as privacy and fairness were rarely discussed in the reviews. Overall, our study provides insights that can inspire further investigations to support the design of smart mobile health apps.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Polytechnique Montreal",
              "dsl": ""
            }
          ],
          "personId": 85336
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Polytechnique Montreal",
              "dsl": ""
            }
          ],
          "personId": 85230
        }
      ]
    },
    {
      "id": 85477,
      "typeId": 12315,
      "title": "Demonstrating SleepGuru: Personalized Sleep Planning System for Real-life Actionability and Negotiability",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1016",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Actionable Sleep",
        "Computational Sleep Model",
        "Personally Optimized Sleep Schedule",
        "Real-life Constraints"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Widely-accepted sleep guidelines advise people about regular bedtimes and sleep hygiene. However, these standard guidelines are often little applicable to our real-life full of duty and responsibility. We propose SleepGuru, an individually actionable sleep planning system featuring one's real-life compatibility and extended forecast. Leveraging wearable activity trackers and commercial mobile calendars, SleepGuru computes individually actionable multi-day sleep schedules that optimize the sleep timings and the user's alertness levels subject to one's real-life constraints. SleepGuru provides individual predictions and adjustability through a mobile-friendly user interface backed by a cloud-side optimization engine.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84911
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84901
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84852
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84904
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "UIUC",
              "dsl": ""
            }
          ],
          "personId": 84770
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84909
        }
      ]
    },
    {
      "id": 85478,
      "typeId": 12320,
      "title": "Towards using Involuntary Body Gestures for Measuring the User Engagement in VR Gaming",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558691"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85478/poster/fcc8ce4c-76a7-a027-bef7-527e1ae6e505.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85478/poster/cdb3ed79-28e9-bcc6-4adf-227c6d34b3c3.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-5283",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Understanding the degree of user engagement in a VR game is vital to provide a better gaming experience. While prior work has suggested self-reports, and biological signal-based methods, measuring game engagement remains a challenge due to its complex nature. In this work, we provide a preliminary exploration of using involuntary body gestures to measure user engagement in VR gaming. Based on data collected from 27 participants performing multiple VR games, we demonstrate a relationship between foot gesture-based models for measuring arousal and physiological responses while engaging in VR games. Our findings show the possibility of using involuntary body gestures to measure engagement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 85374
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 85352
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 85384
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "School of Computer Science & Engineering"
            }
          ],
          "personId": 85201
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "Computer Science and Engineering "
            }
          ],
          "personId": 85135
        }
      ]
    },
    {
      "id": 85479,
      "typeId": 12315,
      "title": "Demonstrating Finger-Based Dexterous Phone Gestures",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558645"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1015",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Interaction techniques",
        "Finger dexterity",
        "Mobile input",
        "Gesture recognition"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "This hands-on demonstration enables participants to experience single-handed “dexterous gestures”, a novel approach for the physical manipulation of a phone using the fine motor skills of fingers. A recognizer is developed for variations of “full” and “half” gestures that spin (yaw axis), rotate (roll axis), and flip (pitch axis), all detected using the built-in phone IMU sensor. A functional prototype demonstrates how recognized dexterous gestures can be used to interact with a variety of current smartphone applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 85376
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Preferred Networks Inc.",
              "dsl": ""
            }
          ],
          "personId": 85183
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 85320
        }
      ]
    },
    {
      "id": 85480,
      "typeId": 12320,
      "title": "HapticPuppet: A Kinesthetic Mid-air Multidirectional Force-Feedback Drone-based Interface",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558694"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-9272",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Providing kinesthetic force-feedback for human-scale interactions is challenging due to the relatively large forces needed. Therefore, robotic actuators are predominantly used to deliver this kind of haptic feedback; however, they offer limited flexibility and spatial resolution. In this work, we introduce HapticPuppet, a drone-based force-feedback interface which can exert multidirectional forces onto the human body. This can be achieved by attaching strings to different parts of the human body such as fingers, hands or ankles, which can then be affixed to multiple coordinated drones - puppeteering the user. HapticPuppet opens up a wide range of potential applications in virtual, augmented and mixed reality, exercising, physiotherapy, remote collaboration as well as haptic guidance. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 85287
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 84632
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 85186
        }
      ]
    },
    {
      "id": 85481,
      "typeId": 12320,
      "title": "LUNAChair: Remote Wheelchair System that Links Up a Remote Caregiver and Wheelchair Surroundings",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558729"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-6809",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "We introduce LUNAChair, a remote control and communication system that uses omnidirectional video to connect a remote caregiver to a wheelchair user and a third person around the wheelchair. With the recent growing need for wheelchairs, much of the wheelchair research has focused on wheelchair control, such as fully automatic driving and remote operation. For wheelchair users, conversations with caregivers and third persons around them are also important. Therefore, we propose a system that connects a wheelchair user and a remote caregiver using omnidirectional cameras, which allows the remote caregiver to control the wheelchair while observing both the wheelchair user and his/her surroundings. Moreover, the system facilitates communication by using gaze and hand pointing estimation from the omnidirectional video.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 85218
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 85246
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Meguro",
              "institution": "Tokyo Institute of Technology",
              "dsl": "Koike Lab"
            }
          ],
          "personId": 85223
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": "School of Computing"
            }
          ],
          "personId": 85210
        }
      ]
    },
    {
      "id": 85482,
      "typeId": 12315,
      "title": "Experience Visual Impairment via Optical See-through Smart Glasses",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1023",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Vision Augmentation",
        "Visual Perception",
        "Visual Impairment",
        "Eye Tracking",
        "Smart Eyewear",
        "Smart Glasses",
        "Optical See-through"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "As the population ages, many will acquire visual impairments. To improve design for these users, it is essential to build awareness of their perspective during everyday routines. \r\n Although several visual impairment simulation toolkits exist in both academia and as commercial products, analog, and static visual impairment simulation tools do not simulate effects with respect to the user's eye movements. Meanwhile, VR and video see-through-based AR simulation methods are constrained by smaller fields of view when compared with the natural human visual field and also suffer from vergence-accommodation conflict (VAC) which correlates with visual fatigue, headache, and dizziness. \r\n This demonstration enables an on-the-go, VAC-free, visually impaired experience by leveraging our optical see-through glasses. The FOV of our glasses is approximately 160 degrees, and participants can experience both losses of central vision and loss of peripheral vision at different severities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 84548
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "YOKOHAMA",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 85196
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 85214
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 84710
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": "KMD"
            }
          ],
          "personId": 85298
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Keio Graduate School of Media Design"
            }
          ],
          "personId": 84744
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 84555
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 85247
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 84676
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmiths University of London",
              "dsl": "Computing"
            }
          ],
          "personId": 84604
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": "KMD"
            }
          ],
          "personId": 84579
        }
      ]
    },
    {
      "id": 85483,
      "typeId": 12320,
      "title": "One-Dimensional Eye-Gaze Typing Interface for People with Locked-in Syndrome",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558732"
        },
        "Poster": {
          "hashSum": "y8LxLpt//S8u4SIQVrGoOknHrQITCprtvXp3Xq5PHPY=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85483/poster/9741e677-8f2a-a7a5-e3ac-dc4f5caa10e1.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85483/poster/21b30882-3330-1f0a-d920-a3948335c743.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3532",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "People with Locked-in syndrome (LIS) suffer from complete loss of voluntary motor functions for speech or hand-writing. They are mentally intact, retaining only the control of vertical eye movements and blinking. In this work, we present a one-dimensional typing interface controlled exclusively by vertical eye movements and dwell-time for them to communicate at will. Hidden Markov Model and Bigram Models are used as auto-completion on both word and sentence level. We conducted two preliminary user studies on non-disabled users. The typing interface achieved 3.75 WPM without prediction and 11.36 WPM with prediction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 85338
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 84719
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 85166
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 84884
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 84647
        }
      ]
    },
    {
      "id": 85484,
      "typeId": 12315,
      "title": "Demonstrating MagneShape: A Non-electrical Pin-Based Shape-Changing Display",
      "isBreak": false,
      "importedId": "uist22d-1020",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Pin-based shape-changing displays can present dynamic shape changes by actuating a number of pins. However, the use of many linear actuators to achieve this makes the electrical structure and mechanical construction of the display complicated. We propose a simple pin-based shape-changing display that outputs shape and motions without any electronic elements. Our display consists of magnetic pins in a pin housing, with a magnetic sheet underneath it. The magnetic sheet has a specific magnetic pattern on its surface, and each magnetic pin has a magnet at its lower end. The repulsive force generated between the magnetic sheet and the magnetic pin levitates the pin vertically, and the height of the pin-top varies depending on the magnetic pattern. We demonstrate several application examples to show how the magnetized pattern changes the levitation height of the pins. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Atsugi",
              "institution": "Nippon Telegraph and Telephone Corporation",
              "dsl": "NTT Communication Science Laboratories"
            }
          ],
          "personId": 84886
        }
      ]
    },
    {
      "id": 85485,
      "typeId": 12320,
      "title": "SpiceWare: Simulating Spice Using Thermally Adjustable Dinnerware to Bridge Cultural Gaps",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558701"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85485/poster/69f48d3a-7709-03ef-5f18-ac893098fa50.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85485/poster/570eaef8-620c-129d-91dc-166607986d84.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3533",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86425
      ],
      "eventIds": [],
      "abstract": "Preference and tolerance towards spicy food may vary depending on culture, location, upbringing, personality and even gender. Due to this, spicy food can often effect the social interaction on the dining table, especially if it is presented as a cultural dish. We propose SpiceWare, a thermally adjustable spoon that alters the perception of spice to improve cross-cultural communication. SpiceWare is a 3D-printed aluminium spoon that houses a thermal peltier that provides thermal feedback up to 45°C which can alter the taste perception of the user. As an initial evaluation, we conducted a workshop among participants of varying cultural backgrounds and observe their interaction when dining on spicy food. We found that the overall interaction was perceived to be more harmonious, and we discuss potential future works on improving the system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Media Design ",
              "dsl": "Keio University"
            }
          ],
          "personId": 85173
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 84555
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 85228
        }
      ]
    },
    {
      "id": 85486,
      "typeId": 12320,
      "title": "SomaFlatables: Supporting Embodied Cognition through Pneumatic Bladders",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558705"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-5356",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Applying the theory of Embodied Cognition through design allows us to create computational interactions that engage our bodies by modifying our body schema. However, in HCI, most of these interactive experiences have been stationed around creating sensing-based systems that leverage our body's position and movement to offer an experience, such as games using Nintendo Wii and Xbox Kinect. In this work, we created two pneumatic inflatables-based prototypes that actuate our body to support embodied cognition in two scenarios by altering the user's body schema. We call these \"SomaFlatables\" and demonstrate the design and implementation of these inflatables-based prototypes that can move and even extend our bodies, allowing for novel bodily experiences. Furthermore, we discuss the future work and limitations of the current implementation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash Uniersity",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 85147
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 85373
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 85364
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Clayton",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 85249
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "Faculty of Engineering and Information Technology"
            },
            {
              "country": "Netherlands",
              "state": "Noord Brabant",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 85273
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Exertion Games Lab, Department of Human-Centred Computing"
            }
          ],
          "personId": 85227
        }
      ]
    },
    {
      "id": 85487,
      "typeId": 12320,
      "title": "SilentWhisper: faint whisper speech using wearable microphone",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558715"
        },
        "Poster": {
          "hashSum": "Cn5LQNPPxbh2pEcPLx7DWWkzg7XOhZuhw7mYL9+qe6g=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85487/poster/a8ef932f-04dc-d532-286b-4b3e41f10a88.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85487/poster/a168c74e-b082-9cf8-be99-3f7c9c91cc91.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-5750",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Voice interaction is a fundamental human capacity, and we can use voice user interfaces just speaking. However, in public spaces, we are hesitant to use them  because of consideration for their surroundings and low privacy. Silent speech, a method that recognizes the movement of speech in silence, has been proposed as a solution to this problem, and it allows us to maintain our privacy when speaking. However, existing silent speech interfaces are burdensome because the sensor must be kept in contact with the face and mouth, and commands must be prepared for each user. In this study, we propose a method to input whispered speech at a quiet volume that cannot be heard by others using a pin microphone. Experimental results show that a recognition rate was 13.9% WER and 6.4% CER for 210 phrases. We showed that privacy-preserving vocal input is possible by whispering voices which are not\r\ncomprehensible to others.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Interdisciplinary Information Studies"
            },
            {
              "country": "Japan",
              "state": "Chiba",
              "city": "Kashiwa",
              "institution": "National Institute of Advanced Industrial Science and Technology",
              "dsl": "Human Augmentation Research Center"
            }
          ],
          "personId": 85145
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 84550
        }
      ]
    },
    {
      "id": 85488,
      "typeId": 12315,
      "title": "DigituSync: A Dual-User Passive Exoskeleton Glove That Adaptively Shares Hand Gestures",
      "isBreak": false,
      "importedId": "uist22d-1049",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Embodied Interaction",
        "Tangible",
        "Touch/Haptic/Pointing/Gesture"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We engineered DigituSync, a passive-exoskeleton that physically links two hands together, enabling two users to adaptively transmit finger movements in real-time. It uses multiple four-bar linkages to transfer both motion and force, while still preserving congruent haptic feedback. Moreover, we implemented a variable-length linkage that allows adjusting the force transmission ratio between the two users and regulates the amount of intervention, which enables users to customize their learning experience. DigituSync's benefits emerge from its passive design: unlike existing haptic devices (motor-based exoskeletons or electrical muscle stimulation), DigituSync has virtually no latency and does not require batteries/electronics to transmit or adjust movements, making it useful and safe to deploy in many settings, such as between students and teachers in a classroom. We validated DigituSync by means of technical evaluations and a user study, demonstrating that it instantly transfers finger motions and forces with the ability of adaptive force transmission, which allowed participants to feel more control over their own movements and to feel the teacher’s intervention was more responsive. We also conducted two exploratory sessions with a music teacher and deaf-blind users, which allowed us to gather experiential insights from the teacher’s side and explore DigituSync in applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84535
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84922
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84544
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84662
        }
      ]
    },
    {
      "id": 85489,
      "typeId": 12315,
      "title": "KineCAM: An Instant Camera for Animated Photographs",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558621"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1047",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "kinegram",
        "animation",
        "fabrication",
        "live capture"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "The kinegram is a classic animation technique that involves sliding\r\na striped overlay across an interlaced image to create the effect of\r\nframe-by-frame motion. While there are known tools for generating\r\nkinegrams from pre-existing videos and images, there exists no\r\nsystem for capturing and fabricating kinegrams in situ. To bridge\r\nthis gap, we created KineCAM, an open source instant camera that\r\ncaptures and prints animated photographs in the form of kinegrams.\r\nKineCAM combines the form factor of instant cameras with the\r\nexpressiveness of animated photographs to explore and extend\r\ncreative applications for instant photography.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85304
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84695
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT CSAIL"
            }
          ],
          "personId": 85235
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85265
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 85490,
      "typeId": 12320,
      "title": "iMarker: Instant and True-to-scale AR with Invisible Markers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558721"
        },
        "Poster": {
          "hashSum": "Nc7lJ0CUJoLDKUJKxTfSvywr9oTVZaS2VKREbgshGS8=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85490/poster/c6b05935-8122-782b-f85d-eb0cc398374d.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85490/poster/d9448721-89d2-02b4-bc1e-17d101e33ddd.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2080",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Augmented Reality (AR) has been widely used in modern mobile devices for various applications. To achieve a stable and precise AR experience, mobile devices are equipped with various sensors (e.g., dual camera, LiDAR) to increase the robustness of camera tracking. Those sensors largely increased the cost of mobile devices and are usually not available on low-cost devices. We propose a novel AR system that leverage the advance of marker-based camera tracking to produce fast and true-to-scale AR rendering on any\r\ndevice with a single camera. Our method enables the computer monitor to be the host of AR markers, without taking up valuable screen space nor impacting the user experience. Unlike traditional marker-based methods, we utilize the difference between human vision and camera system, making AR markers to be invisible to human vision. We propose an efficient algorithm that allows the mobile device to detect those markers accurately and later recover the camera pose for AR rendering. Since the markers are invisible to human vision, we can embed them on any website and the user will not notice the existence of these markers. We also conduct extensive experiments that evaluate the efficacy of our method. The experimental results show that our method is faster and has a more accurate scale of the virtual objects compared to the state-of-the-art AR solution.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 85350
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 85158
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 85237
        }
      ]
    },
    {
      "id": 85491,
      "typeId": 12320,
      "title": "Amplified Carousel: Amplifying the Perception of Vertical Movement using Optical Illusion",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558735"
        },
        "Poster": {
          "hashSum": "7fCuHHFuzMVp75/j9p7cgkX76VMAzpUmwugcwhSs7vI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85491/poster/122d2d6e-1b62-8d20-7e69-b5aa68f37b5e.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85491/poster/5fbe2ea9-4cb4-e4af-6944-a90ac9d062e1.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2519",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "With the spread of virtual reality (VR) attractions, vector generation techniques that enhance the sense of realism are gaining attention. Additionally, mixed reality (MR) attractions, which overlay VR onto a real-world display, are expected to become more prevalent in the future. However, with MR, it is impossible to move all the coordinates of the visual stimuli to generate proper vection effects. Therefore, we have created an optical illusion method that provides a three-dimensional impression of a two-dimensional visual stimulation. The technique amplifies the sensation of vertical movement by placing the illusion on the floor.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Fujisawa",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 85292
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Fujisawa",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 85128
        }
      ]
    },
    {
      "id": 85492,
      "typeId": 12320,
      "title": "LipLearner: Customizing Silent Speech Commands from Voice Input using One-shot Lipreading",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558737"
        },
        "Poster": {
          "hashSum": "O8spSTJCwthdw4UdIoWQIzwrgdn4r+/rjDvnEo1q9Hw=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85492/poster/d9adc3f6-b7fc-249d-5210-ef6f9b5de4cf.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85492/poster/a92ee2b3-1615-3b71-8584-5caba71a08ac.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-4938",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "We present LipLearner, a lipreading-based silent speech interface that enables in-situ command customization on mobile devices. By leveraging contrastive learning to learn efficient representations from existing datasets, it performs instant fine-tuning for unseen users and words using one-shot learning. To further minimize the labor of command registration, we incorporate speech recognition to automatically learn new commands from voice input. Conventional lipreading systems provide limited pre-defined commands due to the time cost and user burden of data collection. In contrast, our technique provides expressive silent speech interaction with minimal data requirements. We conducted a pilot experiment to investigate the real-time performance of LipLearner, and the result demonstrates that an average accuracy of 91.33% is achievable with only one training sample for each command.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Rekimoto Lab, GSII"
            }
          ],
          "personId": 85197
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Interactive Intelligent Systems Laboratory"
            }
          ],
          "personId": 85314
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 84550
        }
      ]
    },
    {
      "id": 85493,
      "typeId": 12320,
      "title": "Over-The-Shoulder Training Between Redundant Wearable Sensors for Unified Gesture Interactions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558725"
        },
        "Poster": {
          "hashSum": "r/txnG6rf4LBNQU7O2abEBQjYZ5KUdvuzEzq/4BZ2fw=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85493/poster/c5c4970b-885e-4b32-9a59-8fce495a0bf5.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85493/poster/38ba101b-f7e7-d192-a784-afaefdeae465.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-9121",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": " Wearable computers are now prevalent, and it is not uncommon to see people wearing multiple wearable devices. These wearable devices are often equipped with sensors to detect the user's interactions and context. As more devices are worn on the user’s body, there is an increasing redundancy between the sensors. For example, swiping gestures on a headphone are detected by its touch sensor, but the movement it caused can also be measured by the sensors in a smartwatch or smart rings. We present a new mechanism to train a gesture recognition model using redundant sensor data so that measurements from other sensors can be used to detect gestures performed on another device even if the device is missing. Our preliminary study with $13$ participants revealed that a unified gesture recognition model for touch gestures achieved $89.2\\%$ accuracy for $25$ gestures ($5$ gestures $\\times$ $5$ scenarios) where gestures were trained by leveraging the available sensors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85380
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85318
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 85204
        }
      ]
    },
    {
      "id": 85494,
      "typeId": 12315,
      "title": "Demonstration of interiqr: Unobtrusive Edible Tags using Food 3D Printing",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1056",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "digital fabrication",
        "food 3D printing",
        "human-food interaction",
        "invisible tag"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We demonstrate interiqr, a method that utilizes the infill parameter in the 3D printing process to embed information inside the food that is difficult to recognize with the human eye. Our key idea is to utilize the air space or secondary materials to generate a specific pattern inside the food without changing the model geometry. As a result, our method exploits the patterns that appear as hidden edible tags to store the data and simultaneously adds them to a 3D printing pipeline. Our demonstration includes the framework that connects the user with a data-embedding interface through the food 3D printing process, and the decoding system allows the user to decode the information inside the 3D printed food through backlight illumination and a simple image processing technique. We also demonstrate our method through the example application scenarios including the food interactivity and food traceability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 84818
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 84926
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 84681
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 84663
        }
      ]
    },
    {
      "id": 85495,
      "typeId": 12315,
      "title": "Extail: a Kinetic Inconspicuous Wareable Hair Extension Device",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558648"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1057",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Wearables",
        "Body extension",
        "Biomimetics",
        "Communication design"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Wearable devices that present the wearer's information have been designed to stand out when worn, making it difficult to conceal their wear. Therefore, we have been working on developing and evaluating a dynamic expression of hair to realize the presentation of the wearer's information in an inconspicuous wearable device. In the precedents of hair interaction, the hairstyles and expressions to which the technique can be applied are limited. In this paper, we focus on the output mechanism and present Extail, a hair extension type device with a control mechanism that moves bundled hair like a tail. The results of a questionnaire survey on the correspondence between the movement of hair bundles and emotional expression were generally consistent with the results of the evaluation of tail devices in related studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Hino",
              "institution": "Tokyo Metropolitan University",
              "dsl": "Graduate School of Systems Design"
            }
          ],
          "personId": 85268
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Hino",
              "institution": "Tokyo Metropolitan University",
              "dsl": "Graduate School of Systems Design"
            }
          ],
          "personId": 85234
        }
      ]
    },
    {
      "id": 85496,
      "typeId": 12320,
      "title": "What’s Cooking? Olfactory Sensing Using Off-the-Shelf Components",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558687"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85496/poster/10bcdd93-164b-5eaf-2d98-230ee39ac31f.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85496/poster/7080b068-2366-9b39-203c-9a9f79f4b2c9.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-4418",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "  We present \\emph{Project Sniff}, a hardware and software exploration into olfactory sensing with an application in digital communication and social presence. Our initial results indicate that a simple hardware design using off-the-shelf sensors and the application of supervised learning to the sensor data allows us to detect several common household scents reliably. As part of this exploration, we developed a scent-sensing IoT prototype and placed it in the kitchen area to sense ``what's cooking?'', and share the olfactory information via a Slack bot. We conclude by outlining our plans for future steps and potential applications of this research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Mercer Island",
              "institution": "Independent",
              "dsl": ""
            }
          ],
          "personId": 85319
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Independent",
              "dsl": ""
            }
          ],
          "personId": 85153
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 85311
        }
      ]
    },
    {
      "id": 85497,
      "typeId": 12320,
      "title": "Tie Memories to E-souvenirs: Personalized Hybrid Tangible AR Souvenirs in the Museum",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558722"
        },
        "Poster": {
          "hashSum": "LboyHwK61D9Dbmw5sBgsAJkEddeN3JAetO3S/YqDHaI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85497/poster/5626a1e7-d65f-19c5-a503-6a0dbb6c3d1a.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85497/poster/a1675fcc-8bd1-40a9-5a11-b19c30599a0b.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2118",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Traditional physical souvenirs in museums have three major limits: monotonous interaction, lack of personalization, and disconnection to the exhibition. To conquer these problems and to make personalized souvenirs a part of the visiting experience, we create a hybrid tangible Augmented Reality(AR) souvenir that combines a physical firework launcher and AR models. An application called AR Firework is designed for customizing the hybrid souvenir as well as interactive learning in an exhibition in the wild. Multiple interaction methods including mobile user interface, hand gestures, and voice are adopted to create a multi-sensory product. As the first research to propose tangible AR souvenirs, we find that they establish a long-lasting connection between visitors and their personal visiting experiences. This paper promotes the understanding of personalization, socialization and tangible AR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 85159
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University ",
              "dsl": "Shanghai of Design"
            }
          ],
          "personId": 85375
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai ",
              "institution": "School of Foreign Languages",
              "dsl": "Shanghai Jiao Tong University "
            }
          ],
          "personId": 85262
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 85279
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "SHANGHAI",
              "institution": "SHANGHAI JIAO TONG UNIVERSITY",
              "dsl": ""
            }
          ],
          "personId": 85178
        }
      ]
    },
    {
      "id": 85498,
      "typeId": 12315,
      "title": "Demonstrating ex-CHOCHIN: Shape/Texture-changing cylindrical interface with deformable origami tessellation.",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558626"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1054",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Shape-changing interface",
        "Flexible display",
        "Tangible interaction",
        "Origami tessellation"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We demonstrate ex-CHOCHIN which is a cylindrical shape/texture- changing display inspired by the chochin, a traditional Japanese foldable lantern. Ex-CHOCHIN achieves complex control over origami such as local deformation and control in the intermedi- ate process of folding by attaching multiple mechanisms to the origami tessellation. It thereby results in flexible deformations that can be adapted to a wide range of shapes, a one-continuous surface without gaps, and even changes in texture. It creates several de- formed shapes from a crease pattern, allowing flexible deformation to function as a display. We have also produced an application using ex-CHOCHIN.\r\nDuring this hands-on demo at UIST, attendees will manipulate the amount of change in shape and texture and interact with ex- CHOCHIN by seeing and touching.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Keio University ",
              "dsl": "Hiroya Tanaka Lab. (4D Fabrication Lab.)"
            }
          ],
          "personId": 85310
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Keio University",
              "dsl": "Hiroya Tanaka Lab. (4D Fabrication Lab.)"
            }
          ],
          "personId": 85399
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Keio University",
              "dsl": "Hiroya Tanaka Lab. (4D Fabrication Lab.)"
            }
          ],
          "personId": 85301
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Fujisawa",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 85291
        }
      ]
    },
    {
      "id": 85499,
      "typeId": 12315,
      "title": "Anywhere Hoop: Virtual Free Throw Training System",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558639"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1055",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "\"Basketball",
        "Augmented reality",
        "Head-mounted display\""
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "To complete successfully a high percentage of free throws in basketball, the shooter must achieve a stable trajectory with the ball. A player must practice shooting repeatedly to shoot a ball with a stable trajectory. However, traditional practice methods require the preparation of a real basketball hoop, which has made it difficult for some players to prepare a practice environment. We propose a training method for free throws using a virtual basketball hoop. In this paper, we present an implementation of the proposed method and experimental results showing its effectiveness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chiyoda-ku",
              "institution": "NTT DOCOMO, INC",
              "dsl": ""
            }
          ],
          "personId": 85267
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 85317
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 85286
        }
      ]
    },
    {
      "id": 85500,
      "typeId": 12320,
      "title": "Wemoji: Towards Designing Complementary Communication Systems in Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558699"
        },
        "Poster": {
          "hashSum": "kicxPO8OMgkuVtf8TD4kx/uJYg04kxfH4KTCEzj6YGM=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85500/poster/bca46863-36ba-f341-4592-a69bc1a15e43.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85500/poster/06db6312-a539-5e4a-b1f4-526cafb7e37f.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3966",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Augmented Reality (AR) can enable new forms of self-expression and communication. However, little is known about how AR experiences should be designed to complement face-to-face communication. We present an initial set of insights derived from the iterative design of a mobile AR app called Wemoji. It enables people to emote or react to one another by spawning visual AR effects in a shared physical space. As an additional communication medium, it can help add volume and dimension to what is exchanged. We outline a design space for AR complementary communication systems, and offer a set of insights from initial testing that points towards how AR can be used to enhance same-time same-place social interactions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 85327
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85381
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 85284
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85243
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Monica",
              "institution": "Snap Inc.",
              "dsl": ""
            }
          ],
          "personId": 85153
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 85359
        }
      ]
    },
    {
      "id": 85501,
      "typeId": 12320,
      "title": "ICEBOAT: An Interactive User Behavior Analysis Tool for Automotive User Interfaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558739"
        },
        "Poster": {
          "hashSum": "UpVbkaXKllAfFyXilY7tYNAa2cjKAl3GBtC2pg/MYhE=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85501/poster/32ef2554-bf8b-944a-163a-66eb892bb3d3.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85501/poster/d84d2909-e671-e893-90c4-559fa274872f.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2636",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "In this work, we present ICEBOAT an interactive tool that enables automotive UX experts to explore how users interact with In-Vehicle Information Systems. Based on large naturalistic driving data continuously collected from production line vehicles, ICEBOAT visualizes drivers' interactions and driving behavior on different levels of detail. Hence, it allows to easily compare different user flows based on performance- and safety-related metrics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "University of Cologne",
              "dsl": ""
            }
          ],
          "personId": 85344
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "TU Berlin",
              "dsl": ""
            }
          ],
          "personId": 85219
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "MBition GmbH",
              "dsl": ""
            }
          ],
          "personId": 85203
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne ",
              "institution": "University of Cologne ",
              "dsl": ""
            }
          ],
          "personId": 85194
        }
      ]
    },
    {
      "id": 85502,
      "typeId": 12320,
      "title": "ARfy: A Pipeline for Adapting 3D Scenes to Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558697"
        },
        "Poster": {
          "hashSum": "Sl9IV+HvHkfR1tFjLQyViNBcvfbUAtADI6/h97+aVSY=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85502/poster/6602f151-0f6f-07c0-ec0b-b6be362478e9.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85502/poster/ce0f4a11-5de0-ec5a-4f1a-22441cac5c2d.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2597",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Virtual content placement in physical scenes is a crucial aspect of augmented reality (AR). This task is particularly challenging when the virtual elements must adapt to multiple target physical environments unknown during development. AR authors use strategies such as manual placement performed by end-users, automated placement powered by author-defined constraints, and procedural content generation to adapt virtual content to physical spaces. Although effective, these options require human effort or annotated virtual assets. As an alternative, we present ARfy, a pipeline to support the adaptive placement of virtual content from pre-existing 3D scenes in arbitrary physical spaces. ARfy does not require intervention by end-users or asset annotation by AR authors. We demonstrate the pipeline capabilities using simulations on a publicly available indoor space dataset. ARfy makes any generic 3D scene automatically AR-ready and provides evaluation tools to facilitate future research on adaptive virtual content placement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 85207
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 85213
        }
      ]
    },
    {
      "id": 85503,
      "typeId": 12315,
      "title": "Silent subwoofer system using myoelectric stimulation to present the acoustic deep bass experiences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558653"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1053",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "VR live concert",
        "EMS application",
        "acoustic interface",
        "multi-modal media"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "This study demonstrates a portable, low-noise system that utilizes electrical muscle stimulation (EMS) to present a body-sensory acoustic experience similar to that experienced during live concerts. Twenty-four participants wore head-mounted displays (HMDs), headphones, and the proposed system and experienced a live concert in a virtual reality (VR) space to evaluate the system. We found that the system was not inferior to a system with loudspeakers and subwoofers, where ambient noise concerns precision in rhythm and harmony. These results could be explained by the user perceiving the EMS experience as a single signal when the EMS stimulation is presented in conjunction with visual and acoustic stimuli (e.g., the kicking of a bass drum, the bass sound generated from the kicking, and the acoustic sensation caused by the bass sound). The proposed method offers a novel EMS-based body-sensory acoustic experience, and the results of this study may lead to an improved experience not only for live concerts in VR space but also for everyday music listening.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Engineering, Information and Systems"
            }
          ],
          "personId": 85202
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "College of Engineering Systems"
            }
          ],
          "personId": 85184
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Engineering, Information and Systems"
            }
          ],
          "personId": 85259
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "Faculty of Engineering, Information and Systems, University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 85181
        }
      ]
    },
    {
      "id": 85504,
      "typeId": 12320,
      "title": "VLOGS: Virtual Laboratory Observation Tool for Monitoring a Group of Students",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558738"
        },
        "Poster": {
          "hashSum": "6tXjm87k2sA/au44Xwu7Nq5IEPbLG7H5EkvfegqUIME=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85504/poster/c6abd126-fefa-f978-52dc-6130ddd255bc.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85504/poster/0f2ab581-8dbf-8ba0-162d-5e8c950c164e.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-6038",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Virtual laboratories (VLs) enable students to conduct lab experiments in the virtual world using Virtual Reality (VR) technology, providing benefits in areas such as availability, safety as well as scalability. While there are existing platforms that provide VLs with rich content as well as research works on promoting effective learning in VLs, less attention has been paid on VLs from a teaching perspective. Students usually learn and practice in VL sessions with limited help from the instructors. Instructors, on the other hand, could only receive limited information on the performance of the students and could not provide timely feedback to facilitate students' learning. In this work, we present a prototype virtual laboratory monitoring tool, created using a design thinking approach, for addressing teaching needs when conducting a virtual laboratory session simultaneously for multiple students, similar to that in a physical lab environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 85212
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Durham",
              "institution": "Durham University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 85126
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 85332
        }
      ]
    },
    {
      "id": 85505,
      "typeId": 12316,
      "durationOverride": 450,
      "title": "Towards Future Health and Well-being: Bridging Behavior Modeling and Intervention",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558524"
        }
      },
      "isBreak": false,
      "importedId": "uist22e-1021",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86421
      ],
      "eventIds": [],
      "abstract": "With the advent of always-available, ubiquitous devices with powerful passive sensing and active interaction capabilities, the opportunities to integrate AI into this ecosystem have matured, providing an unprecedented opportunity to understand and support user well-being. A wide array of research has demonstrated the potential to detect risky behaviors and address health concerns, using human-centered ML to understand longitudinal, passive behavior logs. Unfortunately, it is difficult to translate these findings into deployable applications without better approaches to providing human-understandable relationship explanations between behavior features and predictions (interpretability); and generalizing to new users and new time periods (robustness). My past work has made significant headway in addressing modeling accuracy, interpretability, and robustness. Moreover, my ultimate goal is to build deployable, intelligent interventions for health and well-being that make use of succeeding ML-based behavior models. I believe that just-in-time interventions are particularly well suited to ML support. I plan to test the value of ML for providing users with a better, interpretable, and robust experience in supporting their well-being.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Information School"
            }
          ],
          "personId": 85199
        }
      ]
    },
    {
      "id": 85506,
      "typeId": 12320,
      "title": "Search with Space: Find and Visualize Furniture in Your Space",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558740"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-8853",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Online shopping in the home category enables quick and convenient access to large catalog of products. In particular, users can simultaneously browse for functional requirements, such as size and material, while evaluating aesthetic fit, such as color and style, across hundreds of product offerings. However, the typical user flow requires navigating to an e-commerce retailer's website first, setting the search/filter parameters that may be generic, and then landing on product pages, one at a time, to make a decision. Upon purchase, \"does not fit\" is among the top reasons for returning a product. Amalgamating the above information, we present Search with Space, a novel interactive approach that a) inputs the user's space as a search parameter to b) filter for product matches that will physically fit, and c) visualize these matches in the user's space at true scale and in a format that facilitates simultaneous comparison. Briefly, the user leverages augmented reality (AR) to set a proxy 3d product in the desired location, updates the proxy's dimensions, and takes photos from preset angles. Using spatial information captured with AR, a web-based gallery page is curated with all the product matches that will physically fit and products are shown at true scale in their original photos. The user may now browse products visualized in the context of their space and evaluate based on their shopping criteria, share the gallery page with designers or partners for asynchronous feedback, re-use the photos for a different product class, or re-capture their space with different criteria altogether. Search with Space inverts the typical user journey by starting with the user's space and maintaining that context across all touch points with the catalog.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Wayfair",
              "dsl": ""
            }
          ],
          "personId": 85261
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Wayfair",
              "dsl": ""
            }
          ],
          "personId": 85162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Wayfair",
              "dsl": ""
            }
          ],
          "personId": 85224
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Wayfair",
              "dsl": "Wayfair Next"
            }
          ],
          "personId": 85131
        }
      ]
    },
    {
      "id": 85507,
      "typeId": 12320,
      "title": "iThem: Programming Internet of Things Beyond Trigger-Action Pattern",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558776"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2352",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "With emerging technologies bringing Internet of Things (IoT) devices into domestic environments, trigger-action programming such as IFTTT with its simple if-this-then-that pattern provides an effective way for end-users to connect fragmented intelligent services and program their own smart home/work space automation. While the simplicity of trigger-action programming can be effective for non-programmers with its straightforward concepts and graphical user interface, it does not allow the algorithmic expressivity that a programming language has. For instance, the simple if-this-then-that structure cannot cover complex algorithms that arise from real world scenarios involving multiple conditions or keeping track of a sequence of conditions (e.g., incrementing counters, triggering one action if two conditions are both true). In this exploratory work, we take an alternative approach by creating a programmable channel between application programming interfaces (APIs), which allows programmers to preserve states and to use them to write complex algorithms. We propose iThem, which stands for intelligence of them—internet of things, that allow programmers to author any complex algorithms that can connect different IoT services and fully unleash the freedom of a general programming language. In this poster, we share the design, development, and ongoing validation progress of iThem, which piggybacks on existing programmable IoT system IFTTT, and which allows for a programmable channel that connects triggers and actions in IFTTT with versatility.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 85150
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 85206
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland, College Park",
              "dsl": "Information Studies"
            }
          ],
          "personId": 85270
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 84885
        }
      ]
    },
    {
      "id": 85508,
      "typeId": 12315,
      "title": "TouchVR: A Modality for Instant VR Experience",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558650"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1038",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Virtual Reality",
        "Mobile Interaction",
        "Inertial Sensor"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In near future, we envision instant and ubiquitous access to the VR worlds. However, existing highly portable VR devices usually lack rich and convenient input modality. In response, we introduce TouchVR, a system that enables BoD interaction in instant VR supported by mobile HMDs.\r\n\r\nWe present the overall architecture and prototype of the TouchVR system in Android platform, capable of being easily integrated to Unity-based mobile VR applications. Furthermore, we implement a sample application (360 video player) to demonstrate the usage of our system. Through TouchVR, we expect it to be a step for enriching interaction methods in instant VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 85220
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 84911
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 84909
        }
      ]
    },
    {
      "id": 85509,
      "typeId": 12315,
      "title": "Demonstration of Fibercuit: Techniques to Prototype High-Resolution Flexible and Kirigami Circuits with a Fiber Laser Engraver",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1039",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "fiber laser",
        "laser cut",
        "PCB",
        "circuit board",
        "kirigami",
        "flexible PCB"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "In this demonstration, we showcase Fibercuit, a set of rapid prototyping techniques to fabricate high-resolution, flexible, and kirigami circuits using a fiber laser engraver. A set of examples will be presented in this demo, including custom dice, flex cables, custom end-stop switches, electromagnetic coils, LED earrings and a kirigami crane circuit designed and fabricated using our techniques.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84574
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84613
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84602
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 84881
        }
      ]
    },
    {
      "id": 85510,
      "typeId": 12315,
      "title": "VRhook: A Data Collection Tool for VR Motion Sickness Research",
      "isBreak": false,
      "importedId": "uist22d-1036",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Despite the increasing popularity of VR games, one factor hindering the industry's rapid growth is motion sickness experienced by the users. Symptoms such as fatigue and nausea severely hamper the user experience. Machine Learning methods could be used to automatically detect motion sickness in VR experiences, but generating the extensive labeled dataset needed is a challenging task. It needs either very time consuming manual labeling by human experts or modification of proprietary VR application source codes for label capturing. To overcome these challenges, we developed a novel data collection tool, VRhook, which can collect data from any VR game without needing access to its source code. This is achieved by dynamic hooking, where we can inject custom code into a game's run-time memory to record each video frame and its associated transformation matrices. Using this, we can automatically extract various useful labels such as rotation, speed, and acceleration. In addition, VRhook can blend a customized screen overlay on top of game contents to collect self-reported comfort scores. In this paper, we describe the technical development of VRhook, demonstrate its utility with an example, and describe directions for future research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            }
          ],
          "personId": 84947
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            }
          ],
          "personId": 84573
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Auckland",
              "city": "Auckland",
              "institution": "Auckland Bio engineering Institute, University Of Auckland ",
              "dsl": "Augmented Human Lab"
            },
            {
              "country": "New Zealand",
              "state": "Auckland",
              "city": "Auckland",
              "institution": "Auckland Bio engineering Institute, University Of Auckland ",
              "dsl": "Augmented Human Lab"
            }
          ],
          "personId": 84654
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Aucklad",
              "institution": "University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Aucklad",
              "institution": "University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            }
          ],
          "personId": 84796
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Mawson Lakes",
              "institution": "University of South Australia",
              "dsl": "ITMS"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Mawson Lakes",
              "institution": "University of South Australia",
              "dsl": "ITMS"
            }
          ],
          "personId": 84728
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 84738
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Facebook",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Facebook",
              "dsl": ""
            }
          ],
          "personId": 84845
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Oculus",
              "dsl": "Research"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Oculus",
              "dsl": "Research"
            }
          ],
          "personId": 84701
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Department of Information Systems and Analytics, National University of Singapore",
              "dsl": "Augmented Human Lab"
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Department of Information Systems and Analytics, National University of Singapore",
              "dsl": "Augmented Human Lab"
            }
          ],
          "personId": 84899
        }
      ]
    },
    {
      "id": 85511,
      "typeId": 12315,
      "title": "Demonstrating the Integration of Real-World Distractions in Virtual Reality",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1037",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Virtual Reality",
        "Haptics",
        "Distractions",
        "Presence"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "With the proliferation of consumer-level virtual reality (VR) devices, users started experiencing VR in less controlled envi- ronments, such as in social gatherings and public areas. While the current VR hardware provides an increasingly immersive experi- ence, it ignores stimuli originating from the physical surroundings that distract users from the VR experience. To block distractions from the outside world, many users wear noise-canceling headphones. However, this is insufficient to block loud or transient sounds (e.g., drilling or hammering) and, especially, multi-modal distractions (e.g., air drafts, temperature shifts from an A/C, construction vi- brations, or food smells). To tackle this, in this demonstration, we let participants experience a new approach, in which we directly integrate the distracting stimuli from the user’s physical surround- ings into their virtual reality experience to enhance presence. Using our novel approach, an otherwise distracting wind gust can be di- rectly mapped to the sway of trees in a VR experience that already contains trees. We demonstrate how to integrate a range of distrac- tive stimuli into the VR experience, such as haptics (temperature, vibrations, touch), sounds, and smells.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84689
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84544
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84662
        }
      ]
    },
    {
      "id": 85512,
      "typeId": 12320,
      "title": "Detecting Changes in User Emotions During Bicycle Riding by Sampling Facial Images",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558710"
        },
        "Poster": {
          "hashSum": "JAL4+IjBG/psc2sDnfbefH3p2Aa7lF7iMvVgAZh2dpI=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85512/poster/f1c18e29-04e4-1317-50ad-6575bdb786ad.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85512/poster/4f1405a8-8152-e293-71ac-0c6880ef58d8.jpg"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-1816",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "In the context of mobility as a Service (MaaS), bicycles are an important mode of transport for the first and last mile between the home and other transport modalities. Also, due to covid-19 bicycle users such as food delivery drivers and commuters to work are increasing. To investigate driving experience of bicycle users in context and improve MaaS service quality, we propose and describe a method to automatically detect changes in user emotions during bicycle riding by sampling facial images using a smartphone. We describe the proposed method and how we plan to use it in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Sangyo University",
              "dsl": "Center for Sciences towards Symbiosis among Human, Machine and Data"
            }
          ],
          "personId": 85386
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Sangyo University",
              "dsl": ""
            }
          ],
          "personId": 85215
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Institute of Technology",
              "dsl": "School of Information and Human Science"
            }
          ],
          "personId": 85331
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Prefecture University",
              "dsl": ""
            }
          ],
          "personId": 85324
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ibaraki",
              "institution": "Osaka University",
              "dsl": "Cybermedia Center"
            }
          ],
          "personId": 85171
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Sangyo University",
              "dsl": "Faculty of Computer Science and Engineering"
            }
          ],
          "personId": 85357
        }
      ]
    },
    {
      "id": 85513,
      "typeId": 12320,
      "title": "WireSketch: Bimanual Interactions for 3D Curve Networks in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558726"
        },
        "Poster": {
          "hashSum": "9JkVMx+s8osO0G2mi20E+G+YmVWcxOsRJB6j1p6lgCE=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85513/poster/23496c94-9a9d-e187-553e-10675d09409a.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85513/poster/a2858444-a477-8fd1-5879-1c4df3f267b7.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-3715",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "3D content authoring in immersive environments has the advantage of allowing users to see a design result on its actual scale in real time. We present a system to intuitively create and modify 3D curve networks using bimanual gestures in virtual reality (VR). Our system provides a rich vocabulary of interactions in which both hands are used harmoniously following simple and intuitive grammar, and supports comprehensive manipulation of 3D curve networks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 85356
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 85339
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 85348
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 85349
        }
      ]
    },
    {
      "id": 85514,
      "typeId": 12320,
      "title": "LV-Linker: Supporting Fine-grained User Interaction Analyses by Linking Smartphone Log and Recorded Video Data",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558714"
        },
        "Poster": {
          "hashSum": "mEFQAmIv6qHjiJCd3p3xEcSZoTWzk0c/5uLSYTT1mP0=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85514/poster/9f06300e-9bac-492f-431a-977757bfde09.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85514/poster/2d96f5e0-86c9-9ebd-a5c1-9a094483ef09.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-1935",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "Data-driven mobile design as an important UI/UX research technique often requires analyzing recorded screen video data and time-series usage log data, because it helps to obtain a deeper understanding of fine-grained usage behaviors. However, there is a lack of interactive tools that support simultaneously navigation of both mobile usage log and video data. In this paper, we propose LV-Linker (Log and Video Linker), a web-based data viewer system for synchronizing both smartphone usage log and video data to help researchers quickly to analyze and easily understand user behaviors. We conducted a preliminary user study and evaluated the benefits of linking both data by measuring task completion time, helpfulness, and subjective task workload. Our results showed that offering linked navigation significantly lowers the task completion time and task workload, and promotes data understanding and analysis fidelity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology",
              "dsl": "School of Computing, Interactive Computing lab"
            }
          ],
          "personId": 85161
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 85134
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology",
              "dsl": "School of Computing/ Korea Advanced Institute of Science and Technology/ ICLab"
            }
          ],
          "personId": 85365
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 85232
        }
      ]
    },
    {
      "id": 85515,
      "typeId": 12315,
      "title": "Demonstration of Grid-Coding: An Accessible, Efficient, and Structured Coding Paradigm for Blind and Low-Vision Programmers",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1045",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Accessibility",
        "programming languages",
        "blind and low-vision",
        "programmers",
        "Python",
        "code writing",
        "grid-coding"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We demonstrate Grid Editor, an implementation of our novel Grid-Coding paradigm for addressing code navigation, editing, and associated challenges for blind and low-vision (BLV) programmers with text-based programming. Grid Editor is implemented as a spreadsheet-like web application to render Python code in a structured 2D grid. The Editor represents a single line with a row, a single scope with a column, and replaces traditional whitespace-based indentation with meaningful indentation cells. As a result, Grid-Editor supports quickly navigating source code to find context, detecting syntax errors, managing indentation, and writing syntax-error-free source code.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 84560
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Bangladesh University of Engineering and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 84831
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park ",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 84906
        }
      ]
    },
    {
      "id": 85516,
      "typeId": 12316,
      "durationOverride": 450,
      "title": "Extending Computational Abstractions with Manual Craft for Visual Art Tools",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558527"
        }
      },
      "isBreak": false,
      "importedId": "uist22e-1018",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86421
      ],
      "eventIds": [],
      "abstract": "Programming and computation are powerful tools for manipulating visual forms, but working with these abstractions is challenging for artists who are accustomed to direct manipulation and manual control. The goal of my research is to develop visual art tools that extend programmatic capabilities with manual craft. I do so by exposing computational abstractions as transparent materials that artists may directly manipulate and observe in a process that accommodates their non-linear workflows. Specifically, I conduct empirical research to identify challenges professional artists face when using existing software tools—as well as programming their own—to make art. I apply principles derived from these findings in two projects: an interactive programming environment that links code, numerical information, and program state to artists' ongoing artworks, and an algorithm that automates the rigging of character clothing to bodies to allow for more flexible and customizable 2D character illustrations. Evaluating these interactions, my research promotes authoring tools that support arbitrary execution by adapting to the existing workflows of artists.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 85195
        }
      ]
    },
    {
      "id": 85517,
      "typeId": 12315,
      "title": "Prolonging VR Haptic Experiences by Harvesting Kinetic Energy from the User ",
      "isBreak": false,
      "importedId": "uist22d-1043",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We propose a new technical approach to implement untethered VR haptic devices that contain no battery, yet they can render on-demand haptic feedback. The key is that via our approach, a haptic device charges itself by harvesting the user’s kinetic energy (i.e., movement)—even without the user needing to realize this. This is achieved by integrating the energy-harvesting with the virtual experience, in a responsive manner. Whenever our batteryless haptic device is about to lose power, it switches to harvesting-mode (by engaging its clutch to a generator) and, simultaneously, the VR headset renders an alternative version of the current experience that depicts resistive forces (e.g., rowing a boat in VR). As a result, the user feels realistic haptics that correspond to what they should be feeling in VR, while unknowingly charging the device via their movements. Once the haptic device’s supercapacitors are charged, they wake up its microcontroller to communicate with the VR headset. The VR experience can now use the recently harvested power for on-demand haptics, including vibration, electrical or mechanical force-feedback; this process can be repeated, ad infinitum. We instantiated a version of our concept by implementing an exoskeleton (with vibration, electrical & mechanical force-feedback) that harvests the user’s arm movements. We validated it via a user study, in which participants, even without knowing the device was harvesting, rated its’ VR experience as more realistic & engaging than with a baseline VR setup. Finally, we believe our approach enables haptics for prolonged uses, especially useful in untethered VR setups, since devices capable of haptic feedback are traditionally only reserved for situations with ample power. Instead, with our approach, a user who engages in hours-long VR and grew accustomed to finding a battery-dead haptic device that no longer works, will simply resurrect the haptic device with their movement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84675
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84857
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84662
        }
      ]
    },
    {
      "id": 85518,
      "typeId": 12320,
      "title": "PerSign: Personalized Bangladeshi Sign Letters Synthesis",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558712"
        },
        "Poster": {
          "hashSum": "S4YjjJ5Bdog5RJ9TujkURD2PAOsK7nKDIi6e9HjZUA4=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85518/poster/a36867e7-6349-abd1-466d-3c68cfd36f1e.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85518/poster/c343ba6e-4ea7-3e2d-9eab-6c055df34be8.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-6704",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Bangladeshi Sign Language (BdSL)—like other sign languages— is tough to learn for general people, especially when it comes to expressing letters. In this poster, we propose PerSign, a system that can reproduce a person’s image by introducing sign gestures in it. We make this operation “personalized”, which means the generated image keeps the person’s initial image profile–face, skin tone, attire, background—unchanged while altering the hand, palm, and finger positions appropriately. We use an image-to-image translation technique and build a corresponding unique dataset to accomplish the task. We believe the translated image can reduce the communication gap between signers1 and non-signers without having prior knowledge of BdSL.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Computer Science"
            }
          ],
          "personId": 85156
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science & Technology",
              "dsl": "CSE"
            }
          ],
          "personId": 85341
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 85264
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science and Technology",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 85276
        },
        {
          "affiliations": [
            {
              "country": "Bangladesh",
              "state": "",
              "city": "Dhaka",
              "institution": "Ahsanullah University of Science & Technology",
              "dsl": "Computer Science Engineering"
            }
          ],
          "personId": 85306
        }
      ]
    },
    {
      "id": 85519,
      "typeId": 12320,
      "title": "The Reflective Maker: Using Reflection to Support Skill-learning in Makerspaces",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558716"
        },
        "Poster": {
          "hashSum": "2nW0MD9tUoYCyZ/w9js5/Om/Yqew/SJjL6RpTTmwSY0=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/85519/poster/e7b1c6ee-acea-3a14-3b1b-d609f0b1c529.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/85519/poster/e1d51762-4eed-d241-3239-784a4f5d36c0.pdf"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-4643",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "In recent years, while HCI researchers have developed several systems that leverage the use of reflection for skill learning, the use of reflection-based learning of maker skills remains unexplored. We present ReflectiveMaker - a toolkit for experts and educators to design reflection exercises for novice learners in makerspaces. We describe the three components of our toolkit: (a) a designer interface to author the reflection prompts during fabrication activities, (b) a set of fabrication tools to sense the user’s activities and (c) a reflection diary interface to record the user’s reflections and analyze data on their learning progress. We then outline future work and envision a range of application scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT CSAIL"
            }
          ],
          "personId": 85385
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Cognitive Science and Design Lab"
            }
          ],
          "personId": 85394
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85233
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84695
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84643
        }
      ]
    },
    {
      "id": 85520,
      "typeId": 12315,
      "title": "Demonstrating p5.fab: Direct Control of Digital Fabrication Machines from a Creative Coding Environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558646"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1041",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Calibration",
        "Interactive Fabrication",
        "Computational Design",
        "p5.js",
        "3D Printing",
        "Maintenance",
        "Craft"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Machine settings and tuning are critical for digital fabrication outcomes. However, exploring these parameters is non-trivial. We seek to enable exploration of the full design space of digital fabrication. To do so, we built p5.fab, a system for controlling digital fabrication machines from the creative coding environment p5.js and informed by a qualitative study of 3D printer maintenance practices. p5.fab prioritizes material exploration, fine-tuned control, and iteration in fabrication workflows. We demonstrate p5.fab with examples of 3D prints that cannot be made with traditional 3D printing software, including delicate bridging structures and prints on top of existing objects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85274
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85369
        }
      ]
    },
    {
      "id": 85521,
      "typeId": 12320,
      "title": "Gustav: Cross-device Cross-computer Synchronization of Sensory Signals",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558723"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-2344",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "Temporal synchronization of behavioral and physiological signals collected through different devices (and sometimes through different computers) is a longstanding challenge in HCI, neuroscience, psychology, and related areas. Previous research has proposed to synchronize sensory signals using (1) dedicated hardware; (2) dedicated software; or (3) alignment algorithms. All these approaches are either vendor-locked, non-generalizable, or difficult to adopt in practice. We propose a simple but highly efficient alternative: instrument the stimulus presentation software by injecting supervisory event-related timestamps, followed by a post-processing step over the recorded log files. Armed with this information, we introduce Gustav, our approach to orchestrate the recording of sensory signals across devices and computers. Gustav ensures that all signals coincide exactly with the duration of each experiment condition, with millisecond precision. Gustav is publicly available as open source software.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": ""
            }
          ],
          "personId": 85383
        },
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": ""
            }
          ],
          "personId": 85355
        }
      ]
    },
    {
      "id": 85522,
      "typeId": 12315,
      "title": "Demonstrating Dynamic Toolchains for Machine Control",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558662"
        }
      },
      "isBreak": false,
      "importedId": "uist22d-1042",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "toolpath design",
        "plotting",
        "watercolor",
        "digital fabrication"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Humans are increasingly able to work side-by-side with desktop-scale digital fabrication machines. However, much of the software for controlling these machines does not support live, interactive exploration of their capabilities. We present Dynamic Toolchains, an extensible development framework for building parametric machine control interfaces from reusable modules. Toolchains are built and run in a live environment, removing the repetitive import and export bottleneck between software programs. This enables humans to easily explore how they can use machine precision to manipulate physical materials and achieve unique aesthetic outcomes. In this demonstration, we build a toolchain for computer-controlled watercolor painting and show how it facilitates rapid iteration on brush stroke patterns.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Human Centered Design and Engineering"
            }
          ],
          "personId": 85372
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85369
        }
      ]
    },
    {
      "id": 85523,
      "typeId": 12320,
      "title": "FormSense: A Fabrication Method to Support Shape Exploration of Interactive Prototypes",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558730"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-6821",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86422,
        86423
      ],
      "eventIds": [],
      "abstract": "When exploring the shape of interactive objects, existing prototyping methods can conflict with the iterative process. In this paper, we present FormSense: a simple, fast, and modifiable fabrication method to support the exploration of shape when prototyping interactive objects. FormSense enables touch and pressure sensing through a multi-layer coating approach and a custom touch sensor built from commodity electronic components. We use FormSense to create four interactive prototypes of diverse geometries and materials.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ACME Lab, ATLAS Institute"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 85269
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            }
          ],
          "personId": 85361
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            }
          ],
          "personId": 85176
        }
      ]
    },
    {
      "id": 85524,
      "typeId": 12320,
      "title": "A11yBoard: Using Multimodal Input and Output to Make Digital Artboards Accessible to Blind Users",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3558695"
        }
      },
      "isBreak": false,
      "importedId": "uist22b-9899",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424,
        86425
      ],
      "eventIds": [],
      "abstract": "We present A11yBoard, an interactive multimodal system that makes interpreting and authoring digital artboards, such as presentation slides or vector drawings, accessible to blind and low-vision (BLV) users. A11yBoard combines a web-based application with a mobile touch screen device such as a smartphone or tablet. The artboard is mirrored from the PC onto the touch screen, enabling spatial exploration of the artboard via touch and gesture. In addition, speech recognition and non-speech audio are used for input and output, respectively. Finally, keyboard input is used with a custom search-driven command line interface to access various commands and properties. These modalities combine into a rich, accessible system in which artboard contents, such as shapes, lines, text boxes, and images, can be interpreted, generated, and manipulated with ease. With A11yBoard, BLV users can not only consume accessible content, but create their own as well.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Information School"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Information School"
            }
          ],
          "personId": 85326
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 85333
        }
      ]
    },
    {
      "id": 85525,
      "typeId": 12315,
      "title": "Demonstrating a High-Precision Pen that Senses Translation and Rotation on Passive Surfaces",
      "addons": {},
      "isBreak": false,
      "importedId": "uist22d-1040",
      "source": "PCS",
      "trackId": 11862,
      "tags": [],
      "keywords": [
        "Pen input",
        "Rotation sensing",
        "Haptic feedback"
      ],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We demonstrate our integrated pen input device called DeltaPen, which features two adjacent lens-less optical flow sensors next to its tip, from which it reconstructs directional motion as well as pen rotation. A pressure sensor and high-fidelity haptic actuator complements our pen device while retaining a compact form factor that supports mobile use without specialized surfaces and without external tracking. A built-in inertial measurement unit (streaming magnetometer and accelerometer data) further improves the precision of our device and enables tilt input. We showcase a series of example interactions and small applications that leverage pen translation and rotation. Those examples include a photo sorting application, widgets that utilize haptic feedback as well as rotation input, and a simple painting application.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84626
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84860
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 84652
        }
      ]
    },
    {
      "id": 85548,
      "typeId": 12364,
      "title": "UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561350"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-9633",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We introduce UltraBots, a system that combines ultrasound haptic feedback and robotic actuation for large-area mid-air haptics for VR. Ultrasound haptics can provide precise mid-air haptic feedback and versatile shape rendering, but the interaction area is often limited by the small size of the ultrasound devices, restricting the possible interactions for VR. To address this problem, this paper introduces a novel approach that combines robotic actuation with ultrasound haptics. More specifically, we will attach ultrasound transducer arrays to tabletop mobile robots or robotic arms for scalable, extendable, and translatable interaction areas. We plan to use Sony Toio robots for 2D translation and/or commercially available robotic arms for 3D translation. Using robotic actuation and hand tracking measured by a VR HMD (ex: Oculus Quest), our system can keep the ultrasound transducers underneath the user’s hands to provide on-demand haptics. We demonstrate applications with workspace environments, medical training, education and\r\nentertainment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84761
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 85378
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 84611
        }
      ]
    },
    {
      "id": 85549,
      "typeId": 12364,
      "title": "UltraBat: An Interactive 3D Side-Scrolling Game using Ultrasound Levitation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561344"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-4372",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We present UltraBat, an interactive 3D side-scrolling game inspired by Flappy Bird, in which the game character, a bat, is physically levitated in mid-air using ultrasound. \r\nPlayers aim to navigate the bat through a stalagmite tunnel that scrolls to one side as the bat travels, which is implemented using a pin-array display to create a shape-changing passage.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Electrical Engineering"
            }
          ],
          "personId": 85539
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 85538
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 85535
        }
      ]
    },
    {
      "id": 85550,
      "typeId": 12364,
      "title": "Magic Drops:Food 3D Printing of Colored Liquid Balls by Ultrasound Levitation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561348"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-2525",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We introduce the concept of “Magic Drops\", the process which is all using ultrasound levitation to mix multiple liquid drops into a single one in the air and move it specified position and let it free fall below. For molecular gastronomy application, mixture drops with sodium alginate solution are free-fall into a container filled with calcium lactate solution. With this, drops encased in a calcium alginate film are formed in the container, these are edible and the color and flavor of mixture are also controlled through the process. We will also demonstrate stack these drops to create larger edible structure. Our novel mixture drop control technology has other potential applications such as painting techniques and drug development. Thus, we believe that this concept will become a new technology for mixing liquids in the future.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kagoshima",
              "institution": "Kagoshima University",
              "dsl": ""
            }
          ],
          "personId": 85532
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kagoshima",
              "city": "Kagoshima ",
              "institution": "Kagoshima University",
              "dsl": ""
            }
          ],
          "personId": 85529
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kagoshima",
              "city": "Kagoshima",
              "institution": "Kagoshima University",
              "dsl": "Center for Management of Information Technologies"
            }
          ],
          "personId": 85546
        }
      ]
    },
    {
      "id": 85551,
      "typeId": 12364,
      "title": "LeviCircuits: Adhoc Electrical Circuit Prototyping using Ultrasound Levitation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561352"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-9935",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We propose LeviCircuits, an adhoc circuit prototyping platform used for circuit tele-manipulation via ultrasound levitation. LeviCircuits consist of levitating conductive magnetic nodes that create temporary connections when in contact with one another. These nodes are attached to thin wires and electronic components with small footprints such as LEDs, resistors, and capacitors. By controlling the position of LeviCircuit nodes with an ultrasound array, users are able to dynamically change physical circuit connections without the need for direct contact with the components. \r\nOur LeviCircuit platform enables tele-manipulation of electrical circuits in 3D space for remote learning, debugging, and collaboration, as well as the creation of 3-Dimensional circuit connections.To investigate LeviCircuit's nodes' properties and reliability, we ran a series of preliminary explorations for\r\n(1) The print quality of 3D printed LeviCircuit nodes (2) The quality of electrical contact, connectivity, and conductivity. We conclude from our preliminary explorations that weak magnetic attraction between nodes is necessary to enhance the electrical contact reliability.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85533
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 85547
        }
      ]
    },
    {
      "id": 85552,
      "typeId": 12364,
      "title": "ShadowAstro: Levitating Constellation Silhouette for Spatial Exploration and Learning ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561345"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-7714",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We introduce ShadowAstro, a system that uses the levitating particles' casted shadow to produce a constellation pattern. In contrast to the traditional approach of making astronomical observations via AR, planetarium, and computer screens, we intend to use the shadows created by each levitated bead to construct the silhouette of constellations - a natural geometrical pattern that can be represented by a set of particles. In this proposal, we show that ShadowAstro can help users inspect the 12 constellations on the ecliptic plane and augment users' experience with a projector that will serve as the light source. Through this, we draw a future vision, where ShadowAstro can serve as an interactive tool with educational purposes or an art installation in museum. We believe the concept of designing interactions between the levitated objects and their casted shadows will provide a brand new experience to end user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 84919
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 85527
        }
      ]
    },
    {
      "id": 85553,
      "typeId": 12364,
      "title": "Shadow Play using Ultrasound Levitated Props",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561349"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-9829",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Shadow play is a traditional culture to communicate narratives. However, its inheritance is in danger, and traditional methods have limitations in their expression. We propose a novel system to perform a shadow play by levitating props using an ultrasound speaker array. Our system computes an ideal position of levitating props to create a shadow of the desired image. Shadow play will be performed by displaying a sequence of images as shadows. Since the performance is automated, our work makes shadow play accessible to people for generations. Also, our system allows 6 DoF and floating movement of props, which expands the limit of expression. Through this system, we aim to enhance shadow plays informatically and aesthetically.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 85537
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": ""
            }
          ],
          "personId": 85543
        }
      ]
    },
    {
      "id": 85554,
      "typeId": 12364,
      "title": "DAWBalloon: An Intuitive Musical Interface Using Floating Balloons",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561354"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-7416",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "The development of music synthesis technology has created a way to enjoy listening to music and actively manipulate it. However, it is difficult for an amateur to combine sounds or change pitches to operate a DAW (Digital Audio Workstation). Therefore, we focused on ultrasonic levitation and haptic feedback to develop an appropriate interface for DAW. We propose \"DAWBalloon\", a system that uses ultrasonic levitation arrays to visualize rhythms using floating balloons as a metaphor for musical elements and to combine sounds by manipulating the balloons. DAWBalloon realizes the intuitive manipulation of sounds in three dimensions, even for people without knowledge of music.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa, Yokohama ",
              "institution": "Aoyama Gakuin University ",
              "dsl": "Aoyama Gakuin University, Itoh Lab"
            }
          ],
          "personId": 85528
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa, Kawasaki",
              "institution": "Aoyama Gakuin University",
              "dsl": "Aoyama Gakuin University, Itoh Lab"
            }
          ],
          "personId": 85180
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Aoyama Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 85328
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sagamihara",
              "institution": "Aoyama Gakuin University",
              "dsl": "College of Science and Engineering"
            }
          ],
          "personId": 85353
        }
      ]
    },
    {
      "id": 85555,
      "typeId": 12364,
      "title": "Top-Levi: Multi-User Interactive System Using Acoustic Levitation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561347"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-3699",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "Top-Levi is a public multi-user interactive system that requires a pair of users to cooperate with an audience around them. Based on acoustic levitation technology, Top-Levi leverages a unique attribute of dynamic physical 3D contents displayed and animated in the air: such systems inherently provide different visual information to users depending on where they are around the device. In Top-Levi, there are two primary users on opposite (left/right) sides of the device, and audience members to the front. Each sees different instructions displayed on a floating cube. Their collective task is to cooperate to move the cube from a start point to a final destination by synchronizing their responses to the instructions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "UNIST",
              "dsl": "Design"
            }
          ],
          "personId": 85531
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology",
              "dsl": "Department of Human Factors Engineering"
            }
          ],
          "personId": 85534
        }
      ]
    },
    {
      "id": 85556,
      "typeId": 12364,
      "title": "Improving 3D-Editing Workflows via Acoustic Levitation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561353"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-9784",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We outline how to improve common 3D-editing workflows such as modeling or character animation by utilizing an acoustic levitation kit as an interactive 3D display. Our proposed system allows users to directly interact with models in 3D space and perform multi-point gestures to manipulate them. Editing of complex 3D objects can be enabled by combining the 3D display with an LCD, projector or HMD to display additional context.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 85540
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 85541
        }
      ]
    },
    {
      "id": 85557,
      "typeId": 12364,
      "title": "Garnish into Thin Air",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3526114.3561351"
        }
      },
      "isBreak": false,
      "importedId": "uist22g-4540",
      "source": "PCS",
      "trackId": 11870,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86420
      ],
      "eventIds": [],
      "abstract": "We propose Garnish into Thin Air, dynamic and three-dimensional food presentation with acoustic levitation. In contrast to static plating on the dishes, we enable mid-air 3D decoration, and the whole garnishing process serves as an interactive experience to stimulate users' appetite by leveraging acoustic levitation's capacity to actuate edibles. To achieve Garnish into Thin Air, our system is built to orchestrate a range of edible materials, such as flavored droplets, edible beads, and rice paper cutouts. We demonstrate Garnish into Thin Air with two example experiences, including a glass of cocktail named \"Floral Party\" and a plate of dessert called \"Winter Twig.\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 85544
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 85545
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 84856
        }
      ]
    },
    {
      "id": 85644,
      "typeId": 12389,
      "title": "TickleFoot: Design, Development and Evaluation of a Novel Foot-tickling Mechanism that Can Evoke Laughter",
      "isBreak": false,
      "importedId": "tochi-1",
      "source": "CSV",
      "trackId": 11893,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86198
      ],
      "eventIds": [],
      "abstract": "Tickling\n is a type of sensation that is associated with laughter, smiling, or \nother similar reactions. Psychology research has shown that tickling and\n laughter can significantly relieve stress. Although several tickling \nartifacts have been suggested in prior work, limited knowledge is \navailable if those artifacts could evoke laughter. In this article, we \naim at filling this gap by designing and developing a novel \nfoot-tickling mechanism that can evoke laughter. We first developed an \nactuator that can create tickling sensations along the sole of the foot \nutilising magnet-driven brushes. Then, we conducted two studies to \nidentify the most ticklish locations of the foot’s sole and stimulation \npatterns that can evoke laughter. In a follow-up study with a new set of\n participants, we confirmed that the identified stimuli could evoke \nlaughter. From the participants’ feedback, we derived several \napplications that such a simulation could be useful. Finally, we \nembedded our actuators into a flexible insole, demonstrating the \npotential of a wearable tickling insole.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Auckland",
              "institution": "The University of Auckland"
            }
          ],
          "personId": 85374
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Auckland",
              "institution": "The University of Auckland"
            }
          ],
          "personId": 85640
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "LÃ¼beck",
              "institution": "Technical University of Applied Sciences LÃ¼beck"
            }
          ],
          "personId": 84899
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Auckland",
              "institution": "The University of Auckland"
            }
          ],
          "personId": 85643
        }
      ]
    },
    {
      "id": 85652,
      "typeId": 12389,
      "title": "ANISMA: A Prototyping Toolkit to Explore Haptic Skin Deformation Applications Using Shape-Memory Alloys",
      "isBreak": false,
      "importedId": "tochi-2",
      "source": "CSV",
      "trackId": 11893,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        85639
      ],
      "eventIds": [],
      "abstract": "We\n present ANISMA, a software and hardware toolkit to prototype on-skin \nhaptic devices that generate skin deformation stimuli like pressure, \nstretch, and motion using shape-memory alloys (SMAs). Our toolkit embeds\n expert knowledge that makes SMA spring actuators more accessible to \nhuman–computer interaction (HCI) researchers. Using our software tool, \nusers can design different actuator layouts, program their \nspatio-temporal actuation and preview the resulting deformation behavior\n to verify a design at an early stage. Our toolkit allows exporting the \nactuator layout and 3D printing it directly on skin adhesive. To test \ndifferent actuation sequences on the skin, a user can connect the SMA \nactuators to our customized driver board and reprogram them using our \nvisual programming interface. We report a technical analysis, verify the\n perceptibility of essential ANISMA skin deformation devices with 8 \nparticipants, and evaluate ANISMA regarding its usability and supported \ncreativity with 12 HCI researchers in a creative design task.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Auckland",
              "institution": "The University of Auckland"
            }
          ],
          "personId": 85648
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Auckland",
              "institution": "The University of Auckland"
            }
          ],
          "personId": 85646
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Aachen",
              "institution": "RWTH Aachen University"
            }
          ],
          "personId": 85647
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Aachen",
              "institution": "RWTH Aachen University"
            }
          ],
          "personId": 85649
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Auckland",
              "institution": "The University of Auckland"
            }
          ],
          "personId": 85650
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Aachen",
              "institution": "RWTH Aachen University"
            }
          ],
          "personId": 85651
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Auckland",
              "institution": "The University of Auckland"
            }
          ],
          "personId": 85645
        }
      ]
    },
    {
      "id": 86322,
      "typeId": 12320,
      "title": "UIST'20: HandMorph: a Passive Exoskeleton that Miniaturizes Grasp",
      "isBreak": false,
      "importedId": "old_papers-10",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "We engineered an exoskeleton, which we call HandMorph, that approximates the experience of having a smaller grasping range. It uses mechanical links to transmit motion from the wearer’s fingers to a smaller hand with five anatomically correct fingers. The result is that HandMorph miniaturizes a wearer’s grasping range while transmitting haptic feedback. Unlike other size-illusions based on virtual reality, HandMorph achieves this in the user’s real environment, preserving the user’s physical and social contexts. As such, our device can be integrated into the user’s workflow, e.g., to allow product designers to momentarily change their grasping range into that of a child while evaluating a toy prototype. In our first user study, we found that participants perceived objects as larger when wearing HandMorph, which suggests that their size perception was successfully transformed. In our second user study, we assessed the experience of using HandMorph in designing a simple toy trumpet for children. We found that participants felt more confident in their toy design when using HandMorph to validate its ergonomics.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84535
        },
        {
          "affiliations": [],
          "personId": 86352
        },
        {
          "affiliations": [],
          "personId": 86405
        },
        {
          "affiliations": [],
          "personId": 84675
        },
        {
          "affiliations": [],
          "personId": 86358
        },
        {
          "affiliations": [],
          "personId": 86409
        },
        {
          "affiliations": [],
          "personId": 84662
        }
      ]
    },
    {
      "id": 86323,
      "typeId": 12320,
      "title": "UIST'21: Do We Need a Faster Mouse? Empirical Evaluation of Asynchronicity-Induced Jitter",
      "isBreak": false,
      "importedId": "old_papers-12",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "In gaming, accurately rendering input signals on a display is crucial, both spatially and temporally. However, the asynchronicity between the input and output signal frequencies results in unstable responses called \"jitter.\" A recent research modeled this jitter mathematically; however, the effect of jitter on human performance is unknown. In this study, we investigated the empirical effect of asynchronicity-induced jitter using a state-of-the-art high-performance mouse and monitor device. In the first part, perceptual user experience under different jitter levels was examined using the ISO 4120:2004 triangle test protocol, and a jitter of over 0.3 ms could be perceived by sensitive subjects. In the second part, we measured the pointing task performance for different jitter levels using the ISO 9241-9 (i.e., Fitts' law) test, and found that the pointing performance was unaffected up to a jitter of 1 ms. Finally, we recommended display and mouse combinations based on our results, which indicated the need for a higher mouse polling rate than that of the current standard 1000-Hz USB mouse.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86354
        },
        {
          "affiliations": [],
          "personId": 86400
        },
        {
          "affiliations": [],
          "personId": 86406
        }
      ]
    },
    {
      "id": 86324,
      "typeId": 12320,
      "title": "UIST'21: Route Tapestries: Navigating 360° Virtual Tour Videos Using Slit-Scan Visualizations",
      "isBreak": false,
      "importedId": "old_papers-11",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "An increasingly popular way of experiencing remote places is by viewing 360° virtual tour videos, which show the surrounding view while traveling through an environment. However, finding particular locations in these videos can be difficult because current interfaces rely on distorted frame previews for navigation. To alleviate this usability issue, we propose Route Tapestries, continuous orthographic-perspective projection of scenes along camera routes. We first introduce an algorithm for automatically constructing Route Tapestries from a 360° video, inspired by the slit-scan photography technique. We then present a desktop video player interface using a Route Tapestry timeline for navigation. An online evaluation using a target-seeking task showed that Route Tapestries allowed users to locate targets 22% faster than with YouTube-style equirectangular previews and reduced the failure rate by 75% compared to a more conventional row-of-thumbnail strip preview. Our results highlight the value of reducing visual distortion and providing continuous visual contexts in previews for navigating 360°virtual tour videos.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86404
        },
        {
          "affiliations": [],
          "personId": 86362
        },
        {
          "affiliations": [],
          "personId": 86393
        },
        {
          "affiliations": [],
          "personId": 86385
        },
        {
          "affiliations": [],
          "personId": 84632
        },
        {
          "affiliations": [],
          "personId": 84678
        }
      ]
    },
    {
      "id": 86325,
      "typeId": 12320,
      "title": "UIST'20: SchemaBoard: Supporting Correct Assembly of Schematic Circuits using Dynamic In-Situ Visualization",
      "isBreak": false,
      "importedId": "old_papers-9",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Assembling circuits on breadboards using reference designs is a common activity among makers. While tools like Fritzing offer a simplified visualization of how components and wires are connected, such pictorial depictions of circuits are rare in formal educational materials and the vast bulk of online technical documentation. Electronic schematics are more common but are perceived as challenging and confusing by novice makers. To improve access to schematics, we propose SchemaBoard, a system for assisting makers in assembling and inspecting circuits on breadboards from schematic source materials. SchemaBoard uses an LED matrix integrated underneath a working breadboard to visualize via light patterns where and how components should be placed, or to highlight elements of circuit topology such as electrical nets and connected pins. This paper presents a formative study with 16 makers, the SchemaBoard system, and a summative evaluation with an additional 16 users. Results indicate that SchemaBoard is effective in reducing both the time and the number of errors associated with building a circuit based on a reference schematic, and for inspecting the circuit for correctness after its assembly.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86380
        },
        {
          "affiliations": [],
          "personId": 86401
        },
        {
          "affiliations": [],
          "personId": 86361
        },
        {
          "affiliations": [],
          "personId": 86416
        },
        {
          "affiliations": [],
          "personId": 86381
        },
        {
          "affiliations": [],
          "personId": 84666
        },
        {
          "affiliations": [],
          "personId": 86388
        },
        {
          "affiliations": [],
          "personId": 84673
        }
      ]
    },
    {
      "id": 86326,
      "typeId": 12320,
      "title": "UIST'21: Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots",
      "isBreak": false,
      "importedId": "old_papers-8",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Automated understanding of user interfaces (UIs) from their pixels can improve accessibility, enable task automation, and facilitate interface design without relying on developers to comprehensively provide metadata.\nA first step is to infer what UI elements exist on a screen, but current approaches are limited in how they infer how those elements are semantically grouped into structured interface definitions.\nIn this paper, we motivate the problem of screen parsing, the task of predicting UI elements and their relationships from a screenshot.\nWe describe our implementation of screen parsing and provide an effective training procedure that optimizes its performance.\nIn an evaluation comparing the accuracy of the generated output, we find that our implementation significantly outperforms current systems (up to 23%).\nFinally, we show three example applications that are facilitated by screen parsing: (i) UI similarity search, (ii) accessibility enhancement, and (iii) code generation from UI screenshots.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84842
        },
        {
          "affiliations": [],
          "personId": 86387
        },
        {
          "affiliations": [],
          "personId": 86399
        },
        {
          "affiliations": [],
          "personId": 84952
        }
      ]
    },
    {
      "id": 86327,
      "typeId": 12320,
      "title": "UIST'21: DextrEMS: Achieving Dexterity in Electrical Muscle Stimulation by Combining it with Brakes",
      "isBreak": false,
      "importedId": "old_papers-7",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Electrical muscle stimulation (EMS) is an emergent technique that miniaturizes force feedback, especially popular for untethered haptic devices, such as mobile gaming, VR or AR. However, the actuation displayed by interactive systems based on EMS is coarse and imprecise. EMS systems mostly focus on inducing movements in large muscle groups such as legs, arms and wrists; whereas individual finger poses, which would be required, for example, to actuate a user’s fingers to fingerspell even the simplest letters in sign language, are not possible. The lack of dexterity in EMS stems from two fundamental limitations: (1) lack of independence: when a particular finger is actuated by EMS, the current runs through nearby muscles, causing unwanted actuation of adjacent fingers; and, (2) lack of precision: while it is relatively easy for EMS to start moving a finger, it is very hard for EMS to stop and hold that finger at a precise angle; because, to stop a finger, virtually all EMS systems contract the opposing muscle, typically achieved via controllers (e.g., PID)—unfortunately, even with best controller tuning, this still often results in unwanted oscillations. To tackle these limitations, we propose dextrEMS, an EMS-based haptic device featuring a lightweight mechanical brake attached to each finger joint. The key idea behind dextrEMS is that while the EMS actuates the fingers, it is our mechanical brake that stops the finger in a precise position. Moreover, it is also the brakes that allows dextrEMS to select which fingers are moved by EMS, eliminating unwanted movements by preventing adjacent fingers from moving. We implemented dextrEMS as an untethered haptic device that actuates eight finger joints independently (metacarpophalangeal and proximal interphalangeal joints for four fingers), which we demonstrate in a wide range of haptic applications, such as assisted fingerspelling, a piano tutorial, guitar tutorial and a VR game. Finally, in our technical evaluation, we found that dextrEMS outperformed EMS alone in both finger independence and precision.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84544
        },
        {
          "affiliations": [],
          "personId": 84675
        },
        {
          "affiliations": [],
          "personId": 86376
        },
        {
          "affiliations": [],
          "personId": 84689
        },
        {
          "affiliations": [],
          "personId": 84662
        }
      ]
    },
    {
      "id": 86328,
      "typeId": 12320,
      "title": "UIST'21: Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation",
      "isBreak": false,
      "importedId": "old_papers-6",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "We propose a haptic device that alters the perceived softness of real rigid objects without requiring to instrument the objects. Instead, our haptic device works by restricting the user’s fingerpad lateral deformation via a hollow frame that squeezes the sides of the fingerpad. This causes the fingerpad to become bulgier than it originally was—when users touch an object’s surface with their now-restricted fingerpad, they feel the object to be softer than it is. To illustrate the extent of softness illusion induced by our device, touching the tip of a wooden chopstick will feel as soft as a rubber eraser. Our haptic device operates by pulling the hollow frame using a motor. Unlike most wearable haptic devices, which cover up the user’s fingerpad to create force sensations, our device creates softness while leaving the center of the fingerpad free, which allows the users to feel most of the object they are interacting with. This makes our device a unique contribution to altering the softness of everyday objects, creating “buttons” by softening protrusions of existing appliances or tangibles, or even, altering the softness of handheld props for VR. Finally, we validated our device through two studies: (1) a psychophysics study showed that the device brings down the perceived softness of any object between 50A-90A to around 40A (on Shore A hardness scale); and (2) a user study demonstrated that participants preferred our device for in- teractive applications that leverage haptic props, such as making a VR prop feel softer or making a rigid 3D printed remote control feel softer on its button.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84689
        },
        {
          "affiliations": [],
          "personId": 84675
        },
        {
          "affiliations": [],
          "personId": 84662
        }
      ]
    },
    {
      "id": 86329,
      "typeId": 12320,
      "title": "UIST'20: HMD Light: Sharing In-VR Experience via Head-Mounted Projector for Asymmetric Interaction",
      "isBreak": false,
      "importedId": "old_papers-1",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "We present HMD Light, a proof-of-concept Head-Mounted Display (HMD) implementation that reveals the Virtual Reality (VR) user's experience in the physical environment to facilitate communication between VR and external users in a mobile VR context. While previous work externalized the VR user's experience through an on-HMD display, HMD Light places the display into the physical environment to enable larger display and interaction area. This work explores the interaction design space of HMD Light and presents four applications to demonstrate its versatility. Our exploratory user study observed participant pairs experience applications with HMD Light and evaluated usability, accessibility and social presence between users. From the results, we distill design insights for HMD Light and asymmetric VR collaboration.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84706
        },
        {
          "affiliations": [],
          "personId": 86382
        },
        {
          "affiliations": [],
          "personId": 86414
        },
        {
          "affiliations": [],
          "personId": 86375
        },
        {
          "affiliations": [],
          "personId": 84755
        }
      ]
    },
    {
      "id": 86331,
      "typeId": 12320,
      "title": "UIST'20: Slice of Light: Transparent and Integrative Transition Among Realities in a Multi-HMD-User Environment",
      "isBreak": false,
      "importedId": "old_papers-0",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "This work presents Slice of Light, a visualization design created to enhance transparency and integrative transition between realities of Head-Mounted Display (HMD) users sharing the same physical environment. Targeted at reality-guests, Slice of Light's design enables guests to view other HMD users' interactions contextualized in their own virtual environments while allowing the guests to navigate among these virtual environments. In this paper, we detail our visualization design and the implementation. We demonstrate Slice of Light with a block-world construction scenario that involves a multi-HMD-user environment. VR developer and HCI expert participants were recruited to evaluate the scenario, and responded positively to Slice of Light. We discuss their feedback, our design insights, and the limitations of this work.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84706
        },
        {
          "affiliations": [],
          "personId": 86351
        },
        {
          "affiliations": [],
          "personId": 86382
        },
        {
          "affiliations": [],
          "personId": 84755
        }
      ]
    },
    {
      "id": 86333,
      "typeId": 12320,
      "title": "UIST'21: Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants",
      "isBreak": false,
      "importedId": "old_papers-16",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "We propose a new class of haptic devices that provide haptic sensations by delivering liquid-stimulants to the user’s skin; we call this chemical haptics. Upon absorbing these stimulants, which contain safe and small doses of key active ingredients, receptors in the user’s skin are chemically triggered, rendering distinct haptic sensations. We identified five chemicals that can render lasting haptic sensations: tingling (sanshool), numbing (lidocaine), stinging (cinnamaldehyde), warming (capsaicin), and cooling (menthol). To enable the application of our novel approach in a variety of settings (such as VR), we engineered a self-contained wearable that can be worn anywhere on the user’s skin (e.g., face, arms, legs). Implemented as a soft silicone patch, our device uses micropumps to push the liquid stimulants through channels that are open to the user’s skin, enabling topical stimulants to be absorbed by the skin as they pass through. Our approach presents two unique benefits. First, it enables sensations, such as numbing, not possible with existing haptic devices. Second, our approach offers a new pathway, via the skin’s chemical receptors, for achieving multiple haptic sensations using a single actuator, which would otherwise require combining multiple actuators (e.g., Peltier, vibration motors, electro-tactile stimulation). We evaluated this novel approach by means of two studies. In our first study, we characterized the temporal profiles of sensations elicited by each chemical when applied to the skin. Using these insights, we designed five interactive VR experiences utilizing chemical haptics, and in our second user study, participants rated these VR experiences with chemical haptics as more immersive than without. Finally, as the first work exploring the use of chemical haptics on the skin, we offer recommendations to designers for how they may employ our approach for their interactive experiences.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84858
        },
        {
          "affiliations": [],
          "personId": 86358
        },
        {
          "affiliations": [],
          "personId": 86378
        },
        {
          "affiliations": [],
          "personId": 84662
        }
      ]
    },
    {
      "id": 86334,
      "typeId": 12320,
      "title": "UIST'21: Snowy: Recommending Utterances for Conversational Visual Analysis",
      "isBreak": false,
      "importedId": "old_papers-15",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Natural language interfaces (NLIs) have become a prevalent medium for conducting visual data analysis, enabling people with varying levels of analytic experience to ask questions of and interact with their data. While there have been notable improvements with respect to language understanding capabilities in these systems, fundamental user experience and interaction challenges including the lack of analytic guidance (i.e., knowing what aspects of the data to consider) and discoverability of natural language input (i.e., knowing how to phrase input utterances) persist. To address these challenges, we investigate utterance recommendations that contextually provide analytic guidance by suggesting data features (e.g., attributes, values, trends) while implicitly making users aware of the types of phrasings that an NLI supports. We present SNOWY, a prototype system that generates and recommends utterances for visual analysis based on a combination of data interestingness metrics and language pragmatics. Through a preliminary user study, we found that utterance recommendations in SNOWY support conversational visual analysis by guiding the participants' analytic workflows and making them aware of the system's language interpretation capabilities. Based on the feedback and observations from the study, we discuss potential implications and considerations for incorporating recommendations in future NLIs for visual analysis.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86364
        },
        {
          "affiliations": [],
          "personId": 86383
        }
      ]
    },
    {
      "id": 86335,
      "typeId": 12320,
      "title": "UIST'20: CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications",
      "isBreak": false,
      "importedId": "old_papers-5",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems allowing for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84557
        },
        {
          "affiliations": [],
          "personId": 84668
        },
        {
          "affiliations": [],
          "personId": 84890
        },
        {
          "affiliations": [],
          "personId": 84530
        },
        {
          "affiliations": [],
          "personId": 86391
        },
        {
          "affiliations": [],
          "personId": 86418
        },
        {
          "affiliations": [],
          "personId": 84558
        }
      ]
    },
    {
      "id": 86336,
      "typeId": 12320,
      "title": "UIST'21: Bit Whisperer: Enabling Ad-hoc, Short-range, Walk-Up-and-Share Data Transmissions via Surface-restricted Acoustics",
      "isBreak": false,
      "importedId": "old_papers-18",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Bluetooth requires device pairing to ensure security in data transmission, encumbering a number of ad-hoc, transactional interactions that require both ease-of-use and “good enough” security: e.g., sharing contact information or secure links to people nearby. We introduce Bit Whisperer, an ad-hoc short-range wireless communication system that enables “walk up and share” data transmissions with “good enough” security. Bit Whisperer transmits data to proximate devices co-located on a solid surface through high frequency, inaudible acoustic signals. The physical surface has two benefits: it enhances acoustic signal transmission by reflecting sound waves as they propagate; and, it makes the domain of communication visible, helping users identify exactly with whom they are sharing data without prior pairing. Through a series of technical evaluations, we demonstrate that Bit Whisperer is robust for common use-cases and secure against likely threats. We also implement three example applications to demonstrate the utility of Whisperer: 1-to-1 local contact sharing, 1-to-N private link sharing to open a secure group chat, and 1-to-N local device authentication.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86374
        },
        {
          "affiliations": [],
          "personId": 86396
        },
        {
          "affiliations": [],
          "personId": 86413
        },
        {
          "affiliations": [],
          "personId": 86368
        },
        {
          "affiliations": [],
          "personId": 86355
        },
        {
          "affiliations": [],
          "personId": 86357
        },
        {
          "affiliations": [],
          "personId": 85214
        },
        {
          "affiliations": [],
          "personId": 86370
        },
        {
          "affiliations": [],
          "personId": 86377
        }
      ]
    },
    {
      "id": 86337,
      "typeId": 12320,
      "title": "UIST'21: Capturing Tactile Properties of Real Surfaces for Haptic Reproduction",
      "isBreak": false,
      "importedId": "old_papers-4",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Tactile feedback of an object's surface enables us to discern its material properties and affordances. This understanding is used in digital fabrication processes by creating objects with high-resolution surface variations to influence a user's tactile perception. As the design of such surface haptics commonly relies on knowledge from real-life experiences, it is unclear how to adapt this information for digital design methods. In this work, we investigate replicating the haptics of real materials. Using an existing process for capturing an object's microgeometry, we digitize and reproduce the stable surface information of a set of 15 fabric samples. In a psychophysical experiment, we evaluate the tactile qualities of our set of original samples and their replicas. From our results, we see that direct reproduction of surface variations is able to influence different psychophysical dimensions of the tactile perception of surface textures. While the fabrication process did not preserve all properties, our approach underlines that replication of surface microgeometries benefits fabrication methods in terms of haptic perception by covering a large range of tactile variations. Moreover, by changing the surface structure of a single fabricated material, its material perception can be influenced. We conclude by proposing strategies for capturing and reproducing digitized textures to better resemble the perceived haptics of the originals.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86397
        },
        {
          "affiliations": [],
          "personId": 86371
        },
        {
          "affiliations": [],
          "personId": 86384
        },
        {
          "affiliations": [],
          "personId": 85186
        }
      ]
    },
    {
      "id": 86339,
      "typeId": 12320,
      "title": "UIST'21: Weirding Haptics: In-Situ Prototyping of Vibrotactile Feedback in Virtual Reality through Vocalization",
      "isBreak": false,
      "importedId": "old_papers-3",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Effective haptic feedback in virtual reality (VR) is an essential element for creating convincing immersive experiences. To design such feedback, state-of-the-art VR setups provide APIs for programmatically generating controller vibration patterns. While tools for designing vibrotactile feedback keep evolving, they often require expert knowledge and rarely support direct manipulation methods for mapping feedback to user interactions within the VR environment. To address these challenges, we contribute a novel concept called Weirding Haptics, that supports fast-prototyping by leveraging the user's voice to design such feedback while manipulating virtual objects in-situ. Through a pilot study (N = 9) focusing on how tactile experiences are vocalized during object manipulation, we identify spatio-temporal mappings and supporting features needed to produce intended vocalizations. To study our concept, we built a VR design tool informed by the results of the pilot study. This tool enables users to design tactile experiences using their voice while manipulating objects, provides a set of modifiers for fine-tuning the created experiences in VR, and allows to rapidly compare various experiences by feeling them. Results from a validation study (N = 8) show that novice hapticians can vocalize experiences and refine their designs with the fine-tuning modifiers to match their intentions. We conclude our work by discussing uncovered design implications for direct manipulation and vocalization of vibrotactile feedback in immersive virtual environments.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86397
        },
        {
          "affiliations": [],
          "personId": 86415
        },
        {
          "affiliations": [],
          "personId": 86417
        },
        {
          "affiliations": [],
          "personId": 86373
        },
        {
          "affiliations": [],
          "personId": 86408
        },
        {
          "affiliations": [],
          "personId": 85186
        },
        {
          "affiliations": [],
          "personId": 84664
        }
      ]
    },
    {
      "id": 86340,
      "typeId": 12320,
      "title": "UIST'21: GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications",
      "isBreak": false,
      "importedId": "old_papers-2",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Freehand gesture is an essential input modality for modern Augmented Reality (AR) user experiences. However, developing AR applications with customized hand interactions remains a challenge for end-users. Therefore, we propose GesturAR, an end-to-end authoring tool that supports users to create in-situ freehand AR applications through embodied demonstration and visual programming. During authoring, users can intuitively demonstrate the customized gesture inputs while referring to the spatial and temporal context. Based on the taxonomy of gestures in AR, we proposed a hand interaction model which maps the gesture inputs to the reactions of the AR contents. Thus, users can author comprehensive freehand applications using trigger-action visual programming and instantly experience the results in AR.",
      "authors": [
        {
          "affiliations": [],
          "personId": 84557
        },
        {
          "affiliations": [],
          "personId": 84668
        },
        {
          "affiliations": [],
          "personId": 84890
        },
        {
          "affiliations": [],
          "personId": 86363
        },
        {
          "affiliations": [],
          "personId": 86418
        },
        {
          "affiliations": [],
          "personId": 84558
        }
      ]
    },
    {
      "id": 86342,
      "typeId": 12320,
      "title": "UIST'21: X-Rings: A Hand-mounted 360 Degree Shape Display for Grasping in Virtual Reality",
      "isBreak": false,
      "importedId": "old_papers-21",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "X-Rings is a novel hand-mounted 360 degree shape display for Virtual Reality that renders objects in 3D and responds to user-applied touch and grasping force. Designed as a modular stack of motor-driven expandable rings (5.7-7.7 cm diameter), X-Rings renders radially-symmetric surfaces graspable by the user's whole hand. The device is strapped to the palm, allowing the fingers to freely make and break contact with the device. Capacitance sensors and motor current sensing provide estimates of finger touch states and gripping force. We present the results of a user study evaluating participants’ ability to associate device-rendered shapes with visually-rendered objects as well as a demo application that allows users to freely interact with a variety of objects in a virtual environment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86349
        },
        {
          "affiliations": [],
          "personId": 84541
        },
        {
          "affiliations": [],
          "personId": 86367
        },
        {
          "affiliations": [],
          "personId": 86398
        }
      ]
    },
    {
      "id": 86343,
      "typeId": 12320,
      "title": "UIST'20: TransceiVR: Bridging Asymmetrical Communication Between VR Users and External Collaborators",
      "isBreak": false,
      "importedId": "old_papers-20",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) users often need to work with other users, who observe them outside of VR using an external display. Communication between them is difficult; the VR user cannot see the external user's gestures, and the external user cannot see VR scene elements outside of the VR user's view. We carried out formative interviews with experts to understand these asymmetrical interactions and identify their goals and challenges. From this, we identify high-level system design goals to facilitate asymmetrical interactions and a corresponding space of implementation approaches based on the level of programmatic access to a VR application. We present TransceiVR, a system that utilizes VR platform APIs to enable asymmetric communication interfaces for third-party applications without requiring source code access. TransceiVR allows external users to explore the VR scene spatially or temporally, to annotate elements in the VR scene at correct depths, and to discuss via a shared static virtual display. An initial co-located user evaluation with 10 pairs shows that our system makes asymmetric collaborations in VR more effective and successful in terms of task time, error rate, and task load index. An informal evaluation with a remote expert gives additional insight on utility of features for real world tasks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86392
        },
        {
          "affiliations": [],
          "personId": 85188
        },
        {
          "affiliations": [],
          "personId": 86411
        },
        {
          "affiliations": [],
          "personId": 84767
        }
      ]
    },
    {
      "id": 86344,
      "typeId": 12320,
      "title": "UIST'20: DoThisHere: Using multi-modal interaction to support cross-application tasks on mobile devices",
      "isBreak": false,
      "importedId": "old_papers-23",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Many computing tasks, such as comparison shopping, two-factor authentication, and checking movie reviews, require using multiple apps together. On large screens, \"windows, icons, menus, pointer\" (WIMP) graphical user interfaces (GUIs) support easy sharing of content and context between multiple apps. So, it is straightforward to see the content from one application and write something relevant in another application, such as looking at the map around a place and typing walking instructions into an email. However, although today's smartphones also use GUIs, they have small screens and limited windowing support, making it hard to switch contexts and exchange data between apps.\n\nWe introduce DoThisHere, a multimodal interaction technique that streamlines cross-app tasks and reduces the burden these tasks impose on users. Users can use voice to refer to information or app features that are off-screen and touch to specify where the relevant information should be inserted or is displayed. With DoThisHere, users can access information from or carry information to other apps with less context switching.\nWe conducted a survey to find out what cross-app tasks people are currently performing or wish to perform on their smartphones. Among the 125 tasks that we collected from 75 participants, we found that 59 of these tasks are not well supported currently. DoThisHere is helpful in completing 95% of these unsupported tasks. A user study, where users are shown the list of supported voice commands when performing a representative sample of such tasks, suggests that DoThisHere may reduce expert users' cognitive load; the Query action, in particular, can help users reduce task completion time.",
      "authors": [
        {
          "affiliations": [],
          "personId": 85211
        },
        {
          "affiliations": [],
          "personId": 85275
        },
        {
          "affiliations": [],
          "personId": 85308
        }
      ]
    },
    {
      "id": 86345,
      "typeId": 12320,
      "title": "UIST'20: Human-Centered Electronics Design",
      "isBreak": false,
      "importedId": "old_papers-22",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Mainstream board-level electronics design tools work at the lowest level of design, that of schematics and individual components, but can be tedious and require significant expertise.\nIn this work, we propose a hardware description language (HDL) approach that enables users to design at a higher, system architecture level while also allowing optional lower-level design of reusable subcircuit libraries.\nWe extend the common hierarchical block diagram concept with generators to support user-defined automation, abstract-typed blocks to allow generic components, and electronics modeling to check basic correctness.\nCombined, these help bridge high-level designs to fully specified circuits while bringing the advantages of reusability and encapsulation from object-oriented programming.\nFurthermore, an IDE plugin provides a more familiar block diagram visualization of the textual HDL and supports schematic editor style GUI actions to generate HDL code.\nExample designs generated with this system demonstrate the power and flexibility of this approach.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86410
        },
        {
          "affiliations": [],
          "personId": 86356
        },
        {
          "affiliations": [],
          "personId": 86379
        },
        {
          "affiliations": [],
          "personId": 86390
        },
        {
          "affiliations": [],
          "personId": 86350
        },
        {
          "affiliations": [],
          "personId": 86389
        },
        {
          "affiliations": [],
          "personId": 86394
        }
      ]
    },
    {
      "id": 86346,
      "typeId": 12320,
      "title": "UIST'20: Back-Hand-Pose: 3D Hand Pose Estimation for Wrist-worn Devices via Dorsum Deformation Analysis",
      "isBreak": false,
      "importedId": "old_papers-25",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "The automatic recognition of how people use their hands and fingers in natural settings -- without instrumenting the fingers -- can be useful for many mobile computing applications. To achieve such an interface, we propose a vision-based 3D hand pose estimation framework using a wrist-worn camera. The main challenge is the oblique angle of the wrist-worn camera, which makes the fingers hardly visible. To address this, a special network that observes the deformations on the back of the (dorsal) hand is required. We introduce DorsalNet, a two-stream convolutional neural network to regress finger joint angles from spatio-temporal features of the dorsal hand region (related to the movement of bones, muscle, and tendons). This work is the first vision-based real-time 3D hand pose estimator using visual features from the dorsal hand region. Our system achieves a mean joint-angle error of 8.81 degree for a user-specific model and 9.77 degree for a general model. Further evaluation shows that our system outperforms previous work with an average of 20% higher accuracy in recognizing dynamic gestures, and achieves a 75.1% accuracy of detecting 11 types of grasp motion. We also demonstrate 3 types of applications to use our system as: a control device, an input device, and a grasped object recognizer.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86359
        },
        {
          "affiliations": [],
          "personId": 86369
        },
        {
          "affiliations": [],
          "personId": 86360
        },
        {
          "affiliations": [],
          "personId": 85135
        },
        {
          "affiliations": [],
          "personId": 85210
        },
        {
          "affiliations": [],
          "personId": 86407
        }
      ]
    },
    {
      "id": 86347,
      "typeId": 12320,
      "title": "UIST'20: Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities",
      "isBreak": false,
      "importedId": "old_papers-24",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair’s armrest, a piggybank can reach out an arm to ’steal’ coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such ‘transformables’ using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design. We present Romeo—a design tool for creating transformables embedded into a 3D model to robotically augment the object’s default functionalities. Romeo allows users to express at a high level, (1) which part of the object to be transformed, (2) how it moves following motion points in space, and (3) the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. We validated Romeo with a design session where 8 participants design and create custom transformables using 3D objects of their own choice.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86402
        },
        {
          "affiliations": [],
          "personId": 86386
        },
        {
          "affiliations": [],
          "personId": 84531
        },
        {
          "affiliations": [],
          "personId": 84616
        }
      ]
    },
    {
      "id": 86427,
      "typeId": 12320,
      "title": "SpiceWare: Simulating Spice Using Thermally Adjustable Dinnerware to Bridge Cultural Gaps",
      "isBreak": false,
      "importedId": "757626d4-e06e-4e79-9cbb-f015de550c8d",
      "source": "PCS",
      "trackId": 11864,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86424
      ],
      "eventIds": [],
      "abstract": "Preference and tolerance towards spicy food may vary depending on culture, location, upbringing, personality and even gender. Due to this, spicy food can often effect the social interaction on the dining table, especially if it is presented as a cultural dish. We propose SpiceWare, a thermally adjustable spoon that alters the perception of spice to improve cross-cultural communication. SpiceWare is a 3D-printed aluminium spoon that houses a thermal peltier that provides thermal feedback up to 45°C which can alter the taste perception of the user. As an initial evaluation, we conducted a workshop among participants of varying cultural backgrounds and observe their interaction when dining on spicy food. We found that the overall interaction was perceived to be more harmonious, and we discuss potential future works on improving the system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Media Design ",
              "dsl": "Keio University"
            }
          ],
          "personId": 85173
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 84555
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 85228
        }
      ]
    },
    {
      "id": 89913,
      "typeId": 12316,
      "title": "Artistic User Expressions in AI-powered Creativity Support Tools",
      "isBreak": false,
      "importedId": "786994c0-e4dc-4409-8d6a-30952f651bfe",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Novel AI algorithms introduce a new generation of AI-powered Creativity Support Tools (AI-CSTs). These tools can inspire and surprise users with algorithmic outputs that the users could not expect. However, users can struggle to align their intentions with unexpected algorithmic behaviors. My dissertation research studies how user expressions in art-making AI-CSTs need to be designed. With an interview study with 14 artists and a literature survey on 111 existing CSTs, I first isolate three requirements: 1) allow users to express under-constrained intentions, 2) enable the tool and the user to co-learn the user expressions and the algorithmic behaviors, and 3) allow easy and expressive iteration. Based on these requirements, I introduce two tools, 1) Artinter, which learns how the users express their visual art concepts within their communication process for art commissions, and 2) TaleBrush, which facilitates the under-constrained and iterative expression of use intents through sketching-based story generation. My research provides guidelines for designing user expression interactions for AI-CSTs while demonstrating how they can suggest new designs of AI-CSTs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 85367
        }
      ]
    },
    {
      "id": 89914,
      "typeId": 12316,
      "title": "Design and Fabricate Personal Health Sensing Devices",
      "isBreak": false,
      "importedId": "396d9b0c-7e07-4529-8c52-48118387c66b",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "With the development of low-cost electronics, rapid prototyping techniques, as well as widely available mobile devices (e.g. mobile phones, smart watches), users are able to develop their own basic interactive functional applications, either on top of existing device platforms, or as stand-alone devices. However, the boundary for creating personal health sensing devices, both function prototyping and fabrication -wise, are still high. In this paper, I present my works on designing and fabricating personal health sensing devices with rapid function prototyping techniques and novel sensing technologies. Through these projects and ongoing future research, I am working towards my vision that everyone can design and fabricate highly-customized health sensing devices based on their body form and desired functions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 84905
        }
      ]
    },
    {
      "id": 89915,
      "typeId": 12316,
      "title": "Designing Tools for Autodidactic Learning of Skills",
      "isBreak": false,
      "importedId": "5b06085b-74f8-4a82-9a2c-527f4cb39042",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "In the last decade, HCI researchers have designed and engineered several systems to lower the entry barrier for beginners and support novices in learning hands-on creative skills, such as motor skills, fabrication, circuit prototyping, and design. \r\n\r\nIn my research, I contribute to this body of work by designing tools that enable learning by oneself, also known as autodidactism. My research lies at the intersection of system design, learning sciences, and technologies that support physical skill-learning. Through my research projects, I propose to re-imagine the design of systems for skill-learning through the lens of learner-centric theories and frameworks. \r\n\r\nI present three sets of research projects - (1) adaptive learning of motor skills, (2) game-based learning for fabrication skills, and (3) reflection-based learning of maker skills. Through these projects, I demonstrate how we can leverage existing theories, frameworks, and approaches from the learning sciences to design autodidactic systems for skill- learning.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT CSAIL"
            }
          ],
          "personId": 85385
        }
      ]
    },
    {
      "id": 89916,
      "typeId": 12316,
      "title": "Environmental physical intelligence: Seamlessly deploying sensors and actuators to our everyday life",
      "isBreak": false,
      "importedId": "d175d23a-8206-47bc-a438-e1458957523b",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Weiser has predicted the third generation of computing would result in individuals interacting with many computing devices and ultimately can “weave themselves into the fabric of everyday life until they are indistinguishable from it”. However, how to achieve this seamlessness and what associated interaction should be developed are still under investigation. On the other hand, the material composition, structures and operating logic of a variety of physical objects existing in everyday life determine how we interact with them. The intelligence of the built environment does not only rely on the encoded computational abilities within the “brain” (like the controllers of home appliances), but also the physical intelligence encoded in their “body” (e.g., materials, mechanical structures). In my research, I work on creating computational materials with different encoded material properties (e.g., conductivity, transparency, water-solubility, self-assembly, etc.) that can be seamlessly integrated into our living environment to enrich different modalities of information communication.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Interactive Computing",
              "dsl": "Georgia Institute of Technology"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Interactive Computing",
              "dsl": "Georgia Institute of Technology"
            }
          ],
          "personId": 85343
        }
      ]
    },
    {
      "id": 89917,
      "typeId": 12316,
      "title": "Extending Computational Abstractions with Manual Craft for Visual Art Tools",
      "isBreak": false,
      "importedId": "f8a86d33-cc9e-4a00-96b7-ff94342ae76d",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Programming and computation are powerful tools for manipulating visual forms, but working with these abstractions is challenging for artists who are accustomed to direct manipulation and manual control. The goal of my research is to develop visual art tools that extend programmatic capabilities with manual craft. I do so by exposing computational abstractions as transparent materials that artists may directly manipulate and observe in a process that accommodates their non-linear workflows. Specifically, I conduct empirical research to identify challenges professional artists face when using existing software tools—as well as programming their own—to make art. I apply principles derived from these findings in two projects: an interactive programming environment that links code, numerical information, and program state to artists' ongoing artworks, and an algorithm that automates the rigging of character clothing to bodies to allow for more flexible and customizable 2D character illustrations. Evaluating these interactions, my research promotes authoring tools that support arbitrary execution by adapting to the existing workflows of artists.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 85195
        }
      ]
    },
    {
      "id": 89918,
      "typeId": 12316,
      "title": "Towards Future Health and Well-being: Bridging Behavior Modeling and Intervention",
      "isBreak": false,
      "importedId": "48b6b72f-c77b-4dde-9ecd-06dfd375b614",
      "source": "PCS",
      "trackId": 11866,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "With the advent of always-available, ubiquitous devices with powerful passive sensing and active interaction capabilities, the opportunities to integrate AI into this ecosystem have matured, providing an unprecedented opportunity to understand and support user well-being. A wide array of research has demonstrated the potential to detect risky behaviors and address health concerns, using human-centered ML to understand longitudinal, passive behavior logs. Unfortunately, it is difficult to translate these findings into deployable applications without better approaches to providing human-understandable relationship explanations between behavior features and predictions (interpretability); and generalizing to new users and new time periods (robustness). My past work has made significant headway in addressing modeling accuracy, interpretability, and robustness. Moreover, my ultimate goal is to build deployable, intelligent interventions for health and well-being that make use of succeeding ML-based behavior models. I believe that just-in-time interventions are particularly well suited to ML support. I plan to test the value of ML for providing users with a better, interpretable, and robust experience in supporting their well-being.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Information School"
            }
          ],
          "personId": 85199
        }
      ]
    },
    {
      "id": 89933,
      "typeId": 12320,
      "title": "UIST'21: Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning",
      "isBreak": false,
      "importedId": "old_papers-27",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios. We present Screen2Words, a novel screen summarization approach that automatically encapsulates essential information of a UI screen into a coherent language phrase. Summarizing mobile screens requires a holistic understanding of the multi-modal data of mobile UIs, including text, image, structures as well as UI semantics, motivating our multi-modal learning approach. We collected and analyzed a large-scale screen summarization dataset annotated by human workers. Our dataset contains more than 112k language summarization across ? 22k unique UI screens. We then experimented with a set of deep models with different configurations. Our evaluation of these models with both automatic accuracy metrics and human rating shows that our approach can generate high-quality summaries for mobile screens. We demonstrate potential use cases of Screen2Words and open-source our dataset and model to lay the foundations for further bridging language and user interfaces.",
      "authors": [
        {
          "affiliations": [],
          "personId": 89929
        },
        {
          "affiliations": [],
          "personId": 89932
        },
        {
          "affiliations": [],
          "personId": 89931
        },
        {
          "affiliations": [],
          "personId": 89928
        },
        {
          "affiliations": [],
          "personId": 84678
        },
        {
          "affiliations": [],
          "personId": 89927
        }
      ]
    },
    {
      "id": 89934,
      "typeId": 12320,
      "title": "UIST'20: Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback",
      "addons": {
        "Poster": {
          "hashSum": "fZGA+7jbInz4K4vy7umhXorIxfWQqe7iRxcj1xRYIuM=",
          "previewUrl": "https://files.sigchi.org/conference/attachment/10082/content/89934/poster/493358f1-8eb7-7d8f-02c4-a2b561f4eba6.jpg",
          "type": "poster",
          "url": "https://files.sigchi.org/conference/attachment/10082/content/89934/poster/87ef1d63-c68a-b2af-082b-b33e7ef1f9e3.pdf"
        }
      },
      "isBreak": false,
      "importedId": "old_papers-28",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "institution": "ETH Zürich"
            }
          ],
          "personId": 89926
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "institution": "ETH Zürich"
            }
          ],
          "personId": 89925
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 89924
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "institution": "ETH Zürich"
            }
          ],
          "personId": 89923
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "institution": "ETH Zürich"
            }
          ],
          "personId": 89922
        }
      ]
    },
    {
      "id": 90850,
      "typeId": 12316,
      "title": "UIST'20 Doctoral consortium: An Upcycled IoT: Creating an Inclusive and Frugal IoT Infrastructure by Using the Home's Existing Possessions",
      "isBreak": false,
      "importedId": "07e5a848-9936-4602-ade7-e1b68d908a03",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "The Internet-of-Things (IoT) promises to enhance even the most mundane of objects with computational properties. Yet, IoT has largely focused on new devices while ignoring the home's many existing possessions. Requiring households to replace their possessions to adopt IoT yields substantial costs. Beyond financial, these include waste, work to arrange and orchestrate objects to suit households, and attention investment to acquire new skills. To address these costs, my dissertation worked with 10 American families to design an upcycled approach to IoT that makes use of existing household possessions and then build a system reflective of these findings. The results 1) describe patterns of families' socio-material practices, 2) develop a framework for designing lightweight modification, and 3) presents The IoT Codex—a book of programmable and inexpensive, battery-free interactive devices—to support customizing everyday objects with software and web services using stickers. This work offers a lightweight approach to customizing IoT.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86412
        }
      ]
    },
    {
      "id": 90851,
      "typeId": 12316,
      "title": "UIST'21 Doctoral consortium: Visual Design Reuse Through Style Recognition and Transfer",
      "isBreak": false,
      "importedId": "4f723af6-6a2b-4f6d-a421-4d5e388d3921",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "This work aims to transfer design attributes and styles within and across visual documents such as slides, graphic designs, and non-photorealistic renderings. Consistent style across elements is a hallmark of good graphic design. Many visual stylistic design patterns exist throughout visualizations, presentations, and interactive media experiences (games, visual novels). These patterns often exist in visual style guides, brand guides, and concept art. However, except for structured document layouts (e.g., HTML/CSS), design tools often do not enforce consistent style decisions or must be manually maintained. Synchronizing style guides and designs require significant effort, discouraging exploration and the mixing of new ideas. This work introduces algorithms that recognize implicit patterns and structures in visual documents along with interfaces that let designers operate on these patterns, specifically, to view and apply design changes across pattern instances flexibly. The key benefits of visual redesign through implicit patterns are: (1) removing any dependence on upfront formal style declarations, (2) enabling extraction and distribution of implicit visual patterns, and (3) facilitating the exploration of novel visual design concepts through the mixing of styles.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86403
        }
      ]
    },
    {
      "id": 90852,
      "typeId": 12316,
      "title": "UIST'21 Doctoral consortium: Taking Digital Product Design Coordination to the Next Level by Fluid Zooming and Linecept",
      "isBreak": false,
      "importedId": "89f1e7c6-bf5d-497f-afc5-a179e016e548",
      "source": "CSV",
      "trackId": 11892,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        86426
      ],
      "eventIds": [],
      "abstract": "Currently, we use a multitude of computing environments in our daily lives, and these devices are increasingly becoming an essential part of our everyday lives. For example, someone might start the day by reviewing notifications on a smartwatch, move to browsing the news and quickly responding to emails during breakfast on a smartphone, and then using a laptop and digital whiteboard during work hours to send emails and facilitate meetings. Sometimes, even multiple computing environments are used in parallel to fulfill a task — Waeljas et al. describe these are multi-channeled and crossmedia service experiences. However, because existing software programs are seldom able to provide a seamless experience across multiple devices, users are forced to “act as the bridge connecting their devices”. This bridging creates additional burdens on users, which have been found to increase cognitive load. Additionally, even on the same screen, we use variable viewport sizes to run multiple software applications. These phenomena are more and more observable on handheld devices as well. In digital product design, the use of multiple devices and multiple software is even more augmented as a wide variety of stakeholders with different cultural backgrounds and tool preferences need to work together in order to efficiently deliver scalable and secure digital solutions to billions of customers. Until now, there has yet to be a uniform user interface model and interaction mechanism that can serve as a main metaphor for user interfaces across a wide variety of screen sizes and shapes to access information. Such a uniform solution would allow a user to interact with displayed information in a fully consistent way from watch-sized to whiteboard-sized screens, and would also decrease the development time of digital products. As an additional benefit of a novel user interface type, it would allow the manifestation of new software productivity tools that would be significantly easier to use, which could yield to less time spent on work coordination, hence more time allowed to efficient work, and thus, leisure. This way, great societal benefits could be realized as well.",
      "authors": [
        {
          "affiliations": [],
          "personId": 86395
        }
      ]
    }
  ],
  "people": [
    {
      "id": 84527,
      "firstName": "Kyle",
      "lastName": "Chard",
      "middleInitial": "",
      "importedId": "iKlrfHHZcA6_R12h5QKjgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84528,
      "firstName": "Yehan",
      "lastName": "Fan",
      "middleInitial": "",
      "importedId": "UJdaHDOVdejDfOuOVPC9-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84529,
      "firstName": "Hany",
      "lastName": "Elhassany",
      "middleInitial": "Mohsen",
      "importedId": "cBee7khQdnOkHwNtwcX7Ww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84530,
      "firstName": "Xiyun",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "cMU6fGiQps5q3szib1nD7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84531,
      "firstName": "Jeeeun",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "8h2BUtB4JNpWkQLqy3KMFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84532,
      "firstName": "Gila",
      "lastName": "Schein",
      "middleInitial": "",
      "importedId": "Rw6E9yVPQ5wvdRk_2UW4Xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84533,
      "firstName": "Xinyu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "zP5-RPtG-Ste1c5d5TKA0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84534,
      "firstName": "Jed",
      "lastName": "Brubaker",
      "middleInitial": "R.",
      "importedId": "QLLwl9Gb-7FBeHyHEraZFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84535,
      "firstName": "Jun",
      "lastName": "Nishida",
      "middleInitial": "",
      "importedId": "Ws6-56wJYVvZ4w-Uz7-BjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84536,
      "firstName": "Bernhard",
      "lastName": "Thomaszewski",
      "middleInitial": "",
      "importedId": "o_dKZGYH3HZ81UrQlUStwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84537,
      "firstName": "Holger",
      "lastName": "Regenbrecht",
      "middleInitial": "",
      "importedId": "DWY_L6ecUmwTUbCSy3EAiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84538,
      "firstName": "Pradyumna",
      "lastName": "Chari",
      "middleInitial": "",
      "importedId": "ypWZkJrdXW3DJR6nJhwfZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84539,
      "firstName": "Sebastian",
      "lastName": "Rodriguez",
      "middleInitial": "S.",
      "importedId": "FrL_WEu6wLH-Z11C6ZXKhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84540,
      "firstName": "Jian",
      "lastName": "Liao",
      "middleInitial": "",
      "importedId": "bgO1Xhwb3kPK73wwAJ-uhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84541,
      "firstName": "Eyal",
      "lastName": "Ofek",
      "middleInitial": "",
      "importedId": "UtFS3oDmK7eM3ZgMqSADVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84542,
      "firstName": "Hamid",
      "lastName": "Ghaednia",
      "middleInitial": "",
      "importedId": "dtDFFHtNeiArEg_1QVIqHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84543,
      "firstName": "Ken",
      "lastName": "Nakagaki",
      "middleInitial": "",
      "importedId": "lsYt2lERJ_k5SfHL9Sztcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84544,
      "firstName": "Romain",
      "lastName": "Nith",
      "middleInitial": "",
      "importedId": "Wo0hKV9EQ-0yaKNs_54ggA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84545,
      "firstName": "Aspen",
      "lastName": "Tng",
      "middleInitial": "",
      "importedId": "m3mRiaRuA4mKqJ1jpCmGbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84546,
      "firstName": "Luke",
      "lastName": "Darcy",
      "middleInitial": "Andre",
      "importedId": "YI2csBYNGkk69z1dfg2jxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84547,
      "firstName": "Nora",
      "lastName": "Morsi",
      "middleInitial": "",
      "importedId": "rgarMvnMrhD5aN71-FL6xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84548,
      "firstName": "Qing",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "Q6BVkFDfzqdL3mLrZqrBIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84549,
      "firstName": "Hiroki",
      "lastName": "Kaimoto",
      "middleInitial": "",
      "importedId": "dlBUBDwjvu8spl2RMPnkaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84550,
      "firstName": "Jun",
      "lastName": "Rekimoto",
      "middleInitial": "",
      "importedId": "2Guk0EWER7zUs-hWrmCSsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84551,
      "firstName": "Justin",
      "lastName": "Lubin",
      "middleInitial": "",
      "importedId": "1kOOul5QpZksyvyK4nkx0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84552,
      "firstName": "Yu",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "n4KrA8dWYAC7Y47PtOQ-dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84553,
      "firstName": "Longxiulin",
      "lastName": "Deng",
      "middleInitial": "",
      "importedId": "iavrrxylv6EUYisIy-RMCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84554,
      "firstName": "Aniket",
      "lastName": "Kittur",
      "middleInitial": "",
      "importedId": "dvITl7CjKRGhLNC7tWO8aw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84555,
      "firstName": "Yun Suen",
      "lastName": "Pai",
      "middleInitial": "",
      "importedId": "XlB4ArLajcCfMwb9KBUFBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84556,
      "firstName": "Zhenhong",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "TF7_CvVwb9wqIy3YNPcUNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84557,
      "firstName": "Tianyi",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "oBPFyRs7TRlZaYrUGRdbrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84558,
      "firstName": "Karthik",
      "lastName": "Ramani",
      "middleInitial": "",
      "importedId": "ltnbQF0vXB-RYkZ1s9rlgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84559,
      "firstName": "Ruta",
      "lastName": "Desai",
      "middleInitial": "",
      "importedId": "nG2_FlQmSK_GvnP8CMD-vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84560,
      "firstName": "Md",
      "lastName": "Ehtesham-Ul-Haque",
      "middleInitial": "",
      "importedId": "_-qYoKRt4KbeSzjuzGSDAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84561,
      "firstName": "Martin",
      "lastName": "Taraz",
      "middleInitial": "",
      "importedId": "imRPnat8cKoj8FJRMew_eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84562,
      "firstName": "Tianren",
      "lastName": "Luo",
      "middleInitial": "",
      "importedId": "slLBKiGAjSYbXIuph7-uNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84563,
      "firstName": "Difeng",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "rPMhGPjnC4WP4NPQXlNkzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84564,
      "firstName": "Jian",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "8dq-zKTcAycM099Ji424lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84565,
      "firstName": "Aashini",
      "lastName": "Shah",
      "middleInitial": "",
      "importedId": "xo0raSN5PEeUg00r__1Jxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84566,
      "firstName": "Abe",
      "lastName": "Davis",
      "middleInitial": "",
      "importedId": "GuGJPj5RBrFll7dqys_ikQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84567,
      "firstName": "Peter",
      "lastName": "Kán",
      "middleInitial": "",
      "importedId": "vvjJZIpg5kSZKVm7BkWt6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84568,
      "firstName": "Gautham",
      "lastName": "Mysore",
      "middleInitial": "",
      "importedId": "P8QEWzPFu6tfJnpFkE_3Dw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84569,
      "firstName": "Christian",
      "lastName": "Frueh",
      "middleInitial": "",
      "importedId": "rxaMneFIFFxZWfODzw6fkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84570,
      "firstName": "Romeo",
      "lastName": "Sommerfeld",
      "middleInitial": "",
      "importedId": "nVjRUFt4V7stmLu-JB64rg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84571,
      "firstName": "Sarah",
      "lastName": "Chasins",
      "middleInitial": "E.",
      "importedId": "8vOMlM09isbluPujz5S5bA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84572,
      "firstName": "Kenta",
      "lastName": "Yamamoto",
      "middleInitial": "",
      "importedId": "Ya-G4E6oJSPOjY1vFhJtMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84573,
      "firstName": "Tharindu",
      "lastName": "Kaluarachchi",
      "middleInitial": "Indrajith",
      "importedId": "cdDeg59PQdo3ayzy4lqtJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84574,
      "firstName": "Anup",
      "lastName": "Sathya",
      "middleInitial": "",
      "importedId": "ZjNYEfkCTRnIr8xuZScj-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84575,
      "firstName": "Peli",
      "lastName": "de Halleux",
      "middleInitial": "",
      "importedId": "Ky7DGX_tE8kThIohnvWjtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84576,
      "firstName": "Sophia",
      "lastName": "Gu",
      "middleInitial": "",
      "importedId": "70hYo0d7kpSjuT93yKnfmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84577,
      "firstName": "Chaoran",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "xEuJyd5HfLOCNTqc0mfGhQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84578,
      "firstName": "Chun",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "7Ogdmsb7v7aXGq5BHxNa5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84579,
      "firstName": "Kai",
      "lastName": "Kunze",
      "middleInitial": "",
      "importedId": "PZO_YeskIPmyLzFr_vpQHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84580,
      "firstName": "Sonia",
      "lastName": "Elizondo",
      "middleInitial": "",
      "importedId": "Psvgn87thO3jYfaUf35GsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84581,
      "firstName": "Patrick",
      "lastName": "Baudisch",
      "middleInitial": "",
      "importedId": "o7VwMA3PuVJclKFtwCL87A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84582,
      "firstName": "Sheldon",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "4kw55ZRO403fYVr-ium8QA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84583,
      "firstName": "Siyou",
      "lastName": "Pei",
      "middleInitial": "",
      "importedId": "rGbCH7lyv5hUf0qVfD80NA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84584,
      "firstName": "Hewu",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "STrjcDa8y-N31PdF36iE0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84585,
      "firstName": "Yankang",
      "lastName": "Meng",
      "middleInitial": "",
      "importedId": "3W2ObwWP9AVy_f8KtbiaMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84586,
      "firstName": "Janet",
      "lastName": "Ruppert",
      "middleInitial": "",
      "importedId": "zl-6rFjeJrBE7yWzkj8ctw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84587,
      "firstName": "Naveen",
      "lastName": "Sendhilnathan",
      "middleInitial": "",
      "importedId": "YoXPwLFUbBAR35pGsbvv0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84588,
      "firstName": "Jonas",
      "lastName": "Noack",
      "middleInitial": "",
      "importedId": "PtJx5CGOtR0toyRDfi8T-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84589,
      "firstName": "Teng",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "tHIfaVnGk0hgRd0jb9BRUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84590,
      "firstName": "Feng",
      "lastName": "Tian",
      "middleInitial": "",
      "importedId": "e4x280wrNe6DjEzW6pjtvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84591,
      "firstName": "Michael",
      "lastName": "Hedderich",
      "middleInitial": "A.",
      "importedId": "kMT5aEvka0-x0JOul_pNiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84592,
      "firstName": "Amadou",
      "lastName": "Bah",
      "middleInitial": "Yaye",
      "importedId": "iZyOl-_sJxwtlUq9dfSJYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84593,
      "firstName": "Mengyu",
      "lastName": "Zhong",
      "middleInitial": "",
      "importedId": "L43qjAaof9BiVcRIAhVHFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84594,
      "firstName": "Yoshifumi",
      "lastName": "Kitamura",
      "middleInitial": "",
      "importedId": "XwNdfZ209hp61dXkmG71tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84595,
      "firstName": "Alexandra",
      "lastName": "Ion",
      "middleInitial": "",
      "importedId": "nPzcOO-p9-FlXrBByTE0kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84596,
      "firstName": "Ashrith",
      "lastName": "Shetty",
      "middleInitial": "",
      "importedId": "30dYUysnO1Fr6dzA4ijGXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84597,
      "firstName": "Anhong",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "EE_pQo_cejBcQ4Xv5yZujw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84598,
      "firstName": "Brad",
      "lastName": "Myers",
      "middleInitial": "A",
      "importedId": "2PlVnFcPRlVXwHeaH5ipHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84599,
      "firstName": "Brian",
      "lastName": "Colonna",
      "middleInitial": "",
      "importedId": "98qr1jNmJymOHErBhjkfeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84600,
      "firstName": "Radu-Daniel",
      "lastName": "Vatavu",
      "middleInitial": "",
      "importedId": "8aqyMcEhyX2llYoyIaN2iQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84601,
      "firstName": "Adnan",
      "lastName": "Karim",
      "middleInitial": "",
      "importedId": "GbsWf6j2Dl1pbj_GtvFPQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84602,
      "firstName": "Jyh-Ming",
      "lastName": "Lien",
      "middleInitial": "",
      "importedId": "C7oOFCcFayGnCgJ6NYa_FQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84603,
      "firstName": "Koji",
      "lastName": "Yatani",
      "middleInitial": "",
      "importedId": "J6goLxhWKr5rK6nRcctLUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84604,
      "firstName": "Jamie",
      "lastName": "Ward",
      "middleInitial": "A",
      "importedId": "M7UZiMVpfl8Uu0l80q0QeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84605,
      "firstName": "Ana",
      "lastName": "Villanueva",
      "middleInitial": "M",
      "importedId": "TmVN0J6x6Y6j1T8uyWe9lw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84606,
      "firstName": "Ruolin",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "3Dg6PbJi9flxlFRWGC9d1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84607,
      "firstName": "Stelian",
      "lastName": "Coros",
      "middleInitial": "",
      "importedId": "J6M3Z54WltYVmCJcmKegfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84608,
      "firstName": "Xing-Dong",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "qHjKmi-3r1K3BNbRMD2C6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84609,
      "firstName": "Yanjun",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "klOTd6vvnIEtnWv8YHKxIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84610,
      "firstName": "Julina",
      "lastName": "Coupland",
      "middleInitial": "",
      "importedId": "dkuTaU9J4-8L4_0F1Fb6Gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84611,
      "firstName": "Ryo",
      "lastName": "Suzuki",
      "middleInitial": "",
      "importedId": "GN-jA9I12mj5OyFiQL9Cxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84612,
      "firstName": "Achuta",
      "lastName": "Kadambi",
      "middleInitial": "",
      "importedId": "OGM48Tz0KRijY92NJKqVEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84613,
      "firstName": "Sahra",
      "lastName": "Yusuf",
      "middleInitial": "",
      "importedId": "DnfwxLE6bvV3Y7_FuzgL8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84614,
      "firstName": "Chi-Huan",
      "lastName": "Chiang",
      "middleInitial": "",
      "importedId": "9FALP72EaAIO_cFUL06nfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84615,
      "firstName": "Jaemin",
      "lastName": "Jo",
      "middleInitial": "",
      "importedId": "t__ZfCPXedQZUrpOvZIAiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84616,
      "firstName": "Xiang 'Anthony'",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "KpO_0vxU-ZNKmw9QA3hTuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84617,
      "firstName": "Moritz",
      "lastName": "Dzingel",
      "middleInitial": "",
      "importedId": "q6UPF7NioQtNhTzJr82eIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84618,
      "firstName": "Ye",
      "lastName": "Tao",
      "middleInitial": "",
      "importedId": "-Z8K7gpdIwpjDVQKoqDbag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84619,
      "firstName": "Matthew",
      "lastName": "Brehmer",
      "middleInitial": "",
      "importedId": "z21Mzr8taw2O48c7H4UODA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84620,
      "firstName": "Zhenxuan",
      "lastName": "He",
      "middleInitial": "",
      "importedId": "PN2MmTyP4iG9nLFAin4Dlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84621,
      "firstName": "Blase",
      "lastName": "Ur",
      "middleInitial": "",
      "importedId": "I8AQLcZIDLYLGUESAtV_uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84622,
      "firstName": "Evie",
      "lastName": "Cheng",
      "middleInitial": "Yu-Yen",
      "importedId": "Nri7xYBoxj9j7tY26uprqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84623,
      "firstName": "Grace",
      "lastName": "Liu",
      "middleInitial": "M",
      "importedId": "gg9KtmpqjesxIHfp95IFlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84624,
      "firstName": "Velko",
      "lastName": "Vechev",
      "middleInitial": "",
      "importedId": "PBCPSiDdci1wjwDizznsJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84625,
      "firstName": "Tanya",
      "lastName": "Jonker",
      "middleInitial": "R.",
      "importedId": "iBMEnW-p-vxQsZr9R68pVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84626,
      "firstName": "Andreas",
      "lastName": "Fender",
      "middleInitial": "Rene",
      "importedId": "aGZO97mF9CBleMOKcUJ0Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84627,
      "firstName": "Pashin",
      "lastName": "Raja",
      "middleInitial": "Farsak",
      "importedId": "BwBoMqne5hhBHt5-gm6rDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84628,
      "firstName": "Shangyin",
      "lastName": "Tan",
      "middleInitial": "",
      "importedId": "Rqqkts_kzm3R0xVysZ1gaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84629,
      "firstName": "Chenyang",
      "lastName": "Cai",
      "middleInitial": "",
      "importedId": "d3nYp7C2OmxDRM744XVDtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84630,
      "firstName": "Lindsay",
      "lastName": "Popowski",
      "middleInitial": "",
      "importedId": "T42Mm8SumEo3Be55mGzRrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84631,
      "firstName": "Arthur",
      "lastName": "Fleig",
      "middleInitial": "",
      "importedId": "JFfNlQ-FB5edNShH-h74Kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84632,
      "firstName": "Anthony",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "YGO4_l5XRHyTv-hGHbY83g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84633,
      "firstName": "Hong-Xian",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "dtQPKmGy6GJpfuVZ3hhoMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84634,
      "firstName": "Kaj",
      "lastName": "Grønbæk",
      "middleInitial": "",
      "importedId": "7q6wCAlX85zIMCKTWiMQbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84635,
      "firstName": "Jennifer",
      "lastName": "Mankoff",
      "middleInitial": "",
      "importedId": "t5zpMV-vR3b9i2C9JiCaWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84636,
      "firstName": "Alexander",
      "lastName": "Plopski",
      "middleInitial": "",
      "importedId": "vgAMuqMSsR9NJeuIevCxNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84637,
      "firstName": "Deying",
      "lastName": "Pan",
      "middleInitial": "",
      "importedId": "E5mcgU3lE6y5hgAECvv_5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84638,
      "firstName": "Kyzyl",
      "lastName": "Monteiro",
      "middleInitial": "",
      "importedId": "WHdTqYyms-c_XqcUEoblDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84639,
      "firstName": "Jiatian",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "VsLrfIKikuQa2SXVkSWvDg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84640,
      "firstName": "Bryan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "HPOKEIUSZ5soMnVIZFkXqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84641,
      "firstName": "Ronan",
      "lastName": "Hinchet",
      "middleInitial": "J",
      "importedId": "P5URlLaGjZuyDrzS9XhRfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84642,
      "firstName": "Ian",
      "lastName": "Arawjo",
      "middleInitial": "",
      "importedId": "9Cmnmubftugec7sLlAw1QQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84643,
      "firstName": "Stefanie",
      "lastName": "Mueller",
      "middleInitial": "",
      "importedId": "5vj196FvEHPKhaKPi53vKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84644,
      "firstName": "Carrie",
      "lastName": "Cai",
      "middleInitial": "J",
      "importedId": "IpbG6xKKCWj1t6fVN2m8Qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84645,
      "firstName": "Percy",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "OG1BX_0XIzdWZn3lhaKOrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84646,
      "firstName": "Zeyu",
      "lastName": "Jin",
      "middleInitial": "",
      "importedId": "Ch_mf4B56KL9j2sOcxSEtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84647,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "aY56pNDJZ4aDpSLEQxb3_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84648,
      "firstName": "Tarfah",
      "lastName": "Alrashed",
      "middleInitial": "",
      "importedId": "ALK2715-1tFYDVaL0bhKnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84649,
      "firstName": "Thijs",
      "lastName": "Roumen",
      "middleInitial": "",
      "importedId": "PuZDyg-CJcfJf5qrbCDWpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84650,
      "firstName": "Miryung",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "i2pJRmjCRKUu6NNOXFEQYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84651,
      "firstName": "Diego",
      "lastName": "Martinez Plasencia",
      "middleInitial": "",
      "importedId": "TliXoRsunqNp-JvoYJ2FCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84652,
      "firstName": "Christian",
      "lastName": "Holz",
      "middleInitial": "",
      "importedId": "iLPlU_z1dah9YY__uEeXsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84653,
      "firstName": "Raida",
      "lastName": "Karim",
      "middleInitial": "",
      "importedId": "_ZfoPcTZJ86lRJobViAPGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84654,
      "firstName": "Shamane",
      "lastName": "Siriwardhana",
      "middleInitial": "",
      "importedId": "8_EoUbR08mloA1sH12U4Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84655,
      "firstName": "Yi-Hao",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "mA32XskTkKZ9pUU0qJYRTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84656,
      "firstName": "Shrutarshi",
      "lastName": "Basu",
      "middleInitial": "",
      "importedId": "iPBbETwndQA8tmQ4TkLxzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84657,
      "firstName": "Rubaiat Habib",
      "lastName": "Kazi",
      "middleInitial": "",
      "importedId": "UJXDaT6uZ4fNhdFsUVs55w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84658,
      "firstName": "James",
      "lastName": "Devine",
      "middleInitial": "",
      "importedId": "pUU_tx_0v0SyPCYl8yLE-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84659,
      "firstName": "Chris",
      "lastName": "Harrison",
      "middleInitial": "",
      "importedId": "s4DDgcdMxU0xmPJmhyUsNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84660,
      "firstName": "Zheer",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "AekNariZFfr_q_OcOMZ-iQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84661,
      "firstName": "Tao",
      "lastName": "Dong",
      "middleInitial": "",
      "importedId": "n-Rxsoz8m26NZHeg6ThM_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84662,
      "firstName": "Pedro",
      "lastName": "Lopes",
      "middleInitial": "",
      "importedId": "vu7xKEsb0s2o9WvEByjw7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84663,
      "firstName": "Kosuke",
      "lastName": "Sato",
      "middleInitial": "",
      "importedId": "xsROZ5Z06ANA2hbsJjbRDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84664,
      "firstName": "Jürgen",
      "lastName": "Steimle",
      "middleInitial": "",
      "importedId": "s7t9OOHe8Q59g39YCU0IbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84665,
      "firstName": "Roberto",
      "lastName": "Montano-Murillo",
      "middleInitial": "A",
      "importedId": "pdBQz9ztSjkiwfhc2ELxfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84666,
      "firstName": "Daniel",
      "lastName": "Ashbrook",
      "middleInitial": "",
      "importedId": "32JeI_YKMn-zq1ONOD9hww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84667,
      "firstName": "Ken",
      "lastName": "Pfeuffer",
      "middleInitial": "",
      "importedId": "NrpIHYjQBiGY3BcwqIdH0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84668,
      "firstName": "Xun",
      "lastName": "Qian",
      "middleInitial": "",
      "importedId": "Mzd41Mu9XTzsFqvxf99-oA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84669,
      "firstName": "Leonard",
      "lastName": "Geier",
      "middleInitial": "",
      "importedId": "4V1mHckK7urqegyfgkAnEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84670,
      "firstName": "Madalina",
      "lastName": "Nicolae",
      "middleInitial": "Luciana",
      "importedId": "IRnhxDKl1XTcEk7-vwPL_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84671,
      "firstName": "Napol",
      "lastName": "Rachatasumrit",
      "middleInitial": "",
      "importedId": "1B0MzQehvsWgWfH1QPEcVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84672,
      "firstName": "Chi-Jung",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "ygu5H9uKit8L6QRSsEhJdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84673,
      "firstName": "Andrea",
      "lastName": "Bianchi",
      "middleInitial": "",
      "importedId": "K2M98YEtx8kGpIWWEMYEgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84674,
      "firstName": "Rui",
      "lastName": "Dong",
      "middleInitial": "",
      "importedId": "8J_x-z48prvNJPm2Ev6rIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84675,
      "firstName": "Shan-Yuan",
      "lastName": "Teng",
      "middleInitial": "",
      "importedId": "Fr_MdyDQrCsSwLDzlppG2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84676,
      "firstName": "Junichi",
      "lastName": "Yamaoka",
      "middleInitial": "",
      "importedId": "PF1hNOh4azOewyPHUG5Q3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84677,
      "firstName": "Christoph",
      "lastName": "Thieme",
      "middleInitial": "",
      "importedId": "cZF6Tw_9Xccp1AROB30vgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84678,
      "firstName": "Tovi",
      "lastName": "Grossman",
      "middleInitial": "",
      "importedId": "y0gW9n9_FRp81xTibIb1pg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84679,
      "firstName": "Marcus",
      "lastName": "Ding",
      "middleInitial": "",
      "importedId": "5qrdBpv73ixR3s0o9NL1uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84680,
      "firstName": "Ting",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "IU3OSA2laM0n5yRSDriAmw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84681,
      "firstName": "Daisuke",
      "lastName": "Iwai",
      "middleInitial": "",
      "importedId": "Ck_jbdC7fzNnHEP60hkWgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84682,
      "firstName": "Narin",
      "lastName": "Okazaki",
      "middleInitial": "",
      "importedId": "2xmDeDIt12S4ECrQXt7dFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84683,
      "firstName": "Christine",
      "lastName": "Dierk",
      "middleInitial": "",
      "importedId": "fP6v1ja3w7NaUmwzMAmcdQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84684,
      "firstName": "Ken",
      "lastName": "Takaki",
      "middleInitial": "",
      "importedId": "aUMpjyUeFeyjUicQc6lgPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84685,
      "firstName": "Yang",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "H6pNZe1VMhZtmachw5ibrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84686,
      "firstName": "Michael Xieyang",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "uzbYmbUQzU3rkpwX_i4v_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84687,
      "firstName": "Brian",
      "lastName": "Hall",
      "middleInitial": "D.",
      "importedId": "YoLncNwJG2o0z4Co_V3WBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84688,
      "firstName": "Toki",
      "lastName": "Takeda",
      "middleInitial": "",
      "importedId": "s8oa-mIPHNMxr2aM23V0Pg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84689,
      "firstName": "Yujie",
      "lastName": "Tao",
      "middleInitial": "",
      "importedId": "w5MuQDqF25pQ_W6MnCH77g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84690,
      "firstName": "Man To",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "yOsZo8ZNnEBcHtvm0qcKlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84691,
      "firstName": "João",
      "lastName": "Evangelista Belo",
      "middleInitial": "Marcelo",
      "importedId": "dvHBJlfCWgQhrKbxlIjcRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84692,
      "firstName": "Lukas",
      "lastName": "Fritzsche",
      "middleInitial": "",
      "importedId": "ueRf4sxwsGZv0qrUC7D7yQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84693,
      "firstName": "Ryosuke",
      "lastName": "Aoki",
      "middleInitial": "",
      "importedId": "udwMrtERER59CwCow6LsJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84694,
      "firstName": "Harmanpreet",
      "lastName": "Kaur",
      "middleInitial": "",
      "importedId": "8hXk1vqVs-WqqeLuFDMczQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84695,
      "firstName": "Mackenzie",
      "lastName": "Leake",
      "middleInitial": "",
      "importedId": "iUeS_U98Vg4xQUr78-q7Kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84696,
      "firstName": "Yuxuan",
      "lastName": "Lei",
      "middleInitial": "",
      "importedId": "FA1i3AfzRISWrLurJseMAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84697,
      "firstName": "Tianze",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "XtKu1Qkaapbv6hIPImwwnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84698,
      "firstName": "Martin",
      "lastName": "Nisser",
      "middleInitial": "",
      "importedId": "I6i095_s-O7ci-bCOVlYDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84699,
      "firstName": "Haeun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "07DnQhPkUidi35pF2u3yQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84700,
      "firstName": "Aditya",
      "lastName": "Nittala",
      "middleInitial": "Shekhar",
      "importedId": "AtfZmail5GVrrVsbogMf0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84701,
      "firstName": "James",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "bQBV63dBKpzQVKu7t6QX1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84702,
      "firstName": "Wan-Chen",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "9dyfctixmaNcwtAuhWEs7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84703,
      "firstName": "Kai",
      "lastName": "Mihata",
      "middleInitial": "",
      "importedId": "jAr5Y_r1kao_6FuviRf0QA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84704,
      "firstName": "Miroslav",
      "lastName": "Bachinski",
      "middleInitial": "",
      "importedId": "AYKi0LfBnNWYryIvgeYlsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84705,
      "firstName": "Will",
      "lastName": "Brackenbury",
      "middleInitial": "",
      "importedId": "Oo6tU4cV42QRXy0r8Gz6-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84706,
      "firstName": "Chiu-Hsuan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "VWsqld1RZXNV4bUKycAMEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84707,
      "firstName": "Masahiko",
      "lastName": "Inami",
      "middleInitial": "",
      "importedId": "ymV92K5P9NKDPz9t2QmJBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84708,
      "firstName": "Kuangqi",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "9mrFNia1YIt3yMxX_UuJ0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84709,
      "firstName": "Elin",
      "lastName": "Björling",
      "middleInitial": "A.",
      "importedId": "uAIlEGHvpzbPQAY3izbuPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84710,
      "firstName": "Yifei",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "luaoCVSTTY_SipWh6wqCGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84711,
      "firstName": "Otmar",
      "lastName": "Hilliges",
      "middleInitial": "",
      "importedId": "qct9P8AqtwogZhO0xOogyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84712,
      "firstName": "Peggy",
      "lastName": "Chi",
      "middleInitial": "",
      "importedId": "cibNVEBNbpclSQIzN8YrLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84713,
      "firstName": "Yuki",
      "lastName": "Koyama",
      "middleInitial": "",
      "importedId": "tdeFtc7Ep2XEnkJ3qohlwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84714,
      "firstName": "Erik",
      "lastName": "Langenhan",
      "middleInitial": "",
      "importedId": "gZg4WUD_uEnM1uOBjIN3Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84715,
      "firstName": "Wenjing",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "QXcpeoSppAOEbQ8xSlKk3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84716,
      "firstName": "Wojciech",
      "lastName": "Jarosz",
      "middleInitial": "",
      "importedId": "wi-MtC8J9SSIpTIUpJeI4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84717,
      "firstName": "Lingyun",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "anf4DjW8apv2gTtBT55RPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84718,
      "firstName": "Lydia",
      "lastName": "Chilton",
      "middleInitial": "B",
      "importedId": "jM77DxTwyp1U8TtuVyYGCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84719,
      "firstName": "Leping",
      "lastName": "Qiu",
      "middleInitial": "",
      "importedId": "bJ6Bha0xwvnkTM9tqYX0lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84720,
      "firstName": "Hrvoje",
      "lastName": "Benko",
      "middleInitial": "",
      "importedId": "wlbeOcO0F7b-yrwrphSmNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84721,
      "firstName": "Xuewei",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "tPdRL9t_ZNnnQgbVxQB98g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84722,
      "firstName": "Andrés",
      "lastName": "Lucero",
      "middleInitial": "",
      "importedId": "ZiIVkcBbtonl5luNH-rKeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84723,
      "firstName": "Meredith",
      "lastName": "Morris",
      "middleInitial": "Ringel",
      "importedId": "5yIvWdXUvJUOaHe6nlRO7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84724,
      "firstName": "Yuwen",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "jjGTCNfGxiVrn7hvXxlvXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84725,
      "firstName": "Yukang",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "3VxTjW6x8TBLTaM9O3Qh2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84726,
      "firstName": "Ishan",
      "lastName": "Chatterjee",
      "middleInitial": "",
      "importedId": "S8IFANAmyzzQ1FUZgZGddQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84727,
      "firstName": "Takeru",
      "lastName": "Hashimoto",
      "middleInitial": "",
      "importedId": "eDrbw5Burt9qbTDpw5E4VQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84728,
      "firstName": "Mark",
      "lastName": "Billinghurst",
      "middleInitial": "",
      "importedId": "QlX1SYXVMZkLEc_X2oWX1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84729,
      "firstName": "Xue",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "XNyimXh2mo8NW1aDfu3EOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84730,
      "firstName": "Zhipeng",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "3SAjQF1S2E3cLBzH2JmkTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84731,
      "firstName": "Raf",
      "lastName": "Ramakers",
      "middleInitial": "",
      "importedId": "D5UPmdoyrXN_yI0FHEzJgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84732,
      "firstName": "Hyung-Kwon",
      "lastName": "Ko",
      "middleInitial": "",
      "importedId": "7TNCJqdZdFYoVko19rzCpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84733,
      "firstName": "Bohyoung",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "s5TFxkaHs5TJe18hMFQxnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84734,
      "firstName": "Liang-Jin",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "4id8H6ArC2ca4vaMV5wkNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84735,
      "firstName": "Daniel",
      "lastName": "Weld",
      "middleInitial": "S",
      "importedId": "iopluiDMx007wNu9A4yNGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84736,
      "firstName": "Conrad",
      "lastName": "Lempert",
      "middleInitial": "",
      "importedId": "R_LGhNf_v2hROWTcym2ojQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84737,
      "firstName": "Andrew",
      "lastName": "Howes",
      "middleInitial": "",
      "importedId": "CobKcZCekqotp775irFYfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84738,
      "firstName": "Robert",
      "lastName": "Lindeman",
      "middleInitial": "W.",
      "importedId": "Tbcq5CY2MSefQ7PvqrZkmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84739,
      "firstName": "Andrew",
      "lastName": "Macvean",
      "middleInitial": "",
      "importedId": "S33ki-5u0n5sK2kD8a6oug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84740,
      "firstName": "Mathias",
      "lastName": "Lystbæk",
      "middleInitial": "N.",
      "importedId": "Tj3NjTKO6GPpocMqwX-RrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84741,
      "firstName": "Ziyi",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "zRaAg-C-1-mHnAEf3QsrGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84742,
      "firstName": "Amy",
      "lastName": "Pavel",
      "middleInitial": "",
      "importedId": "mWHY4paujYm0BGpPaankqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84743,
      "firstName": "Mira",
      "lastName": "Haberfellner",
      "middleInitial": "Alida",
      "importedId": "RYqj9kz4seqmxp9_unhpVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84744,
      "firstName": "Giulia",
      "lastName": "Barbareschi",
      "middleInitial": "",
      "importedId": "zQI_WMsL-y3to63Pg_mKfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84745,
      "firstName": "Chao-Hsien",
      "lastName": "Ting",
      "middleInitial": "",
      "importedId": "9li2X1Sm7ELnnD6mjgopnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84746,
      "firstName": "Liang",
      "lastName": "He",
      "middleInitial": "",
      "importedId": "FeIfGLSBkBwF8kzpq-ZsTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84747,
      "firstName": "Vivek",
      "lastName": "Kwatra",
      "middleInitial": "",
      "importedId": "2641ZZbxqSeTh5LOYpRlAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84748,
      "firstName": "Edith",
      "lastName": "Law",
      "middleInitial": "",
      "importedId": "FE15AcQdoXpEPmGc-0BrsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84749,
      "firstName": "Elena",
      "lastName": "Glassman",
      "middleInitial": "L.",
      "importedId": "QmlJycOy4LH6PdhoUoRyEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84750,
      "firstName": "Karim",
      "lastName": "Benharrak",
      "middleInitial": "",
      "importedId": "GA_J0WcA3n7JaIT_6YI-RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84751,
      "firstName": "Muhammad",
      "lastName": "Abdullah",
      "middleInitial": "",
      "importedId": "uMJEArnyBaOHD0nBraCwfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84752,
      "firstName": "Daesik",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "AYeJdYPPnXgHxgxpVJLk0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84753,
      "firstName": "Daehwa",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "wofoprAgV6gzBo3Cazk5RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84754,
      "firstName": "Yan",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "nMaLQpu3pR6H8kYCJ1NR1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84755,
      "firstName": "Liwei",
      "lastName": "Chan",
      "middleInitial": "",
      "importedId": "KB4Ew3efN16WCB23RZP3hA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84756,
      "firstName": "Karthik",
      "lastName": "Mahadevan",
      "middleInitial": "",
      "importedId": "ZTJVejnFuGJVN931XGNp4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84757,
      "firstName": "Han",
      "lastName": "Qiao",
      "middleInitial": "",
      "importedId": "ObR7CFb0cf6HF5mv7GU8Ow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84758,
      "firstName": "Mark",
      "lastName": "McGill",
      "middleInitial": "",
      "importedId": "ob1mmWPesH21Yfh_UxGqsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84759,
      "firstName": "Marc",
      "lastName": "Teyssier",
      "middleInitial": "",
      "importedId": "9ExYnNlmMZIrFprSSixNHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84760,
      "firstName": "Te-Yen",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "6gULAFNgA33JONjOq1B8tQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84761,
      "firstName": "Mehrad",
      "lastName": "Faridan",
      "middleInitial": "",
      "importedId": "rGXs39D1nd_7hWuZn0WkJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84762,
      "firstName": "Gwanmo",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "ab8Knk04n-JaWdm135r3Tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84763,
      "firstName": "Venkatesh",
      "lastName": "Potluri",
      "middleInitial": "",
      "importedId": "lwH56TDZS0dH5fL6w51oOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84764,
      "firstName": "Tapan",
      "lastName": "Parikh",
      "middleInitial": "",
      "importedId": "xyPB9nk0u9NqTRas1KHqxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84765,
      "firstName": "Michael",
      "lastName": "Haller",
      "middleInitial": "",
      "importedId": "2ywHXzQ612PNovLVgpOnBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84766,
      "firstName": "Bradley",
      "lastName": "Breneisen",
      "middleInitial": "",
      "importedId": "PwXlnFRZw-OVmw7e2zOxfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84767,
      "firstName": "Bjoern",
      "lastName": "Hartmann",
      "middleInitial": "",
      "importedId": "UeMY1SsBRzOg4bVxZSCmuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84768,
      "firstName": "Hyeonsu",
      "lastName": "Kang",
      "middleInitial": "B",
      "importedId": "gCmsrQ1XRAePogWZ7jZMEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84769,
      "firstName": "Lea",
      "lastName": "Verou",
      "middleInitial": "",
      "importedId": "ayUdJFNb8V4oY5073Wz9WQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84770,
      "firstName": "JaeEun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "7KkdrEEgg9REel6DukhEvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84771,
      "firstName": "Yuki",
      "lastName": "Onishi",
      "middleInitial": "",
      "importedId": "My0h56Gnf9Y3w4X8P3J5lw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84772,
      "firstName": "Oliver",
      "lastName": "Adameck",
      "middleInitial": "",
      "importedId": "_8ZZtpRSDNh6biAcRZ6mIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84773,
      "firstName": "Bing-Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "-lrGWSm4EFKCsnZiKuIu3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84774,
      "firstName": "Thomas",
      "lastName": "Kern",
      "middleInitial": "",
      "importedId": "c1P9fNubH55KH3l-JoPl0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84775,
      "firstName": "Masaharu",
      "lastName": "Hirose",
      "middleInitial": "",
      "importedId": "BLzvq1-D2B5iug0Blgza9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84776,
      "firstName": "Chia-Sheng",
      "lastName": "Hung",
      "middleInitial": "",
      "importedId": "J5DDlSuIex49ojRjXY5nSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84778,
      "firstName": "Zhengzhe",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "pnenrv-7MnVSj52a1Ksn8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84779,
      "firstName": "Xia",
      "lastName": "Su",
      "middleInitial": "",
      "importedId": "J4pksFIR03W2_In-JLQNYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84780,
      "firstName": "Kazuyuki",
      "lastName": "Fujita",
      "middleInitial": "",
      "importedId": "e-KfkiPFbFPUYASETBkUtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84781,
      "firstName": "Xingyu",
      "lastName": "Liu",
      "middleInitial": "\"Bruce\"",
      "importedId": "BB-GhbxpYiO9H3ClxDe2YQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84782,
      "firstName": "Jonathan",
      "lastName": "Bragg",
      "middleInitial": "",
      "importedId": "5juTx1AoMViy24GcPbDLsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84783,
      "firstName": "Masataka",
      "lastName": "Goto",
      "middleInitial": "",
      "importedId": "tHzzCOYBm1fj6WK73AufPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84784,
      "firstName": "Joseph Chee",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "zuS02m6iwKI6qMyzZ93-Sg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84785,
      "firstName": "Andrew",
      "lastName": "Kuznetsov",
      "middleInitial": "",
      "importedId": "6VoX4Bp0uioQsleoX1I__Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84786,
      "firstName": "HANG",
      "lastName": "ZHAO",
      "middleInitial": "",
      "importedId": "f_-DHIdonT-qwXrpVrZu-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84787,
      "firstName": "Jörg",
      "lastName": "Müller",
      "middleInitial": "",
      "importedId": "9GFxpxzkFRM4VikuvGPEKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84788,
      "firstName": "Steve",
      "lastName": "Hodges",
      "middleInitial": "",
      "importedId": "gRv7CYVQNVZB2DZUjxLdvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84789,
      "firstName": "Michael",
      "lastName": "Roberts",
      "middleInitial": "",
      "importedId": "2mcsOxAz2Vj7uFpVJ7uo1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84790,
      "firstName": "Xin",
      "lastName": "Yi",
      "middleInitial": "",
      "importedId": "3dZ_qNHb5C6MaIhYFjdp0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84791,
      "firstName": "Zhongyi",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "SkyHXjYANOq5lqmLiB2IYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84792,
      "firstName": "Mingming",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "8DRvsbdX9oLqirWN3umFnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84793,
      "firstName": "Rong-Hao",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "P5KiSriu_URnO56cXxb8Zw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84794,
      "firstName": "Jaewook",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "Ko2N9T8zAz-zPPa8nd9IBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84795,
      "firstName": "Chi-Hao",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "RZ5ozYR5_Ojof-ZpQj7yNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84796,
      "firstName": "Vanessa",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "xMal5bxJ4N4eVl77G8eGJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84797,
      "firstName": "Casper",
      "lastName": "Harteveld",
      "middleInitial": "",
      "importedId": "AdKhnmom2oLh__sU6QgtKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84798,
      "firstName": "Markus",
      "lastName": "Klar",
      "middleInitial": "",
      "importedId": "ZA0blB-jyfETjwo8IFZanw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84799,
      "firstName": "Dhanya",
      "lastName": "Jayagopal",
      "middleInitial": "",
      "importedId": "qTQCNf3EDMB0INVSqeRVLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84800,
      "firstName": "Anthony",
      "lastName": "DeArmas",
      "middleInitial": "J",
      "importedId": "q23b6foOHamG_0eVY67xrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84801,
      "firstName": "Yu-Tzu",
      "lastName": "Chao",
      "middleInitial": "",
      "importedId": "joFFRSMcT2yOxs29Opi7Yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84802,
      "firstName": "Imtiaz",
      "lastName": "Rahman",
      "middleInitial": "",
      "importedId": "UxiIqC7d3MyRNg7JHgW34Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84803,
      "firstName": "Ovidiu-Ciprian",
      "lastName": "Ungurean",
      "middleInitial": "",
      "importedId": "tzU-AvgTALY4-FiAVzXdgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84804,
      "firstName": "Jiaji",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "ngCZ_WbPTjbycakkfHYywQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84805,
      "firstName": "Tobias",
      "lastName": "Langlotz",
      "middleInitial": "",
      "importedId": "mYrVKjYUdhZLQJBbqdqYGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84806,
      "firstName": "Samin",
      "lastName": "Farajian",
      "middleInitial": "",
      "importedId": "y2SDkMn15ys-EwBA2QylvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84807,
      "firstName": "Jeppe",
      "lastName": "Kristensen",
      "middleInitial": "Theiss",
      "importedId": "yCJYrfIFcKawuZb99QIWTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84808,
      "firstName": "Zhanhui",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "DXCME3MK42FFdTieSJs9qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84809,
      "firstName": "Michael",
      "lastName": "Nebeling",
      "middleInitial": "",
      "importedId": "7Kex_0dKVNSpniXyyTYpBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84810,
      "firstName": "Bo",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "khJdyOZVWBSFkBgBhrL3eQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84811,
      "firstName": "Shinji",
      "lastName": "Miyahara",
      "middleInitial": "",
      "importedId": "IF6PFlFe0EP_n4w1Nhaf3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84812,
      "firstName": "Ling-Chien",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "VBY0ztbw7dOJNf53wGhiwQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84813,
      "firstName": "Shanshan",
      "lastName": "Yao",
      "middleInitial": "",
      "importedId": "xwW6BhrVKZIhzc7k1FNgNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84814,
      "firstName": "Yoichi",
      "lastName": "Ochiai",
      "middleInitial": "",
      "importedId": "oPTUR6agW0OdIumyZpymlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84815,
      "firstName": "Gilbert",
      "lastName": "Bernstein",
      "middleInitial": "",
      "importedId": "ZfsVGjUlH61gPg59GJVwrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84816,
      "firstName": "Jeffrey",
      "lastName": "Lipton",
      "middleInitial": "Ian",
      "importedId": "JMRd5EFZHc_ZASO3UsyyiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84817,
      "firstName": "Jon",
      "lastName": "Froehlich",
      "middleInitial": "E.",
      "importedId": "WzZztLsZHV7nWIHWf-9_EA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84818,
      "firstName": "Parinya",
      "lastName": "Punpongsanon",
      "middleInitial": "",
      "importedId": "raaqUQZBcRAt6RIy5HIHSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84819,
      "firstName": "Aakar",
      "lastName": "Gupta",
      "middleInitial": "",
      "importedId": "XMRioOOL5rNvcM8E9h2LDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84820,
      "firstName": "Bongshin",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "fYM7dKthqZ4SngFo-Q0RJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84821,
      "firstName": "Junzhe",
      "lastName": "Ji",
      "middleInitial": "",
      "importedId": "HBersIGARR2I5GvFAxTk1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84822,
      "firstName": "Yu",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "e8rWewnb2rSS1gJ7UAiipA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84823,
      "firstName": "Chutian",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "kTZL4q-PRkFlSQikd8Y6Cg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84824,
      "firstName": "Yumeng",
      "lastName": "Zhuang",
      "middleInitial": "",
      "importedId": "xT8wSiyEsQqZahMEH7Hv6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84825,
      "firstName": "Sangho",
      "lastName": "Suh",
      "middleInitial": "",
      "importedId": "LCIY9oz1aK9g8onEOgleUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84826,
      "firstName": "Hechuan",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "2-P1ebUGm_stEEulh8x6lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84827,
      "firstName": "John",
      "lastName": "Thompson",
      "middleInitial": "R",
      "importedId": "bHSNTcS-t6ah9H4crPYqEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84828,
      "firstName": "Paolo",
      "lastName": "Burelli",
      "middleInitial": "",
      "importedId": "f3LTZRjPpzlgZzAas9zNlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84829,
      "firstName": "Takatoshi",
      "lastName": "Yoshida",
      "middleInitial": "",
      "importedId": "OKz6wIOFLDOuKlaDK1pZmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84830,
      "firstName": "Amber",
      "lastName": "Horvath",
      "middleInitial": "",
      "importedId": "AenyFtpLOwQCNPYcD7PtjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84831,
      "firstName": "Syed Mostofa",
      "lastName": "Monsur",
      "middleInitial": "",
      "importedId": "RcrDI-MpdbwF2kpwaI-npA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84832,
      "firstName": "Dingzeyu",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "_lqhHWED2jy-w5GrjE_BsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84833,
      "firstName": "Shoi",
      "lastName": "Higashiyama",
      "middleInitial": "",
      "importedId": "fvjTs8J4JEsF2DY8q1YLkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84834,
      "firstName": "Chiao",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "hkcGY5zjY8GDiuTTznLGBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84835,
      "firstName": "Florian",
      "lastName": "Fischer",
      "middleInitial": "",
      "importedId": "AbFZu6hv_QZt_zUJKSzBYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84836,
      "firstName": "Aleksi",
      "lastName": "Ikkala",
      "middleInitial": "",
      "importedId": "ILB9l3NGkRNVN4PxdAmYmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84837,
      "firstName": "Jessica",
      "lastName": "Feuston",
      "middleInitial": "L.",
      "importedId": "b7TbzIiAJuLAYFBug_AyJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84838,
      "firstName": "Raahul",
      "lastName": "Natarrajan",
      "middleInitial": "",
      "importedId": "N79XswfuLt1jm5wDjDBGPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84839,
      "firstName": "Yue",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "5t1nNvyKtDf0yxfOhRqLDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84840,
      "firstName": "Hyunyoung",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "ntYqo9wpGHLR5HKb0w560Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84841,
      "firstName": "Payod",
      "lastName": "Panda",
      "middleInitial": "",
      "importedId": "TWLrzUisMJqmD-TUhodqoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84842,
      "firstName": "Jason",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "AX66TvfSdtUHX3ZgCBIB1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84843,
      "firstName": "Jonathan",
      "lastName": "Sutton",
      "middleInitial": "",
      "importedId": "NfzpalVWXthXBvjhG6DnTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84844,
      "firstName": "Shih-Hao",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "NmYBSMtaoU7OyW1SY58HDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84845,
      "firstName": "Richard",
      "lastName": "Yao",
      "middleInitial": "",
      "importedId": "5uaLfQot9Gj2GTtKOJ5CWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84846,
      "firstName": "Xiaoying",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "3qGxucbmvfz93Ic_tR3G3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84847,
      "firstName": "Patricia",
      "lastName": "Alves-Oliveira",
      "middleInitial": "",
      "importedId": "GkjoH8ctblSs8YLfJj6h2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84848,
      "firstName": "Vivian",
      "lastName": "Chan",
      "middleInitial": "Hsinyueh",
      "importedId": "ilPCiWR_rNvPu0mRXYO_iw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84849,
      "firstName": "Ruyu",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "5WXV-zfi7KFCM8fAqBLKJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84850,
      "firstName": "Farshid",
      "lastName": "Salemi Parizi",
      "middleInitial": "",
      "importedId": "WELYGwe9XSr0dqY_oPF85A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84851,
      "firstName": "Yasuaki",
      "lastName": "Kakehi",
      "middleInitial": "",
      "importedId": "2Jl-Pws_aOFCgYMfNPc1YA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84852,
      "firstName": "Sungnam",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "rUZuV7H3jY2APIdBKjZo7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84853,
      "firstName": "Zeyu",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "mUgdEK5HN9HRQbpOsze3_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84854,
      "firstName": "Juling",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "JbaFP2dF2b518C81j25yGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84855,
      "firstName": "Florian",
      "lastName": "Lehmann",
      "middleInitial": "",
      "importedId": "Nf9XHogSmhkGkZjT1qUsyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84856,
      "firstName": "Lung-Pan",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "eA4F65PMG-ZnFOFgV9JhsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84857,
      "firstName": "K.",
      "lastName": "Wu",
      "middleInitial": "D.",
      "importedId": "ryp3UsA6ZnBwcP7qqnubxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84858,
      "firstName": "Jasmine",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "7zOfpMLFbNPM2fpIVwBnYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84859,
      "firstName": "Hsin-Ruey",
      "lastName": "Tsai",
      "middleInitial": "",
      "importedId": "gqmjc2BHoqyh3jfqKcAIIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84860,
      "firstName": "Guy",
      "lastName": "Lüthi",
      "middleInitial": "",
      "importedId": "1r0p04dja7grvpVr4XHUXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84861,
      "firstName": "Faraz",
      "lastName": "Faruqi",
      "middleInitial": "",
      "importedId": "smhcmuVPPGNqCG1o_uJ7dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84862,
      "firstName": "Steven",
      "lastName": "Dow",
      "middleInitial": "P.",
      "importedId": "8QHJbjDXFqKcG-dlzKVHwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84863,
      "firstName": "Yuta",
      "lastName": "Itoh",
      "middleInitial": "",
      "importedId": "SL_W6xgAN8h-_ktvw92OUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84864,
      "firstName": "Laura-Bianca",
      "lastName": "Bilius",
      "middleInitial": "",
      "importedId": "A0ZY1qM8Hiy3Q2ycIQsSxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84865,
      "firstName": "Yitao",
      "lastName": "Fan",
      "middleInitial": "",
      "importedId": "py-6QLdjiLMVurlUNR0qhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84866,
      "firstName": "Arinobu",
      "lastName": "Niijima",
      "middleInitial": "",
      "importedId": "1hXdXCGzoscaaHtE_sgpbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84867,
      "firstName": "Matt-Heun",
      "lastName": "Hong",
      "middleInitial": "",
      "importedId": "c8Hamgy3Nr7V2y0J9Q6GPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84868,
      "firstName": "Hongnan",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "EcVpo-7u8tpC-wRrhF6DGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84869,
      "firstName": "Shingo",
      "lastName": "Kitagawa",
      "middleInitial": "",
      "importedId": "K4N-_XUYKtX9TnLBdu9Ibg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84870,
      "firstName": "Yingyi",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "wV_V4EVAkvWHZBd2nIgXfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84871,
      "firstName": "Shwetak",
      "lastName": "Patel",
      "middleInitial": "",
      "importedId": "IVnjxjYkN3trolk1IiWHlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84872,
      "firstName": "Eammon",
      "lastName": "Littler",
      "middleInitial": "",
      "importedId": "-AU704DpyLtONF9_UQgZmw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84873,
      "firstName": "Tadeusz",
      "lastName": "Pforte",
      "middleInitial": "",
      "importedId": "2bYJSNT7WoyLI-VqptiVEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84874,
      "firstName": "Guanyun",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "aNj3BsCl2b0b7FACxYJdKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84875,
      "firstName": "Bjarne",
      "lastName": "Sievers",
      "middleInitial": "",
      "importedId": "zDcdd5KNqdsvP_K-KBGlyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84876,
      "firstName": "Michael",
      "lastName": "Bernstein",
      "middleInitial": "S.",
      "importedId": "batNJuLMTvp1yvs1G1gBHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84877,
      "firstName": "Perttu",
      "lastName": "Hämäläinen",
      "middleInitial": "",
      "importedId": "esoYDjUrwF1Ck2SScARemg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84878,
      "firstName": "Joon Gi",
      "lastName": "Shin",
      "middleInitial": "",
      "importedId": "-dnkYoLl8k6ohshcN_BI5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84879,
      "firstName": "Subin",
      "lastName": "An",
      "middleInitial": "",
      "importedId": "iqGXb2zo9rocODoL9XLBRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84880,
      "firstName": "Kongpyung (Justin)",
      "lastName": "Moon",
      "middleInitial": "",
      "importedId": "xz3jZ0bAUFuiiktNiFffhQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84881,
      "firstName": "Huaishu",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "cISP1gspzEJZEfcOkN6sIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84882,
      "firstName": "Yashaswini",
      "lastName": "Makaram",
      "middleInitial": "",
      "importedId": "gQ1z8hEBhRlIt5ugHljWPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84883,
      "firstName": "Zhigeng",
      "lastName": "Pan",
      "middleInitial": "",
      "importedId": "W0Dr8GS8ud00IqMvmReiiA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84884,
      "firstName": "Yuntao",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "pTW1Zvq8601esh3qGSR0qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84885,
      "firstName": "Sang Won",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "6iT8RQIiG7T69Z5mNDGzNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84886,
      "firstName": "Kentaro",
      "lastName": "Yasu",
      "middleInitial": "",
      "importedId": "A7K1mK1GUhouzVCC5QCyRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84887,
      "firstName": "Nathan",
      "lastName": "Hahn",
      "middleInitial": "",
      "importedId": "QpF1tVSXmauH4Yk-OX0Nug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84888,
      "firstName": "Jinwook",
      "lastName": "Seo",
      "middleInitial": "",
      "importedId": "zuuM3P8Mg6jprfbgkCqg2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84889,
      "firstName": "Aaron",
      "lastName": "Elmore",
      "middleInitial": "",
      "importedId": "1zmLCVVQhxXWmX4x3EtjTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84890,
      "firstName": "Fengming",
      "lastName": "He",
      "middleInitial": "",
      "importedId": "Re2GVmjbFjrqLUzBI0L4bQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84891,
      "firstName": "Noyan",
      "lastName": "Evirgen",
      "middleInitial": "",
      "importedId": "tSrY4sH7Qo1QjfyLBmm5pg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84892,
      "firstName": "Asier",
      "lastName": "Marzo",
      "middleInitial": "",
      "importedId": "fQBnOT2vkF-srKd-CVFgJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84893,
      "firstName": "Willa Yunqi",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "piUdVxFzSKIxwdS0owntrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84894,
      "firstName": "Maya",
      "lastName": "Cakmak",
      "middleInitial": "",
      "importedId": "YGo_6Y2veS53wLbFNiWijw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84895,
      "firstName": "Ruei-Che",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "fj-AI7DGbP0P11WOCt3b2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84896,
      "firstName": "Carlos",
      "lastName": "Tejada",
      "middleInitial": "",
      "importedId": "uxlUPUXbsUd1bcpLca0sCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84897,
      "firstName": "Qiping",
      "lastName": "Pan",
      "middleInitial": "",
      "importedId": "j_WCAF1h982z8T6VSabu7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84898,
      "firstName": "Si",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "Xsu-fY8IlVRbStukw7Mw7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84899,
      "firstName": "Suranga",
      "lastName": "Nanayakkara",
      "middleInitial": "",
      "importedId": "BLBvqg3aFav-8-OggSRxoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84900,
      "firstName": "Lyn",
      "lastName": "Bartram",
      "middleInitial": "",
      "importedId": "OIvjiK-aou4psmDGWZpG9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84901,
      "firstName": "Hyojin",
      "lastName": "Ju",
      "middleInitial": "",
      "importedId": "3N9WBpPeSmcww2l3tj98eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84902,
      "firstName": "Donghan",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "oBbLhU_vIlGGsXpax_ibAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84903,
      "firstName": "Anna",
      "lastName": "Feit",
      "middleInitial": "Maria",
      "importedId": "fRUBGy6UaA-NDMjP-V3WGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84904,
      "firstName": "Minki",
      "lastName": "Cheon",
      "middleInitial": "",
      "importedId": "uVb1zsSXcx7oD_C1MyiqvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84905,
      "firstName": "Junyi",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "e55KVEA-R2fiLofj_Ci0fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84906,
      "firstName": "Syed Masum",
      "lastName": "Billah",
      "middleInitial": "",
      "importedId": "fk_oHW4xdx90gCXhWb40Tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84907,
      "firstName": "Christian",
      "lastName": "Guckelsberger",
      "middleInitial": "",
      "importedId": "Ts1b9GXuC5GvJK_Pk0X4cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84908,
      "firstName": "Kevin",
      "lastName": "Pu",
      "middleInitial": "",
      "importedId": "G0sYcmOgMBs-sF4VcCp5Bw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84909,
      "firstName": "Inseok",
      "lastName": "Hwang",
      "middleInitial": "",
      "importedId": "zfzoqAuOJsvTmBGkKyShKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84910,
      "firstName": "Daniel",
      "lastName": "Buschek",
      "middleInitial": "",
      "importedId": "-PmisC4rRA8oAfHrwVVdYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84911,
      "firstName": "Jungeun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "RiwySLG8PNCy12-V7KAKMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84912,
      "firstName": "Valkyrie",
      "lastName": "Savage",
      "middleInitial": "",
      "importedId": "S07-pS_XhfJSxmXiqXqgBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84913,
      "firstName": "Danielle",
      "lastName": "Szafir",
      "middleInitial": "Albers",
      "importedId": "vze5ZEbaToKesAszPPs4fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84914,
      "firstName": "Amanpreet",
      "lastName": "Singh",
      "middleInitial": "",
      "importedId": "pwNZJ-JVGFaSv1jvPdx14A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84915,
      "firstName": "David",
      "lastName": "Karger",
      "middleInitial": "R",
      "importedId": "QHNqy5InRREuNW9Wu9SUng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84916,
      "firstName": "Hai",
      "lastName": "Dang",
      "middleInitial": "",
      "importedId": "sbimNJ2MBwJrSQvb3mLEVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84917,
      "firstName": "Takuji",
      "lastName": "Narumi",
      "middleInitial": "",
      "importedId": "6C7aUtZkLRJIYT0_TSY-SQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84918,
      "firstName": "Zhong-Yi",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "4oq2cKNZD0gjea8cFAd1RQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84919,
      "firstName": "Jiatong",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "DFjdJBbIrY7p1NFA1eeEbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84920,
      "firstName": "Xiaojun",
      "lastName": "Bi",
      "middleInitial": "",
      "importedId": "fpWqo1rUmyvEL7HUbPbz6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84921,
      "firstName": "Graham",
      "lastName": "Wilson",
      "middleInitial": "",
      "importedId": "sGtFXnJgdG9Og0TCkppwNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84922,
      "firstName": "Yudai",
      "lastName": "Tanaka",
      "middleInitial": "",
      "importedId": "bZXoz-mKCKaq3xd2DrrlTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84923,
      "firstName": "Ben",
      "lastName": "Lafreniere",
      "middleInitial": "",
      "importedId": "ZCec69xnESZNP5_GEGvG_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84924,
      "firstName": "Doug",
      "lastName": "Downey",
      "middleInitial": "",
      "importedId": "255nMOAJ-mwYfkkReXbHvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84925,
      "firstName": "Seung Kwon",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "5xB9Z_M95ZQhwQBCFdlTHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84926,
      "firstName": "Yamato",
      "lastName": "Miyatake",
      "middleInitial": "",
      "importedId": "jhTMJ98kcBgcdShZAiVejA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84927,
      "firstName": "Shivesh",
      "lastName": "Jadon",
      "middleInitial": "Singh",
      "importedId": "I4GgBu7Uag4YqoGRBtz2RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84928,
      "firstName": "Stefanie",
      "lastName": "Zollmann",
      "middleInitial": "",
      "importedId": "KqIZ_GgTpaXx8XNsZn-vtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84929,
      "firstName": "Antti",
      "lastName": "Oulasvirta",
      "middleInitial": "",
      "importedId": "jUiRwkj0eP82dZVQLtfCZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84930,
      "firstName": "Rainey",
      "lastName": "Fu",
      "middleInitial": "",
      "importedId": "sF1z2lIHEFM56LTWz0v6WQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84931,
      "firstName": "Irfan",
      "lastName": "Essa",
      "middleInitial": "",
      "importedId": "QnPiiTwuquo17pE2EoLKHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84932,
      "firstName": "Ebrima",
      "lastName": "Jarjue",
      "middleInitial": "Haddy",
      "importedId": "bbg3kQC1b1gQabVSplKzww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84933,
      "firstName": "Shigeo",
      "lastName": "Yoshida",
      "middleInitial": "",
      "importedId": "5HP2zckIbFI65epvHO_I6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84934,
      "firstName": "Joseph",
      "lastName": "Schwab",
      "middleInitial": "H",
      "importedId": "1xyV8hv0UDw1LPriNSO9uQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84935,
      "firstName": "Daniel",
      "lastName": "Medeiros",
      "middleInitial": "",
      "importedId": "STkQ0KbP4yrwwsBQFvw5Tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84936,
      "firstName": "Shohei",
      "lastName": "Katakura",
      "middleInitial": "",
      "importedId": "OutkrIA7fjSxwAe3iWUIhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84937,
      "firstName": "Jacqueline",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "TnmGZmN80TpiopsmwTAqwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84938,
      "firstName": "Joon Sung",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "F1Jsyz-cSSkB-vvuBl8udg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84939,
      "firstName": "Yongsung",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "8nizuR09Y2mSwzTpzFAbdQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84940,
      "firstName": "Eimontas",
      "lastName": "Jankauskis",
      "middleInitial": "",
      "importedId": "Z-LtahWzvxCjhRgfzeeqrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84941,
      "firstName": "Tianyi",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "7taies-lvmk7BcDr022wLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84942,
      "firstName": "Maneesh",
      "lastName": "Agrawala",
      "middleInitial": "",
      "importedId": "vEgkfTRwqF0MI1NyYHSNKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84943,
      "firstName": "TJ",
      "lastName": "Rhodes",
      "middleInitial": "",
      "importedId": "8RWsLWr1PV8Aqm_oaZrCWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84944,
      "firstName": "Nathan",
      "lastName": "DeVrio",
      "middleInitial": "",
      "importedId": "c87oHJSEuNpDHogPqJA6gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84945,
      "firstName": "Jiasheng",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "KHjNqBmP5cM1cLgUQ20MNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84946,
      "firstName": "Litao",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "7YBNL-RNL8vAsQsP0IDjVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84947,
      "firstName": "Elliott",
      "lastName": "Wen",
      "middleInitial": "",
      "importedId": "TttGgsPDgtXEVA4b42Tvuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84948,
      "firstName": "Kazuki",
      "lastName": "Takashima",
      "middleInitial": "",
      "importedId": "Y23h22sxdG2T0yx4JY6Azg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84949,
      "firstName": "Laurenz",
      "lastName": "Seidel",
      "middleInitial": "",
      "importedId": "8vRFBUN2OEL2Og-CBNBGwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84950,
      "firstName": "Gavin",
      "lastName": "Miller",
      "middleInitial": "S. P.",
      "importedId": "RmYFKr_XCjevlOrqEfSnfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84951,
      "firstName": "Stephen",
      "lastName": "Brewster",
      "middleInitial": "Anthony",
      "importedId": "5oR5tpEfy7_GSFNHxRxKQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84952,
      "firstName": "Jeffrey",
      "lastName": "Bigham",
      "middleInitial": "P",
      "importedId": "8nDgzwamTG7TuXrNxpBBuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84953,
      "firstName": "Lucian",
      "lastName": "Covarrubias",
      "middleInitial": "",
      "importedId": "tAjkWYeHw0GLA3wEJQDQEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84954,
      "firstName": "Srishti",
      "lastName": "Palani",
      "middleInitial": "",
      "importedId": "l2VZObNbp4VGFOy2ibMNrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84955,
      "firstName": "Marion",
      "lastName": "Koelle",
      "middleInitial": "",
      "importedId": "kEXDR06q99XOOFE0vFpI8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84956,
      "firstName": "Roderick",
      "lastName": "Murray-Smith",
      "middleInitial": "",
      "importedId": "LwxWm0xz2dftVoo6W2MTKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84957,
      "firstName": "Lauren",
      "lastName": "Marsh",
      "middleInitial": "A.",
      "importedId": "9lFxUYQhRUfJbM2sa7Bkjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84958,
      "firstName": "Vivian",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "5bGuu44RwxNnfK1WBTVUnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84959,
      "firstName": "Youyou",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "GmqHW2Is305oW02x_rjhLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84960,
      "firstName": "Roland",
      "lastName": "Aigner",
      "middleInitial": "",
      "importedId": "cZfw3cbro4ziSMll3iP9_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84961,
      "firstName": "Hua",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "Hc9HlM5tBxESAkrEGj9z0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84962,
      "firstName": "Yikai",
      "lastName": "Cui",
      "middleInitial": "",
      "importedId": "J4QKDCWGrwq-4sC0ykC61g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85126,
      "firstName": "Santawat",
      "lastName": "Thanyadit",
      "middleInitial": "",
      "importedId": "oTvlbunC-x6yQWclIxqcMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85127,
      "firstName": "Jin",
      "lastName": "Ryu",
      "middleInitial": "",
      "importedId": "yQh8OUgITbGFEmpA3T3nAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85128,
      "firstName": "Yasuto",
      "lastName": "Nakanishi",
      "middleInitial": "",
      "importedId": "lQn0gxgvpBHyPgLRwrGkhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85129,
      "firstName": "Homura",
      "lastName": "Kawamura",
      "middleInitial": "",
      "importedId": "k8Virgu2kiGaGGoQ6aA8hQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85130,
      "firstName": "Jiani",
      "lastName": "Zeng",
      "middleInitial": "",
      "importedId": "QlZHJduyL8PGc1JWLLOcfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85131,
      "firstName": "Shrenik",
      "lastName": "Sadalgi",
      "middleInitial": "",
      "importedId": "nPTTOFIZgdOo6vjLyKP0gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85132,
      "firstName": "Matthew",
      "lastName": "Komar",
      "middleInitial": "L.",
      "importedId": "yr3u35NDMT09VAgl_QBQtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85133,
      "firstName": "Evan",
      "lastName": "Pezent",
      "middleInitial": "",
      "importedId": "EbCWBvGpm70dEvKPaw2zSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85134,
      "firstName": "Sangwook",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "7-xO3SPxd2t5euLldSd75Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85135,
      "firstName": "Aaron",
      "lastName": "Quigley",
      "middleInitial": "J",
      "importedId": "MibYPMlrFfkZJFjG9MoptQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85136,
      "firstName": "Florian",
      "lastName": "Müller",
      "middleInitial": "",
      "importedId": "3gnZEiql5UVDZAWwJ8LD7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85137,
      "firstName": "Sam",
      "lastName": "Lally",
      "middleInitial": "",
      "importedId": "SrDEYbxGuFI0owi7czSTWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85138,
      "firstName": "Yuhei",
      "lastName": "Imai",
      "middleInitial": "",
      "importedId": "t_VxYW9dAf4CLCqA_n-X3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85139,
      "firstName": "Shea",
      "lastName": "Robinson",
      "middleInitial": "",
      "importedId": "0pfpps9bMKzPafTvHH7i3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85140,
      "firstName": "Michael",
      "lastName": "Wessely",
      "middleInitial": "",
      "importedId": "qRALpo5vu6f15uoXT7L8hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85141,
      "firstName": "Tashfiq",
      "lastName": "Ahmed",
      "middleInitial": "",
      "importedId": "M9EMrzdbc2pvJ2vI8hgTBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85142,
      "firstName": "Stephen",
      "lastName": "MacNeil",
      "middleInitial": "",
      "importedId": "B4lp0htvBcqTBYIuQJUiGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85143,
      "firstName": "Shiina",
      "lastName": "Takano",
      "middleInitial": "",
      "importedId": "1RoQlqWb5IlRFNKQekAPhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85144,
      "firstName": "Myeongseong",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "gmlhH7P3FLDp9dSixPHNNw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85145,
      "firstName": "Hirotaka",
      "lastName": "Hiraki",
      "middleInitial": "",
      "importedId": "q7g1mZpZnwW0YelNEL3zUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85146,
      "firstName": "Nick",
      "lastName": "Colonnese",
      "middleInitial": "",
      "importedId": "XXRxqHvvUkn4WQL000-5vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85147,
      "firstName": "Aryan",
      "lastName": "Saini",
      "middleInitial": "",
      "importedId": "H1Rts5KoHFb2ZMPLWDo1iQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85148,
      "firstName": "Takanori",
      "lastName": "Nishino",
      "middleInitial": "",
      "importedId": "W41KWpMCAQFH1N_ArUO5XA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85149,
      "firstName": "Yoshio",
      "lastName": "Ishiguro",
      "middleInitial": "",
      "importedId": "KELHV2x_XjeoZMQKSV5V1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85150,
      "firstName": "Marx",
      "lastName": "Wang",
      "middleInitial": "Boyuan",
      "importedId": "wb0IHJtxM5OfLHmpL9pb_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85151,
      "firstName": "Zixia",
      "lastName": "Zheng",
      "middleInitial": "",
      "importedId": "BMc-S3rAfqRmW82A-goNqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85152,
      "firstName": "Myung Jin",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "SWFAKyqAJJhfOX9wG4IBWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85153,
      "firstName": "Rajan",
      "lastName": "Vaish",
      "middleInitial": "",
      "importedId": "Uf3eWAiARyKtdTS7jMIArg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85154,
      "firstName": "Kota",
      "lastName": "Araki",
      "middleInitial": "",
      "importedId": "EU2zu9HtS5SKLILz4aN_9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85155,
      "firstName": "Masatoshi",
      "lastName": "Hamanaka",
      "middleInitial": "",
      "importedId": "WxMgc_YluJaF5Oj64yDhZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85156,
      "firstName": "Mohammad Imrul",
      "lastName": "Jubair",
      "middleInitial": "",
      "importedId": "T0NhOsmveD_Dr8M4YMab6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85157,
      "firstName": "Wanhui",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "dorqXTsYOVHnG5CBZGSmaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85158,
      "firstName": "Ryan",
      "lastName": "Rossi",
      "middleInitial": "",
      "importedId": "2Y4JZCP0NlLH4-1OrbIqTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85159,
      "firstName": "Yixiao",
      "lastName": "Kang",
      "middleInitial": "",
      "importedId": "auC7vW2pWnwqU1KkoLzMcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85160,
      "firstName": "Zeyu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "YsTO8sPaYRW7y1rUJI6Jgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85161,
      "firstName": "Hansoo",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "Pt9fKuXpLrDgroyAw59aUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85162,
      "firstName": "Niveditha",
      "lastName": "Samudrala",
      "middleInitial": "",
      "importedId": "ojzOOLK5zZZSit8dCfFvFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85163,
      "firstName": "Jiajia",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "4m1AdDemtvQBjStVnw5ivg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85164,
      "firstName": "Wenxin",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "vnPWrOd1xz2BDC3ZeJEyOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85165,
      "firstName": "Mariko",
      "lastName": "Chiba",
      "middleInitial": "",
      "importedId": "bKDt9ERkeXz_pjjDcXqO5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85166,
      "firstName": "Mingyuan",
      "lastName": "Zhong",
      "middleInitial": "",
      "importedId": "dQO5CKZoTOADE9_fxqfASw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85167,
      "firstName": "Alexandra",
      "lastName": "Slabakis",
      "middleInitial": "",
      "importedId": "4fyLO5sCCXcB4HZPIARlJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85168,
      "firstName": "Yuki",
      "lastName": "Kakui",
      "middleInitial": "",
      "importedId": "oPAUe-dZqage2zd5sroXHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85169,
      "firstName": "Mengjie",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "Na9LVjeoyGbkxXO4xTZ5Vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85170,
      "firstName": "Dominik",
      "lastName": "Schön",
      "middleInitial": "",
      "importedId": "Z4hyyGSPDtY18hRG6c3Umg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85171,
      "firstName": "Shinji",
      "lastName": "Shimojo",
      "middleInitial": "",
      "importedId": "gCzFe07jFBQqtJsvil0HyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85172,
      "firstName": "Ashraful",
      "lastName": "Islam",
      "middleInitial": "",
      "importedId": "UDyWya4rus8FwZt0_fKBnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85173,
      "firstName": "Shunyi",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "JGMVGsq60BAisKEbSFbbtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85174,
      "firstName": "Zhuoyue",
      "lastName": "Lyu",
      "middleInitial": "",
      "importedId": "ZsASiPVMooyt76i1fY1sZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85175,
      "firstName": "Kazuya",
      "lastName": "Takeda",
      "middleInitial": "",
      "importedId": "WCxxgDCUQDr5pi2EgbqkjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85176,
      "firstName": "Jasmine",
      "lastName": "Ou",
      "middleInitial": "",
      "importedId": "boD213OYIVfWonF4vLxVQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85177,
      "firstName": "Myounghoon",
      "lastName": "Jeon",
      "middleInitial": "",
      "importedId": "Jj0c3guSwqoC-DO7ttWDZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85178,
      "firstName": "Xubo",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "6blPxc8ZzM87xSTdSB5tZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85179,
      "firstName": "Ricardo",
      "lastName": "Gonzalez Penuela",
      "middleInitial": "E.",
      "importedId": "IFyHZRp-LZVuYxDBb2IwsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85180,
      "firstName": "Ayumu",
      "lastName": "Ogura",
      "middleInitial": "",
      "importedId": "1oJ6gm4ASsYMijrB13E44A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85181,
      "firstName": "Koichi",
      "lastName": "Mizutani",
      "middleInitial": "",
      "importedId": "yparbzj5umbPjrp8wO-F2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85182,
      "firstName": "Miguel",
      "lastName": "Bruns",
      "middleInitial": "",
      "importedId": "SolchVwRqw0P0yIX7z-sCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85183,
      "firstName": "Fabrice",
      "lastName": "Matulic",
      "middleInitial": "",
      "importedId": "yR21ibENOww9Yc3so4LODw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85184,
      "firstName": "Ryo",
      "lastName": "Kashiwabara",
      "middleInitial": "",
      "importedId": "-CMLDVITkKfPPImP5J_orQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85185,
      "firstName": "Shiri",
      "lastName": "Azenkot",
      "middleInitial": "",
      "importedId": "z9BHmIM2WFZyU8rL8YPTRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85186,
      "firstName": "Antonio",
      "lastName": "Krüger",
      "middleInitial": "",
      "importedId": "7BVGaBoskVivG5iBzHVi8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85187,
      "firstName": "Johannes",
      "lastName": "Kreutz",
      "middleInitial": "",
      "importedId": "7nhDm4MJ-HTDrVYcz-Ky5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85188,
      "firstName": "Cuong",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "Zr6kWI7nXq7mFI0bN2adwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85189,
      "firstName": "Peiyu",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "R5qH1pGOcFasVgJR6bCbgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85190,
      "firstName": "Hugo",
      "lastName": "Nicolau",
      "middleInitial": "",
      "importedId": "6pg_rrG91DnuYzl1ZCvEJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85191,
      "firstName": "Suzuha",
      "lastName": "Harada",
      "middleInitial": "",
      "importedId": "GByoT-QEm4EqncLhBVFe9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85192,
      "firstName": "Wren",
      "lastName": "Poremba",
      "middleInitial": "",
      "importedId": "ULBJjnrpk1ZkLJ5owHMBDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85193,
      "firstName": "Hanbit",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "aFKfLaPfArbdtzAxvz8UqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85194,
      "firstName": "Andreas",
      "lastName": "Vogelsang",
      "middleInitial": "",
      "importedId": "t5eAvcWQW7hW_8TgW06K1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85195,
      "firstName": "Jingyi",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "Z261FiwxZTVz9kmen0433w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85196,
      "firstName": "Xiongqi",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "diQhS25cKt3nueN5mPduEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85197,
      "firstName": "Zixiong",
      "lastName": "Su",
      "middleInitial": "",
      "importedId": "n1BzWkXSq9CyFumdm5H98w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85198,
      "firstName": "Yuki",
      "lastName": "Sakashita",
      "middleInitial": "",
      "importedId": "Y3j-uLK1Vl22yHCBnpNlpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85199,
      "firstName": "Xuhai",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "kZTgMM28kznZj_RjAAperg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85200,
      "firstName": "Majed",
      "lastName": "Samad",
      "middleInitial": "",
      "importedId": "nW1qg8m4aFjFYLFMifMqqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85201,
      "firstName": "Gelareh",
      "lastName": "Mohammadi",
      "middleInitial": "",
      "importedId": "Dci-0EF6i1u98NbbRhnSQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85202,
      "firstName": "Keiichi",
      "lastName": "Zempo",
      "middleInitial": "",
      "importedId": "5EXLcJOvdZpzRMR4gs1ZAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85203,
      "firstName": "Christoph",
      "lastName": "Lingenfelder",
      "middleInitial": "",
      "importedId": "0DLiSW2LjvjMYCojEmUfkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85204,
      "firstName": "Seongkook",
      "lastName": "Heo",
      "middleInitial": "",
      "importedId": "8u6ENy5fbi7xGnVehduTFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85205,
      "firstName": "Ching-Wen",
      "lastName": "Hung",
      "middleInitial": "",
      "importedId": "yddadgl3HkQf_5i5nWJjCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85206,
      "firstName": "Daniel",
      "lastName": "Manesh",
      "middleInitial": "",
      "importedId": "LnmKKmSFDsqTXa7KQ6tblg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85207,
      "firstName": "Arthur",
      "lastName": "Caetano",
      "middleInitial": "",
      "importedId": "igWWQQU6S_5tsJaeJgdEmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85208,
      "firstName": "Robert",
      "lastName": "LiKamWa",
      "middleInitial": "",
      "importedId": "CUA3v03V_fkBpmPXw7oNug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85209,
      "firstName": "Chenxin",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "7hLIG7lFgfJCbrNHFc1Jtg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85210,
      "firstName": "Hideki",
      "lastName": "Koike",
      "middleInitial": "",
      "importedId": "RDQHw03ag01lqULc0xa4_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85211,
      "firstName": "Jackie",
      "lastName": "Yang",
      "middleInitial": "(Junrui)",
      "importedId": "IQsl-k8PL0D9P8n6TWUOmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85212,
      "firstName": "Pak Ming",
      "lastName": "Fan",
      "middleInitial": "",
      "importedId": "qEIoQOK_pmRCGKQGSMFlgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85213,
      "firstName": "Misha",
      "lastName": "Sra",
      "middleInitial": "",
      "importedId": "Ud_BsQ2Tnwvc9smICZF5hg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85214,
      "firstName": "Thad",
      "lastName": "Starner",
      "middleInitial": "",
      "importedId": "sWKS6wUrUI9sUgYhmC3G8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85215,
      "firstName": "Ryuta",
      "lastName": "Yamaguchi",
      "middleInitial": "",
      "importedId": "ejAIXMFaINNpnnJZ2WjgYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85216,
      "firstName": "Kazuki",
      "lastName": "Kawamura",
      "middleInitial": "",
      "importedId": "C0iAkRH148fQ9uSY61BYFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85217,
      "firstName": "Haruki",
      "lastName": "Takahashi",
      "middleInitial": "",
      "importedId": "valmwVaVJvEDw-u5ctqCxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85218,
      "firstName": "Luna",
      "lastName": "Takagi",
      "middleInitial": "",
      "importedId": "IABtHNnSpz5tAopeHd4_fA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85219,
      "firstName": "Kim",
      "lastName": "Gülle",
      "middleInitial": "Julian",
      "importedId": "uBT2fWr38VyM3V9ow29OyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85220,
      "firstName": "Sungjae",
      "lastName": "Cho",
      "middleInitial": "",
      "importedId": "uU5pMLgrlczDShllpS4YHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85221,
      "firstName": "Ryota",
      "lastName": "Gomi",
      "middleInitial": "",
      "importedId": "KjSEEIyRxrKOAZwseEg-mA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85222,
      "firstName": "Shiling",
      "lastName": "Dai",
      "middleInitial": "",
      "importedId": "-81rIUI0Yi9EC91GXTB90Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85223,
      "firstName": "Jefferson",
      "lastName": "Pardomuan",
      "middleInitial": "",
      "importedId": "AJgfpolxXYKc4-bDhMlJ5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85224,
      "firstName": "Nicole",
      "lastName": "Tan",
      "middleInitial": "",
      "importedId": "vbBzG1c-xk5LZqx3h5tBRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85225,
      "firstName": "Danli",
      "lastName": "Luo",
      "middleInitial": "",
      "importedId": "kekpTaIpk8Hb8PNqThtoSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85226,
      "firstName": "Thomas",
      "lastName": "Kosch",
      "middleInitial": "",
      "importedId": "xNJhF8Um1cSTuL-8ebzsgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85227,
      "firstName": "Florian",
      "lastName": "Mueller",
      "middleInitial": "‘Floyd’",
      "importedId": "l8cWCEh-tWQZbtxUv2Azag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85228,
      "firstName": "Kouta",
      "lastName": "Minamizawa",
      "middleInitial": "",
      "importedId": "cGigxc4f9I7rVVWPqzvZSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85229,
      "firstName": "Donghyeon",
      "lastName": "Ko",
      "middleInitial": "",
      "importedId": "KopPJAUFDHqGEX1jXFB-0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85230,
      "firstName": "Jinghui",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "FZR8WjN6928EQZGcGiVKTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85231,
      "firstName": "Mustafa Doga",
      "lastName": "Dogan",
      "middleInitial": "",
      "importedId": "iFn4fAUAYoPYRz95BeIWHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85232,
      "firstName": "Uichin",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "pqQx9PFJboSwansrrWi2PA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85233,
      "firstName": "Brent",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "EiIXSdUmtCckQu06rSqxiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85234,
      "firstName": "Tetsuaki",
      "lastName": "Baba",
      "middleInitial": "",
      "importedId": "YEoVNMfldn0l4sMhLPExJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85235,
      "firstName": "Catalina",
      "lastName": "Monsalve Rodriguez",
      "middleInitial": "",
      "importedId": "UwONwUU9nNqW5S6-OK4RgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85236,
      "firstName": "Ningjing (Anita)",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "b2Hcj-sKhws2wKKOP9iMxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85237,
      "firstName": "Eunyee",
      "lastName": "Koh",
      "middleInitial": "",
      "importedId": "PMoKh21thXimpL9JTzoZJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85238,
      "firstName": "Wen",
      "lastName": "Ying",
      "middleInitial": "",
      "importedId": "pksbxG9YNE7dgwZ2i8JCcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85239,
      "firstName": "Martin",
      "lastName": "Schmitz",
      "middleInitial": "",
      "importedId": "2TAkNV6pUi3rf_iBjrN1nQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85240,
      "firstName": "Ana",
      "lastName": "Paiva",
      "middleInitial": "",
      "importedId": "P5UYtmO7rwDcMq9R6AIN2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85241,
      "firstName": "Christina",
      "lastName": "Trice",
      "middleInitial": "",
      "importedId": "pRsXQAR1yesrjCHcJd8yTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85242,
      "firstName": "Changsung",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "t1MfsUfswP4qxfQVnL0o6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85243,
      "firstName": "Benny",
      "lastName": "Tang",
      "middleInitial": "J.",
      "importedId": "fq5B0_PImVqXfxK6UE1gJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85245,
      "firstName": "Jeremia",
      "lastName": "Lo",
      "middleInitial": "",
      "importedId": "6ZLiCe4I41GyElI0RH5iuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85246,
      "firstName": "Shio",
      "lastName": "Miyafuji",
      "middleInitial": "",
      "importedId": "PAxdxLeLLLTTxRcI8JgRbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85247,
      "firstName": "Jing",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "gC3yWr85O933tWLuGGuQDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85248,
      "firstName": "Frank",
      "lastName": "Liu",
      "middleInitial": "Wencheng",
      "importedId": "Rq5to2A97JBJ60w5lS6TpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85249,
      "firstName": "Nathalie",
      "lastName": "Overdevest",
      "middleInitial": "",
      "importedId": "jBKNSFNdPUDd0nbrZDm-QQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85250,
      "firstName": "Honghao",
      "lastName": "Deng",
      "middleInitial": "",
      "importedId": "kVo7W-iDkKXNduc49PAhIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85251,
      "firstName": "Jay",
      "lastName": "Kolvenbag",
      "middleInitial": "",
      "importedId": "4vWR-2bTtLqymj8unxHMqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85252,
      "firstName": "Hiroyuki",
      "lastName": "Manabe",
      "middleInitial": "",
      "importedId": "GPbdshGJqVFsnL-tpVbWZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85253,
      "firstName": "Beenish",
      "lastName": "Chaudhry",
      "middleInitial": "Moalla",
      "importedId": "U5RYDNGIwSMrUOLh1GIlHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85254,
      "firstName": "Priyanshu",
      "lastName": "Agarwal",
      "middleInitial": "",
      "importedId": "ezCbKg_P_ZqnCE6EkcY_Mw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85255,
      "firstName": "Aditya Shekhar",
      "lastName": "Nittala",
      "middleInitial": "",
      "importedId": "Etjddl9lyseaIki_U3rDrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85256,
      "firstName": "Rong Kang (Ron)",
      "lastName": "Chew",
      "middleInitial": "",
      "importedId": "MpuaZ52P2_5fTmW4O-YtGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85257,
      "firstName": "Hank",
      "lastName": "Duhaime",
      "middleInitial": "",
      "importedId": "fFHQEmuKqYom54vA6BhaSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85258,
      "firstName": "Doil",
      "lastName": "Kwon",
      "middleInitial": "",
      "importedId": "gWlxupcjTvQLRkRfrFW-GA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85259,
      "firstName": "Naoto",
      "lastName": "Wakatsuki",
      "middleInitial": "",
      "importedId": "Ya5cyMqwr6pxj68BhmWBmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85260,
      "firstName": "Mora",
      "lastName": "Pochettino",
      "middleInitial": "",
      "importedId": "PTUe5zcT9JWTBmsilPO4hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85261,
      "firstName": "Rachana",
      "lastName": "Sreedhar",
      "middleInitial": "",
      "importedId": "-BHs2NHbRK_17r_72OnAdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85262,
      "firstName": "Meiqi",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "b70bu5njsgRT-1nyKvhoUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85263,
      "firstName": "Foisal",
      "lastName": "Reza",
      "middleInitial": "",
      "importedId": "jnb4_UE-cP8ZQGCpCa60lA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85264,
      "firstName": "Tashfiq Nahiyan",
      "lastName": "Khan",
      "middleInitial": "",
      "importedId": "fhSJUMZuxQt_nPhYh3m-JQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85265,
      "firstName": "Miranda",
      "lastName": "Cai",
      "middleInitial": "",
      "importedId": "951yp1vxx5Nf6V9nxvlONw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85266,
      "firstName": "Keita",
      "lastName": "Kuwayama",
      "middleInitial": "",
      "importedId": "INzeklv0wU4lzc8uUZgt7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85267,
      "firstName": "Masayasu",
      "lastName": "Sumiya",
      "middleInitial": "",
      "importedId": "YEB1rFLHUhbjikMvs5Vb0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85268,
      "firstName": "Yuiko",
      "lastName": "Suyama",
      "middleInitial": "",
      "importedId": "gYoWRCPHGCUNWPe9D3c1uQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85269,
      "firstName": "Derrek",
      "lastName": "Chow",
      "middleInitial": "",
      "importedId": "dy70oKupDd4r-oYrh5rC5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85270,
      "firstName": "Ruipu",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "C2myrzqwiAPSe-LrFk8-AQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85271,
      "firstName": "Mizuki",
      "lastName": "Yabutani",
      "middleInitial": "",
      "importedId": "ovd4g26QmOr5EkXxr_HRvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85272,
      "firstName": "Chung Han",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "OHL8Szv8AYaFiZ_9KZYfJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85273,
      "firstName": "Elise",
      "lastName": "van den Hoven",
      "middleInitial": "",
      "importedId": "yEOYXDitD4SG38W22HELpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85274,
      "firstName": "Blair",
      "lastName": "Subbaraman",
      "middleInitial": "",
      "importedId": "O8mlnnO59wUNWSj-cZUJMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85275,
      "firstName": "Monica",
      "lastName": "Lam",
      "middleInitial": "",
      "importedId": "N6fvBDT1U8YZ8nYoqjlC9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85276,
      "firstName": "Ullash",
      "lastName": "Bhattacharjee",
      "middleInitial": "",
      "importedId": "56J9t7Suvh4pO7eb2tK9mA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85277,
      "firstName": "Yuhan",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "IEGXnMudE5kZK0tuI3AZnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85278,
      "firstName": "Tatsuya",
      "lastName": "Maeda",
      "middleInitial": "",
      "importedId": "PL_W1mQSUrofQptOtW2DnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85279,
      "firstName": "Xuanhui",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "NZbUYe5EQplVX6tImNZRdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85280,
      "firstName": "Yunyi",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "9lMZRMQRW9AOL2wSso2vbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85281,
      "firstName": "Isabel",
      "lastName": "Neto",
      "middleInitial": "",
      "importedId": "8OR5Vqg5QLvic17Yw62Asg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85282,
      "firstName": "Yunyi(Joyce)",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "JD5Dtl2ZwWotXwCwb3WzOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85283,
      "firstName": "Scotty",
      "lastName": "Hardwig",
      "middleInitial": "",
      "importedId": "kq7KHmPT2-_P3rzfngvVrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85284,
      "firstName": "Cathy Mengying",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "dkc8r1KzapyIhtY5n8lkOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85285,
      "firstName": "Amy",
      "lastName": "Winters",
      "middleInitial": "",
      "importedId": "bOZScyQC-KLmvoiOHKrIJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85286,
      "firstName": "Keiichi",
      "lastName": "Ochiai",
      "middleInitial": "",
      "importedId": "ft96FySMjHTPPWWjt95Y3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85287,
      "firstName": "Martin",
      "lastName": "Feick",
      "middleInitial": "",
      "importedId": "0D4Zr0Q_AnaOfaM9Sq655Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85288,
      "firstName": "Hinako",
      "lastName": "Kuroki",
      "middleInitial": "",
      "importedId": "_R69iILdONND6Ra86HyTfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85289,
      "firstName": "Tyler",
      "lastName": "Peng",
      "middleInitial": "L",
      "importedId": "Vhm8_Ckr7xIsfdyjqHEYGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85290,
      "firstName": "Johanna",
      "lastName": "Karras",
      "middleInitial": "Suvi",
      "importedId": "qmfsL0auJ8WVYLmjJP1Z7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85291,
      "firstName": "Hiroya",
      "lastName": "Tanaka",
      "middleInitial": "",
      "importedId": "ncpG7aaecb8Oze39YuurQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85292,
      "firstName": "Mondo",
      "lastName": "Saito",
      "middleInitial": "",
      "importedId": "WoGXGwO_zpig8YLMf4d-Rg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85293,
      "firstName": "Woohun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "SJfYp_T2Hm0qxpZzutq2RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85294,
      "firstName": "Hongning",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "upeRD5CVR9AS6hj5_L_-mQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85295,
      "firstName": "Rui",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "HXXYR_ufg8ovtZ5O0EzxsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85296,
      "firstName": "Eunice",
      "lastName": "Jun",
      "middleInitial": "",
      "importedId": "vqX_4F0zlxG1K9bAoTlgYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85297,
      "firstName": "Hasanath",
      "lastName": "Jamy",
      "middleInitial": "",
      "importedId": "aWtqUgkMnnXcxxAu3d-8-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85298,
      "firstName": "George",
      "lastName": "Chernyshov",
      "middleInitial": "",
      "importedId": "32U9mqNKpSmDx82rX3dAWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85299,
      "firstName": "Kento",
      "lastName": "Ohtani",
      "middleInitial": "",
      "importedId": "KFmPUJxZyMPpkgHplt4XXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85300,
      "firstName": "Sebastian",
      "lastName": "Günther",
      "middleInitial": "",
      "importedId": "pRPw2I8IGOtf3Bfov2Ek6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85301,
      "firstName": "Claire",
      "lastName": "Okabe",
      "middleInitial": "",
      "importedId": "88bJU4g_3s-IOAgtjPyxAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85302,
      "firstName": "Lei",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "RzymDQ1zY-KSFGkjCgapEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85303,
      "firstName": "Sriram",
      "lastName": "Subramanian",
      "middleInitial": "",
      "importedId": "fStD3h939qqVtVzIdtNnLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85304,
      "firstName": "Ticha",
      "lastName": "Sethapakdi",
      "middleInitial": "",
      "importedId": "pv_GfnYk0QboAq5qXRzIxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85305,
      "firstName": "Changyo",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "098kWELGyd9x1oRh_FKISA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85306,
      "firstName": "Tanjila",
      "lastName": "Joti",
      "middleInitial": "",
      "importedId": "1iaT6qaHqHv03BiW-BNJDg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85307,
      "firstName": "Keito",
      "lastName": "Uwaseki",
      "middleInitial": "",
      "importedId": "3Ep1ejVj_ABw8PJbtd3Pzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85308,
      "firstName": "James",
      "lastName": "Landay",
      "middleInitial": "A.",
      "importedId": "4SCweeMHjVUnsarWRtZXOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85309,
      "firstName": "Andreas",
      "lastName": "Pointner",
      "middleInitial": "",
      "importedId": "bq6QVGinYcojAYx39DIbdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85310,
      "firstName": "Mai",
      "lastName": "Ohira",
      "middleInitial": "",
      "importedId": "ry7DdO-5IizHfvQfFvTcqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85311,
      "firstName": "Andrés",
      "lastName": "Monroy-Hernández",
      "middleInitial": "",
      "importedId": "GX5pL3XryTRF0QDCi-Luaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85312,
      "firstName": "Axel",
      "lastName": "Kilian",
      "middleInitial": "",
      "importedId": "uo-otA5CdpNBroa_Unokmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85313,
      "firstName": "Ryan",
      "lastName": "Wirjadi",
      "middleInitial": "",
      "importedId": "VTrXY2I3o6RiBiq4AUewpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85314,
      "firstName": "Shitao",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "tmEIxM1RxUIbUQ3gHNM4AQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85315,
      "firstName": "Takeshi",
      "lastName": "Naemura",
      "middleInitial": "",
      "importedId": "qc0uRc9kefvrfEWLoX_CJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85316,
      "firstName": "Ryuji",
      "lastName": "Hirayama",
      "middleInitial": "",
      "importedId": "hrOer8zf9GyqI4hB60t1dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85317,
      "firstName": "Wataru",
      "lastName": "Yamada",
      "middleInitial": "",
      "importedId": "Df_804SKMJEnpoxlEzl-aQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85318,
      "firstName": "Adil",
      "lastName": "Rahman",
      "middleInitial": "",
      "importedId": "x4QSWXV3o5-SG4BTv_xfQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85319,
      "firstName": "Sven",
      "lastName": "Kratz",
      "middleInitial": "",
      "importedId": "xi_zJU67NKfVUiNI9HqObw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85320,
      "firstName": "Daniel",
      "lastName": "Vogel",
      "middleInitial": "",
      "importedId": "VH8XpxsJEUlDmsjNcBbGOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85321,
      "firstName": "Alayna",
      "lastName": "Ricard",
      "middleInitial": "",
      "importedId": "VdjmMOHwQ7JDhvN2U9BV7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85322,
      "firstName": "Julie",
      "lastName": "Dorsey",
      "middleInitial": "",
      "importedId": "pyPdxItd_kl5TpgIc_T6rA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85323,
      "firstName": "Marcia",
      "lastName": "O'Malley",
      "middleInitial": "K",
      "importedId": "aUiB-r9OJVAjSZhW_Nchow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85324,
      "firstName": "Tomoki",
      "lastName": "Yoshikawa",
      "middleInitial": "",
      "importedId": "hsfqsYf4GOVDvilXqjFg_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85325,
      "firstName": "Yueming",
      "lastName": "Bao",
      "middleInitial": "",
      "importedId": "TbokcICU7DUaiv31nj6XnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85326,
      "firstName": "Zhuohao",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "uKGU9zWN5qVUbfiFk7f1kA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85327,
      "firstName": "Joanne",
      "lastName": "Leong",
      "middleInitial": "",
      "importedId": "obXQ5hwc_vCy3naa1Cx5mQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85328,
      "firstName": "Kodai",
      "lastName": "Ito",
      "middleInitial": "",
      "importedId": "zSvK8BwG6kiGBhtV0WyIjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85329,
      "firstName": "Jaeho",
      "lastName": "Sung",
      "middleInitial": "",
      "importedId": "x88sBq-CC_O8Lj6JtQQlpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85330,
      "firstName": "Jasper",
      "lastName": "Tran O'Leary",
      "middleInitial": "",
      "importedId": "lS66sIrbFV1L7mJzsqUeDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85331,
      "firstName": "Panote",
      "lastName": "Siriaraya",
      "middleInitial": "",
      "importedId": "oNfOfhYj7Imf3Be0uU3CTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85332,
      "firstName": "Ting-Chuen",
      "lastName": "Pong",
      "middleInitial": "",
      "importedId": "QLWx58EFKy0CnsjtuyafKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85333,
      "firstName": "Jacob",
      "lastName": "Wobbrock",
      "middleInitial": "O.",
      "importedId": "u7VUTOERCrBjVBdNyeGk5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85334,
      "firstName": "Yaqing",
      "lastName": "Chai",
      "middleInitial": "",
      "importedId": "SR2jd-xLt3vJl_8lori6QQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85335,
      "firstName": "Wataru",
      "lastName": "Yamada",
      "middleInitial": "",
      "importedId": "tjDXE7R_FfUs2xbzVmpkbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85336,
      "firstName": "Banafsheh",
      "lastName": "Mohajeri",
      "middleInitial": "",
      "importedId": "dzu3FzvBtF9iE8KJ03zlnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85337,
      "firstName": "Shogo",
      "lastName": "Fukushima",
      "middleInitial": "",
      "importedId": "AvjAlI4LRQfCH0g49_5foA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85338,
      "firstName": "Michael",
      "lastName": "Cross",
      "middleInitial": "",
      "importedId": "8Sd6hjp9XiymmzlALuF6tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85339,
      "firstName": "Taegyu",
      "lastName": "Jin",
      "middleInitial": "",
      "importedId": "77QhoxYk83C95LGab8EYaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85340,
      "firstName": "Seokwoo",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "jXZPOVyP5nI3QbajQ5fSnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85341,
      "firstName": "Ali",
      "lastName": "Ahnaf",
      "middleInitial": "",
      "importedId": "vFg0KsdkAgYob5Uz3iLiMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85342,
      "firstName": "Ahmad",
      "lastName": "Taka",
      "middleInitial": "",
      "importedId": "Tvs1TKGuNUWHu8wtsKt3Xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85343,
      "firstName": "Tingyu",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "OW6mkxUkQ8ja21RxkLaW3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85344,
      "firstName": "Patrick",
      "lastName": "Ebel",
      "middleInitial": "",
      "importedId": "NteKveYuvk7U4_udF0ZdjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85345,
      "firstName": "Hong-Sheng",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "-ASoKBEN3rflXS-VeunuLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85346,
      "firstName": "Angelica",
      "lastName": "Bonilla Fominaya",
      "middleInitial": "M.",
      "importedId": "TWPgzHuj_SV26jgLADiwjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85347,
      "firstName": "Ali",
      "lastName": "Shtarbanov",
      "middleInitial": "",
      "importedId": "IiJn9F2aLgQwcehlassTgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85348,
      "firstName": "Joon Hyub",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "0oozWOxlyLLPrL4QxAFHxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85349,
      "firstName": "Seok-Hyung",
      "lastName": "Bae",
      "middleInitial": "",
      "importedId": "mHgyrQnjuhRIke1ZUM_TkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85350,
      "firstName": "Chang",
      "lastName": "Xiao",
      "middleInitial": "",
      "importedId": "zj-3Auk1zTBUkAHqgdGAYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85351,
      "firstName": "Veerapatr",
      "lastName": "Yotamornsunthorn",
      "middleInitial": "",
      "importedId": "afdoWyI69MYdZD5tRVGewg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85352,
      "firstName": "Rukshani",
      "lastName": "Somarathna",
      "middleInitial": "",
      "importedId": "FbRIsHkvis8cpWOZfAqMbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85353,
      "firstName": "Yuichi",
      "lastName": "Itoh",
      "middleInitial": "",
      "importedId": "cPhtts1PeXTRRwjSRvfhyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85354,
      "firstName": "Sara",
      "lastName": "Mlakar",
      "middleInitial": "",
      "importedId": "5yRXC3-t5raU2arNyf4JYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85355,
      "firstName": "Luis",
      "lastName": "Leiva",
      "middleInitial": "A.",
      "importedId": "zEw1asvR5_8wPjGUPy5Fbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85356,
      "firstName": "Sang-Hyun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "pkmAxwavD-M_2Z9lxx3BRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85357,
      "firstName": "Yukiko",
      "lastName": "Kawai",
      "middleInitial": "",
      "importedId": "LKF-7RTA_fA48ZpaasrQdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85358,
      "firstName": "Arafat Ibne",
      "lastName": "Yousuf",
      "middleInitial": "",
      "importedId": "s3XkazPyRoZ-Jd2U4ZbWvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85359,
      "firstName": "Pattie",
      "lastName": "Maes",
      "middleInitial": "",
      "importedId": "Apx00vbso3FewB_L4nWE_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85360,
      "firstName": "Alec",
      "lastName": "Jacobson",
      "middleInitial": "",
      "importedId": "6a1M4p12kgmmFD-KGk2RQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85361,
      "firstName": "Gracie",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "lBszMR5m4JhjN5uARZq-Ww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85362,
      "firstName": "Takuto",
      "lastName": "Nakamura",
      "middleInitial": "",
      "importedId": "bNfAZvqyGjS4a4WZsKSsrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85363,
      "firstName": "Guy",
      "lastName": "Hoffman",
      "middleInitial": "",
      "importedId": "TB0t8BqJB_-ZFgcF6dzbug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85364,
      "firstName": "Rakesh",
      "lastName": "Patibanda",
      "middleInitial": "",
      "importedId": "xIfPgx5wQMXkmffqHxrMwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85365,
      "firstName": "Youngji",
      "lastName": "Koh",
      "middleInitial": "",
      "importedId": "OOLh18qBppR35rdjigffpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85366,
      "firstName": "Mohsena",
      "lastName": "Ashraf",
      "middleInitial": "",
      "importedId": "3f2g9olUcvwAprldwpgVmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85367,
      "firstName": "John",
      "lastName": "Chung",
      "middleInitial": "Joon Young",
      "importedId": "PGlpEVdS4_1vxTCbCETfCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85368,
      "firstName": "Jina",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "b9UP0rYetnT_peTTMo3R1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85369,
      "firstName": "Nadya",
      "lastName": "Peek",
      "middleInitial": "",
      "importedId": "bCuJMThz4u2trsJEGdqTyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85370,
      "firstName": "Xiemin",
      "lastName": "Wei",
      "middleInitial": "",
      "importedId": "N5c7-MhG5lF44EadA_BJ7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85371,
      "firstName": "Max",
      "lastName": "Mühlhäuser",
      "middleInitial": "",
      "importedId": "IxtRKZB2rjeAvVGqICIm1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85372,
      "firstName": "Hannah",
      "lastName": "Twigg-Smith",
      "middleInitial": "Rose",
      "importedId": "Xa--VOEHd48alXj_kMUuig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85373,
      "firstName": "Haotian",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "HV4DNjoqJcNQez9rOhuQdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85374,
      "firstName": "Don Samitha",
      "lastName": "Elvitigala",
      "middleInitial": "",
      "importedId": "Hs_eu2OjQBPkrDYRgbKczA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85375,
      "firstName": "Zhenglin",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "7t1dikXYL-8DJeQ4nrvDaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85376,
      "firstName": "Yen-Ting",
      "lastName": "Yeh",
      "middleInitial": "",
      "importedId": "JESJtChsg4P398TQs_Hq2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85377,
      "firstName": "Paul",
      "lastName": "Asente",
      "middleInitial": "",
      "importedId": "zQIFvzNTYsvzNijpo8Wyow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85378,
      "firstName": "Marcus",
      "lastName": "Friedel",
      "middleInitial": "",
      "importedId": "aCICO_Y7cuHE8qkaFtbBTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85379,
      "firstName": "Alice",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "n3bys9SS4u7RnyZeVUE14Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85380,
      "firstName": "Md Aashikur Rahman",
      "lastName": "Azim",
      "middleInitial": "",
      "importedId": "ya5t-YOyLMtwf8RHdW5BYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85381,
      "firstName": "Olivia",
      "lastName": "Seow",
      "middleInitial": "",
      "importedId": "hVzoC04I0qi9fO0MYKQ9IQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85382,
      "firstName": "James",
      "lastName": "Hardwick",
      "middleInitial": "",
      "importedId": "Q3k7Wnh281zFR7Ttv3sAjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85383,
      "firstName": "Kayhan",
      "lastName": "Latifzadeh",
      "middleInitial": "",
      "importedId": "NpbhS7TOmU_OL3EZNcs-gQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85384,
      "firstName": "Yijun",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "AvMcVaPlbbAaeZyAv8ZiqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85385,
      "firstName": "Dishita",
      "lastName": "Turakhia",
      "middleInitial": "G",
      "importedId": "JGrZLelgzqVp_TcWytYllQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85386,
      "firstName": "Felix",
      "lastName": "Dollack",
      "middleInitial": "",
      "importedId": "UDz1EvPZoGUm4PueCz55og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85387,
      "firstName": "Yuichi",
      "lastName": "Mashiba",
      "middleInitial": "",
      "importedId": "zEu2mVBDTR43_y7q_izY5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85388,
      "firstName": "Kai Ji Kevin",
      "lastName": "Feng",
      "middleInitial": "",
      "importedId": "JbK7flHgCMQKn-gBEURicg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85389,
      "firstName": "Alex",
      "lastName": "Vuong",
      "middleInitial": "",
      "importedId": "h6OfzAqqTbhe4Mjj6gFc6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85390,
      "firstName": "Naoki",
      "lastName": "Kimura",
      "middleInitial": "",
      "importedId": "HCsLfJz__PwZxazHRITsdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85391,
      "firstName": "Zachary",
      "lastName": "Duer",
      "middleInitial": "",
      "importedId": "LqbM6FEoLBtpdz422xH-dA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85393,
      "firstName": "Ali",
      "lastName": "Israr",
      "middleInitial": "",
      "importedId": "TZRW2AdjdD9IITMoV9PyyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85394,
      "firstName": "Peiling",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "WAuTiM5ZgtuM7Q8rthGcZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85395,
      "firstName": "Benjamin",
      "lastName": "Smolin",
      "middleInitial": "E.",
      "importedId": "dAOmSxW9jaFD9iqSMEgh8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85396,
      "firstName": "Shahabedin",
      "lastName": "Sagheb",
      "middleInitial": "",
      "importedId": "o4GHYoiEkMyMcXkx2wyuLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85397,
      "firstName": "Tomohito",
      "lastName": "Suzuki",
      "middleInitial": "",
      "importedId": "4aL2U2cWNfPVIXpIKPdgKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85398,
      "firstName": "Thomas",
      "lastName": "Preindl",
      "middleInitial": "",
      "importedId": "bEcgPL_r3U--MNJvGi6zsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85399,
      "firstName": "Soya",
      "lastName": "Eguchi",
      "middleInitial": "",
      "importedId": "yU_S5FaDRuedwgSWQB_Sdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85400,
      "firstName": "Sarah",
      "lastName": "Kushner",
      "middleInitial": "Anne",
      "importedId": "KlUn3Z6Q9-uCInZEP20Ycg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85527,
      "firstName": "Chenfeng",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "8NioDhxMAahWU_PG1s5fMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85528,
      "firstName": "Mai",
      "lastName": "Kamihori",
      "middleInitial": "",
      "importedId": "1LBqbWeEAqMp4AXLlrNUog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85529,
      "firstName": "Kaito",
      "lastName": "Yamao",
      "middleInitial": "",
      "importedId": "xE77yYx2ga2fTihsmwCaZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85531,
      "firstName": "Jiwan",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "moaLtaqn5MffvuzNuFd0Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85532,
      "firstName": "Hiroki",
      "lastName": "Kawahara",
      "middleInitial": "",
      "importedId": "3tdnzDHbBsJEcV2OM1IQ_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85533,
      "firstName": "Marwa",
      "lastName": "AlAlawi",
      "middleInitial": "",
      "importedId": "Et85KPRF7bO0zVqYuARZrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85534,
      "firstName": "Hyunjae",
      "lastName": "Gil",
      "middleInitial": "",
      "importedId": "25eJB6OqCVJsRZzmQJrwAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85535,
      "firstName": "Mike",
      "lastName": "Chen",
      "middleInitial": "Y.",
      "importedId": "lHUnsX0Tf6ZdyKH8WPjT9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85537,
      "firstName": "Shutaro",
      "lastName": "Aoyama",
      "middleInitial": "",
      "importedId": "JPgVd05XOPCokJ14PhJCug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85538,
      "firstName": "Hong-En",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "4lEcJ_fhHhixCeBJPQpAZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85539,
      "firstName": "Wei-Hsin",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "m7_zf8XKu9Obxx1-eyey0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85540,
      "firstName": "Antonius",
      "lastName": "Naumann",
      "middleInitial": "",
      "importedId": "4QLzIHufEpiWM5lSGhyrlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85541,
      "firstName": "Paul",
      "lastName": "Methfessel",
      "middleInitial": "",
      "importedId": "HBI_0gq7MnLtCXqO2iacoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85543,
      "firstName": "Kei",
      "lastName": "Asano",
      "middleInitial": "",
      "importedId": "0ewjYN4J1Z8o6Lrvctq3bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85544,
      "firstName": "Ching-Yi",
      "lastName": "Tsai",
      "middleInitial": "",
      "importedId": "MOBgRF-g5hWFk7L_Iyn8Lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85545,
      "firstName": "Chen-Kuo",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "BEYrRUOmMYXpqzi0-GfyYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85546,
      "firstName": "Kentaro",
      "lastName": "Oda",
      "middleInitial": "",
      "importedId": "yNOhF_oe37FIcC8_yl-wDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85547,
      "firstName": "Brandon",
      "lastName": "Wong",
      "middleInitial": "M",
      "importedId": "V5wyO3V6U7q69kKGIr6PyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85627,
      "firstName": "Ehud",
      "lastName": "Sharlin",
      "middleInitial": "",
      "importedId": "qWDVW9DZNkt9PRyucpJ8Fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85628,
      "firstName": "Paul H.",
      "lastName": "Dietz",
      "middleInitial": "",
      "importedId": "UhmEUGHnU44VY1GAV_B5CA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85629,
      "firstName": "Parth",
      "lastName": "Patel",
      "middleInitial": "",
      "importedId": "7_2-VcpdjPHbvgqPA2Msqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 85640,
      "firstName": "Roger",
      "lastName": "Boldu",
      "importedId": "person-2",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 85643,
      "firstName": "Denys J. C.",
      "lastName": "Matthies",
      "importedId": "person-4",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 85645,
      "firstName": "Suranga Chandima",
      "lastName": "Nanayakkara",
      "importedId": "person-11",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 85646,
      "firstName": "Sachith",
      "lastName": "Muthukumarana",
      "importedId": "person-6",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 85647,
      "firstName": "Nur Al-Huda",
      "lastName": "Hamdan",
      "importedId": "person-7",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 85648,
      "firstName": "Moritz Alexander",
      "lastName": "Messerschmidt",
      "importedId": "person-5",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 85649,
      "firstName": "Adrian",
      "lastName": "Wagner",
      "importedId": "person-8",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 85650,
      "firstName": "Haimo",
      "lastName": "Zhang",
      "importedId": "person-9",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 85651,
      "firstName": "Jan",
      "lastName": "Borchers",
      "importedId": "person-10",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86349,
      "firstName": "Eric",
      "lastName": "Gonzalez",
      "middleInitial": "J",
      "importedId": "5EmhrfnJ",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86350,
      "firstName": "Ryan",
      "lastName": "Nuqui",
      "importedId": "S1ptgORL",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86351,
      "firstName": "Chia-En",
      "lastName": "Tsai",
      "importedId": "ntyUG5FU",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86352,
      "firstName": "Soichiro",
      "lastName": "Matsuda",
      "importedId": "4QaAZTvw",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86354,
      "firstName": "Auejin",
      "lastName": "Ham",
      "importedId": "UrLCZDUh",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86355,
      "firstName": "Phoebe",
      "lastName": "Welch",
      "middleInitial": "J.",
      "importedId": "pSzQb5cs",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86356,
      "firstName": "Rohit",
      "lastName": "Ramesh",
      "importedId": "oiAMzlfd",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86357,
      "firstName": "Chengzhi",
      "lastName": "Shi",
      "importedId": "QWhl3lHM",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86358,
      "firstName": "Ziwei",
      "lastName": "Liu",
      "importedId": "4EtOUT8b",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86359,
      "firstName": "Erwin",
      "lastName": "Wu",
      "importedId": "oMxkTqjT",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86360,
      "firstName": "Hui-Shyong",
      "lastName": "Yeo",
      "importedId": "cUMoCd5i",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86361,
      "firstName": "Ramkrishna",
      "lastName": "Prasad",
      "importedId": "Mxt79qpy",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86362,
      "firstName": "Jiahe",
      "lastName": "Lyu",
      "importedId": "0tPGByvR",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86363,
      "firstName": "Xiyun",
      "lastName": "H",
      "importedId": "IKPa4V4d",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86364,
      "firstName": "Arjun",
      "lastName": "Srinivasan",
      "importedId": "eHicO0gm",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86367,
      "firstName": "Mar",
      "lastName": "Gonzalez-Franco",
      "importedId": "1IM97EQF",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86368,
      "firstName": "Steven",
      "lastName": "Craig",
      "middleInitial": "R.",
      "importedId": "smkYYhtY",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86369,
      "firstName": "Ye",
      "lastName": "Yuan",
      "importedId": "pV6DlHMx",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86370,
      "firstName": "Gregory",
      "lastName": "Abowd",
      "middleInitial": "D.",
      "importedId": "rbYK7sl6",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86371,
      "firstName": "Michal",
      "lastName": "PiovarÄi",
      "importedId": "OqHH4YoM",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86373,
      "firstName": "Emmanouil",
      "lastName": "Potetsianakis",
      "importedId": "pV73ctPz",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86374,
      "firstName": "Youngwook",
      "lastName": "Do",
      "importedId": "4pikNIbf",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86375,
      "firstName": "Yuan-Syun",
      "lastName": "Ye",
      "importedId": "U9OzZuX3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86376,
      "firstName": "Pengyu",
      "lastName": "Li",
      "importedId": "x9uHzfTG",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86377,
      "firstName": "Sauvik",
      "lastName": "Das",
      "importedId": "2eNUzRpC",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86378,
      "firstName": "Jas",
      "lastName": "Brooks",
      "importedId": "26OsiYvF",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86379,
      "firstName": "Connie",
      "lastName": "Chi",
      "importedId": "q716RYj8",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86380,
      "firstName": "Yoonji",
      "lastName": "Kim",
      "importedId": "kAlrnxkf",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86381,
      "firstName": "Youngkyung",
      "lastName": "Choi",
      "importedId": "9cvBHWP3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86382,
      "firstName": "Seraphina",
      "lastName": "Yong",
      "importedId": "MdKYNRRS",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86383,
      "firstName": "Vidya",
      "lastName": "Setlur",
      "importedId": "Ki5bae8u",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86384,
      "firstName": "Bernd",
      "lastName": "Bickel",
      "importedId": "bNFMinqo",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86385,
      "firstName": "Ravin",
      "lastName": "Balakrishnan",
      "importedId": "JdjoGURn",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86386,
      "firstName": "Meilin",
      "lastName": "Cui",
      "importedId": "05eyyiQx",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86387,
      "firstName": "Xiaoyi",
      "lastName": "Zhang",
      "importedId": "HDqEFaGI",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86388,
      "firstName": "Ian",
      "lastName": "Oakley",
      "importedId": "SU5beA2l",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86389,
      "firstName": "Prabal",
      "lastName": "Dutta",
      "importedId": "zLOh8fm3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86390,
      "firstName": "Nikhil",
      "lastName": "Jain",
      "importedId": "9GCuTuYV",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86391,
      "firstName": "Ke",
      "lastName": "Huo",
      "importedId": "u4LiZvXD",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86392,
      "firstName": "Balasaravanan",
      "lastName": "Kumaravel",
      "middleInitial": "Thoravi",
      "importedId": "gePm2hgs",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86393,
      "firstName": "Mauricio",
      "lastName": "Sousa",
      "importedId": "Zd1vAsI5",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86394,
      "firstName": "BjÃ¶rn",
      "lastName": "Hartmann",
      "importedId": "SjXl3NNX",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86395,
      "firstName": "David",
      "lastName": "Kutas",
      "middleInitial": "T",
      "importedId": "3H8VMlQ6",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86396,
      "firstName": "Siddhant",
      "lastName": "Singh",
      "importedId": "m20U33VI",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86397,
      "firstName": "Donald",
      "lastName": "Degraen",
      "importedId": "ZhmYd1ZM",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86398,
      "firstName": "Mike",
      "lastName": "Sinclair",
      "importedId": "Lp2mY7at",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86399,
      "firstName": "Jeff",
      "lastName": "Nichols",
      "importedId": "jUQ9kQ99",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86400,
      "firstName": "Junsu",
      "lastName": "Lim",
      "importedId": "qgeb6xGn",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86401,
      "firstName": "Hyein",
      "lastName": "Lee",
      "importedId": "AoFg4hi9",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86402,
      "firstName": "Jiahao",
      "lastName": "Li",
      "importedId": "aQRGALh3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86403,
      "firstName": "Jeremy",
      "lastName": "Warner",
      "importedId": "OEf2DXR8",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86404,
      "firstName": "Jiannan",
      "lastName": "Li",
      "importedId": "qAwWcU2t",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86405,
      "firstName": "Hiroshi",
      "lastName": "Matsui",
      "importedId": "sFyH2vpV",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86406,
      "firstName": "Sunjun",
      "lastName": "Kim",
      "importedId": "hFuxBKTo",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86407,
      "firstName": "Kris",
      "lastName": "Kitan",
      "middleInitial": "M.",
      "importedId": "FemhFd8I",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86408,
      "firstName": "Seref",
      "lastName": "GÃ¼ngÃ¶r",
      "importedId": "eppfu46d",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86409,
      "firstName": "Kenji",
      "lastName": "Suzuki",
      "importedId": "CDZpS4eb",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86410,
      "firstName": "Richard",
      "lastName": "Lin",
      "importedId": "6POEp65N",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86411,
      "firstName": "Stephen",
      "lastName": "DiVerdi",
      "importedId": "ciIb22U7",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86412,
      "firstName": "Kristin",
      "lastName": "Williams",
      "importedId": "iS1ObHgp",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86413,
      "firstName": "Zhouyu",
      "lastName": "Li",
      "importedId": "8kK1hxGL",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86414,
      "firstName": "Hsin-Yu",
      "lastName": "Chen",
      "importedId": "FpWI63CT",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86415,
      "firstName": "Bruno",
      "lastName": "Fruchard",
      "importedId": "tXoRQt8D",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86416,
      "firstName": "Seungwoo",
      "lastName": "Je",
      "importedId": "S9m0jfmT",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86417,
      "firstName": "Frederik",
      "lastName": "Smolders",
      "importedId": "qxIMD2wb",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 86418,
      "firstName": "Yuanzhi",
      "lastName": "Cao",
      "importedId": "W59hk04e",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89824,
      "firstName": "Karan",
      "lastName": "Ahuja",
      "importedId": "fe6c8a56-689e-4351-accb-f83c330146a2",
      "source": "SYS",
      "affiliations": []
    },
    {
      "id": 89825,
      "firstName": "Jane",
      "lastName": "E",
      "middleInitial": "L.",
      "importedId": "45234afb-bbaf-417e-94ff-28f5841654c0",
      "source": "SYS",
      "affiliations": []
    },
    {
      "id": 89826,
      "firstName": "Eric",
      "lastName": "Whitmire",
      "importedId": "491a8b87-adc3-4a38-a5bf-4a3c75dff396",
      "source": "SYS",
      "affiliations": []
    },
    {
      "id": 89827,
      "firstName": "Scott",
      "lastName": "Carter",
      "importedId": "22d24efe-be95-4b81-b476-15a1c1ee8207",
      "source": "SYS",
      "affiliations": []
    },
    {
      "id": 89828,
      "firstName": "Ruofei",
      "lastName": "Du",
      "importedId": "04ee6a6e-25e4-4bd3-9e99-27871c2c5b30",
      "source": "SYS",
      "affiliations": []
    },
    {
      "id": 89829,
      "firstName": "Frederik",
      "lastName": "Brudy",
      "importedId": "f2556279-c5ba-4440-830b-1313a2d05d4e",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Canada",
          "state": "Ontario",
          "city": "Toronto",
          "institution": "Autodesk Research"
        }
      ]
    },
    {
      "id": 89830,
      "firstName": "Amanda",
      "lastName": "Swearngin",
      "importedId": "980840fd-9972-43d0-a092-367488c2446b",
      "source": "SYS",
      "affiliations": []
    },
    {
      "id": 89922,
      "firstName": "Otmar",
      "lastName": "Hilliges",
      "importedId": "omni5",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89923,
      "firstName": "Christian",
      "lastName": "Holz",
      "importedId": "omni4",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89924,
      "firstName": "David",
      "lastName": "Lindlbauer",
      "importedId": "omni3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89925,
      "firstName": "Juan",
      "lastName": "Zarate",
      "middleInitial": "Jose",
      "importedId": "omni2",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89926,
      "firstName": "Thomas",
      "lastName": "Langerak",
      "importedId": "omni1",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89927,
      "firstName": "Yang",
      "lastName": "Li",
      "importedId": "s2w5",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89928,
      "firstName": "Zhourong",
      "lastName": "Chen",
      "importedId": "s2w4",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89929,
      "firstName": "Bryan",
      "lastName": "Wang",
      "importedId": "s2w1",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89931,
      "firstName": "Xin",
      "lastName": "Zhou",
      "importedId": "s2w3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 89932,
      "firstName": "Gang",
      "lastName": "Li",
      "importedId": "s2w2",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 90642,
      "firstName": "Lining",
      "lastName": "Yao",
      "importedId": "ama1",
      "source": "CSV",
      "affiliations": []
    }
  ],
  "recognitions": []
}