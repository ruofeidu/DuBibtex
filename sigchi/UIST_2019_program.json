{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10030,
    "startDate": 1571529600000,
    "endDate": 1571788800000,
    "shortName": "UIST",
    "name": "UIST 2019",
    "year": 2019,
    "fullName": "32nd Annual ACM Symposium on User Interface Software and Technology",
    "url": "http://uist.acm.org/uist2019/",
    "location": "New Orleans, Louisiana, USA",
    "timeZoneOffset": -300,
    "logoUrl": "https://files.sigchi.org/conference/logo/61e8a996-abfd-4569-d982-19677ff04305.png",
    "hideVideoLinksBeforeConference": false,
    "timeZoneName": "America/Chicago"
  },
  "sponsors": [
    {
      "id": 10043,
      "name": "Chatham Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/84667f52-66ea-4812-a286-35de19704374.png",
      "levelId": 10043,
      "order": 1
    },
    {
      "id": 10044,
      "name": "Autodesk",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/dd775cbc-8e01-ded2-372b-6d5af1b61ace.png",
      "levelId": 10045,
      "order": 3
    },
    {
      "id": 10045,
      "name": "Facebook Reality Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/818f3e39-0b00-3191-0637-de3676c14251.png",
      "levelId": 10045,
      "order": 4
    },
    {
      "id": 10046,
      "name": "Adobe",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/5c1a8c22-2b59-1147-3d29-14720a708aa7.png",
      "levelId": 10045,
      "order": 2
    },
    {
      "id": 10047,
      "name": "Microsoft",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/7385455d-4848-3f1d-421f-71b0e71739c3.png",
      "levelId": 10046,
      "order": 7
    },
    {
      "id": 10048,
      "name": "Google",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/f7784797-5fa2-f2f1-169c-1184cea3583e.png",
      "levelId": 10046,
      "order": 6
    },
    {
      "id": 10049,
      "name": "Synaptics",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/aac26a01-a2aa-b954-4ba5-852647adc6eb.png",
      "levelId": 10046,
      "order": 9
    },
    {
      "id": 10050,
      "name": "Tactual",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/571ae8e3-25db-115e-a1ce-baaabe4eaab5.png",
      "levelId": 10046,
      "order": 10
    },
    {
      "id": 10084,
      "name": "Apple",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/6ebe7485-5570-a5ca-5fab-8fd5ffdb4a94.png",
      "levelId": 10046,
      "order": 5
    },
    {
      "id": 10051,
      "name": "Mozilla Dot Design",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/54db6442-924e-683a-c57e-6765398cebc9.png",
      "levelId": 10046,
      "order": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10044,
      "name": "Gold Sponsors",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10045,
      "name": "Silver Sponsors",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10046,
      "name": "Bronze Sponsors",
      "rank": 4,
      "isDefault": false
    },
    {
      "id": 10025,
      "name": "Sponsors",
      "rank": 5,
      "isDefault": true
    },
    {
      "id": 10043,
      "name": "Platinum Sponsors",
      "rank": 1,
      "isDefault": false
    }
  ],
  "floors": [
    {
      "id": 10082,
      "name": "Level Two",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/64d0be8c-a3e2-4440-4db1-9da62854ba6e.png",
      "roomIds": [
        10310
      ]
    },
    {
      "id": 10081,
      "name": "Lobby",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/6104da7e-6a7f-5612-6ba3-52563c4581ea.png",
      "roomIds": [
        10308,
        10309,
        10314,
        10315,
        10313,
        10316,
        10312
      ]
    },
    {
      "id": 10080,
      "name": "Lower Level",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/d428d3ce-9acb-ffa9-1989-25b71073e2a4.png",
      "roomIds": [
        10317
      ]
    },
    {
      "id": 10085,
      "name": "Demo Map",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/7a63ccb7-ac07-6cd4-0e9c-e6a7fca59f4f.png"
    },
    {
      "id": 10086,
      "name": "Poster Map",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/e5a7a265-f76f-22ce-6bb3-462f3b72b65b.png"
    },
    {
      "id": 10087,
      "name": "Aquarium Map",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/157c6e23-96a7-5f09-2f73-3949601e88ef.png"
    },
    {
      "id": 10083,
      "name": "Walking/Biking from hotel to Aquarium",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/f244c962-94be-6954-ad92-3f399113b964.png",
      "roomIds": [
        10311
      ]
    }
  ],
  "rooms": [
    {
      "id": 10308,
      "name": "North Ballroom",
      "typeId": 11339,
      "setup": "Theatre"
    },
    {
      "id": 10309,
      "name": "South Ballroom\n",
      "typeId": 11339,
      "setup": "Theatre"
    },
    {
      "id": 10317,
      "name": "Bourbon",
      "typeId": 11341,
      "setup": "Theatre",
      "capacity": "20"
    },
    {
      "id": 10310,
      "name": "Teche/Belle Grove and Terrace",
      "typeId": 11342,
      "setup": "Theatre",
      "note": "Committee\n"
    },
    {
      "id": 10314,
      "name": "Foyer",
      "typeId": 11342,
      "setup": "Theatre",
      "capacity": "300",
      "note": "Refreshment (Coffee & Snacks)"
    },
    {
      "id": 10315,
      "name": "Foyer & Evangeline",
      "typeId": 11342,
      "setup": "Theatre",
      "capacity": "380",
      "note": "Welcome reception & poster sessions"
    },
    {
      "id": 10313,
      "name": "Grand Ballroom",
      "typeId": 11336,
      "setup": "Theatre",
      "capacity": "550",
      "note": "Keynotes, UIST visions, closing ceremony"
    },
    {
      "id": 10311,
      "name": "The Aquarium",
      "typeId": 11342,
      "setup": "Theatre",
      "note": "Conference reception"
    },
    {
      "id": 10316,
      "name": "Archade",
      "typeId": 11339,
      "setup": "Theatre",
      "note": "Registration"
    },
    {
      "id": 10312,
      "name": "Foyer, Evangeline, Fleur de Lis, Courtyard, Royal Conti",
      "typeId": 11342,
      "setup": "Theatre",
      "capacity": "550",
      "note": "Demo reception"
    }
  ],
  "tracks": [
    {
      "id": 10574,
      "name": "UIST 2019 Papers",
      "typeId": 11339
    }
  ],
  "contentTypes": [
    {
      "id": 11341,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 11342,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 11333,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 11334,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 11335,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 11336,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Invited Talks"
    },
    {
      "id": 11337,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 11338,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90,
      "displayName": "Panels"
    },
    {
      "id": 11340,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 90
    },
    {
      "id": 11339,
      "name": "Paper",
      "color": "#08519c",
      "duration": 18,
      "displayName": "Papers"
    }
  ],
  "timeSlots": [
    {
      "id": 11129,
      "type": "SESSION",
      "startDate": 1571734800000,
      "endDate": 1571740200000
    },
    {
      "id": 11130,
      "type": "BREAK",
      "startDate": 1571740200000,
      "endDate": 1571742000000
    },
    {
      "id": 11131,
      "type": "SESSION",
      "startDate": 1571742000000,
      "endDate": 1571747400000
    },
    {
      "id": 11132,
      "type": "LUNCH",
      "startDate": 1571747400000,
      "endDate": 1571752800000
    },
    {
      "id": 11133,
      "type": "SESSION",
      "startDate": 1571752800000,
      "endDate": 1571760000000
    },
    {
      "id": 11134,
      "type": "SESSION",
      "startDate": 1571760000000,
      "endDate": 1571763600000
    },
    {
      "id": 11135,
      "type": "SESSION",
      "startDate": 1571763600000,
      "endDate": 1571767200000
    },
    {
      "id": 11136,
      "type": "BREAK",
      "startDate": 1571770800000,
      "endDate": 1571779800000
    },
    {
      "id": 11141,
      "type": "SESSION",
      "startDate": 1571655600000,
      "endDate": 1571661000000
    },
    {
      "id": 11142,
      "type": "LUNCH",
      "startDate": 1571661000000,
      "endDate": 1571666400000
    },
    {
      "id": 11143,
      "type": "SESSION",
      "startDate": 1571666400000,
      "endDate": 1571673600000
    },
    {
      "id": 11144,
      "type": "BREAK",
      "startDate": 1571673600000,
      "endDate": 1571675400000
    },
    {
      "id": 11145,
      "type": "SESSION",
      "startDate": 1571675400000,
      "endDate": 1571680800000
    },
    {
      "id": 11146,
      "type": "SESSION",
      "startDate": 1571680800000,
      "endDate": 1571691600000
    },
    {
      "id": 11127,
      "type": "SESSION",
      "startDate": 1571562000000,
      "endDate": 1571594400000
    },
    {
      "id": 11148,
      "type": "SESSION",
      "startDate": 1571821200000,
      "endDate": 1571826600000
    },
    {
      "id": 11149,
      "type": "BREAK",
      "startDate": 1571826600000,
      "endDate": 1571828400000
    },
    {
      "id": 11150,
      "type": "SESSION",
      "startDate": 1571828400000,
      "endDate": 1571833800000
    },
    {
      "id": 11151,
      "type": "LUNCH",
      "startDate": 1571833800000,
      "endDate": 1571839200000
    },
    {
      "id": 11153,
      "type": "SESSION",
      "startDate": 1571839200000,
      "endDate": 1571844600000
    },
    {
      "id": 11154,
      "type": "SESSION",
      "startDate": 1571844600000,
      "endDate": 1571848200000
    },
    {
      "id": 11155,
      "type": "SESSION",
      "startDate": 1571848200000,
      "endDate": 1571851800000
    },
    {
      "id": 11156,
      "type": "SESSION",
      "startDate": 1571851800000,
      "endDate": 1571853600000
    },
    {
      "id": 11128,
      "type": "BREAK",
      "startDate": 1571733000000,
      "endDate": 1571734800000
    },
    {
      "id": 11139,
      "type": "SESSION",
      "startDate": 1571648400000,
      "endDate": 1571652000000
    },
    {
      "id": 11140,
      "type": "BREAK",
      "startDate": 1571652000000,
      "endDate": 1571655600000
    },
    {
      "id": 11147,
      "type": "BREAK",
      "startDate": 1571819400000,
      "endDate": 1571821200000
    },
    {
      "id": 11152,
      "type": "SESSION",
      "startDate": 1571835600000,
      "endDate": 1571838300000
    },
    {
      "id": 11305,
      "type": "SESSION",
      "startDate": 1571594400000,
      "endDate": 1571605200000
    },
    {
      "id": 11308,
      "type": "SESSION",
      "startDate": 1571646600000,
      "endDate": 1571648400000
    },
    {
      "id": 11138,
      "type": "BREAK",
      "startDate": 1571645700000,
      "endDate": 1571648400000
    }
  ],
  "sessions": [
    {
      "id": 1883,
      "name": "Augmented and Mixed Reality",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        19438
      ],
      "contentIds": [
        7163,
        7455,
        3299,
        6758,
        5300,
        7921
      ],
      "timeSlotId": 11143
    },
    {
      "id": 1136,
      "name": "Media Authoring",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        11920
      ],
      "contentIds": [
        5834,
        3599,
        4059,
        7518,
        3153,
        3485
      ],
      "timeSlotId": 11143
    },
    {
      "id": 2131,
      "name": "Knitting, Weaving, Fabrics",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        24713
      ],
      "contentIds": [
        3054,
        3326,
        5762,
        6122,
        2742
      ],
      "timeSlotId": 11141
    },
    {
      "id": 1064,
      "name": "Software and Hardware Development",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        24714
      ],
      "contentIds": [
        7590,
        7151,
        5561,
        6524,
        4451
      ],
      "timeSlotId": 11141
    },
    {
      "id": 2090,
      "name": "Soft, Silky, Stretchy",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        18873
      ],
      "contentIds": [
        3837,
        6958,
        7741,
        6146,
        3877
      ],
      "timeSlotId": 11145
    },
    {
      "id": 1123,
      "name": "Accessibility",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        25075
      ],
      "contentIds": [
        3047,
        4131,
        4526,
        7558
      ],
      "timeSlotId": 11145
    },
    {
      "id": 1632,
      "name": "Statistics and Interactive Machine Learning",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        24721
      ],
      "contentIds": [
        2731,
        7090,
        4427,
        7954,
        4332
      ],
      "timeSlotId": 11131
    },
    {
      "id": 1489,
      "name": "VR Headsets",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        16701
      ],
      "contentIds": [
        3587,
        6870,
        3684,
        4713,
        7460
      ],
      "timeSlotId": 11129
    },
    {
      "id": 1898,
      "name": "Human-Robot Interaction",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        25077
      ],
      "contentIds": [
        6477,
        6913,
        4735,
        7687,
        8115
      ],
      "timeSlotId": 11129
    },
    {
      "id": 2137,
      "name": "Physical Displays",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        18694
      ],
      "contentIds": [
        7720,
        5851,
        7096,
        6667,
        4038
      ],
      "timeSlotId": 11131
    },
    {
      "id": 2256,
      "name": "Fabrication",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        25076
      ],
      "contentIds": [
        5065,
        6431,
        4192,
        5813,
        4543,
        5511
      ],
      "timeSlotId": 11133
    },
    {
      "id": 2494,
      "name": "Haptics and Illusions",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        17851
      ],
      "contentIds": [
        6994,
        4991,
        4307,
        4340,
        8083,
        7403
      ],
      "timeSlotId": 11133
    },
    {
      "id": 1447,
      "name": "Text",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        19359
      ],
      "contentIds": [
        3410,
        3378,
        2842,
        4117,
        3205
      ],
      "timeSlotId": 11148
    },
    {
      "id": 1656,
      "name": "Haptics",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        24725
      ],
      "contentIds": [
        5320,
        7951,
        5502,
        4762,
        4365
      ],
      "timeSlotId": 11148
    },
    {
      "id": 1481,
      "name": "Sensing",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        25078
      ],
      "contentIds": [
        6703,
        5969,
        6260,
        8030,
        3337
      ],
      "timeSlotId": 11150
    },
    {
      "id": 1864,
      "name": "Touch Input",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        24301
      ],
      "contentIds": [
        3816,
        6831,
        7920,
        5542,
        7104
      ],
      "timeSlotId": 11150
    },
    {
      "id": 2068,
      "name": "Walking, Jumping, Roaming",
      "typeId": 11339,
      "roomId": 10308,
      "chairIds": [
        24730
      ],
      "contentIds": [
        3657,
        3749,
        5104,
        7461,
        6904
      ],
      "timeSlotId": 11153
    },
    {
      "id": 2050,
      "name": "3D and VR Input",
      "typeId": 11339,
      "roomId": 10309,
      "chairIds": [
        8841
      ],
      "contentIds": [
        3982,
        4686,
        6254,
        2952,
        3680
      ],
      "timeSlotId": 11153
    }
  ],
  "events": [
    {
      "id": 24693,
      "name": "UIST Visions",
      "typeId": 11342,
      "roomId": 10313,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571763600000,
      "endDate": 1571767200000,
      "presenterIds": []
    },
    {
      "id": 24694,
      "name": "Welcome Remarks",
      "typeId": 11342,
      "roomId": 10313,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571646600000,
      "endDate": 1571648400000,
      "presenterIds": []
    },
    {
      "id": 24579,
      "name": "Doctoral Symposium",
      "typeId": 11342,
      "roomId": 10317,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571562000000,
      "endDate": 1571594400000,
      "presenterIds": []
    },
    {
      "id": 24586,
      "name": "Lunch Break",
      "typeId": 11342,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571661000000,
      "endDate": 1571666400000,
      "presenterIds": []
    },
    {
      "id": 24592,
      "name": "Lunch Break",
      "typeId": 11342,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571747400000,
      "endDate": 1571752800000,
      "presenterIds": []
    },
    {
      "id": 24598,
      "name": "Lunch Break",
      "typeId": 11342,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571833800000,
      "endDate": 1571839200000,
      "presenterIds": []
    },
    {
      "id": 24580,
      "name": "Registration",
      "typeId": 11342,
      "roomId": 10316,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571587200000,
      "endDate": 1571605200000,
      "presenterIds": []
    },
    {
      "id": 24581,
      "name": "Reception",
      "typeId": 11342,
      "roomId": 10315,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571594400000,
      "endDate": 1571605200000,
      "presenterIds": []
    },
    {
      "id": 24587,
      "name": "Coffee Break",
      "typeId": 11342,
      "roomId": 10314,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571673600000,
      "endDate": 1571675400000,
      "presenterIds": []
    },
    {
      "id": 24588,
      "name": "Demo Reception",
      "typeId": 11342,
      "roomId": 10312,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571680800000,
      "endDate": 1571691600000,
      "presenterIds": []
    },
    {
      "id": 24589,
      "name": "Registration",
      "typeId": 11342,
      "roomId": 10316,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571733000000,
      "endDate": 1571767200000,
      "presenterIds": []
    },
    {
      "id": 24590,
      "name": "Coffee & Tea",
      "typeId": 11342,
      "roomId": 10314,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571733000000,
      "endDate": 1571734800000,
      "presenterIds": []
    },
    {
      "id": 24591,
      "name": "Coffee Break",
      "typeId": 11342,
      "roomId": 10314,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571740200000,
      "endDate": 1571742000000,
      "presenterIds": []
    },
    {
      "id": 24593,
      "name": "Coffee Break and Poster Session",
      "typeId": 11342,
      "roomId": 10315,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571760000000,
      "endDate": 1571763600000,
      "presenterIds": []
    },
    {
      "id": 24594,
      "name": "Conference Reception & Student Innovation Contest",
      "typeId": 11342,
      "roomId": 10311,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571770800000,
      "endDate": 1571779800000,
      "presenterIds": []
    },
    {
      "id": 24595,
      "name": "Registration",
      "typeId": 11342,
      "roomId": 10316,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571819400000,
      "endDate": 1571853600000,
      "presenterIds": []
    },
    {
      "id": 24596,
      "name": "Coffee & Tea",
      "typeId": 11342,
      "roomId": 10314,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571819400000,
      "endDate": 1571821200000,
      "presenterIds": []
    },
    {
      "id": 24597,
      "name": "Coffee Break",
      "typeId": 11342,
      "roomId": 10314,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571826600000,
      "endDate": 1571828400000,
      "presenterIds": []
    },
    {
      "id": 24599,
      "name": "Town Hall",
      "typeId": 11342,
      "roomId": 10308,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571837400000,
      "endDate": 1571839200000,
      "presenterIds": []
    },
    {
      "id": 24600,
      "name": "Coffee Break and Poster Session",
      "typeId": 11342,
      "roomId": 10315,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571844600000,
      "endDate": 1571848200000,
      "presenterIds": []
    },
    {
      "id": 24601,
      "name": "Closing Keynote:  \"Virtual Conferences\" by Cristina (Crista) Lopes",
      "typeId": 11342,
      "roomId": 10313,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571848200000,
      "endDate": 1571851800000,
      "presenterIds": []
    },
    {
      "id": 24602,
      "name": "Closing Ceremony",
      "typeId": 11342,
      "roomId": 10313,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571851800000,
      "endDate": 1571853600000,
      "presenterIds": []
    },
    {
      "id": 24603,
      "name": "Registration",
      "typeId": 11342,
      "roomId": 10316,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571644800000,
      "endDate": 1571680800000,
      "presenterIds": []
    },
    {
      "id": 24583,
      "name": "Coffee & Tea",
      "typeId": 11342,
      "roomId": 10314,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571645700000,
      "endDate": 1571648400000,
      "presenterIds": []
    },
    {
      "id": 24584,
      "name": "Opening Keynote: \"How We Make Impact\" by Elizabeth Gerber",
      "typeId": 11342,
      "roomId": 10313,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571648400000,
      "endDate": 1571652000000,
      "presenterIds": []
    },
    {
      "id": 24585,
      "name": "Coffee Break",
      "typeId": 11342,
      "roomId": 10314,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571652000000,
      "endDate": 1571655600000,
      "presenterIds": []
    }
  ],
  "contents": [
    {
      "id": 6913,
      "typeId": 11339,
      "title": "ShapeBots: Shape-changing Swarm Robots",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce shape-changing swarm robots, a new class of systems that consist of a swarm of self-transformable robots. These robots can both individually and collectively change their shape to display information, actuate objects, act as tangible controllers, visualize data, and provide affordances. To demonstrate this idea, we present ShapeBots, a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (extend up to 20cm) in both horizontal and vertical directions. The modular design of each linear actuator enables various shapes and topologies of self-transformation. We illustrate potential applications scenarios and discuss how this new type of interface can open up new possibilities for the future of ubiquitous and distributed shape-changing interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            }
          ],
          "personId": 8292
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            }
          ],
          "personId": 22737
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 16960
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            }
          ],
          "personId": 10206
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 23102
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute "
            }
          ],
          "personId": 11025
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado, Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 11396
        }
      ],
      "sessionIds": [
        1898
      ],
      "eventIds": []
    },
    {
      "id": 6146,
      "typeId": 11339,
      "title": "Morphlour: Personalized Flour-based Morphing Food Induced by Dehydration or Hydration Method",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we explore personalized morphing food that enhances traditional food with new HCI capabilities, rather than replacing the chef and authentic ingredients (e.g., flour) with an autonomous machine and heterogeneous mixtures (e.g., gel). Thus, we contribute a unique transformation mechanism of kneaded and sheeted flour-based dough, with an integrated design strategy for morphing food during two general cooking methods: dehydration (e.g., baking) or hydration (e.g., water boiling). We also enrich the design space of morphing food by demonstrating several application cases. We end by discussing hybrid cooking between human and design tool to ensure accuracy while preserving customizability for morphing food.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang Univeristy",
              "dsl": ""
            }
          ],
          "personId": 10848
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 16097
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Architecture"
            }
          ],
          "personId": 18505
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 21301
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 19146
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 10772
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 21792
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore ",
              "institution": "Singapore ",
              "dsl": ""
            }
          ],
          "personId": 21354
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 14792
        }
      ],
      "sessionIds": [
        2090
      ],
      "eventIds": []
    },
    {
      "id": 5762,
      "typeId": 11339,
      "title": "Tessutivo: Contextual Interactions on Interactive Fabrics with Inductive Sensing",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present Tessutivo, a contact-based inductive sensing technique for contextual interactions on interactive fabrics. Our technique recognizes conductive objects (mainly metallic) that are commonly found in households and workplaces, such as keys, coins, and electronic devices. We built a prototype containing six by six spiral-shaped coils made of conductive thread, sewn onto a four-layer fabric structure. We carefully designed the coil shape parameters to maximize the sensitivity based on a new inductance approximation formula. Through a ten- participant study, we evaluated the performance of our proposed sensing technique across 27 common objects. We yielded 93.9% real-time accuracy for object recognition. We conclude by presenting several applications to demonstrate the unique interactions enabled by our technique.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science Department"
            },
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 22695
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Anhui",
              "city": "HeFei",
              "institution": "USTC",
              "dsl": "EE"
            },
            {
              "country": "China",
              "state": "Anhui",
              "city": "HeFei",
              "institution": "USTC",
              "dsl": "EE"
            }
          ],
          "personId": 11051
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": ""
            }
          ],
          "personId": 16411
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 14215
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 18975
        }
      ],
      "sessionIds": [
        2131
      ],
      "eventIds": []
    },
    {
      "id": 3587,
      "typeId": 11339,
      "title": "FaceWidgets: Exploring Tangible Interaction on Face with Head-Mounted Displays",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present FaceWidgets, a device integrated with the backside of a head-mounted display (HMD) that enables tangible interactions using physical controls. To allow for near range-to-eye interactions, our first study suggested displaying the virtual widgets at 20 cm from the eye positions, which is 9 cm from the HMD backside. We propose two novel interactions, widget canvas and palm-facing gesture, that can help users avoid double vision and allow them to access the interface as needed. Our second study showed that displaying a hand reference improved performance of face widgets interactions. We developed two applications of FaceWidgets, a fixed-layout 360 video player and a contextual input for smart home control. Finally, we compared four hand visualizations against the two applications in an exploratory study. Participants considered the transparent hand as the most suitable and responded positively to our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "Institute of Computer Science and Engineering",
              "dsl": "National Chiao Tung University"
            }
          ],
          "personId": 19524
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "Institute of Multimedia Engineering",
              "dsl": "National Chiao Tung University"
            }
          ],
          "personId": 15860
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "Computer Science",
              "dsl": "National Chiao Tung University"
            }
          ],
          "personId": 17816
        }
      ],
      "sessionIds": [
        1489
      ],
      "eventIds": []
    },
    {
      "id": 3205,
      "typeId": 11339,
      "title": "TipText: Eyes-Free Text Entry on a Fingertip Keyboard",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose and investigate a new text entry technique using micro thumb-tip gestures. Our technique features a miniature QWERTY keyboard residing invisibly on the first segment of the user’s index finger. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. The keyboard layout was optimized for eyes-free input by utilizing a spatial model reflecting the users’ natural spatial awareness of key locations on the index finger. We present our approach of designing and optimizing the keyboard layout through a series of user studies and computer simulated text entry tests over 1,146,484 possibilities in the design space. The outcome is a 2×3 grid with the letters highly confining to the alphabetic and spatial arrangement of QWERTY. Our one-day user  evaluation showed that participants achieved an average text entry speed of 11.9 WPM and were able to type as fast as 13.3 WPM towards the end of the experiment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 10117
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 22130
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 22695
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "West Lebanon",
              "institution": "Dartmouth College",
              "dsl": "XDiscovery Lab"
            }
          ],
          "personId": 10905
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 13686
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 12320
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 8753
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 18724
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 16121
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 18975
        }
      ],
      "sessionIds": [
        1447
      ],
      "eventIds": []
    },
    {
      "id": 7558,
      "typeId": 11339,
      "title": "GesturePod: Enabling On-device Gesture-based Interaction for White Cane Users",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "People using white canes for navigation find it challenging to concurrently access devices such as smartphones. Building on recent research on abandonment of specialized devices, we explore a new touch free mode of interaction, wherein a person with visual impairment performs gestures on their existing white cane to trigger tasks on their smartphone. We present an easy-to-integrate GesturePod, that clips on to any white cane and enables the detection of gestures performed with the cane. GesturePod, thereby, helps perform certain tasks on a smartphone without touch, or removing the phone from a pocket or bag. We discuss challenges that arise in building the pod and design decisions we made. We propose a novel, efficient machine learning pipeline to train and deploy a gesture recognition model. Our in-lab study shows that GesturePod achieves 92% gesture-recognition accuracy and can help perform some common smartphone tasks faster. Our in-wild study suggests that GesturePod is a promising interaction tool for smartphone, especially in constrained outdoor scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "KA",
              "city": "Bengaluru",
              "institution": "Microsoft Research Lab India",
              "dsl": ""
            }
          ],
          "personId": 19225
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "Microsoft Research India",
              "dsl": "Microsoft Research Lab"
            }
          ],
          "personId": 11751
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "KA",
              "city": "Bangalore",
              "institution": "Microsoft Research Lab India",
              "dsl": ""
            }
          ],
          "personId": 24006
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "KA",
              "city": "Bangalore",
              "institution": "Microsoft Research Lab India",
              "dsl": ""
            }
          ],
          "personId": 21880
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bengaluru",
              "institution": "Microsoft Research India",
              "dsl": ""
            }
          ],
          "personId": 8673
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "KA",
              "city": "Bangalore",
              "institution": "Microsoft Research India",
              "dsl": ""
            }
          ],
          "personId": 18910
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Microsoft Research Lab India",
              "dsl": ""
            }
          ],
          "personId": 20775
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "KA",
              "city": "Bangalore",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 10494
        }
      ],
      "sessionIds": [
        1123
      ],
      "eventIds": []
    },
    {
      "id": 7687,
      "typeId": 11339,
      "title": "GhostAR: A Time-space Editor for Embodied Authoring of Human-Robot Collaborative Task with Augmented Reality",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present GhostAR, a time-space editor for authoring and acting Human-Robot-Collaborative (HRC) tasks in-situ. Our system adopts an embodied authoring approach in Augmented Reality (AR), for spatially editing the actions and programming the robots through demonstrative role-playing. We propose a novel HRC workflow that externalizes user's authoring as demonstrative and editable AR ghost, allowing for spatially situated visual referencing, realistic animated simulation, and collaborative action guidance. We develop a dynamic time warping (DTW) based collaboration model which takes the real-time captured motion as inputs, maps it to the previously authored human actions, and outputs the corresponding robot actions to achieve adaptive collaboration. We emphasize an in-situ authoring and rapid iterations of joint plans without an offline training process. Further, we demonstrate and evaluate the effectiveness of our workflow through HRC use cases and a three-session user study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 13648
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 17501
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 11479
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purude University",
              "dsl": ""
            }
          ],
          "personId": 14177
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "C Design Lab, School of Mechanical Engineering"
            }
          ],
          "personId": 13442
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "School of Mechanical Engineering"
            }
          ],
          "personId": 14089
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 15840
        }
      ],
      "sessionIds": [
        1898
      ],
      "eventIds": []
    },
    {
      "id": 5511,
      "typeId": 11339,
      "title": "Ondulé: Designing and Controlling 3D Printable Springs",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present Ondulé, a novel computational design tool to add elastic deformation behaviors to static 3D models using a combination of 3D-printed springs and mechanical joints. Springs are unique because they can exert expressive deformation behaviors and store mechanical energy. Informed by spring theory and our empirical mechanical experiments, we introduce spring and joint-based design techniques that support a range of parameterizable deformation behaviors, including compress, extend, twist, bend, and various combinations. To enable users to design and add these 3D-printable deformations to their models, we introduce a custom spring design tool for Rhino. Here, users can convert selected geometries into springs, customize spring stiffness, and parameterize their design to obtain a desired deformation behavior. To demonstrate the feasibility of our approach and the breadth of new 3D-printable designs that it enables, we showcase a set of example applications from launching rocket toys to tangible storytelling props. We conclude with a discussion of key challenges and open research questions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 24262
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 22313
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 18295
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Makeability Lab"
            }
          ],
          "personId": 22358
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 18196
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 19269
        }
      ],
      "sessionIds": [
        2256
      ],
      "eventIds": []
    },
    {
      "id": 2952,
      "typeId": 11339,
      "title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Modern virtual reality controllers offer direct, quick, and easy manipulation of virtual objects. However, precisely placing objects using direct manipulation is still challenging in 3D environments. This paper presents Plane, Ray, & Point, a set of novel interaction techniques that enable quick object alignment and manipulation in virtual reality. The interaction techniques use hand gestures to create constraints that separate manipulation degrees of freedom. The user can create a Ray by outstretching the index finger or the thumb and use it to limit the rotation or translation of an object to a single axis. By opening both the index finger and the thumb, the user can create a Plane, and use it to limit the object’s movement along the 2D plane. Such gesture-invoked constraints help users quickly align and place virtual objects. We evaluate the applicability and use-cases of our technique in an expert user study; as well as evaluate its learnability in an informal user study with novice users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 18359
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 23647
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 22303
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            }
          ],
          "personId": 10576
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 25074
        }
      ],
      "sessionIds": [
        2050
      ],
      "eventIds": []
    },
    {
      "id": 3337,
      "typeId": 11339,
      "title": "PrivateTalk: Activating Voice Input with Hand-On-Mouth Gesture Detected by Bluetooth Earphones",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce PrivateTalk, an on-body interaction technique to activate voice input with the Hand-On-Mouth gesture. When \"talking\" to a computing device (e.g., a smartphone), a user covers his/her mouth with the hand from one side. PrivateTalk provides two essential benefits for voice input simultaneously. First, the Hand-On-Mouth gesture protects talking privacy by lowering down the voice spreading out and shielding the mouth and lip movements from seen by people around. Second, it uses an on-body hand gesture which removes the need for Wake-Up-Word and is more accessible than a physical/software button when the device is not in hand. We propose a novel sensing technique to recognize this asymmetric Hand-On-Mouth gesture with Bluetooth earphones (e.g., AirPod). When the mouth is covered from one side, the amplitude and Mel-frequency cepstral coefficients (MFCC) of the two audio signals from earphone's microphones are significantly different. This feature is stable and robust across users, and we leverage it to recognize the Hand-On-Mouth gesture.  Evaluation results show that the recognition accuracy is as high as 98.33\\%. Users' subjective feedback indicates that PrivateTalk is intuitive and consistently appreciated by users. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Computer Science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Computer Science and Technology"
            }
          ],
          "personId": 11292
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 12698
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 9137
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            },
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 13683
        }
      ],
      "sessionIds": [
        1481
      ],
      "eventIds": []
    },
    {
      "id": 6667,
      "typeId": 11339,
      "title": "LeviProps: Animating Levitated Optimized Fabric Structures using Holographic Acoustic Tweezers ",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "LeviProps are tangible structures used to create interactive mid-air experiences. They are composed of an acoustically-transparent lightweight piece of fabric and attached beads that act as levitated anchors. This combination enables real-time 6 Degrees-of-Freedom control of levitated structures which are larger and more diverse than those possible with previous acoustic approaches. LeviProps can be used as free-form interactive elements and also as projection surfaces. We developed an authoring tool to support the creation of LeviProps. Our tool employs the outline of the prop and the user constraints to compute the optimum locations for the anchors (i.e. maximizing trapping forces), increasing prop stability and maximum size. The tool produces a final LeviProp design which can be fabricated following a simple procedure. This paper evaluates our approach and showcases example applications such as interactive storytelling, games and mid-air displays.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "East Sussex",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": "Interact lab"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Ultrahaptics",
              "dsl": ""
            }
          ],
          "personId": 8454
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Navarre",
              "city": "Pamplona",
              "institution": "Universidad Publica de Navarra",
              "dsl": "UpnaLab"
            }
          ],
          "personId": 18056
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": ""
            }
          ],
          "personId": 13611
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": ""
            }
          ],
          "personId": 20950
        }
      ],
      "sessionIds": [
        2137
      ],
      "eventIds": []
    },
    {
      "id": 4365,
      "typeId": 11339,
      "title": "HapSense: A Soft Haptic I/O Device with Uninterrupted Dual Functionalities of Force Sensing and Vibrotactile Actuation",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present HapSense, a single-volume soft haptic I/O device with uninterrupted dual functionalities of force sensing and vibrotactile actuation. To achieve both input and output functionalities, we employ a ferroelectric electroactive polymer as core functional material with a multilayer structure design. We introduce a haptic I/O hardware that supports tunable high driving voltage waveform for vibrotactile actuation while in-situ sensing a change in capacitance from contact force. With mechanically soft nature of fabricated structure, HapSense can be embedded onto various object surfaces including but not limited to furniture, garments, and the human body. Through a series of experiments and evaluations, we characterized physical properties of HapSense and validated the feasibility of using soft haptic I/O with real users. We demonstrated a variety of interaction scenarios using HapSense.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            }
          ],
          "personId": 17530
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": "Applied Sciences Group"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": "Applied Sciences Group"
            }
          ],
          "personId": 13756
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            }
          ],
          "personId": 20330
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 19544
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            }
          ],
          "personId": 12631
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            }
          ],
          "personId": 15569
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Corporation",
              "dsl": ""
            }
          ],
          "personId": 23970
        }
      ],
      "sessionIds": [
        1656
      ],
      "eventIds": []
    },
    {
      "id": 3982,
      "typeId": 11339,
      "title": "True Touch: Precise Touch Detection for On-Skin AR/VR Interfaces",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Contemporary AR/VR systems use in-air gestures or handheld controllers for interactivity. This overlooks the skin as a convenient surface for tactile, touch-driven interactions, which are generally more accurate and comfortable than free space interactions. We developed RFTouch, an electrical method that enables very precise touch segmentation by using the body as an RF waveguide. We combine this method with computer vision, enabling a system with both high tracking precision and robust touch detection. Our system requires no cumbersome instrumentation of the fingers or hands, requiring only a single wristband and sensors integrated into the headset. We quantify the accuracy of our approach through a user study and demonstrate how it can enable touchscreen-like interactions on the skin.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 15094
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 18121
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 16840
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 23373
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 8507
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 18694
        }
      ],
      "sessionIds": [
        2050
      ],
      "eventIds": []
    },
    {
      "id": 3599,
      "typeId": 11339,
      "title": "Optimizing Portrait Lighting at Capture-Time Using a 360 Camera as a Light Probe",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present a capture-time tool designed to help casual photographers orient their subject to achieve a user-specified target facial appearance. The inputs to our tool are an HDR environment map of the scene captured using a 360 camera, and a target facial appearance, selected from a gallery of common studio lighting styles. Our tool computes the optimal orientation for the subject to achieve the target lighting using a computationally efficient precomputed radiance transfer-based approach. It tells the photographer how far to rotate about the subject and provides real-time feedback showing how close the photographer's current view is to the view at the target orientation. Optionally, our tool can suggest how to orient a secondary external light source (e.g. a phone screen) about the subject's face to further improve the match to the target lighting. We demonstrate the effectiveness of our approach in a variety of indoor and outdoor scenes using many different subjects to achieve a variety of looks. A user evaluation suggests that our tool is very useful and reduces the mental effort required by photographers to produce well-lit portraits. We discuss why our approach is suitable for implementation directly in camera hardware.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 13274
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 12261
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 12568
        }
      ],
      "sessionIds": [
        1136
      ],
      "eventIds": []
    },
    {
      "id": 7951,
      "typeId": 11339,
      "title": "Virtual Muscle Force: Communicating Kinesthetic Forces Through Pseudo-Haptic Feedback and Muscle Input",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Natural haptic feedback in virtual reality (VR) is complex and challenging, due to the intricacy of necessary stimuli and respective hardware. Pseudo-haptic feedback aims at providing haptic feedback without providing actual haptic stimuli but by using other\r\nsensory channels (e.g. visual cues) for feedback. \r\nWe propose a new approach to enhance the interaction that includes kinesthetic feedback by forcing a user to tense her muscles even if no physical counterpart exists and call this pseudo-haptic interaction. We combine this additional feature with current state-of-the-art pseudo-haptic feedback. A comparison of our solution to existing approaches as well as to no kinesthetic feedback at all shows that our approach significantly increases immersion, enjoyment as well as the perceived quality of kinesthetic feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "Institute of Mediainformatics",
              "dsl": "Ulm University"
            }
          ],
          "personId": 21269
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "BW",
              "city": "Ulm",
              "institution": "institute of media informatics",
              "dsl": "Ulm University"
            }
          ],
          "personId": 12888
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "Institute of Mediainformatics",
              "dsl": "Ulm University"
            }
          ],
          "personId": 15585
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "Institute of Mediainformatics",
              "dsl": "Ulm University"
            }
          ],
          "personId": 11649
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "University of Ulm",
              "dsl": ""
            }
          ],
          "personId": 25057
        }
      ],
      "sessionIds": [
        1656
      ],
      "eventIds": []
    },
    {
      "id": 7954,
      "typeId": 11339,
      "title": "Tea: A High-level Language and Runtime System for Automating Statistical Analysis",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Though statistical analyses are centered on research questions and hypotheses, current statistical analysis tools are not. Users must first translate their hypotheses into specific statistical tests and then perform API calls with functions and parameters. To do so accurately requires that users have statistical expertise. To lower this barrier to valid, replicable statistical analysis, we introduce Tea, a high-level declarative language and runtime system. In Tea, users express their study design, any parametric assumptions, and their hypotheses. Tea compiles these high-level specifications into a constraint satisfaction problem that determines the set of valid statistical tests, and then executes them to test the hypothesis. We evaluate Tea using a suite of statistical analyses drawn from popular tutorials. We show that Tea generally matches the choices of experts while automatically switching to non-parametric tests when parametric assumptions are not met. We simulate the effect of mistakes made by non-expert users and show that Tea automatically avoids both false negatives and false positives that could be produced by the application of incorrect statistical tests.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 25063
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 11510
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 17935
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Electrical Engineering and Computer Sciences"
            }
          ],
          "personId": 15786
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts Amherst",
              "dsl": "College of Information and Computer Sciences"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 18792
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 24239
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 15182
        }
      ],
      "sessionIds": [
        1632
      ],
      "eventIds": []
    },
    {
      "id": 8083,
      "typeId": 11339,
      "title": "PseudoBend: Producing Haptic Illusions of Stretching, Bending, and Twisting Using Grain Vibrations",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present a technique for haptic feedback that creates the illusion that a rigid device is being stretched, bent, or twisted. The method uses a single 6-DOF force sensor and a vibrotactile actuator to render grain vibrations to simulate the vibrations produced during object deformation based on the changes in force or torque exerted on a device. Because this method does not require any moving parts aside from the vibrotactile actuator, devices designed using this method can be small and lightweight. Psychophysical studies conducted using a prototype that implements this method confirmed that the method could be used to successfully create the illusion of deformation and could also change users’ perception of stiffness by changing the virtual stiffness parameters.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 23647
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 15458
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 25074
        }
      ],
      "sessionIds": [
        2494
      ],
      "eventIds": []
    },
    {
      "id": 4117,
      "typeId": 11339,
      "title": "SpaceInk: Making Space for In-Context Annotations",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "When editing or reviewing a document, people directly overlay ink marks on content. For instance, they underline words, or circle elements in a figure. These overlay marks often accompany in-context annotations in the form of handwritten footnotes and marginalia. People tend to put annotations close to the content that elicited them, but have to compose with the often-limited whitespace. We introduce SpaceInk, a design space of pen+touch techniques that make room for in-context annotations by dynamically reflowing documents. We identify representative techniques in this design space, spanning both new ones and existing ones. We evaluate them in a user study, with results that inform the design of a prototype system. Our system lets users concentrate on capturing fleeting thoughts, streamlining the overall annotation process by enabling the fluid inverleaving of space-making gestures with freeform ink.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Univ. Paris-Sud, CNRS, INRIA, Université Paris-Saclay",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Voiron",
              "institution": "TecKnowMetrix",
              "dsl": ""
            }
          ],
          "personId": 11291
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Univ. Paris-Sud, CNRS, INRIA, Université Paris-Saclay",
              "dsl": ""
            }
          ],
          "personId": 23532
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 12954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 16308
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Univ. Paris-Sud, CNRS, INRIA, Université Paris-Saclay.",
              "dsl": ""
            }
          ],
          "personId": 18625
        }
      ],
      "sessionIds": [
        1447
      ],
      "eventIds": []
    },
    {
      "id": 2842,
      "typeId": 11339,
      "title": "Gaze-Assisted Typing for Smart glasses",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Text entry is expected to be a common task for smart glasses users. Using a touchpad on the temple is a common approach, and using eye tracking is a promising approach. However, each approach has its own limitations. For more efficient text entry, we present Gaze-Assisted Typing (GAT), which uses both a touchpad and eye tracking. We first examined a GAT with a minimal eye input load, and could show the GAT (8.87 wpm) was 51% faster than a two-step touch input typing method (M-SwipeBoard: 5.85 wpm). We also compared GATs with varying numbers of touch gestures. The results show that a GAT requiring five different touch gestures was most preferred, while all GATs were equally efficient. Finally, we compared GAT with touch-only typing (SwipeZone) and eye-only typing (adjustable dwell) using an eye track-able HWD. The results show GAT (11.04 wpm) was 25.4% faster than the eye-only typing (8.81 wpm), was 29.4% faster than the touch-only typing (8.53 wpm), and was the most preferred.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "HCI Lab"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 18397
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": "HCI Lab"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 19504
        }
      ],
      "sessionIds": [
        1447
      ],
      "eventIds": []
    },
    {
      "id": 4762,
      "typeId": 11339,
      "title": "Mantis: A Scalable, Lightweight and Accessible Architecture to Build Multiform Force Feedback Systems",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Mantis is a highly scalable system architecture that democratizes haptic devices by enabling designers to create accurate, multiform and accessible force feedback systems. Mantis uses brushless DC motors to control a laser-cut actuated arm, custom electronic controllers, and an admittance control scheme to achieve stable high-quality haptic rendering. It enables common desktop form factors but also: large workspaces (multiple arm lengths); multiple arm workspaces; and mobile workspaces. It also uses accessible components and costs significantly less than typical high-fidelity force feedback solutions which are often confined to labs. We present our design and show that Mantis can reproduce the haptic fidelity of common robotic arms. We demonstrate its multiform ability by implementing five systems: a single desktop-sized device, a single large workspace device, a large workspace system with 4 points of feedback; a mobile system and a wearable one.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": ""
            }
          ],
          "personId": 17616
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": ""
            }
          ],
          "personId": 16801
        }
      ],
      "sessionIds": [
        1656
      ],
      "eventIds": []
    },
    {
      "id": 3485,
      "typeId": 11339,
      "title": "Sketch-n-Sketch: Output-Directed Programming for SVG",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "For creative tasks, programmers face a choice: Use a GUI and sacrifice flexibility, or write code and sacrifice ergonomics?\r\n\r\nTo obtain both flexibility and ease of use, prior work on a system called Sketch-n-Sketch proposed a workflow that we call output-directed programming, in which direct manipulation of the program's graphical output corresponds to writing code in a general-purpose programming language, and where edits not possible with the mouse can still be affected through ordinary text edits to the program. Sketch-n-Sketch demonstrated that output-directed programming is feasible in the domain of vector graphics: some parametric designs can be created entirely by output-directed interactions, without any textual programming.\r\n\r\nIn this paper, we propose that output-directed programming be viewed not just as manipulation of output alone, but as actions taken with respect to the entire program execution history.\r\n\r\nWithin this new conceptual framework, we explore a variety of novel techniques that extend the expressive power of output-directed programming in Sketch-n-Sketch. We relax syntactic restrictions, expose intermediate execution states for manipulation, and add new program transformations for SVG tasks. To demonstrate the improved expressiveness, we implement a dozen new parametric designs (without text-based edits), including the first demonstration of recursion in an output-directed programming setting.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 9785
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 20893
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 9767
        }
      ],
      "sessionIds": [
        1136
      ],
      "eventIds": []
    },
    {
      "id": 7455,
      "typeId": 11339,
      "title": "Context-Aware Online Adaptation of Mixed Reality Interfaces",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e. whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load, and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we present an evaluation with a dual task paradigm. Our approach resulted in similar task performance as a traditional UI, and decreased secondary tasks interactions by 36%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Advanced Interactive Technologies Lab"
            }
          ],
          "personId": 8841
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Advanced Interactive Technologies Lab"
            }
          ],
          "personId": 18989
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Advanced Interactive Technologies Lab"
            }
          ],
          "personId": 11460
        }
      ],
      "sessionIds": [
        1883
      ],
      "eventIds": []
    },
    {
      "id": 6431,
      "typeId": 11339,
      "title": "StackMold: Rapid Prototyping of Functional Multi-Material Objects with Selective Levels of Surface Details",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present StackMold, a DIY molding technique to prototype multi-material and multi-colored objects with embedded electronics. The key concept of our approach is a novel multi-stage mold buildup in which casting operations are interleaved with the assembly of the mold to form independent compartments for casting different materials. To build multi-stage molds, we contribute novel algorithms that computationally design and optimize the mold and casting procedure. By default, the multi-stage mold is fabricated in slices using a laser cutter. For regions that require more surface detail, a high-fidelity 3D-printed mold subsection can be incorporated. StackMold is an integrated end-to-end system, supporting all stages of the process: it provides a UI to specify material and detail regions of a 3D object; it generates fabrication files for the molds; and it produces a step-by-step casting instruction manual.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "-",
              "city": "Diepenbeek",
              "institution": "Expertise Centre for Digital Media",
              "dsl": "Hasselt University - tUL"
            }
          ],
          "personId": 14569
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KULeuven",
              "dsl": ""
            }
          ],
          "personId": 11340
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 8506
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "-",
              "city": "Diepenbeek",
              "institution": "Expertise Centre for Digital Media",
              "dsl": "Hasselt University - tUL- Flanders Make"
            }
          ],
          "personId": 9848
        }
      ],
      "sessionIds": [
        2256
      ],
      "eventIds": []
    },
    {
      "id": 4131,
      "typeId": 11339,
      "title": "StateLens: A Reverse Engineering Solution for Making Existing Dynamic Touchscreens Accessible",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Blind people frequently encounter inaccessible dynamic touchscreens in their everyday lives that are difficult, frustrating, and often impossible to use independently. Touchscreens are often the only way to control everything from coffee machines and payment terminals, to subway ticket machines and in-flight entertainment systems. Interacting with dynamic touchscreens is difficult non-visually because the visual user interfaces change, interactions often occur over multiple different screens, and it is easy to accidentally trigger interface actions while exploring the screen. To solve these problems, we introduce StateLens - a three-part reverse engineering solution that makes existing dynamic touchscreens accessible. First, StateLens reverse engineers the underlying state diagrams of existing interfaces using point-of-view videos found online or taken by users using a hybrid crowd-computer vision pipeline. Second, using the state diagrams, StateLens automatically generates conversational agents to guide blind users through specifying the tasks that the interface can perform, allowing the StateLens iOS application to provide interactive guidance and feedback so that blind users can access the interface. Finally, a set of 3D-printed accessories enable people who are blind to explore capacitive touchscreens without risk of triggering accidental touches on the interface. Our technical evaluation shows that StateLens can accurately reconstruct interface from stationary, hand-held, and web videos; and, a user study of the complete system demonstrates that StateLens successfully enables blind users to access otherwise inaccessible dynamic touchscreens.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 25061
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 13780
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 10921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies Institute"
            }
          ],
          "personId": 21402
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 24018
        }
      ],
      "sessionIds": [
        1123
      ],
      "eventIds": []
    },
    {
      "id": 7460,
      "typeId": 11339,
      "title": "MagicalHands: Mid-Air Hand Gestures for Animating in VR",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We seek to understand the preferred hand gestures for canonical animation authoring tasks in virtual reality (VR) environments. We first perform a gesture elicitation study for animation authoring to understand user preferences for a spatio-temporal bare-handed interaction system in VR. Specifically, we focus on the creation and editing of dynamic, physical phenomena (e.g., particle systems, deformation, coupling), where the mapping from gesture to animation is ambiguous and indirect. We present commonly observed mid-air gestures from the study, eliciting a set of rich interaction techniques---from direct manipulation to abstract demonstrations. To this end, we extend existing gesture taxonomies to the rich spatiotemporal interaction space needed by animators for authoring in both space and time. We distill our findings into a set of design guidelines for the construction of natural user interfaces for VR-based animation systems.\r\n\r\nFinally, based on our guidelines, we develop a proof-of-concept gestural animation system in VR. Our results, as well as feedback from user evaluation, suggest that the expressive qualities of hand gestures effectively allow users to animate in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 18370
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 16448
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe",
              "dsl": "Adobe Research"
            }
          ],
          "personId": 10615
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe",
              "dsl": "Adobe Research"
            }
          ],
          "personId": 12582
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 13399
        }
      ],
      "sessionIds": [
        1489
      ],
      "eventIds": []
    },
    {
      "id": 3749,
      "typeId": 11339,
      "title": "DreamWalker: Substituting Real-World Walking Experiences with a Virtual Reality",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We explore a future in which people spend considerably more time in virtual reality, even during moments when walk between locations in the real world. We present DreamWalker, a VR system that enables such real-world walking while users explore large virtual environments in a headset. Provided with a real-world destination, DreamWalker finds a similar path in a preauthored VR environment and then guides the user's real-walking VR. DreamWalker's tracking system fuses GPS locations, inside-out tracking, and RGBD frames to 1) continuously and accurately position the user in the real world, 2) sense walkable paths and obstacles in real time, and 3) represent paths through a dynamically changing scene in VR to redirect the user towards the chosen destination. We show DreamWalker's versatility through users walking three paths across a large campus while enjoying preauthored VR worlds, supplemented with a variety of obstacle avoidance and redirection techniques. In our evaluation, 8 participants walked through campus on a 15-minute route, experiencing virtual Manhattan full of animated cars, people, and other objects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 9209
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": ""
            }
          ],
          "personId": 19421
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 10723
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 13723
        }
      ],
      "sessionIds": [
        2068
      ],
      "eventIds": []
    },
    {
      "id": 7461,
      "typeId": 11339,
      "title": "Drift-Correction Techniques for Scale-Adaptive VR Navigation",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Scale adaptive techniques for VR navigation enable users to navigate spaces larger than the real space available, while allowing precise interaction when required. However, due to these techniques gradually scaling displacements as the user moves, they introduce a Drift effect. That is, a user returning to the same point in VR will not return to the same point in the real space. This mismatch between the real/virtual spaces can grow over time, and turn the techniques unusable (i.e., users cannot reach their target locations). In this paper, we characterise and analyse the effects of Drift, highlighting its potential detrimental effects. We then propose two techniques to correct Drift effects and use a data driven approach (using navigation data from real users with a specific scale adaptive technique) to tune them, compare their performance and chose an optimum correction technique and configuration. Our user study, applying our technique in a different environment and with two different scale adaptive navigation techniques, shows that our correction technique can significantly reduce Drift effects and extend the life-span of the navigation techniques (i.e., time that they can be used before Drift draws targets unreachable), while not hindering users’ experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of sussex",
              "dsl": "Department of Informatics / Interact Lab "
            }
          ],
          "personId": 18673
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": "Informatics"
            }
          ],
          "personId": 8983
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": ""
            }
          ],
          "personId": 13611
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": "Engineering and Informatics"
            }
          ],
          "personId": 20950
        }
      ],
      "sessionIds": [
        2068
      ],
      "eventIds": []
    },
    {
      "id": 3877,
      "typeId": 11339,
      "title": "Soft Inkjet Circuits: Rapid Multi-Material Fabrication of Soft Circuits using a Commodity Inkjet Printer",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Despite the increasing popularity of soft interactive devices, their fabrication remains complex and time-consuming. We contribute a process for rapid do-it-yourself fabrication of soft circuits using a conventional desktop inkjet printer. It supports inkjet printing of circuits that are stretchable, ultrathin, high resolution, and integrated with a wide variety of materials used for prototyping. We introduce multi-ink functional printing on a desktop printer for realizing multi-material devices, including conductive and isolating inks. We further present DIY techniques to enhance compatibility between inks and substrates and the circuits’ elasticity. This enables circuits on a wide set of materials including temporary tattoo paper, textiles, and thermoplastic. Four application cases demonstrate versa-tile uses for realizing stretchable devices, e-textiles, body-based and re-shapeable interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "INM - Leibniz Institute for New Materials",
              "dsl": ""
            }
          ],
          "personId": 11224
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Saarland University"
            },
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "Inria",
              "dsl": "Potioc"
            }
          ],
          "personId": 10684
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "INM - Leibniz Institute for New Materials",
              "dsl": "Structure Formation Group"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": "Colloid and Interface Chemistry"
            }
          ],
          "personId": 16956
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 8753
        }
      ],
      "sessionIds": [
        2090
      ],
      "eventIds": []
    },
    {
      "id": 5542,
      "typeId": 11339,
      "title": "Accurate and Low-Latency Sensing of Touch Contact on Any Surface with Finger-Worn IMU Sensor",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Head-mounted Mixed Reality (MR) systems enable touch interaction on any physical surface. However, optical methods (i.e., with cameras on the headset) have difficulty in determining the touch contact accurately. We show that a finger ring with Inertial Measurement Unit (IMU) can substantially improve the accuracy of contact sensing from 84.74% to 98.61% (f1 score), with a low latency of 10 ms. We tested different ring wearing positions and tapping postures (e.g., with different fingers and parts). Results show that an IMU-based ring worn on the proximal phalanx of the index finger can accurately sense touch contact of most usable tapping postures. Participants preferred wearing a ring for better user experience. Our approach can be used in combination with the optical touch sensing to provide robust and low-latency contact detection.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 9324
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 12698
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 11245
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 12480
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Computer Science and Technology, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 15733
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 11395
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer science and Technology"
            }
          ],
          "personId": 19091
        }
      ],
      "sessionIds": [
        1864
      ],
      "eventIds": []
    },
    {
      "id": 7590,
      "typeId": 11339,
      "title": "Unakite: Scaffolding Developers’ Decision-Making Using the Web",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Developers spend a significant portion of their time searching for solutions and methods online. While numerous tools have been developed to support this exploratory process, in many cases the answers to developers’ questions involve trade-offs among multiple valid options and not just a single solution. Through interviews, we discovered that developers express a desire for help with decision-making and understanding trade-offs. Through an analysis of Stack Overflow posts, we observed that many answers describe such trade-offs. These findings suggest that tools designed to help a developer capture information and make decisions about trade-offs can provide crucial benefits for both the developers and others who want to understand their design rationale.  In this work, we probe this hypothesis with a prototype system named Unakite that captures,  organizes,  and keeps track of information about trade-offs and builds a comparison table, which can be saved for later as the design rationale. Our evaluation results show that Unakite reduces the cost of collecting tradeoff-related information by 45%, and that the resulting comparison table speeds up a subsequent developer’s ability to understand the trade-offs by about a factor of 3.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 9289
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Oberlin",
              "institution": "Oberlin College",
              "dsl": ""
            }
          ],
          "personId": 20754
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institue"
            }
          ],
          "personId": 15640
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 18288
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 21157
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University ",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 12801
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Oberlin",
              "institution": "Oberlin College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 17497
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 25056
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 10154
        }
      ],
      "sessionIds": [
        1064
      ],
      "eventIds": []
    },
    {
      "id": 7720,
      "typeId": 11339,
      "title": "BubBowl: Display Vessel Using Electrolysis Bubbles in Drinkable Beverages",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Research has been conducted regarding a display that presents digital information using bubbles. Conventional bubble displays require moving parts, because it is common that air taken from outside of the water is used to present pixels. However, it is difficult to increase the number of pixels at a low cost. We propose a novel bubble display using gas generated from electrolysis, and present a cup-type system that generates a dot matrix pattern with 10 × 10 pixels on the surface of a beverage. Our technique requires neither a gas supply from the outside nor moving parts. Using the proposed electrolysis method, a higher-resolution display can be easily realized using a PCB with a higher density of matrix electrodes. Moreover, the method is simple and practical, and it can be utilized in daily life, such as for presenting information with bubbles on the surface of coffee in a cup. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo",
              "institution": "Ochanomizu University",
              "dsl": ""
            }
          ],
          "personId": 9814
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "Ochanomizu University",
              "dsl": "Graduate School of Humanities and Sciences"
            }
          ],
          "personId": 21920
        }
      ],
      "sessionIds": [
        2137
      ],
      "eventIds": []
    },
    {
      "id": 2731,
      "typeId": 11339,
      "title": "Is this Real? Generating Synthetic Data that Looks Real",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Synner is a tool that helps users generate real-looking synthetic data by visually and declaratively specifying the properties of the dataset such as each field's statistical distribution, its domain, and its relationship to other fields. It provides instant feedback on every user interaction by updating multiple visualizations of the generated dataset and even suggests data generation specifications from a few user examples and interactions. Synner visually communicates the inherent randomness of statistical data generation. Our evaluation of Synner demonstrates its effectiveness at generating realistic data when compared with Mockaroo, a popular data generation tool, and with hired developers who coded data generation scripts for a fee.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 24282
        },
        {
          "affiliations": [
            {
              "country": "United Arab Emirates",
              "state": "Abu Dhabi",
              "city": "Abu Dhabi",
              "institution": "New York University Abu Dhabi",
              "dsl": ""
            }
          ],
          "personId": 22814
        }
      ],
      "sessionIds": [
        1632
      ],
      "eventIds": []
    },
    {
      "id": 4526,
      "typeId": 11339,
      "title": "Designing AR Visualizations to Facilitate Stair Navigation for People with Low Vision",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Navigating stairs is one of the most dangerous mobility challenges for people with low vision (PLV), who have visual impairments that fall short of blindness. Prior research contributed systems for stair navigation that provide audio or tactile feedback, but PLV have usable vision and don’t typically use nonvisual aids. We conducted the first exploration of augmented reality (AR) visualizations to facilitate stair navigation for PLV. We designed visualizations for a projection-based AR platform and smartglasses, considering the different characteristics of these platforms. For projection,  we designed visual highlights that are projected directly on the stairs. In contrast, for smartglasses that have a limited vertical field of view, we designed visualizations that indicate the user’s position on the stairs, without directly augmenting the stairs themselves. We evaluated our visualizations on each platform with 12 PLV. We found that the visualizations for projection AR increased participants’ walking speed. Moreover, our designs on both platforms largely increased participants’ self-reported psychological security.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": ""
            }
          ],
          "personId": 21588
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": ""
            }
          ],
          "personId": 10688
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 19630
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 11496
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 21328
        }
      ],
      "sessionIds": [
        1123
      ],
      "eventIds": []
    },
    {
      "id": 6958,
      "typeId": 11339,
      "title": "Skin-On Interfaces: A Bio-Driven Approach for Artificial Skin Design to Cover Interactive Devices",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a paradigm called Skin-On interfaces, in which interactive devices have their own (artificial) skin, thus enabling new forms of input gestures for end-users (e.g. twist, scratch). Our work explores the design space of Skin-On interfaces by following a bio-driven approach:  (1) from a sensory point of view, we study how to reproduce the look and feel of the human skin through three user studies; (2) from a gestural point of view, we study what kind of gestures such interfaces enable by looking at how human beings use real skin as a communication medium or in interfaces; (3) from a technical point of view, we explore and discuss different ways of fabricating interfaces that mimic human skin sensitivity and can recognize the gestures observed in the previous study; (4) we assemble the insights of our three exploratory facets to implement a series of Skin-On interfaces and also contribute by providing a toolkit that enables easy reproduction and fabrication",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Université Paris Saclay",
              "dsl": "LTCI, Télécom ParisTech"
            }
          ],
          "personId": 23317
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, CNRS, ISIR",
              "dsl": ""
            }
          ],
          "personId": 23930
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "CNRS, ISIR",
              "dsl": "Sorbonne Université"
            }
          ],
          "personId": 25073
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Université Paris Saclay",
              "dsl": "LTCI, Télécom ParisTech"
            }
          ],
          "personId": 25066
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 10177
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": ""
            }
          ],
          "personId": 16801
        }
      ],
      "sessionIds": [
        2090
      ],
      "eventIds": []
    },
    {
      "id": 6831,
      "typeId": 11339,
      "title": "Modeling the Uncertainty in 2D Moving Target Selection",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding the selection uncertainty of moving targets is a fundamental research problem in HCI. However, the only few works in this domain mainly focus on selecting 1D moving targets with certain input devices, where the model generalizability has not been extensively investigated. In this paper, we propose a 2D Ternary-Gaussian model to describe the selection uncertainty manifested in endpoint distribution for moving target selection. We explore and compare two candidate methods to generalize the problem space from 1D to 2D tasks, and evaluate their performances with three input modalities including mouse, stylus, and finger touch. By applying the proposed model in assisting target selection, we achieved 56.7% improvement in selection speed and 78.8% improvement in pointing accuracy. In addition, we found that when predicting pointing errors, our model can fit the data of error rates with 0.94 R2.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Chinese Academy of Sciences",
              "dsl": "Institute of Software"
            }
          ],
          "personId": 23725
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 25059
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 12170
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 25068
        }
      ],
      "sessionIds": [
        1864
      ],
      "eventIds": []
    },
    {
      "id": 6703,
      "typeId": 11339,
      "title": "Opisthenar: Hand Poses and Finger Tapping Recognition by Observing Back of Hand Using Embedded Wrist Camera",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce a vision-based technique to recognize hand poses and gestures by simply observing changes on the back of the hand. Our approach employs a camera on the wrist, which we envisage  can be included in a wrist-worn device such as a smartwatch, fitness tracker or wristband. However, in this configuration the fingers are occluded from the view of the camera. The oblique angle and placement of the camera make typical vision-based techniques difficult to adopt. Our alternative approach observes small changes and movements in the shape, tendons, skin and bone on the back of the hand. We uses a deep neural network to train and recognize both static hand poses and dynamic gestures. \r\nWhile this is a challenging configuration for sensing, we tested the recognition with a real-time user test and can achieve a high recognition rate of 89.4% (static) and 67.5% (dynamic).\r\nOur results further demonstrate that our approach can generalize across sessions and to new users. Namely, users can remove and replace the wrist-worn device while new users can employ a previously trained system, to a certain extent. This form of sensing affords a range of new interaction capabilities from one-handed to subtle inputs or eyes-free to orientation invariant interactions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Fife",
              "institution": "University of St Andrews",
              "dsl": ""
            }
          ],
          "personId": 19288
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": "School of Computing"
            }
          ],
          "personId": 18815
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "UVR Lab"
            }
          ],
          "personId": 13026
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Fife",
              "city": "St Andrews",
              "institution": "University of St Andrews",
              "dsl": "SACHI, School of Computer Science"
            }
          ],
          "personId": 11763
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": "School of Computing"
            }
          ],
          "personId": 21614
        }
      ],
      "sessionIds": [
        1481
      ],
      "eventIds": []
    },
    {
      "id": 3378,
      "typeId": 11339,
      "title": "Type, Then Correct: Intelligent Text Correction Techniques for Mobile Text Entry Using Neural Networks",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Current text correction processes on mobile touch devices are laborious: users either extensively use backspace, or navigate the cursor to the error position, make a correction, and navigate back, usually by employing multiple taps or drags over small targets. In this paper, we present three novel text correction techniques to improve the efficiency of the correction process: Drag-n-Drop, Drag-n-Throw, and Magic Key. All of the techniques skip error-deletion and cursor-positioning procedures, and instead allow the user to type the correction first, and then apply that correction to a previously committed error. Specifically, Drag-n-Drop allows a user to drag a correction and drop it on the error position. Drag-n-Throw lets a user drag a correction from the keyboard suggestion list and “throw” it to the approximate area of the error text. Our deep learning algorithm determines the most likely error in the targeted area and applies the correction. Magic Key allows a user to type a correction and tap a designated key to highlight possible error candidates. The user can navigate among these candidates by dragging atop the key, and can apply a correction by tapping the key. We evaluated these techniques in both text correction and transcription tasks. Our experiment results show that correction with the new techniques was significantly faster than de facto cursor and backspace-based correction. Our techniques apply to any touch-based text entry method.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "The Information School"
            }
          ],
          "personId": 13395
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 22018
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "The Information School"
            }
          ],
          "personId": 21890
        }
      ],
      "sessionIds": [
        1447
      ],
      "eventIds": []
    },
    {
      "id": 7090,
      "typeId": 11339,
      "title": "Bespoke: Interactively Synthesizing Custom GUIs from Command-Line Applications By Demonstration",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Despite their usability problems, command-line applications are frequently used by programmers, researchers, system administrators, and data scientists. To give these power users the well-known benefit of GUIs, we created Bespoke, an interactive system that synthesizes form-based GUIs by observing user demonstrations of command-line apps. To our knowledge, Bespoke is the first attempt to use demonstration-based techniques to synthesize GUIs from command-line invocations. To assess the versatility and limitations of Bespoke, we ran an exploratory study where we let participants use it to create a custom GUI in a domain that personally motivated them. They ended up creating a diverse set of GUIs for domains such as cloud computing management, prototyping machine learning, transcribing lecture videos, integrated circuit design, remote code deployment, and gaming server management. Participants reported that the main benefit of these bespoke GUIs is that they exposed only the most relevant subset of options required for their needs. In contrast, any official GUI made by the app's manufacturer would usually include far more panes, menus, and settings since they must accommodate a wider range of possible use cases.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": ""
            }
          ],
          "personId": 11260
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Cognitive Science"
            }
          ],
          "personId": 9263
        }
      ],
      "sessionIds": [
        1632
      ],
      "eventIds": []
    },
    {
      "id": 8115,
      "typeId": 11339,
      "title": "A 3D Printer Head as a Robotic Manipulator",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce new ways of using 3D printer head as a 3-axis robotic manipulator to support advanced fabrication such as breaking support materials, assembling separately printed parts and actuating printed objects on a build-plate. To achieve these advanced fabrication techniques, we develop specific printing methods and redesign a low-cost commodity fused deposition modeling (FDM) 3D printer to that can attach/detach printed end-effectors which change the function of the 3D printer head (e.g. hook, break, and rotate printed objects).\r\nBy combining our explored printing methods and advanced fabrication techniques, a low-cost FDM 3D printer print out kinetic objects one-off such as bevel gears, springs and linkage mechanisms. In addition, this technique enables actuating printed functional objects on a build-plate that need a power source and actuators such as a coffee mill. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 12033
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 16952
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 12887
        }
      ],
      "sessionIds": [
        1898
      ],
      "eventIds": []
    },
    {
      "id": 5300,
      "typeId": 11339,
      "title": " LightAnchors: Appropriating Point Lights for Spatially-Anchored Augmented Reality Interfaces",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented reality requires precise and instant overlay of digital information onto everyday objects. We present our work on LightAnchors, a new method for displaying spatially-anchored data. We take advantage of pervasive point lights – such as LEDs and light bulbs – for both inview anchoring and data transmission. These lights are blinked at high speed to encode binary data. We built a proof-of-concept application that runs on iOS without any hardware or software modifications. We ran a study to characterize the performance of our system and built ten example demos to highlight the potential of our approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 21851
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 15757
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 19438
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 24301
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 18694
        }
      ],
      "sessionIds": [
        1883
      ],
      "eventIds": []
    },
    {
      "id": 5813,
      "typeId": 11339,
      "title": "Turn-by-Wire: Computationally Mediated Physical Fabrication",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Current digital fabrication tools allow users to exert precise computer control over the making process.  \r\nHowever, the `push-to-make' usage model of these tools limits not only their use, but also stifles our own mental models for how we should be able to directly, fluidly, and creatively interact with these tools.\r\nIn this paper, we investigate how a new class of hybrid-controlled machines can collaborate with novice and expert users alike to yield a more lucid making experience. \r\nWe demonstrate these ideas through our system, Turn-by-Wire.\r\nBy combining the capabilities of a traditional lathe with haptic input controllers that modulate both position and force, we detail a series of novel interaction metaphors that invite an adaptable making process spanning digital, model-centric, computer control, and embodied, adaptive, human control. \r\nWe evaluate our system through a user study and discuss how these concepts generalize to other fabrication tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "Computer Sciences"
            }
          ],
          "personId": 14340
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "Computer Sciences"
            }
          ],
          "personId": 24175
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "Siemens Corporate Technology",
              "dsl": "Artificial & Human Intelligence Research"
            }
          ],
          "personId": 14116
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "Siemens Corporate Technology",
              "dsl": "Artificial & Human Intelligence Research"
            }
          ],
          "personId": 15884
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": ""
            }
          ],
          "personId": 21672
        }
      ],
      "sessionIds": [
        2256
      ],
      "eventIds": []
    },
    {
      "id": 2742,
      "typeId": 11339,
      "title": "Knitting Skeletons: A Computer-Aided Design Tool for Shaping and Patterning of Knitted Garments",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "This work presents a novel system for simple garment composition and surface patterning which aims to allow anyone to design customized knitted garments.  Our tool combines ideas from CAD software and image editing: it allows the composition of (1) new parametric knitted primitives, and (2) stitch pattern layers with different resampling behaviours. The underlying course-based representation enables complex customization with automated layout and real-time patterning feedback. We show a variety of garments and patterns created with our tool, as well as our ability to transfer shape and pattern customizations across users. Finally, we make our design tool available to stimulate research in computational knitting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 14311
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 16755
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 23823
        }
      ],
      "sessionIds": [
        2131
      ],
      "eventIds": []
    },
    {
      "id": 7096,
      "typeId": 11339,
      "title": "TilePoP: Tile-type Pop-up Prop for Virtual Reality",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present TilePoP, a new type of pneumatically-actuated interface deployed as floor tiles which dynamically pop up to large shapes to construct proxy objects for whole-body interactions in Virtual Reality. TilePoP consists of a 2D array of stacked cube-shaped airbags designed with specific folding structures, enabling each airbag to be inflated into a physical proxy and deflated back to a tile when not in use. TilePoP is capable of providing haptic feedback for the whole body and can even support human body weight. Thus it affords new interaction possibilities in VR. We describe the design and implementation in details. We finally demonstrate the applications and conducted a preliminary user evaluation to understand the experiences of using TilePoP.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 16542
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 10606
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 21651
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 23592
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 17816
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 14909
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 22165
        }
      ],
      "sessionIds": [
        2137
      ],
      "eventIds": []
    },
    {
      "id": 5561,
      "typeId": 11339,
      "title": "X-Droid: A Quick and Easy Android Prototyping Framework with a Single App Illusion",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present X-Droid, a framework that provides Android app developers an ability to quickly produce functional prototypes. Our work is motivated by the need for such ability and the lack of tools that provide the ability. Developers want to produce a functional prototype rapidly in order to test out potential features in real-life situations. However, current prototyping tools for mobile apps are limited to creating non-functional UI mockups that do not demonstrate actual features. With X-Droid, developers can create a new app that imports various kinds of functionality provided by other existing Android apps. In doing so, developers do not need to understand how other Android apps are implemented and do not need access to their source code. X-Droid provides a developer tool that enables developers to use the UIs of other Android apps and import desired functions into their prototypes. X-Droid also provides a run-time system that executes other apps’ functionality in the background on off-the-shelf Android devices for seamless integration. Our evaluation shows that with the help of X-Droid, a developer was able to import a function from an existing Android app into a new prototype with only 55 lines of Java code, while the function itself requires 10,334 lines of Java code to implement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 16393
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 17080
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 10537
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo, The State University of New York",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 15972
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 15207
        }
      ],
      "sessionIds": [
        1064
      ],
      "eventIds": []
    },
    {
      "id": 7741,
      "typeId": 11339,
      "title": "M-Hair: Creating Novel Tactile Feedback by Augmenting the Body Hair to Respond to Magnetic Field",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present M-Hair, a novel method for providing tactile feedback by stimulating only the body hair without touching the skin.   It works by applying passive magnetic materials to the body hair, making it responsive to external magnetic forces/fields. We investigated the feasibility of actuating such augmented hair with external magnetic fields. We conducted a user study to understand the key characteristics and limitations in the perception of dynamic shape display on the hair, as well as demonstrated the applicability of this novel feedback in enhancing VR experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Auckland",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute,"
            }
          ],
          "personId": 18281
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Auckland",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute,"
            }
          ],
          "personId": 12127
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Augmented Human Lab, Auckland Bioengineering Institute"
            }
          ],
          "personId": 9323
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Auckland Bioengineering Institute, The University of Auckland",
              "dsl": "Augmented Human Lab"
            }
          ],
          "personId": 23896
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Auckland Bioengineering Institute, The University of Auckland",
              "dsl": "Augmented Human Lab"
            }
          ],
          "personId": 10655
        }
      ],
      "sessionIds": [
        2090
      ],
      "eventIds": []
    },
    {
      "id": 4543,
      "typeId": 11339,
      "title": "SpringFit: Joints and Mounts That Fabricate On Any Laser Cutter",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Joints are crucial to laser cutting as they allow making three-dimensional objects; mounts are crucial because they allow embedding technical components, such as motors. Unfortunately, mounts and joints tend to fail when trying to fabricate a model on a different laser cutter or from a different material. In our survey of 200 models found online, 65% of models we downloaded from an online re-pository did not fabricate properly because of this. We trace this issue back to the way mounts and joints hold objects in place, which is by forcing them into is slightly smaller openings. Such “press fit” mechanisms unfortunately are susceptible to the small changes in diameter that occur when switching to a machine that removes more or less material (“kerf”), as well as to the changes in stiffness, as they occur when switching materials. Thus the issue with downloaded models.\r\nWe present a software tool called springFit that resolves this problem by replacing the problematic press fit-based mounts with what we call canti¬lever-based mounts and joints. A cantilever spring is simply a long thin piece of material that pushes against the object to be held. Unlike press fits, cantilever springs are robust against variations in kerf and material; they can even handle very high varia-tions, simply by using longer springs. SpringFit converts entire models by replacing all contained mounts, notch joints, finger joints, and t-joints. In our technical evalua-tion springFit successfully converted 11/14 models from the web which fabricated in acrylic and wood and across different laser cutters. SpringFit is a first step towards what we want to call “portable” laser cutting.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 12509
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 12179
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 15226
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 22161
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": ""
            }
          ],
          "personId": 12665
        }
      ],
      "sessionIds": [
        2256
      ],
      "eventIds": []
    },
    {
      "id": 7104,
      "typeId": 11339,
      "title": "Multi-Touch Kit: A Do-It-Yourself Technique for Capacitive Multi-Touch Sensing Using a Commodity Microcontroller",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Mutual capacitance-based multi-touch sensing is now a ubiquitous and high-fidelity input technology. However, due to the complexity of electrical and signal processing requirements, it remains very challenging to create interface prototypes with custom-designed multi-touch input surfaces. In this paper, we introduce Multi-Touch Kit, a technique enabling electronics novices to rapidly prototype customized capacitive multi-touch sensors. In contrast to existing techniques, it works with a commodity microcontroller and open-source software, and does not require any specialized hardware. Evaluation results show that our approach enables multi-touch sensors that have a high spatial and temporal resolution, and can accurately detect multiple simultaneous touches. A set of application examples demonstrate the versatile uses of our approach for sensors of different scales, curvature, and materials. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 21675
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 16515
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments"
            }
          ],
          "personId": 13898
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 8753
        }
      ],
      "sessionIds": [
        1864
      ],
      "eventIds": []
    },
    {
      "id": 4038,
      "typeId": 11339,
      "title": "milliMorph - Fluid-Driven Thin Film Shape-Change Materials for Interaction Design",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents a design space, a fabrication system and applications of creating fluidic chambers and channels at millimeter scale for tangible actuated interfaces. The ability to design and fabricate millifluidic chambers allows one to create high frequency actuation, sequentially control of flows and high resolution design on thin film materials. We propose a four dimensional design space of creating those fluidic chambers, a novel heat sealing system that enables easy and precise millifluidics fabrication, and application demonstrations of the fabricated materials for haptics, ambient devices and robotics. As shape-change materials are increasingly integrated in interaction design, milliMorph enriches library of the fluid-driven shape-change mateirals, and demonstrates new design opportunities that is unique at millimeter scale for product and interaction design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 8671
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 17770
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Tangible Media Group"
            }
          ],
          "personId": 10234
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Media Lab"
            }
          ],
          "personId": 14883
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 18436
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 15769
        }
      ],
      "sessionIds": [
        2137
      ],
      "eventIds": []
    },
    {
      "id": 5320,
      "typeId": 11339,
      "title": "SCALE: Enhancing Force-based Interaction by Processing Load Data from Load Sensitive Modules",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "SCALE provides a framework of processing load data from distributed load-sensitive modules for exploring force-based interaction. Force conveys not only the force vector itself but also rich information of activities, including way of touch, object location and body motion. Our system captures these interactions on a single pipeline of load data processing. Furthermore, we have expanded the interaction area from a flat 2D surface to 3D volume by building a mathematical framework, which enables us to capture the vertical height of a touch point. This technical invention opens broad applications, including general  shape capturing and remote pointing. \r\nWe have packaged the framework into a physical prototyping kit, and conducted a workshop with product designers to evaluate our system in practical scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Media Lab"
            }
          ],
          "personId": 20230
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 11216
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 15769
        }
      ],
      "sessionIds": [
        1656
      ],
      "eventIds": []
    },
    {
      "id": 5065,
      "typeId": 11339,
      "title": "Robiot: A Design Tool for Actuating Everyday Objects with Automatically Generated 3D Printable Mechanisms",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Users can now easily communicate digital information with an Internet of Things; in contrast, there remains a lack of support to automate physical tasks that involve legacy static objects,e.g., adjusting a desk lamp’s angle for optimal brightness, turning on/off a manual faucet when washing dishes, sliding a window to maintain a preferred indoor temperature. Automating these simple physical tasks has the potential to improve people’s quality of life, which is particularly important for people with a disability or in situational impairment.\r\nWe present Robiot—a design tool for generating mechanisms that can be attached to, motorized, and actuating legacy static objects to perform simple physical tasks. Users only need to take a short video manipulating an object to demonstrate an intended physical behavior. Robiot then extracts requisite parameters and automatically generates 3D models of the enabling actuation mechanisms by performing a scene and motion analysis of the 2D video in alignment with the object’s3D model. In an hour long design session, six participants used Robiot to actuate seven everyday objects, imbuing them with the robotic capability to automate various physical tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "Mechnical Engineering"
            }
          ],
          "personId": 11008
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Computer Science"
            }
          ],
          "personId": 18781
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "ECE Department"
            }
          ],
          "personId": 25075
        }
      ],
      "sessionIds": [
        2256
      ],
      "eventIds": []
    },
    {
      "id": 3657,
      "typeId": 11339,
      "title": "Redirected Jumping: Perceptual Detection Rates for Curvature Gains",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Redirected walking (RDW) techniques provide a way to explore a virtual space that is larger than the available physical space by imperceptibly manipulating the virtual world view or motions. These manipulations introduce conflicts between real and virtual cues (e.g., visual-vestibular conflicts) which can be disturbing when detectable by users. The empirically established detection thresholds of rotation manipulation for RDW still require a large physical tracking space and are therefore impractical for general-purpose Virtual Reality (VR) applications. We investigate Redirected Jumping (RDJ) as a new locomotion metaphor for redirection to partially address this limitation, and because jumping is a common interaction for environments like games.\r\nWe investigated the detection rates for different curvature gains during RDJ. The probability of users detecting RDJ appears substantially lower than that of RDW, meaning designers can get away with greater manipulations with RDJ than with RDW. We postulate that the excessive vertical (up/down) movement present when jumping introduces increased vestibular noise compared to normal walking, thereby supporting greater rotational manipulations. Our study suggests that the potential combination of metaphors (e.g., walking and jumping) could further reduce the required physical space for locomotion in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 23644
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "Center for Advanced Computer Studies"
            }
          ],
          "personId": 24225
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HITLab NZ"
            }
          ],
          "personId": 14284
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 21579
        }
      ],
      "sessionIds": [
        2068
      ],
      "eventIds": []
    },
    {
      "id": 5834,
      "typeId": 11339,
      "title": "Sketchforme: Composing Sketched Scenes from Text Descriptions for Interactive Applications",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Sketching and natural languages are effective communication media for interactive applications. We introduce Sketchforme, the first neural-network-based system that can generate sketches based on text descriptions specified by users. Sketchforme is capable of gaining high-level and low-level understanding of multi-object sketched scenes without being trained on sketched scene datasets annotated with text descriptions. The sketches composed by Sketchforme are expressive and realistic: we show in our user study that these sketches convey descriptions better than human-generated sketches in multiple cases, and 36.5% of those sketches are considered to be human-generated. We develop multiple interactive applications using these generated sketches, and show that Sketchforme can significantly improve language learning applications and support intelligent language-based sketching assistants.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 15376
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Computer Science"
            }
          ],
          "personId": 10726
        }
      ],
      "sessionIds": [
        1136
      ],
      "eventIds": []
    },
    {
      "id": 4427,
      "typeId": 11339,
      "title": "PUMICE: A Multi-Modal Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Natural language programming is a promising approach to enable end users to instruct new tasks for intelligent agents. However, our formative study found that end users would often use unclear, ambiguous or vague concepts when naturally instructing tasks in natural language, especially when specifying conditionals. Existing systems have limited support for letting the user teach agents new concepts or explaining unclear concepts. In this paper, we describe a new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations. Users can also define new procedures and concepts by demonstrating and referring to contents within GUIs of existing mobile apps. We demonstrate this approach in PUMICE, an end-user programmable agent that implements this approach. A lab study with 10 users showed its usability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 23316
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "Amherst College",
              "dsl": ""
            }
          ],
          "personId": 22101
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 13423
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 20696
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Machine Learning Department"
            }
          ],
          "personId": 11608
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 10154
        }
      ],
      "sessionIds": [
        1632
      ],
      "eventIds": []
    },
    {
      "id": 6477,
      "typeId": 11339,
      "title": "Bodystorming Human-Robot Interactions",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Designing and implementing human-robot interactions requires numerous skills, from having a rich understanding of social interactions and the capacity to articulate their subtle requirements, to the ability to then program a social robot with the many facets of such a complex interaction. Although designers are best-suited to develop and implement these interactions due to their inherent understanding of the context and its requirements, these skills are a barrier to enabling designers to rapidly explore and prototype ideas: it is impractical for designers to also be experts on social interaction behaviors, and the technical challenges associated with programming a social robot are prohibitive. In this work, we introduce Improv, which allows designers to act out, or bodystorm, multiple demonstrations of an interaction. These demonstrations are automatically captured and translated into prototypes for the design team using program synthesis. We evaluate Improv in multiple design sessions involving pairs of designers bodystorming interactions and observing the resulting models on a robot. We build on the findings from these sessions to improve the capabilities of Improv and demonstrate the use of these capabilities in a second set of design sessions with two pairs of designers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin–Madison",
              "dsl": "Computer Sciences Department"
            }
          ],
          "personId": 20802
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "La Crosse",
              "institution": "University of Wisconsin - La Crosse",
              "dsl": "College of Science and Health"
            }
          ],
          "personId": 23478
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "La Crosse",
              "institution": "University of Wisconsin–La Crosse",
              "dsl": ""
            }
          ],
          "personId": 14749
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": ""
            }
          ],
          "personId": 22624
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 12771
        }
      ],
      "sessionIds": [
        1898
      ],
      "eventIds": []
    },
    {
      "id": 4686,
      "typeId": 11339,
      "title": "Eye&Head: Synergetic Eye and Head Movement for Pointing and Selection",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": " Eye gaze involves the coordination of eye and head movement to acquire gaze targets, but existing approaches to gaze pointing are based on eye-tracking in abstraction from head motion. We propose to leverage the synergetic movement of eye and head, and identify design principles for Eye&Head gaze interaction. We introduce three novel techniques that build on the distinction of head-supported versus eyes-only gaze, to enable dynamic coupling of gaze and pointer, hover interaction, visual exploration around pre-selections, and iterative and fast confirmation of targets. We demonstrate Eye&Head interaction on applications in virtual reality, and evaluate our techniques against baselines in pointing and confirmation studies. Our results show that Eye&Head techniques enable novel gaze behaviours that provide users with more control and flexibility in fast gaze pointing and selection. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lancaster",
              "institution": "Lancaster University",
              "dsl": ""
            }
          ],
          "personId": 20882
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lancaster",
              "institution": "Lancaster University",
              "dsl": ""
            }
          ],
          "personId": 23836
        }
      ],
      "sessionIds": [
        2050
      ],
      "eventIds": []
    },
    {
      "id": 3153,
      "typeId": 11339,
      "title": "HairBrush for Immersive Data-Driven Hair Modeling",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "While hair is an essential component of virtual humans, it is also one of the most challenging and time-consuming digital assets to create. Existing automatic techniques lack the generality and flexibility for users to create the exact intended hairstyles. Meanwhile, manual authoring interfaces often require considerable skills and experiences from character modelers, and are difficult to navigate for intricate 3D hair structures. We propose an interactive hair modeling system that can help create complex hairstyles that would otherwise take weeks or months with existing tools. Modelers, including novice users, can focus on the overall intended hairstyles and local hair deformations, as our system intelligently suggests the desired hair parts.\r\n\r\nOur method combines the flexibility of manual authoring and the convenience of data-driven automation. Since hair contains intricate 3D structures such as buns, knots, and strands, they are inherently challenging to create from scratch using traditional 2D interfaces. Our system provides a new 3D hair authoring interface for immersive interaction in virtual reality (VR). We use a strip-based representation, which is commonly adopted in real-time games due to rendering efficiency and modeling flexibility. The output strips can be converted to other hair formats such as strands. Users can draw high-level guide strips, from which our system predicts the most plausible hairstyles in the dataset via a trained deep neural network. Each hairstyle in our dataset is composed of multiple variations, serving as blendshapes to fit the user drawings via global blending and local deformation. The fitted hair models are visualized as interactive suggestions, that the user can select, modify, or ignore. We conducted a user study to confirm that our system can significantly reduce manual labor while improve the output quality for modeling a variety of hairstyles that are challenging to create using existing techniques.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Institute for Creative Technologies",
              "dsl": ""
            }
          ],
          "personId": 16039
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 15714
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Institute for Creative Technologies",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Pinscreen",
              "dsl": ""
            }
          ],
          "personId": 8850
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Pinscreen",
              "dsl": ""
            }
          ],
          "personId": 8368
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Detroit",
              "institution": "Wayne State University",
              "dsl": ""
            }
          ],
          "personId": 22852
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Institute for Creative Technologies",
              "dsl": ""
            }
          ],
          "personId": 8468
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Institute for Creative Technologies",
              "dsl": "University of Southern California"
            }
          ],
          "personId": 15394
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 23454
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San jose",
              "institution": "Adobe Inc.",
              "dsl": "Adobe Research"
            }
          ],
          "personId": 18283
        }
      ],
      "sessionIds": [
        1136
      ],
      "eventIds": []
    },
    {
      "id": 5969,
      "typeId": 11339,
      "title": "Sozu: Self-Powered Radio Tags for Building-Scale Activity Sensing",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Robust, wide-area sensing of human environments has been a long-standing research goal. We present Sozu, a new low-cost sensing system that can detect a wide range of events wirelessly, through walls and without line of sight, at whole-building scale. To achieve this in a battery-free manner, Sozu tags convert energy from activities that they sense into RF broadcasts, acting like miniature self-powered radio stations. We describe the results from a series of iterative studies, culminating in a deployment study with 30 instrumented objects. Results show that Sozu is very accurate, with true positive event detection exceeding 99%, with almost no false positives. Beyond event detection, we show that Sozu can be extended to detect the state, intensity, count, and rate of events. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 15094
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Human-Computer Interaction Institute, Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 22022
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Human Computer Interaction Institute",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 20342
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "CMU",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 12964
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 18694
        }
      ],
      "sessionIds": [
        1481
      ],
      "eventIds": []
    },
    {
      "id": 3410,
      "typeId": 11339,
      "title": "Beyond the Input Stream: Making Text Entry Evaluations More Flexible with Transcription Sequences",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Text entry method-independent evaluation tools are often used to conduct text entry experiments and compute performance metrics, like words per minute and error rates. The input stream paradigm of Soukoreff & MacKenzie (2001, 2003) still remains prevalent, which presents a string for transcription and uses a serial character representation for encoding the text entry process. Although an advance over prior paradigms, the input stream paradigm is unable to support many modern text entry features. To address these limitations, we present “transcription sequences:” for each new input, a snapshot of the entire transcribed string unto that point is captured. By assembling transcription sequences and comparing adjacent strings, we can compute all prior metrics, reduce artificial constraints on text entry evaluations, and introduce new metrics. We conducted a study with 18 participants who typed 1620 phrases using a laptop keyboard, on-screen keyboard, and smartphone keyboard using features such as auto-correction, word prediction, and copy-and-paste. We also evaluated non-keyboard methods Dasher, gesture typing, and T9. Our results show that modern features and methods can be accommodated, prior metrics can be correctly computed, and new metrics can reveal insights. We validated our algorithms using ground truth based on cursor positioning, confirming 100% accuracy. We also provide a new tool, TextTest++, to facilitate web-based evaluations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "The Information School"
            }
          ],
          "personId": 13395
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "The Information School"
            }
          ],
          "personId": 21890
        }
      ],
      "sessionIds": [
        1447
      ],
      "eventIds": []
    },
    {
      "id": 6994,
      "typeId": 11339,
      "title": "SWISH: A Shifting-Weight Interface of Simulated Hydrodynamics for Haptic Perception of Virtual Fluid Vessels",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Current VR/AR systems are unable to reproduce the physical sensation of fluid vessels, due to the shifting nature of fluid motion. To this end, we introduce SWISH, an ungrounded mixed-reality interface, capable of affording the users a realistic haptic sensation of fluid behaviors in vessels. The chief mechanism behind SWISH is in the use of virtual reality tracking and motor actuation to actively relocate the center of gravity of a handheld vessel, emulating the moving center of gravity of a handheld vessel that contains fluid. In addition to solving challenges related to reliable and efficient motor actuation, our SWISH designs place an emphasis on reproducibility, scalability, and availability to the maker culture. \r\n\r\nOur virtual-to-physical coupling uses Nvidia Flex’s Unity integration for virtual fluid dynamics with a 3D printed augmented vessel containing a motorized mechanical actuation system. To evaluate the effectiveness and perceptual efficacy of SWISH, we conduct a user study with 24 participants, 7 vessel actions, and 2 virtual fluid viscosities in a virtual reality environment. In all cases, the users on average reported that the SWISH bucket generates accurate tactile sensations for the fluid behavior. This opens the potential for multi-modal interactions with programmable fluids in virtual environments for chemistry education, worker training, and immersive entertainment. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Arts, Media and Engineering"
            }
          ],
          "personId": 19297
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Electrical, Computer and Energy Engineering"
            }
          ],
          "personId": 22584
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "TEMPE",
              "institution": "Arizona State University",
              "dsl": "School of Electrical, Computer and Energy Engineering"
            }
          ],
          "personId": 18941
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Arts, Media and Engineering"
            }
          ],
          "personId": 22979
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Arts, Media and Engineering"
            },
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Electrical, Computer and Energy Engineering"
            }
          ],
          "personId": 12822
        }
      ],
      "sessionIds": [
        2494
      ],
      "eventIds": []
    },
    {
      "id": 4307,
      "typeId": 11339,
      "title": "Mise-Unseen: Using Eye Tracking to Hide Virtual Reality Scene Changes in Plain Sight ",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Creating or arranging objects at runtime is needed in many virtual reality applications, but such changes are noticed when they occur inside the user’s field of view. We present Mise-Unseen, a software system that applies such scene changes covertly inside the user’s field of view. Mise-Unseen leverages gaze tracking to create models of user attention, intention, and spatial memory to determine if and when to inject a change. We present seven applications of Mise-Unseen to unnoticeably modify the scene within view (i) to hide that task difficulty is adapted to the user, (ii) to adapt the experience to the user’s preferences, (iii) to time the use of low fidelity effects, (iv) to detect user choice for passive haptics even when lacking physical props, (v) to sustain physical locomotion despite a lack of physical space, (vi) to reduce motion sickness during virtual locomotion, and (vii) to verify user understanding during story progression. We evaluated Mise-Unseen and our applications in a user study with 15 participants and find that while gaze data indeed supports obfuscating changes inside the field of view, a change is rendered unnoticeably by using gaze in combination with common masking techniques.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Human Computer Interaction"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 9265
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 13723
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 10723
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 21833
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 19421
        }
      ],
      "sessionIds": [
        2494
      ],
      "eventIds": []
    },
    {
      "id": 6870,
      "typeId": 11339,
      "title": "ElastImpact: 2.5D Multilevel Instant Impact Using Elasticity on Head-Mounted Displays",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Impact is a common effect in both daily life and virtual reality (VR) experiences, e.g., being punched, hit or bumped. Impact force is instantly produced, which is distinct from other force feedback, e.g., push and pull. We propose ElastImpact to provide 2.5D instant impact on a head-mounted display (HMD) for realistic and versatile VR experiences. ElastImpact consists of three impact devices. Each device uses a DC motor to extend an elastic band and block it with a mechanical brake to store the impact power. When releasing the brake, it provides impact instantly. Two impact devices are affixed on both sides of the head and connected with the HMD to provide the normal direction impact toward the face (i.e., 0.5D in z-axis). The other device is connected with a proxy collider and barrel in front of the HMD and rotated by a motor in the tangential plane of the face to provide 2D impact (i.e., xy-plane). By performing a just-noticeable difference (JND) study, we realize users’ impact force perception distinguishability on the heads in the normal direction and tangential plane, separately. Based on the results, we combine normal and tangential normal as 2.5D impact, and performed a VR experience study to verify that the 2.5D impact from ElastImpact significantly enhances realism.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 12577
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 22165
        }
      ],
      "sessionIds": [
        1489
      ],
      "eventIds": []
    },
    {
      "id": 4059,
      "typeId": 11339,
      "title": "Videostrates: Collaborative, Distributed and Programmable Video Manipulation",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present Videostrates, a concept and prototype system that enables real-time collaborative editing of both recorded material and live video streams. Videostrates supports live and recorded video composition with a declarative HTML-based notation, as well as creating both simple and sophisticated editing tools that can be combined and used collaboratively. Videostrates is programmable and unleashes the power of the modern web platform for video manipulation. We demonstrate its potential through three use scenarios: collaborative video editing with multiple tools and devices; orchestration of multiple live streams that are recorded and broadcast to a popular streaming platform; and programmatic creation of video using WebGL and shaders for blue screen effects. These scenarios only scratch the surface of the potential of Videostrates, which generates a new interaction paradigm for collaborative video manipulation with a fully programmable interface.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Digital Design and Information Studies"
            }
          ],
          "personId": 22389
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Centre for Digital Creativity"
            }
          ],
          "personId": 21855
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Center for Advanced Visualisation and Interaction"
            }
          ],
          "personId": 21886
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Center for Advanced Visualisation and Interaction"
            }
          ],
          "personId": 11339
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Univ. Paris-Sud, CNRS, Inria, Université Paris-Saclay",
              "dsl": "LRI"
            }
          ],
          "personId": 19792
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Université Paris-Saclay",
              "dsl": "LRI"
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Inria",
              "dsl": "ExSitu"
            }
          ],
          "personId": 25079
        }
      ],
      "sessionIds": [
        1136
      ],
      "eventIds": []
    },
    {
      "id": 5851,
      "typeId": 11339,
      "title": "Interactive 360-Degree Glasses-Free Tabletop 3D Display",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present an interactive 360-degree tabletop display system for collaborative work around a round table. Users are able to see 3D objects on the tabletop display anywhere around the table without 3D-glasses.The system uses a visual perceptual mechanism for smooth motion parallax in the horizontal direction with fewer projectors than previous works.A 360-degree camera mounted above the table and an image recognition detect users' positions around the table and the heights of their faces (eyes) as they move around the table in real-time.Those mechanics help display correct vertical and horizontal direction motion parallax for different users simultaneously.Our system also has a user interaction function with a tablet device that manipulates 3D objects displayed on the table.This function supports collaborative work and communication between users. We implemented a prototype system and demonstrated the collaborative features of the 360-degree tabletop display system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Hikarino-oka 1-1, Yokosuka-shi",
              "dsl": "NTT Service Evolution Laboratories, NTT Corporation"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Hokkaido",
              "institution": "Kita 14, Nishi 9, Kita-ku, Sapporo-shi",
              "dsl": "Human Computer Interaction Laboratory, Graduate School of Information Science and Technology, Hokkaido University"
            }
          ],
          "personId": 16922
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hokkaido",
              "city": "Sapporo",
              "institution": "Hokkaido University",
              "dsl": "Human-Computer Interaction Lab"
            }
          ],
          "personId": 20992
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "State/Province",
              "city": "Yokosuka-shi",
              "institution": "NTT Service Evolution Laboratories, NTT Corporation",
              "dsl": ""
            }
          ],
          "personId": 23886
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sapporo-shi",
              "institution": "Hokkaido University",
              "dsl": "Hokkaido University"
            }
          ],
          "personId": 22129
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sapporo",
              "institution": "Hokkaido University",
              "dsl": "Division of Computer Science and Information Technology"
            }
          ],
          "personId": 23262
        }
      ],
      "sessionIds": [
        2137
      ],
      "eventIds": []
    },
    {
      "id": 8030,
      "typeId": 11339,
      "title": "RFTouchPads: Batteryless and Wireless Modular Touch Sensor Pads Based on RFID",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents RFTouchPads, a system of battery-less wireless modular hardware designs of 2D touch sensor pads based on ultra-high frequency (UHF) radio-frequency identification (RFID) technology. We connect an antenna to multiple touch-sensing ID modules in parallel to provide 2D touch input with a single antenna. Each touch-sensing RFID module connects only one of its endpoints to the antenna, so it is normally off as it gets insufficient energy to operate. When a finger touches the circuit trace that is attached to an endpoint of a chip, the finger functions as another half of the antenna that triggers the connected chip on, and then the presence of its ID indicates the finger touch location. We proposed two hardware designs, StickerPad and TilePad, based on this principle. StickerPad is a flexible 3 × 3 touch sensing pad that allows for the applications on curved surfaces, such as the human body. TilePad is a modular 3 × 3 touch sensing pad that supports modular area expansion by tiling and provides more flexible deployment as its antenna is folded. Our implementation of this design allows 2D touch input to be reliability detected from 2 m away from a remote antenna of an RFID reader. The proposed battery-less, wireless and extensible modular hardware design provides fine-grained, less constrained 2D touch inputs in various ubiquitous computing applications  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 14864
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 23648
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 11934
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "Communication and Multimedia Lab",
              "dsl": "National Taiwan University"
            }
          ],
          "personId": 13501
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 12426
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 22165
        }
      ],
      "sessionIds": [
        1481
      ],
      "eventIds": []
    },
    {
      "id": 7518,
      "typeId": 11339,
      "title": "View-Dependent Video Textures for 360° Video",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "A major concern for filmmakers creating 360° video is ensuring that the viewer does not miss important narrative elements because they are looking in the wrong direction. This paper introduces gated clips which do not play past a gate time until a filmmaker-defined viewer gaze condition is met, such as looking at the region of interest (ROI). Until the condition is met, we seamlessly loop playback using view-dependent video textures, which extend standard video textures by adapting the looping behavior to the portion of the scene in the viewer’s field of view.   We use our desktop GUI to edit live action and computer animated 360◦videos, and show them to casual viewers. Study participants prefer our looping videos over the standard versions and are able to successfully see all of the looping videos’ ROIs without fear of missing anything.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Stanford University"
            }
          ],
          "personId": 22015
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 12568
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe",
              "dsl": ""
            }
          ],
          "personId": 16701
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe",
              "dsl": ""
            }
          ],
          "personId": 8590
        }
      ],
      "sessionIds": [
        1136
      ],
      "eventIds": []
    },
    {
      "id": 4192,
      "typeId": 11339,
      "title": "Photo-Chromeleon: Re-Programmable Multi-Color Textures Using Photochromic Dyes",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present a method to create re-programmable multi-color textures that are made from a single material only. The key idea builds on the use of photochromic inks that can switch their appearance from transparent to colored when exposed to the light of a certain wavelength. By mixing cyan, magenta, and yellow (CMY) photochromic dyes into a single solution and leveraging the different absorption spectra of each dye, we can control each color channel in the solution separately. Our approach can transform single-material fabrication techniques, such as coating, into high-resolution multi-color processes. \r\n\r\nWe discuss the material mixing procedure, modifications to the light source, and the algorithm to control each color channel. We then show the results from an experiment in which we evaluated the available color space and the resolution of our textures. Finally, we demonstrate our user interface that allows users to transform virtual textures onto physical objects and show a range of application examples.\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 8879
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": "Bristol Interaction Group"
            }
          ],
          "personId": 9266
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 15419
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 19245
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 23530
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 8347
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT CSAIL",
              "dsl": ""
            }
          ],
          "personId": 18006
        }
      ],
      "sessionIds": [
        2256
      ],
      "eventIds": []
    },
    {
      "id": 3680,
      "typeId": 11339,
      "title": "INVANER: INteractive VAscular Network Editing and Repair",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Vascular network visualization is an essential aspect of the daily practice of medical doctors working with vascular systems. \r\nAccurately representing vascular networks, not only graphically but also in a way that encompasses their structure, can be used to\r\nrun simulations, plan medical procedures or identify real-life defects, for example.\r\nA vascular network is thus constructed from a 3D medical image sequence via segmentation and skeletonization.\r\nMany automatic algorithms exist to do so but tend to fail for specific corner cases.\r\nOn the other hand, manual methods exist as well but are tedious to use and require a lot of time.\r\nIn this paper, we introduce an interactive vascular network skeletonization system called INVANER that relies on a graph-like representation of the network's structure.\r\nA general skeleton is obtained with an automatic method and medical practitioners are allowed to manually repair the local defects where this method fails.\r\nOur system uses graph-related tools with local effects and introduces two novel tools, dedicated to solving two common problems arising when automatically extracting the centerlines of vascular structures: so-called \"Kissing Vessels\" and a type of phenomenon we call \"Dotted Vessels.\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Vaud",
              "city": "Lausanne",
              "institution": "Ecole Polytechnique Fédérale de Lausanne",
              "dsl": ""
            }
          ],
          "personId": 12791
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 11920
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Department of Creative Informatics"
            }
          ],
          "personId": 9246
        }
      ],
      "sessionIds": [
        2050
      ],
      "eventIds": []
    },
    {
      "id": 3299,
      "typeId": 11339,
      "title": "Loki: Facilitating Remote Instruction of Physical Tasks Using Bi-Directional Mixed-Reality Telepresence",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Remotely instructing and guiding users in physical tasks has offered the promise of training surgeons remotely, guiding complex repair tasks by an expert, or enabling novel learning workflows. While it has been the subject of many research projects,  current  approaches  are  often  limited  in  the communication   bandwidth   (lacking   context,   spatial information) or interactivity (unidirectional, asynchronous) between expert and learner. To address some of these issues we explore the design space of bi-directional mixed-reality telepresence systems for teaching physical tasks, and present Loki, a novel system which explores the various dimensions of this space. Loki leverages video, audio and spatial capture along with mixed reality presentation methods to allow users to explore and annotate the local and remote environments, and record and review their own performance as well as their peer’s. In this paper, we explore the design space of mixed reality telepresence for physical tasks, contribute the system design  of  Loki  which  enables  transitions  between  the elements of this design space, and validate its utility through a varied set of scenarios",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": ""
            }
          ],
          "personId": 17903
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 17851
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 18443
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 21283
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Autodesk Research",
              "dsl": ""
            }
          ],
          "personId": 15374
        }
      ],
      "sessionIds": [
        1883
      ],
      "eventIds": []
    },
    {
      "id": 4451,
      "typeId": 11339,
      "title": "Proxino: Enabling Prototyping of Virtual Circuits With Physical Proxies",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We propose blending the virtual and physical worlds for prototyping circuits using physical proxies. With physical proxies, real-world components (e.g. a motor, or light sensor) can be used with a virtual counterpart for a circuit designed in software. We demonstrate this concept in Proxino, and elucidate the new scenarios it enables for makers, such as remote collaboration with physically distributed electronics components. We compared our hybrid system and its output with designs of real circuits to determine the difference through a system evaluation and observed minimal differences. We then present the results of an informal study with 9 users, where we gathered feedback on the effectiveness of our system in different working conditions (with a desktop, using a mobile, and with a remote collaborator). We conclude by sharing our lessons learned from our system and discuss directions for future research that blend physical and virtual prototyping for electronic circuits.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "West Lebanon",
              "institution": "Dartmouth College",
              "dsl": "XDiscovery Lab"
            }
          ],
          "personId": 10905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 22695
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 14215
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 18975
        }
      ],
      "sessionIds": [
        1064
      ],
      "eventIds": []
    },
    {
      "id": 3684,
      "typeId": 11339,
      "title": "Masque: Exploring Lateral Skin Stretch Feedback on the Face with Head-Mounted Displays",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We propose integrating an array of skin stretch modules with an head-mounted display (HMD) to provide two-dimensional skin stretch feedback on the user’s face. Skin stretch has been found effective to induce the perception of force (e.g. weight or inertia) and to enable directional haptic cues. However, its potential as an HMD output for virtual reality (VR) remains to be exploited. Our explorative study firstly investigated the design of shear tactors. Based on our results, Masque has been implemented as an HMD prototype actuating six shear tactors positioned on the HMD’s face interface. A comfort study was conducted to ensure that skin stretches generated by Masque are acceptable to all participants. The following two perception-based studies examined the minimum changes in skin stretch distance and stretch angles that are detectable by participants. The results help us to design haptic profiles as well as our prototype applications. Finally, the user evaluation indicates that participants welcomed Masque and regarded skin stretch feedback as a worthwhile addition to HMD output.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University of  Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 20388
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 14909
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 19112
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 19172
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 18960
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "HsinChu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 19461
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 17551
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 22165
        }
      ],
      "sessionIds": [
        1489
      ],
      "eventIds": []
    },
    {
      "id": 6758,
      "typeId": 11339,
      "title": "InfoLED: Augmenting LED Indicator Lights for Device Positioning and Communication",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented Reality (AR) has the potential to expand our capability for interacting with and comprehending our surrounding environment.  However, current AR devices treat electronic appliances no different than common non-interactive objects, which substantially limits the functionality of AR.  We present InfoLED, a positioning and communication system based on indicator light that enables appliances to transmit their location, device IDs, and status information to the AR client, without changing their visual design.  By leveraging human insensitivity to high frequency brightness flickering, InfoLED transmits all those information without disturbing the original functionality as an indicator light.  We envision InfoLED can be used in three categories of application: malfunctioning device diagnosis, appliances control, and multi-appliances configuration.\r\n\r\nWe conducted three user study, measuring the performance of the InfoLED system, the human readability of the patterns and colors displayed on the InfoLED, and users' overall preference for InfoLED.  The study results showed that InfoLED can work properly from a distance up to 7 meters in an indoor condition and it did not interfere with our participants' ability in comprehending the high-level patterns and colors of the indicator light. Overall, study subjects preferred InfoLED to an ArUco-based baseline system, and reported less cognitive load when using our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 9209
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 25064
        }
      ],
      "sessionIds": [
        1883
      ],
      "eventIds": []
    },
    {
      "id": 3047,
      "typeId": 11339,
      "title": "Supporting Elder Connectedness through Cognitively Sustainable Design Interactions with the Memory Music Box",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Isolation is one of the largest contributors to a lack of wellbeing, increased anxiety and loneliness in older adults. In collaboration with elders in living facilities, we designed the\r\nMemory Music Box: a low-threshold platform to increase connectedness. The research community has contributed notable research in support of elders through monitoring, tracking and memory augmentation. Despite the Information and Communication Technologies field (ICT) advances in providing new opportunities for connection, challenges in accessibility increase the gap between elders and their loved ones. We\r\napproach this challenge by embedding a familiar form factor with innovative applications, performing design evaluations with our key target group and incorporating multi-iteration\r\nlearnings. These findings culminate in a novel design that facilitates elders in crossing technology and communication barriers. Based on these findings, we discuss how future inclusive technologies for the older adults’ need to balance ease of use, subtlety and Cognitively Sustainable Design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 13662
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 13725
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 16524
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 9548
        }
      ],
      "sessionIds": [
        1123
      ],
      "eventIds": []
    },
    {
      "id": 3816,
      "typeId": 11339,
      "title": "Ohmic-Sticker: Force-to-Motion Type Input Device that Extends Capacitive Touch Surface",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We propose ``Ohmic-Sticker'', a novel force-to-motion type input device to extend capacitive touch surfaces. It realizes various types of force-sensitive inputs by simply attaching on to commercial touchpads or touchscreens. A simple force-sensitive-resistor (FSR)-based structure enables thin (less than 2 mm) form factors and battery-less operation. The applied force vector is detected as the leakage current from the corresponding touch surface electrodes by using Ohmic-Touch technology. Ohmic-Sticker can be used for adding force-sensitive interactions to touch surfaces, such as analog push buttons, TrackPoint-like devices, and full 6 DoF controllers for navigating virtual spaces. In this paper, we report a series of investigations on the design requirements of Ohmic-Sticker and some prototypes.We also evaluate the performance of Ohmic-Sticker as a pointing device. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chiyoda-ku",
              "institution": "Yahoo! Japan",
              "dsl": ""
            }
          ],
          "personId": 12967
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 11027
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Ochanomizu University",
              "dsl": ""
            }
          ],
          "personId": 21920
        }
      ],
      "sessionIds": [
        1864
      ],
      "eventIds": []
    },
    {
      "id": 4713,
      "typeId": 11339,
      "title": "Learning Cooperative Personalized Policies from Gaze Data",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "An ideal Mixed Reality (MR) system would only present virtual\r\ninformation (e.g., a label) when it is useful to the person.\r\nHowever, figuring out when a label is useful is challenging;\r\nit depends on a variety of factors, including the current task,\r\nprevious knowledge, context, etc. In this paper, we propose a\r\nReinforcement Learning (RL) method to learn when to show\r\nor hide an object’s label given eye movement data. We demonstrate\r\nthe capabilities of this approach by showing that an intelligent\r\nagent can learn cooperative policies that better support\r\nusers in a visual search task than design heuristics. Furthermore,\r\nwe show the applicability of our approach in realistic\r\nenvironments and use cases (e.g., grocery shopping). By posing\r\nMR object labeling as an RL control problem we can learn\r\npolicies implicitly by observing users’ behavior without requiring\r\nexperience sampling or any other form of supervision.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 19511
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 17623
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 20795
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 16401
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 25074
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Advanced Interactive Technologies Lab"
            }
          ],
          "personId": 11460
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 8507
        }
      ],
      "sessionIds": [
        1489
      ],
      "eventIds": []
    },
    {
      "id": 6122,
      "typeId": 11339,
      "title": "3D Printed Fabric: Techniques for Design and 3D Weaving Programmable Textiles",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present a 3D printing technique for a soft and flexible textile fabrication using a consumer grade fused deposition modeling (FDM) printer. By controlling the movement of the print header, the FDM alternately weaves the stringing fibers across a row of pillars. Owing to the structure of the fibers, which support and strengthen the pillars from each side, a thin and flexible sheet of fabric can be printed upright while the fibers are being weaved. In addition, this technique enables users to employ two material colors to design a pattern and prototype an interactive object through a variety of off-the-shelf material properties such as conductive filament. We detail a technique for weaving a textile and introduce a list of parameters that enable users to design various textiles. We demonstrate examples to show the feasibility of our approach and numerous applications to integrate printed textiles in a custom object design. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 11692
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Computer Science"
            }
          ],
          "personId": 18781
        }
      ],
      "sessionIds": [
        2131
      ],
      "eventIds": []
    },
    {
      "id": 7403,
      "typeId": 11339,
      "title": "CapstanCrunch: A Haptic VR Controller with User-supplied Force Feedback",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce CapstanCrunch, a force resisting, ungrounded haptic controller that renders haptic feedback for touching and grasping both rigid and compliant objects in a VR environment. In contrast to previous controllers, CapstanCrunch renders human-scale forces without the use of large, high force, electrically power consumptive and expensive actuators. Instead, CapstanCrunch integrates a friction-based capstan-plus-cord variable-resistance brake mechanism that is dynamically controlled by a small internal motor. The capstan mechanism magnifies the motor’s force by a factor of around 40. Compared to active force control devices, it is low cost, low electrical power, robust, safe, fast and quiet, while providing high force control to user interaction. We describe the design and implementation of CapstanCrunch and demonstrate its use in a series of VR scenarios. Finally, we evaluate the performance of CapstanCrunch in two user studies and compare our controller with an active haptic controller with the ability to simulate different levels of convincing object rigidity and/or compliance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft",
              "dsl": "Microsoft Research"
            }
          ],
          "personId": 21377
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 10723
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 21833
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": ""
            }
          ],
          "personId": 19421
        }
      ],
      "sessionIds": [
        2494
      ],
      "eventIds": []
    },
    {
      "id": 4332,
      "typeId": 11339,
      "title": "Mallard: Turn the Web into a Contextualized Prototyping Environment for Machine Learning",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Machine learning (ML) can be hard to master, but what first trips up novices is something much more mundane: the incidental complexities of installing and configuring software development environments. Everyone has a web browser, so can we let people experiment with ML within the context of any webpage they visit? This paper's contribution is the idea that the web can serve as a contextualized prototyping environment for ML by enabling analyses to occur within the context of data on actual webpages rather than in isolated silos. We realized this idea by building Mallard, a browser extension that scaffolds acquiring and parsing web data, prototyping and debugging ML models, and augmenting webpages with ML-driven results and interactions. To demonstrate the versatility of Mallard, we performed a case study where we used it to prototype nine ML-based browser apps, including augmenting Amazon and Twitter websites with sentiment analysis, augmenting restaurant menu websites with OCR-based search, style transfer on Google image search results, and using real-time face tracking to control a Pac-Man game. These case studies show that Mallard is capable of supporting a diverse range of hobbyist-level ML prototyping projects.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 17687
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Cognitive Science"
            }
          ],
          "personId": 9263
        }
      ],
      "sessionIds": [
        1632
      ],
      "eventIds": []
    },
    {
      "id": 6254,
      "typeId": 11339,
      "title": "Resized Grasping in VR: Estimating Thresholds for Object Discrimination",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Previous work in VR has demonstrated how individual physical objects can represent multiple virtual objects in different locations by redirecting the user's hand. We show how individual objects can represent multiple virtual objects of different sizes by resizing the user's grasp. We redirect the positions of the user's fingers by visual translation gains, inducing an illusion that can make physical objects seem larger or smaller. We present a discrimination experiment to estimate the thresholds of resizing virtual objects from physical objects, without the user reliably noticing a difference. The results show that the size difference is easily detected when a physical object is used to represent an object less than 90% of its size. When physical objects represent larger virtual objects, however, then scaling is tightly coupled to the physical object's size: smaller physical objects allow more virtual resizing (up to a 50% larger virtual size). Resized Grasping considerably broadens the scope of using illusions to provide rich haptic experiences in virtual reality. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 13136
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 9352
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": ""
            }
          ],
          "personId": 16253
        }
      ],
      "sessionIds": [
        2050
      ],
      "eventIds": []
    },
    {
      "id": 3054,
      "typeId": 11339,
      "title": "KnitPick: Programming and Modifying Complex Knitted Textures for Machine and Hand Knitting",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": " Knitting creates complex, soft objects with unique and controllable texture properties that can be used to create interactive objects.\r\n However, little work addresses the challenges of using knitted textures.\r\n We present KnitPick: a pipeline for interpreting pre-existing hand-knitting texture patterns into a directed-graph representation of knittable structures (KnitGraphs) which can be output to machine and hand-knitting instructions.\r\n Using KnitPick, we contribute a measured and photographed data set of \\totaltextures{} knitted textures. Based on findings from this data set, we contribute two algorithms for manipulating KnitGraphs. KnitCarving shapes a graph while respecting a texture, and KnitPatching combines graphs with disparate textures while maintaining a consistent shape. Using these algorithms and textures in our data set we are able to create three Knitting based interactions: roll, tug, and slide. KnitPick is the first system to bridge the gap between hand- and machine-knitting when creating complex knitted textures.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon Universtiy",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 25058
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 24233
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 21329
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 12351
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 25070
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 20695
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 25062
        }
      ],
      "sessionIds": [
        2131
      ],
      "eventIds": []
    },
    {
      "id": 7151,
      "typeId": 11339,
      "title": "Mercury: Empowering Programmers' Mobile Practices with Microproductivity",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "There has been considerable research on how software can enhance programmers' productivity within their workspace. In this paper, we instead explore how software might help programmers make productive use of their time while away from their workspace. We interviewed 10 software engineers and surveyed 78 others and found that while programmers often do work while mobile, their existing mobile work practices are primarily exploratory (e.g., capturing thoughts or performing online research). In contrast, they want to be doing work that is more grounded in their existing code (e.g., code review or bug triage). Based on these findings, we introduce Mercury, a system that guides programmers in making progress on-the-go with auto-generated microtasks derived from their source code's current state. A study of Mercury with 20 programmers revealed that they could make meaningful progress with Mercury while mobile with little effort or attention. Our findings suggest an opportunity exists to support the continuation of programming tasks across devices and help programmers resume coding upon returning to their workspace.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 25071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 14181
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 11162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 8986
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 13090
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": "Microsoft Research AI"
            }
          ],
          "personId": 13573
        }
      ],
      "sessionIds": [
        1064
      ],
      "eventIds": []
    },
    {
      "id": 7920,
      "typeId": 11339,
      "title": "Tip-Tap: Battery-free Discrete 2D Fingertip Input",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We describe Tip-Tap, a wearable input technique that can be implemented without batteries using a custom RFID tag. It recognizes 2-dimensional discrete touch events by sensing the intersection between two arrays of  contact points: one array along the index fingertip and the other along the thumb tip. A formative study identifies locations on the index finger that are reachable by different parts of the thumb tip, and the results determine the pattern of contacts points used for the technique. Using a reconfigurable 3 by 3 evaluation device, a second study shows eyes-free accuracy is 86% after a very short period, and adding bumpy or magnetic passive haptic feedback to contacts is not necessary. Finally, two battery-free prototypes using a new RFID tag design demonstrates how Tip-Tap can be implemented in a glove or tattoo form factor. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "National Research Council Canada",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 12978
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "waterloo",
              "institution": "university of waterloo",
              "dsl": ""
            }
          ],
          "personId": 22225
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 16807
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 16645
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 23299
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 18610
        }
      ],
      "sessionIds": [
        1864
      ],
      "eventIds": []
    },
    {
      "id": 5104,
      "typeId": 11339,
      "title": "The Memory Palace: Exploring Visual-Spatial Paths for Strong, Memorable, Infrequent Authentication",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Many accounts and devices require only infrequent authentication by an individual, and thus authentication secrets should be both secure and memorable without much reinforcement. Inspired by people's strong visual-spatial memory, we introduce a novel system to help address this problem: the Memory Palace. The Memory Palace encodes authentication secrets as paths through a 3D virtual labyrinth navigated in the first-person perspective. We ran two experiments to iteratively design and evaluate the Memory Palace. In the first, we found that visual-spatial secrets are most memorable if navigated in a 3D first-person perspective. In the second, we comparatively evaluated the Memory Palace against Android's 9-dot pattern lock along three dimensions: memorability after one week, resilience to shoulder surfing, and speed. We found that relative to 9-dot, complexity-controlled secrets in the Memory Palace were significantly more memorable after one week, were much harder to break through shoulder surfing, and were not significantly slower to enter.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 21918
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 18946
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 13220
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh ",
              "institution": "Human-Computer Interaction",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 15111
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 25065
        }
      ],
      "sessionIds": [
        2068
      ],
      "eventIds": []
    },
    {
      "id": 7921,
      "typeId": 11339,
      "title": "MeCap: Whole-Body Digitization for Low-Cost VR/AR Headsets",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Low-cost, smartphone-powered VR/AR headsets are becoming more popular. These basic devices – little more than plastic or cardboard shells – lack advanced features, such as controllers for the hands, limiting their interactive capability. Moreover, even high-end consumer headsets lack the ability to track the body and face. For this reason, interactive experiences like social VR are underdeveloped. We introduce MeCap, which enables commodity VR headsets to be augmented with powerful motion capture (“MoCap”) and user-sensing capabilities at very low cost (under $5). Using only a pair of hemispherical mirrors and the existing rear-facing camera of a smartphone, MeCap provides real-time estimates of a wearer’s 3D body pose, hand pose, facial expression, physical appearance and surrounding environment – capabilities which are either absent in contemporary VR/AR systems or which require specialized hardware and controllers. We evaluate the accuracy of each of our tracking features, the results of which show imminent feasibility.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 21851
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 18694
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 24301
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 19438
        }
      ],
      "sessionIds": [
        1883
      ],
      "eventIds": []
    },
    {
      "id": 4340,
      "typeId": 11339,
      "title": "Pull-Ups: Enhancing Suspension Activities in Virtual Reality with Body-Scale Kinesthetic Force Feedback",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We present Pull-Ups, a suspension kit that can suggest a range of body postures and thus enables various exercise styles of users perceiving the kinesthetic force feedback by suspending their weight with arm exertion during the interaction. Pull-Ups actuates the user's body to move up to 15 cm by pulling his or her hands using a pair of pneumatic artificial muscle groups. Our studies informed the discernible kinesthetic force feedbacks that were then exploited for the design of kinesthetic force feedback in three physical activities: kitesurfing, paragliding, and space invader. Our final study on user experiences suggested that a passive suspension kit alone added substantially to users' perceptions of realism and enjoyment (all above neutral) with passive physical support, while sufficient active feedback can further level them up. In addition, we found that both passive and active feedback of the suspension kit significantly reduced motion sickness in simulated kitesurfing and paragliding compared to when no suspension kit (thus no feedback) was provided. This work suggests that a passive suspension kit is cost-effective as a home exercise kit, while active feedback can further level up user experience, though at the cost of the installation (e.g., an air compressor in our prototype).\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": "Institute of Multimedia Engineering"
            }
          ],
          "personId": 12294
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "Institute of Multimedia Engineering",
              "dsl": "National Chiao Tung University"
            }
          ],
          "personId": 20344
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "Computer Science",
              "dsl": "National Chiao Tung University"
            }
          ],
          "personId": 17816
        }
      ],
      "sessionIds": [
        2494
      ],
      "eventIds": []
    },
    {
      "id": 6260,
      "typeId": 11339,
      "title": "LabelAR: A Spatial Guidance Interface for Fast Computer Vision Image Collection",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Computer vision is applied in an ever expanding range of applications, many of which require custom training data to perform well.  We present a novel interface for rapid collection and labeling of training images to improve computer vision based object detectors. LabelAR leverages the spatial tracking capabilities of an AR-enabled camera, allowing users to place persistent bounding volumes that stay centered on real-world objects. The interface then guides the user to move the camera to cover a wide variety of viewpoints. We eliminate the need for post-hoc manual labeling of images by automatically projecting 2D bounding boxes around objects in the images as they are captured from AR-marked viewpoints. Across 12 users, LabelAR significantly outperforms existing approaches in terms of the trade-off between model performance and collection time. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 21901
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 13191
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 16176
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 20392
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 21283
        }
      ],
      "sessionIds": [
        1481
      ],
      "eventIds": []
    },
    {
      "id": 6904,
      "typeId": 11339,
      "title": "CAVRN: An Exploration and Evaluation of a Collective Audience Virtual Reality Nexus Experience",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "The virtual reality ecosystem has seen an immense gain in momentum around the gaming, entertainment, and enterprise markets, but is hampered by limitations in concurrent user count, throughput, and accessibility to mass audiences. Through analysis of the current state of the virtual reality ecosystem and relevant aspects of traditional media, we propose a set of design hypotheses for practical and effective seated virtual reality experiences of scale. Said hypotheses manifest in the Collective Audience Virtual Reality Nexus (CAVRN), a framework and management system for large-scale (30+ user) virtual reality deployment in a theater-like physical setting. A mixed methodology study of CAVE, an experience implemented using CAVRN, garners rich insights into the proposed hypotheses. Findings on content design, audience representation, and audience interaction are discussed at length.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 21308
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "NYU",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 20666
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "NYU",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 15448
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "CREATE Lab"
            }
          ],
          "personId": 11726
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 22303
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 23319
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Dept of Computer Science, NYU",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 13176
        }
      ],
      "sessionIds": [
        2068
      ],
      "eventIds": []
    },
    {
      "id": 7163,
      "typeId": 11339,
      "title": "Portal-ble: Intuitive Free-hand Manipulation in Unbounded Smartphone-based Augmented Reality",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "How  should  we  implement  direct  hand  manipulation  for smartphone-based augmented reality (AR)  in  larger  spaces?  We  conduct  an experience prototyping exercise to show that this may be harder than it might seem, as existing principles from prior literature were found to have issues with users.  We identify issues in perception (moving to the object, reaching for the object), manipulation (successfully grabbing and orienting the object), and behavioral understanding (knowing how to use the smartphone as a viewport).  Then, we introduce a combination of object-based feedback and accommodations to aid user interaction with virtual objects.   Our resulting system implementation, Portal-ble, was more successful than baseline approaches in a user study for grabbing static and moving objects and for object manipulation, and was less mentally demanding.  Put together, the Portal-ble system improves intuition and aids interacting with the virtual world in larger physical spaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 12024
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 16948
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "Computer Science"
            },
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 10747
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 10820
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 10704
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 12975
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 9708
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 9857
        }
      ],
      "sessionIds": [
        1883
      ],
      "eventIds": []
    },
    {
      "id": 6524,
      "typeId": 11339,
      "title": "CircuitStyle: A System for Peripherally Reinforcing  Best Practices in Hardware Computing",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Instructors of hardware computing face many challenges including maintaining awareness of student progress, allocating their time adequately between lecturing and helping individual students, and keeping students engaged even while debugging problems. Based on formative interviews with 5 electronics instructors, we found that many circuit style behaviors could help novice users prevent or efficiently debug common problems. Drawing inspiration from the software engineering practice of coding style, these circuit style behaviors consist of best-practices and guidelines for implementing circuit prototypes that do not interfere with the functionality of the circuit, but help a circuit be more readable, less error-prone, and easier to debug. To examine if these circuit style behaviors could be peripherally enforced, aid an in-person instructor’s ability to facilitate a workshop, and not monopolize instructor’s attention, we developed CircuitStyle, a teaching aid for in-person hardware computing workshops. To evaluate the effectiveness of our tool, we deployed our system in an in-person maker-space workshop. The instructor appreciated CircuitStyle’s ability to provide a broad understanding of the workshop’s progress and the potential for our system to help instructors of various backgrounds better engage and understand the needs of their classroom. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 24268
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 22695
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 9048
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 19359
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 18975
        }
      ],
      "sessionIds": [
        1064
      ],
      "eventIds": []
    },
    {
      "id": 3837,
      "typeId": 11339,
      "title": "Self-healing UI: Mechanically and Electrically Self-healing Materials for Sensing and Actuation Interfaces",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Living things in nature have long been utilizing the ability to “heal” their wounds on the soft bodies to survive in the outer environment. In order to impart this self-healing property to our daily life interface, we propose Self-healing UI, a soft-bodied interface that can intrinsically self-heal damages without external stimuli or glue. The key material to achieving Self-healing UI is MWCNTs-PBS, a composite material of a self-healing polymer polyborosiloxane (PBS) and a filler material multi-walled carbon nanotubes (MWCNTs), which retain mechanical and electrical self-healability. We developed a hybrid model that combines PBS, MWCNTs-PBS, and other common soft materials including fabric and silicone to build interface devices with self-healing, sensing, and actuation capability. These devices were implemented by layer-by-layer stacking fabrication without glue or any post-processing, by leveraging the materials’ inherent self-healing property between two layers. We then demonstrated sensing primitives and interactive applications that extend the design space of soft interface with their ability to transform, conform, reconfigure, heal, and fuse, which we believe can enrich the toolbox of human-computer interaction (HCI).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 12273
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University ",
              "dsl": "Electrical and Computing Engineering"
            }
          ],
          "personId": 13242
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh ",
              "institution": "Carnegie Mellon University ",
              "dsl": "Materials Science and Engineering"
            }
          ],
          "personId": 24145
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Materials Science and Engineering"
            }
          ],
          "personId": 20458
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 23310
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 17914
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Materials Science and Engineering",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 17938
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 14792
        }
      ],
      "sessionIds": [
        2090
      ],
      "eventIds": []
    },
    {
      "id": 5502,
      "typeId": 11339,
      "title": "Tactlets: Adding Tactile Feedback to 3D Objects Using Custom Printed Controls",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Rapid prototyping of tactile output on 3D objects promises to enable a more widespread use of the tactile channel for ubiquitous, tangible and wearable computing. Existing prototyping approaches, however, have limited tactile output capabilities, require advanced skills for design and fabrication, or are incompatible with curved object geometries. In this paper, we present a novel digital fabrication approach for printing custom, high-resolution controls for electro-tactile output with integrated touch sensing on interactive objects. It supports curved geometries of everyday objects.  We contribute a design tool for modeling, testing and refining tactile input and output at a high level of abstraction, based on parameterized tactile controls. We further contribute an inventory of 10 parametric  Tactlet controls that integrate sensing of user input with real-time tactile feedback. We present two approaches for printing Tactlets on 3D objects, using conductive inkjet printing or FDM 3D-printing. Empirical results from a psychophysical study and findings from two practical application cases confirm the functionality and practical feasibility of the Tactlets approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 12210
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 17467
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 16515
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 8753
        }
      ],
      "sessionIds": [
        1656
      ],
      "eventIds": []
    },
    {
      "id": 3326,
      "typeId": 11339,
      "title": "SensorSnaps: Integrating Wireless Sensor Nodes into Fabric Snap Fasteners for Textile Interfaces",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Adding electronics to textiles can be time-consuming and requires technical expertise. We introduce SensorSnaps, low-power wireless sensor nodes that seamlessly integrate into caps of fabric snap fasteners. SensorSnaps provide a new technique to quickly and intuitively augment any location on the clothing with sensors. SensorSnaps securely attach and detach from ubiquitous commercial snap fasteners. With inertial measurement units, the SensorSnaps detect tap and rotation gestures, as well as body motion tracking.  We optimized the power consumption for SensorSnaps to work continuously for 45 minutes and up to 4 hours in capacitive touch standby mode. We present applications where SensorSnaps are used as gesture interfaces for a music player controller, chorded keyboard, cursor control, and motion tracking suit. The user study showed that SensorSnap could be attached in around 71 seconds, similar to attaching off-the-shelf snaps and participants found the gestures easy to learn and perform.  SensorSnaps will allow anyone to add sophisticated sensing capacities to ubiquitous snap fasteners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT media lab"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": "Google Research"
            }
          ],
          "personId": 14081
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 16756
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 18873
        }
      ],
      "sessionIds": [
        2131
      ],
      "eventIds": []
    },
    {
      "id": 4991,
      "typeId": 11339,
      "title": "Aero-plane: a Handheld Force-Feedback Device that Renders Weight Motion Illusion on a Virtual 2D Plane",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "Force feedback is said to be the next frontier in virtual reality (VR). Recently, with consumers pushing forward with unthethered VR, researchers turned away from solutions based on bulky hardware (e.g., exoskeletons and robotic arms) and started exploring smaller portable or wearable devices. However, when it comes to rendering inertial forces, such as when moving a heavy object around or when interacting with objects with unique mass properties, current ungrounded force feedback devices are unable to provide quick weight shifting sensations that can realistically simulate weight changes over 2D surfaces. In this paper we introduce Aero-plane, a force-feedback handheld controller based on two miniature jet-propellers that can render shifting weights of up to 14 N within 0.3 seconds. Through two user studies we: (1) characterize the users’ ability to perceive and correctly recognize different motion paths on a virtual plane while using our device; and, (2) tested the level of realism and immersion of the controller when used in two VR applications (a rolling ball on a plane, and using kitchen tools of different shape and size). Lastly, we present a set of applications that further explore different usage cases and alternative form-factors for our device.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 14273
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 14791
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 22051
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 25069
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 18975
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 13605
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 9854
        }
      ],
      "sessionIds": [
        2494
      ],
      "eventIds": []
    },
    {
      "id": 4735,
      "typeId": 11339,
      "title": "Third-Person Piloting: Increasing Situational Awareness using a Spatially Coupled Second Drone",
      "trackId": 10574,
      "tags": [],
      "keywords": [],
      "abstract": "We propose Third-Person Piloting, a novel drone manipulation\r\ninterface that increases situational awareness using an interactive\r\nthird-person perspective from a second, spatially coupled\r\ndrone. The pilot uses a controller with a manipulatable miniature\r\ndrone. Our algorithm understands the relationship between\r\nthe pilot’s eye position and the miniature drone and ensures\r\nthat the same spatial relationship is maintained between\r\nthe two real drones in the sky. This allows the pilot to obtain\r\nvarious third-person perspectives by changing the orientation\r\nof the miniature drone while maintaining standard the primary\r\ndrone control using the conventional controller. We design and\r\nimplement a working prototype with programmable drones\r\nand propose several representative operation scenarios. We\r\ngather user feedback to obtain initial insights of our interface\r\ndesign from novices, advanced beginners, and experts. Result\r\nshows that our interface was positively evaluated by all of\r\nthem, and their feedback suggests the additional interactive\r\nthird-person perspective increases spatial awareness and helps\r\ntheir primary drone manipulation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Research Institute of Electrical Communication",
              "dsl": ""
            }
          ],
          "personId": 16661
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": ""
            }
          ],
          "personId": 25072
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 25067
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Smart System Institute",
              "dsl": "National University of Singapore"
            }
          ],
          "personId": 12663
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 25060
        }
      ],
      "sessionIds": [
        1898
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 14340,
      "firstName": "Rundong",
      "lastName": "Tian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18436,
      "firstName": "Haipeng",
      "lastName": "Mi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19461,
      "firstName": "Ruei-Che",
      "lastName": "Chang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12294,
      "firstName": "Yuan-Syun",
      "lastName": "Ye",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16393,
      "firstName": "Donghwi",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18443,
      "firstName": "George",
      "lastName": "Fitzmaurice",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15374,
      "firstName": "Tovi",
      "lastName": "Grossman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15376,
      "firstName": "Forrest",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16401,
      "firstName": "James",
      "lastName": "Hillis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16411,
      "firstName": "Lei",
      "lastName": "Yan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11291,
      "firstName": "Hugo",
      "lastName": "Romat",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11292,
      "firstName": "Yukang",
      "lastName": "Yan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9246,
      "firstName": "Hirofumi",
      "lastName": "Seo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12320,
      "firstName": "Xiaojun",
      "lastName": "Bi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15394,
      "firstName": "Yajie",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23592,
      "firstName": "Tzu-Sheng",
      "lastName": "Kuo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9263,
      "firstName": "Philip",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19504,
      "firstName": "Geehyuk",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9265,
      "firstName": "Sebastian",
      "lastName": "Marwecki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9266,
      "firstName": "Isabel",
      "lastName": "Qamar",
      "middleInitial": "P. S.",
      "affiliations": []
    },
    {
      "id": 19511,
      "firstName": "Christoph",
      "lastName": "Gebhardt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22584,
      "firstName": "Frank",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17467,
      "firstName": "Martin",
      "lastName": "Feick",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15419,
      "firstName": "Michael",
      "lastName": "Wessely",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12351,
      "firstName": "Jessica",
      "lastName": "Hodgins",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16448,
      "firstName": "Rubaiat Habib",
      "lastName": "Kazi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19524,
      "firstName": "Wen-Jie",
      "lastName": "Tseng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18505,
      "firstName": "Humphrey",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9289,
      "firstName": "Michael Xieyang",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11339,
      "firstName": "Rolf",
      "lastName": "Bagge",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21579,
      "firstName": "Robert",
      "lastName": "Lindeman",
      "middleInitial": "W.",
      "affiliations": []
    },
    {
      "id": 11340,
      "firstName": "Danny",
      "lastName": "Leen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13395,
      "firstName": "Mingrui",
      "lastName": "Zhang",
      "middleInitial": "Ray",
      "affiliations": []
    },
    {
      "id": 21588,
      "firstName": "Yuhang",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13399,
      "firstName": "Karan",
      "lastName": "Singh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15448,
      "firstName": "Nicholas Gregory",
      "lastName": "Vitovitch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19544,
      "firstName": "Shantanu",
      "lastName": "Thakurdesai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17497,
      "firstName": "Cynthia",
      "lastName": "Taylor",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23644,
      "firstName": "Sungchul",
      "lastName": "Jung",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17501,
      "firstName": "Tianyi",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23647,
      "firstName": "Seongkook",
      "lastName": "Heo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22624,
      "firstName": "Aws",
      "lastName": "Albarghouthi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23648,
      "firstName": "Jr-Ling",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15458,
      "firstName": "Jaeyeon",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8292,
      "firstName": "Ryo",
      "lastName": "Suzuki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9323,
      "firstName": "Juan Pablo",
      "lastName": "Forero Cortes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9324,
      "firstName": "Yizheng",
      "lastName": "Gu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21614,
      "firstName": "Hideki",
      "lastName": "Koike",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13423,
      "firstName": "Justin",
      "lastName": "Jia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17530,
      "firstName": "Sang Ho",
      "lastName": "Yoon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13442,
      "firstName": "Ke",
      "lastName": "Huo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16515,
      "firstName": "Anusha",
      "lastName": "Withana",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11395,
      "firstName": "Xiaoying",
      "lastName": "Wei",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11396,
      "firstName": "Daniel",
      "lastName": "Leithinger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9352,
      "firstName": "Aske",
      "lastName": "Mottelson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24713,
      "firstName": "Emily",
      "lastName": "Whiting",
      "affiliations": []
    },
    {
      "id": 12426,
      "firstName": "Rong-Hao",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24714,
      "firstName": "Elena",
      "lastName": "Glassman",
      "affiliations": []
    },
    {
      "id": 16524,
      "firstName": "Janelle",
      "lastName": "Sands",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17551,
      "firstName": "Jo-Yu",
      "lastName": "Lo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24721,
      "firstName": "Scott",
      "lastName": "Klemmer",
      "affiliations": []
    },
    {
      "id": 21651,
      "firstName": "Chi-Huan",
      "lastName": "Chiang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24725,
      "firstName": "Karon",
      "lastName": "MacLean",
      "affiliations": []
    },
    {
      "id": 24730,
      "firstName": "Sean",
      "lastName": "Follmer",
      "affiliations": []
    },
    {
      "id": 8347,
      "firstName": "Parinya",
      "lastName": "Punpongsanon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16542,
      "firstName": "Shan-Yuan",
      "lastName": "Teng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22695,
      "firstName": "Jun",
      "lastName": "Gong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21672,
      "firstName": "Eric",
      "lastName": "Paulos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21675,
      "firstName": "Narjes",
      "lastName": "Pourjafarian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23725,
      "firstName": "Jin",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19630,
      "firstName": "Brenda",
      "lastName": "Castro",
      "middleInitial": "Veronica",
      "affiliations": []
    },
    {
      "id": 8368,
      "firstName": "Koki",
      "lastName": "Nagano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18610,
      "firstName": "Daniel",
      "lastName": "Vogel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20666,
      "firstName": "Connor",
      "lastName": "Defanti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13501,
      "firstName": "Han-Wei",
      "lastName": "Hsieh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12480,
      "firstName": "Weiqi",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18625,
      "firstName": "Caroline",
      "lastName": "Appert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11460,
      "firstName": "Otmar",
      "lastName": "Hilliges",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17616,
      "firstName": "Gareth",
      "lastName": "Barnaby",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15569,
      "firstName": "FLAVIO",
      "lastName": "RIBEIRO",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22737,
      "firstName": "Clement",
      "lastName": "Zheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11479,
      "firstName": "Xun",
      "lastName": "Qian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17623,
      "firstName": "Bas",
      "lastName": "van Opheusden",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20695,
      "firstName": "James",
      "lastName": "McCann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20696,
      "firstName": "Kirielle",
      "lastName": "Singarajah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12509,
      "firstName": "Thijs",
      "lastName": "Roumen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15585,
      "firstName": "Thomas",
      "lastName": "Dreja",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11496,
      "firstName": "Steven",
      "lastName": "Feiner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14569,
      "firstName": "Tom",
      "lastName": "Valkeneers",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18673,
      "firstName": "Roberto",
      "lastName": "Montano-Murillo",
      "middleInitial": "A",
      "affiliations": []
    },
    {
      "id": 11510,
      "firstName": "Maureen",
      "lastName": "Daum",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10494,
      "firstName": "Prateek",
      "lastName": "Jain",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13573,
      "firstName": "Adam",
      "lastName": "Fourney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16645,
      "firstName": "Ningshan",
      "lastName": "Ouyang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18694,
      "firstName": "Chris",
      "lastName": "Harrison",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8454,
      "firstName": "Rafael",
      "lastName": "Morales González",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23823,
      "firstName": "Wojciech",
      "lastName": "Matusik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20754,
      "firstName": "Jane",
      "lastName": "Hsieh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8468,
      "firstName": "Weikai",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16661,
      "firstName": "Ryotaro",
      "lastName": "Temma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17687,
      "firstName": "Xiong",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12568,
      "firstName": "Maneesh",
      "lastName": "Agrawala",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15640,
      "firstName": "Nathan",
      "lastName": "Hahn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23836,
      "firstName": "Hans",
      "lastName": "Gellersen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22814,
      "firstName": "Azza",
      "lastName": "Abouzied",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21792,
      "firstName": "Jianxun",
      "lastName": "Cui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12577,
      "firstName": "Hsin-Ruey",
      "lastName": "Tsai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18724,
      "firstName": "Hongbo",
      "lastName": "Fu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13605,
      "firstName": "Pedro",
      "lastName": "Lopes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12582,
      "firstName": "Wilmot",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20775,
      "firstName": "Manik",
      "lastName": "Varma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10537,
      "firstName": "Jihoon",
      "lastName": "Ko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13611,
      "firstName": "Sriram",
      "lastName": "Subramanian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8506,
      "firstName": "Daniel",
      "lastName": "Ashbrook",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20795,
      "firstName": "Brian",
      "lastName": "Hecox",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8507,
      "firstName": "Hrvoje",
      "lastName": "Benko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16701,
      "firstName": "Stephen",
      "lastName": "DiVerdi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20802,
      "firstName": "David",
      "lastName": "Porfirio",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 22852,
      "firstName": "Haotian",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21833,
      "firstName": "Mar",
      "lastName": "Gonzalez-Franco",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9548,
      "firstName": "Janet",
      "lastName": "Baker",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 23886,
      "firstName": "Hideaki",
      "lastName": "Takada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19792,
      "firstName": "Michel",
      "lastName": "Beaudouin-Lafon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13648,
      "firstName": "Yuanzhi",
      "lastName": "Cao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10576,
      "firstName": "Wolfgang",
      "lastName": "Stuerzlinger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12631,
      "firstName": "Di",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23896,
      "firstName": "Haimo",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11608,
      "firstName": "Tom",
      "lastName": "Mitchell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21851,
      "firstName": "Karan",
      "lastName": "Ahuja",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18781,
      "firstName": "Jeeeun",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13662,
      "firstName": "Rebecca",
      "lastName": "Kleinberger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21855,
      "firstName": "Christian",
      "lastName": "Remy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15714,
      "firstName": "Li-yi",
      "lastName": "Wei",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18792,
      "firstName": "Emery",
      "lastName": "Berger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17770,
      "firstName": "Jifei",
      "lastName": "Ou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10606,
      "firstName": "Cheng-Lung",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13683,
      "firstName": "Minxing",
      "lastName": "Xie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16755,
      "firstName": "Liane",
      "lastName": "Makatura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16756,
      "firstName": "Tomás",
      "lastName": "Vega Gálvez",
      "middleInitial": "Alfonso",
      "affiliations": []
    },
    {
      "id": 15733,
      "firstName": "Shuchang",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13686,
      "firstName": "Aditya",
      "lastName": "Nittala",
      "middleInitial": "Shekhar",
      "affiliations": []
    },
    {
      "id": 12663,
      "firstName": "Koh",
      "lastName": "SUEDA",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10615,
      "firstName": "Danny",
      "lastName": "Kaufman",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 21880,
      "firstName": "Nadeem",
      "lastName": "Shaheer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12665,
      "firstName": "Patrick",
      "lastName": "Baudisch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23930,
      "firstName": "Gilles",
      "lastName": "Bailly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21886,
      "firstName": "Janus Bager",
      "lastName": "Kristensen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18815,
      "firstName": "Erwin",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11649,
      "firstName": "Florian",
      "lastName": "Geiselhart",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21890,
      "firstName": "Jacob",
      "lastName": "Wobbrock",
      "middleInitial": "O.",
      "affiliations": []
    },
    {
      "id": 15757,
      "firstName": "Sujeath",
      "lastName": "Pareddy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21901,
      "firstName": "Michael",
      "lastName": "Laielli",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8590,
      "firstName": "Aaron",
      "lastName": "Hertzmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20882,
      "firstName": "Ludwig",
      "lastName": "Sidenmark",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17816,
      "firstName": "Liwei",
      "lastName": "Chan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15769,
      "firstName": "Hiroshi",
      "lastName": "Ishii",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12698,
      "firstName": "Chun",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13723,
      "firstName": "Andrew",
      "lastName": "Wilson",
      "middleInitial": "D",
      "affiliations": []
    },
    {
      "id": 13725,
      "firstName": "Alexandra",
      "lastName": "Rieger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14749,
      "firstName": "Allison",
      "lastName": "Sauppe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20893,
      "firstName": "Justin",
      "lastName": "Lubin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21918,
      "firstName": "Sauvik",
      "lastName": "Das",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10655,
      "firstName": "Suranga",
      "lastName": "Nanayakkara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21920,
      "firstName": "Itiro",
      "lastName": "Siio",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16801,
      "firstName": "Anne",
      "lastName": "Roudaut",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23970,
      "firstName": "Jim",
      "lastName": "Holbery",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16807,
      "firstName": "Ziyang",
      "lastName": "Shan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15786,
      "firstName": "Sarah",
      "lastName": "Chasins",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 11692,
      "firstName": "Haruki",
      "lastName": "Takahashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18873,
      "firstName": "Alex",
      "lastName": "Olwal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17851,
      "firstName": "Fraser",
      "lastName": "Anderson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13756,
      "firstName": "Siyuan",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10684,
      "firstName": "Joan",
      "lastName": "ROO",
      "middleInitial": "Sol",
      "affiliations": []
    },
    {
      "id": 10688,
      "firstName": "Elizabeth",
      "lastName": "Kupferstein",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22979,
      "firstName": "Assegid",
      "lastName": "Kidane",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24006,
      "firstName": "Chirag",
      "lastName": "Pabbaraju",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14791,
      "firstName": "Myung Jin",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14792,
      "firstName": "Lining",
      "lastName": "Yao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16840,
      "firstName": "Yanjun",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11726,
      "firstName": "Corinne",
      "lastName": "Brenner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10704,
      "firstName": "Haoming",
      "lastName": "Lai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24018,
      "firstName": "Jeffrey",
      "lastName": "Bigham",
      "middleInitial": "Philip",
      "affiliations": []
    },
    {
      "id": 13780,
      "firstName": "Junhan",
      "lastName": "Kong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20950,
      "firstName": "Diego",
      "lastName": "Martinez Plasencia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18910,
      "firstName": "Vivek",
      "lastName": "Seshadri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8671,
      "firstName": "Qiuyu",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15840,
      "firstName": "Pawan",
      "lastName": "Rao",
      "middleInitial": "Sudhindra",
      "affiliations": []
    },
    {
      "id": 25056,
      "firstName": "Aniket",
      "lastName": "Kittur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25057,
      "firstName": "Enrico",
      "lastName": "Rukzio",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8673,
      "firstName": "Harsha Vardhan",
      "lastName": "Simhadri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25058,
      "firstName": "Megan",
      "lastName": "Hofmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25059,
      "firstName": "Feng",
      "lastName": "Tian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12771,
      "firstName": "Bilge",
      "lastName": "Mutlu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10723,
      "firstName": "Eyal",
      "lastName": "Ofek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25060,
      "firstName": "Yoshifumi",
      "lastName": "Kitamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25061,
      "firstName": "Anhong",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10726,
      "firstName": "John",
      "lastName": "Canny",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25062,
      "firstName": "Jennifer",
      "lastName": "Mankoff",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11751,
      "firstName": "Don",
      "lastName": "Dennis",
      "middleInitial": "Kurian",
      "affiliations": []
    },
    {
      "id": 25063,
      "firstName": "Eunice",
      "lastName": "Jun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25064,
      "firstName": "James",
      "lastName": "Landay",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 25065,
      "firstName": "Jason I.",
      "lastName": "Hong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25066,
      "firstName": "Eric",
      "lastName": "Lecolinet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25067,
      "firstName": "Kazuyuki",
      "lastName": "Fujita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25068,
      "firstName": "Xiangmin",
      "lastName": "Fan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9708,
      "firstName": "John F.",
      "lastName": "Hughes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25069,
      "firstName": "Byungjoo",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25070,
      "firstName": "Scott",
      "lastName": "Hudson",
      "middleInitial": "E",
      "affiliations": []
    },
    {
      "id": 25071,
      "firstName": "Alex",
      "lastName": "Williams",
      "middleInitial": "C",
      "affiliations": []
    },
    {
      "id": 17903,
      "firstName": "Balasaravanan",
      "lastName": "Thoravi Kumaravel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25072,
      "firstName": "Kazuki",
      "lastName": "Takashima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25073,
      "firstName": "Catherine",
      "lastName": "Pelachaud",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25074,
      "firstName": "Daniel",
      "lastName": "Wigdor",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25075,
      "firstName": "Xiang 'Anthony'",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11763,
      "firstName": "Aaron",
      "lastName": "Quigley",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 15860,
      "firstName": "Li-Yang",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25076,
      "firstName": "Michelle",
      "lastName": "Annett",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25077,
      "firstName": "Walter",
      "lastName": "Lasecki",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 25078,
      "firstName": "Gierad",
      "lastName": "Laput",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25079,
      "firstName": "Wendy",
      "lastName": "Mackay",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 12791,
      "firstName": "Valentin",
      "lastName": "Nigolian",
      "middleInitial": "Zenon",
      "affiliations": []
    },
    {
      "id": 17914,
      "firstName": "Yoshihiro",
      "lastName": "Kawahara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10747,
      "firstName": "Xiangyu",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18941,
      "firstName": "Alireza",
      "lastName": "Bahremand",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22015,
      "firstName": "Sean",
      "lastName": "Liu",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 20992,
      "firstName": "Daisuke",
      "lastName": "Sakamoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12801,
      "firstName": "Shaun",
      "lastName": "Burley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22018,
      "firstName": "He",
      "lastName": "Wen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18946,
      "firstName": "Taehoon",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22022,
      "firstName": "Yasha",
      "lastName": "Iravantchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15884,
      "firstName": "Florian",
      "lastName": "Michahelles",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17935,
      "firstName": "Jared",
      "lastName": "Roesch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14864,
      "firstName": "Meng-Ju",
      "lastName": "Hsieh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18960,
      "firstName": "Yeu-Luen",
      "lastName": "Chiu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17938,
      "firstName": "Mohammad",
      "lastName": "Islam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10772,
      "firstName": "Catherine",
      "lastName": "Mondoa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12822,
      "firstName": "Robert",
      "lastName": "LiKamWa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16922,
      "firstName": "Motohiro",
      "lastName": "Makiguchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18975,
      "firstName": "Xing-Dong",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22051,
      "firstName": "Woojin",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14883,
      "firstName": "André",
      "lastName": "Haben",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9767,
      "firstName": "Ravi",
      "lastName": "Chugh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18989,
      "firstName": "Anna",
      "lastName": "Feit",
      "middleInitial": "Maria",
      "affiliations": []
    },
    {
      "id": 8753,
      "firstName": "Jürgen",
      "lastName": "Steimle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16948,
      "firstName": "Jiaju",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16952,
      "firstName": "Yuto",
      "lastName": "Kuroki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9785,
      "firstName": "Brian",
      "lastName": "Hempel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16956,
      "firstName": "Tobias",
      "lastName": "Kraus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14909,
      "firstName": "Da-Yuan",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23102,
      "firstName": "Ellen Yi-Luen",
      "lastName": "Do",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16960,
      "firstName": "Yasuaki",
      "lastName": "Kakehi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10820,
      "firstName": "Benjamin",
      "lastName": "Attal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13898,
      "firstName": "Joe",
      "lastName": "Paradiso",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24145,
      "firstName": "Siyuan",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22101,
      "firstName": "Marissa",
      "lastName": "Radensky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18006,
      "firstName": "Stefanie",
      "lastName": "Mueller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9814,
      "firstName": "Ayaka",
      "lastName": "Ishii",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12887,
      "firstName": "Keita",
      "lastName": "Watanabe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12888,
      "firstName": "Gabriel",
      "lastName": "Haas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10848,
      "firstName": "Ye",
      "lastName": "Tao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15972,
      "firstName": "Steven",
      "lastName": "Ko",
      "middleInitial": "Y.",
      "affiliations": []
    },
    {
      "id": 24175,
      "firstName": "Vedant",
      "lastName": "Saran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22129,
      "firstName": "Kengo",
      "lastName": "Honda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22130,
      "firstName": "Wong",
      "lastName": "Pui Chung",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9848,
      "firstName": "Raf",
      "lastName": "Ramakers",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9854,
      "firstName": "Andrea",
      "lastName": "Bianchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9857,
      "firstName": "Jeff",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18056,
      "firstName": "Asier",
      "lastName": "Marzo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8841,
      "firstName": "David",
      "lastName": "Lindlbauer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11920,
      "firstName": "Takeo",
      "lastName": "Igarashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22161,
      "firstName": "Felix",
      "lastName": "Grzelka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8850,
      "firstName": "Hao",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19091,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22165,
      "firstName": "Bing-Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10905,
      "firstName": "Te-Yen",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12954,
      "firstName": "Nathalie",
      "lastName": "Henry Riche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11934,
      "firstName": "Chin-Yuan",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24225,
      "firstName": "Christoph",
      "lastName": "Borst",
      "middleInitial": "W",
      "affiliations": []
    },
    {
      "id": 12964,
      "firstName": "Swarun",
      "lastName": "Kumar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21157,
      "firstName": "Emily",
      "lastName": "Deng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16039,
      "firstName": "Jun",
      "lastName": "Xing",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12967,
      "firstName": "Kaori",
      "lastName": "Ikematsu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19112,
      "firstName": "Shuo-wen",
      "lastName": "Hsu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24233,
      "firstName": "Lea",
      "lastName": "Albaugh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10921,
      "firstName": "Michael",
      "lastName": "Rivera",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 12975,
      "firstName": "James",
      "lastName": "Tompkin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8879,
      "firstName": "Yuhua",
      "lastName": "Jin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24239,
      "firstName": "Rene",
      "lastName": "Just",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12978,
      "firstName": "Keiko",
      "lastName": "Katsuragawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17080,
      "firstName": "Soo Young",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24262,
      "firstName": "Liang",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18121,
      "firstName": "Wolf",
      "lastName": "Kienzle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19146,
      "firstName": "Guanyun",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24268,
      "firstName": "Josh",
      "lastName": "Davis",
      "middleInitial": "Urban",
      "affiliations": []
    },
    {
      "id": 22225,
      "firstName": "Ju",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24282,
      "firstName": "Miro",
      "lastName": "Mannino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23262,
      "firstName": "Tetsuo",
      "lastName": "Ono",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16097,
      "firstName": "Youngwook",
      "lastName": "Do",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13026,
      "firstName": "Juyoung",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19172,
      "firstName": "Chu-En",
      "lastName": "Hou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24301,
      "firstName": "Mayank",
      "lastName": "Goel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15094,
      "firstName": "Yang",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12024,
      "firstName": "Jing",
      "lastName": "Qian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16121,
      "firstName": "Kening",
      "lastName": "Zhu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11008,
      "firstName": "Jiahao",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12033,
      "firstName": "Shohei",
      "lastName": "Katakura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14081,
      "firstName": "Artem",
      "lastName": "Dementyev",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23299,
      "firstName": "Omid",
      "lastName": "Abari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20230,
      "firstName": "Takatoshi",
      "lastName": "Yoshida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15111,
      "firstName": "Joanne",
      "lastName": "Lo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14089,
      "firstName": "Karthik",
      "lastName": "Ramani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23310,
      "firstName": "Jianzhe",
      "lastName": "Gu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11025,
      "firstName": "Mark",
      "lastName": "Gross",
      "middleInitial": "D",
      "affiliations": []
    },
    {
      "id": 11027,
      "firstName": "Masaaki",
      "lastName": "Fukumoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23316,
      "firstName": "Toby",
      "lastName": "Li",
      "middleInitial": "Jia-Jun",
      "affiliations": []
    },
    {
      "id": 18196,
      "firstName": "Francois",
      "lastName": "Guimbretiere",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21269,
      "firstName": "Michael",
      "lastName": "Rietzler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23317,
      "firstName": "Marc",
      "lastName": "Teyssier",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23319,
      "firstName": "Kris",
      "lastName": "Layng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8983,
      "firstName": "Patricia",
      "lastName": "Cornelio",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19225,
      "firstName": "Shishir",
      "lastName": "Patil",
      "middleInitial": "G",
      "affiliations": []
    },
    {
      "id": 8986,
      "firstName": "Ryen",
      "lastName": "White",
      "middleInitial": "W",
      "affiliations": []
    },
    {
      "id": 22303,
      "firstName": "Haijun",
      "lastName": "Xia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13090,
      "firstName": "Shamsi",
      "lastName": "Iqbal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21283,
      "firstName": "Bjoern",
      "lastName": "Hartmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14116,
      "firstName": "Mareike",
      "lastName": "Kritzler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22313,
      "firstName": "Huaishu",
      "lastName": "Peng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11051,
      "firstName": "Yu",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19245,
      "firstName": "Aradhana",
      "lastName": "Adhikari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16176,
      "firstName": "Giscard",
      "lastName": "Biamby",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21301,
      "firstName": "Yi-Chin",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21308,
      "firstName": "Sebastian",
      "lastName": "Herscher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19269,
      "firstName": "Jon",
      "lastName": "Froehlich",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 23373,
      "firstName": "Shiu",
      "lastName": "Ng",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 15182,
      "firstName": "Katharina",
      "lastName": "Reinecke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13136,
      "firstName": "Joanna",
      "lastName": "Bergström",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21328,
      "firstName": "Shiri",
      "lastName": "Azenkot",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21329,
      "firstName": "Ticha",
      "lastName": "Sethapakdi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22358,
      "firstName": "Ravikanth",
      "lastName": "Konjeti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19288,
      "firstName": "Hui-Shyong",
      "lastName": "Yeo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9048,
      "firstName": "Yunxin",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12127,
      "firstName": "Sambhav",
      "lastName": "Jain",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14177,
      "firstName": "Manav",
      "lastName": "Wadhawan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19297,
      "firstName": "Shahabedin",
      "lastName": "Sagheb",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14181,
      "firstName": "Harmanpreet",
      "lastName": "Kaur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15207,
      "firstName": "Sung-Ju",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18281,
      "firstName": "Roger",
      "lastName": "Boldu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21354,
      "firstName": "Wen",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20330,
      "firstName": "woo suk",
      "lastName": "lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18283,
      "firstName": "Byungmoon",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18288,
      "firstName": "Angelina",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22389,
      "firstName": "Clemens",
      "lastName": "Klokmose",
      "middleInitial": "Nylandsted",
      "affiliations": []
    },
    {
      "id": 20342,
      "firstName": "Haojian",
      "lastName": "Jin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18295,
      "firstName": "Michelle",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13176,
      "firstName": "Ken",
      "lastName": "Perlin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20344,
      "firstName": "Hsin-Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15226,
      "firstName": "Julius Cosmo Romeo",
      "lastName": "Rudolph",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16253,
      "firstName": "Jarrod",
      "lastName": "Knibbe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21377,
      "firstName": "Mike",
      "lastName": "Sinclair",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10117,
      "firstName": "Zheer",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14215,
      "firstName": "Teddy",
      "lastName": "Seyed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13191,
      "firstName": "James",
      "lastName": "Smith",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12170,
      "firstName": "Nianlong",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12179,
      "firstName": "Jotaro",
      "lastName": "Shigeyama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11162,
      "firstName": "Jaime",
      "lastName": "Teevan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21402,
      "firstName": "Frank",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23454,
      "firstName": "Jingwan",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19359,
      "firstName": "Parmit",
      "lastName": "Chilana",
      "middleInitial": "K",
      "affiliations": []
    },
    {
      "id": 20388,
      "firstName": "Chi",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13220,
      "firstName": "David",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20392,
      "firstName": "Trevor",
      "lastName": "Darrell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10154,
      "firstName": "Brad",
      "lastName": "Myers",
      "middleInitial": "A",
      "affiliations": []
    },
    {
      "id": 9137,
      "firstName": "Yingtian",
      "lastName": "Shi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12210,
      "firstName": "Daniel",
      "lastName": "Groeger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16308,
      "firstName": "Ken",
      "lastName": "Hinckley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23478,
      "firstName": "Evan",
      "lastName": "Fisher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18359,
      "firstName": "Devamardeep",
      "lastName": "Hayatpur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13242,
      "firstName": "Fang",
      "lastName": "Qin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14273,
      "firstName": "Seungwoo",
      "lastName": "Je",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10177,
      "firstName": "Andrew",
      "lastName": "Conn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18370,
      "firstName": "Rahul",
      "lastName": "Arora",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14284,
      "firstName": "Simon",
      "lastName": "Hoermann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11216,
      "firstName": "Xiaoyan",
      "lastName": "Shen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11224,
      "firstName": "Arshad",
      "lastName": "Khan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13274,
      "firstName": "Jane",
      "lastName": "E",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 19421,
      "firstName": "Christian",
      "lastName": "Holz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18397,
      "firstName": "Sunggeun",
      "lastName": "Ahn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10206,
      "firstName": "Tom",
      "lastName": "Yeh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12261,
      "firstName": "Ohad",
      "lastName": "Fried",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14311,
      "firstName": "Alexandre",
      "lastName": "Kaspar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23530,
      "firstName": "Katarina",
      "lastName": "Bulovic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20458,
      "firstName": "Huai-Yu",
      "lastName": "Cheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23532,
      "firstName": "Emmanuel",
      "lastName": "Pietriga",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11245,
      "firstName": "Zhipeng",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19438,
      "firstName": "Robert",
      "lastName": "Xiao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12273,
      "firstName": "Koya",
      "lastName": "Narumi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9209,
      "firstName": "Jackie",
      "lastName": "Yang",
      "middleInitial": "(Junrui)",
      "affiliations": []
    },
    {
      "id": 10234,
      "firstName": "João Henrique",
      "lastName": "Wilbert",
      "middleInitial": "Santos",
      "affiliations": []
    },
    {
      "id": 11260,
      "firstName": "Priyan",
      "lastName": "Vaithilingam",
      "middleInitial": "",
      "affiliations": []
    }
  ],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 43,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false
  }
}