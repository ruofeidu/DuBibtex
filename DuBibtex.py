# -*- coding: utf-8 -*-
"""DuBibtex merges duplicated bibtex items and resolves missing DOIs.

This script assumes the first line of each bibtex item contains its bib ID.
This is typically true if the bibtex item is from Google Scholar or DBLP.

Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)
Reference: http://www.bibtex.org/Format/
Sources of DOI: Bing, arXiv, ACM, IEEE, Springer, Caltech, Wiley
"""
import configparser
import json
import re
import requests
from typing import Any, Dict, List, Optional, TextIO, Tuple

import bibtexparser

# --- Configuration Constants ---

# These constants are loaded from 'config.ini' once at startup.
_CONFIG = configparser.ConfigParser()
_CONFIG.read("config.ini")

_SECTION = "DuBibtex"
_SEARCH_DOI: bool = _CONFIG.getboolean(_SECTION, "searchDOI")
_INPUT_FILE_LIST: List[str] = _CONFIG.get(_SECTION,
                                          "inputFileList").strip().split(",")
_OUTPUT_FILE: str = _CONFIG.get(_SECTION, "outputFile").strip()
_USE_OFFLINE_DOI: bool = _CONFIG.getboolean(_SECTION, "useOfflineDOI")
_PRINT_SELF_INFO: bool = _CONFIG.getboolean(_SECTION, "printSelfInfo")
_KEEP_COMMENTS: bool = _CONFIG.getboolean(_SECTION, "keepComments")
_DEBUG_BIB_CRAWLER: bool = _CONFIG.getboolean(_SECTION, "debugBibCrawler")
_DEBUG_STATISTICS: bool = _CONFIG.getboolean(_SECTION, "debugStatistics")
_DEFAULT_ADDRESS: bool = _CONFIG.getboolean(_SECTION, "defaultAddress")
_DEFAULT_PUBLISHER: bool = _CONFIG.getboolean(_SECTION, "defaultPublisher")
_DOI_JSON_FILE: str = _CONFIG.get(_SECTION, "doiJsonFile").strip()
_MIN_YEAR: int = _CONFIG.getint(_SECTION, "minYear")
_TIME_OUT: int = _CONFIG.getint(_SECTION, "timeOut")
_HEADER: Dict[str, str] = {
    "User-Agent": _CONFIG.get(_SECTION, "header").strip()
}
_DOI_TO_URL: bool = _CONFIG.getboolean(_SECTION, "DOI2URL")
_REMOVE_URL: bool = _CONFIG.getboolean(_SECTION, "removeUrl")
_OPTIMIZE_BIB_ID: bool = _CONFIG.getboolean(_SECTION, "optimizeBibId")
_FIELD_REMOVAL_LIST: List[str] = _CONFIG.get(
    _SECTION, "fieldRemovalList").strip().split(",")

# --- Auto-Generated Comments ---

_AUTO_COMMENT_CREDIT: str = '% Automatically generated by DuBibTeX.\n'
_AUTO_COMMENT_URL: str = '% https://github.com/ruofeidu/DuBibtex\n'

# --- Regular Expressions ---

_RE_BIB: re.Pattern = re.compile(r'\s*\@(\w+)\s*\{\s*([^\{\,\}]+),')
_RE_ITEM: re.Pattern = re.compile(r'\s*(\w+)\s*=\s*[\{"]\s*(.*)\s*[\}"]')
_RE_ITEM2: re.Pattern = re.compile(r'\s*(\w+)\s*=\s*[\{"]\{\s*(.*)\s*[\}"]\}')
_RE_ENDL: re.Pattern = re.compile(r'\s*}\s*')
_RE_ABBR: re.Pattern = re.compile(r'@string', flags=re.IGNORECASE)
_RE_DOI_JSON: re.Pattern = re.compile(r'doi\.org\\?\/([\w\d\.\-\\\/]+)',
                                      flags=re.MULTILINE)
_RE_DOI_URL: re.Pattern = re.compile(r'doi\.org\/([\w\d\.\-\\\/]+)',
                                     flags=re.MULTILINE)
_RE_DOI_ACM_URL: re.Pattern = re.compile(
    r'https:\/\/dl\.acm\.org\/doi\/(?:\w+\/)?([\w\d\.\-\\\/]+)',
    flags=re.MULTILINE)
_RE_DOI_JAVASCRIPT: re.Pattern = re.compile(r'doi\"\:\"([\w\d\.\-\\\/]+)\"',
                                            flags=re.MULTILINE)
_RE_DOI_TEXT: re.Pattern = re.compile(r'"DOI":"([\w\.\\\/]*)"',
                                      flags=re.MULTILINE)
_RE_DOI_SPRINGER: re.Pattern = re.compile(r'chapter\/([\w\.\\\/\_\-]+)',
                                          flags=re.MULTILINE)
_RE_DOI_WILEY: re.Pattern = re.compile(r'doi\/abs\/([\w\.\\\/\_\-]+)',
                                       flags=re.MULTILINE)
_RE_DOI_CALTECH: re.Pattern = re.compile(
    r'authors\.library\.caltech\.edu\/(\d+)', flags=re.MULTILINE)
_RE_DOI_PUBMED: re.Pattern = re.compile(r'nlm\.nih\.gov\/pubmed\/(\d+)',
                                        flags=re.MULTILINE)
_RE_URL_ARXIV: re.Pattern = re.compile(r'arxiv\.org\/pdf\/([\d\.]+)',
                                       flags=re.MULTILINE)
_RE_ACM: re.Pattern = re.compile(r'citation\.cfm\?id\=([\d\.]+)',
                                 flags=re.MULTILINE)
_RE_IEEE: re.Pattern = re.compile(r'ieee\.org(?:\/abstract)?\/document\/(\d+)',
                                  flags=re.MULTILINE)
_RE_NEURIPS: re.Pattern = re.compile(r'proceedings.neurips.cc',
                                     flags=re.MULTILINE)
_RE_YEAR: re.Pattern = re.compile(r'\w+(\d+)')
_RE_ARXIV_DOI: re.Pattern = re.compile(r'arXiv:(\d{4}\.\d{5})')


def request_url(url: str) -> str:
  """Sends a GET request to a URL with a standard header.

    Args:
      url: The URL to request.

    Returns:
      The text content of the response.
    """
  return requests.get(url, headers=_HEADER, timeout=_TIME_OUT).text


class BibTexManager:
  """Manages the processing, cleaning, and writing of BibTeX entries."""

  def __init__(self, output_file: str, use_offline_doi: bool):
    """Initializes the BibTexManager.

        Args:
          output_file: Path to the output .bib file.
          use_offline_doi: If True, uses a local JSON file for DOI lookups.
        """
    self.fout: TextIO = open(output_file, 'w', encoding='utf-8')
    if _PRINT_SELF_INFO:
      self.fout.write(f'{_AUTO_COMMENT_CREDIT}{_AUTO_COMMENT_URL}')

    self.bib_dict: Dict[str, Dict[str, Any]] = {}
    self.doi_dict: Dict[str, str] = {}
    if use_offline_doi:
      with open(_DOI_JSON_FILE, encoding='utf-8') as f:
        self.doi_dict = json.load(f)

    self.num_missing: int = 0
    self.num_duplicated: int = 0
    self.num_fixed: int = 0

  def process_entry(self, entry: Dict[str, Any]):
    """Processes a single parsed BibTeX entry.

        This method checks for duplicates, cleans fields, searches for missing
        DOIs, and writes the final entry to the output file.

        Args:
          entry: A dictionary representing a single BibTeX entry.
        """
    bib_id = entry.get('ID', '')
    bib_type = entry.get('ENTRYTYPE', 'misc')

    if not bib_id or bib_id.lower() in self.bib_dict:
      print(f"* Duplicated or empty ID found: {bib_id}")
      self.num_duplicated += 1
      return

    self.bib_dict[bib_id.lower()] = entry
    self.cur = entry
    self.bib = bib_id
    self.cur['type'] = bib_type  # Keep original type for processing.

    self.write_current_item()

  def write_current_item(self):
    """Validates, cleans, and writes the current BibTeX item to a file."""
    # This debug_bib call is fine as it's just showing the original title for context
    # self.debug_bib(self.cur.get('title', ''))

    self._ensure_year()
    self._optimize_bib_id()
    self._add_defaults()

    # Check for missing DOI and attempt to find it.
    if (_SEARCH_DOI and int(self.cur.get('year', 0)) > _MIN_YEAR and
        'doi' not in self.cur and
        self.cur.get('type', '').lower() not in ['misc', 'book', 'techreport']):
      self._find_doi()

    if 'doi' in self.cur:
      self.cur['doi'] = fix_underscore(self.cur['doi'])
      self.doi_dict[self.bib] = self.cur['doi']

    self._write_entry_to_file()

  def _write_entry_to_file(self):
    """Formats and writes the final BibTeX entry to the output file."""
    entry_type = self.cur.get('type', 'misc').lower()
    self.fout.write(f'@{entry_type}{{{self.bib},\n')

    # Prepare fields for writing
    fields_to_write = self.cur.copy()
    del fields_to_write['ID']
    del fields_to_write['ENTRYTYPE']
    del fields_to_write['type']

    if _REMOVE_URL and 'url' in fields_to_write:
      del fields_to_write['url']

    num_fields = len(fields_to_write)
    for i, (key, value) in enumerate(fields_to_write.items()):
      if key in _FIELD_REMOVAL_LIST:
        continue

      # Capitalize specific fields
      if key in ['booktitle', 'journal', 'title']:
        # FIX: Explicitly remove an outer pair of braces for 'title'
        # if bibtexparser has left them in. This ensures 'value' is pure content.
        if key == 'title':
          stripped_value = value.strip()
          # This regex will match if the string starts and ends with a curly brace
          # and capture the content inside. re.DOTALL allows matching across newlines.
          match = re.match(r'^{(.*)}$', stripped_value, re.DOTALL)
          if match:
            # If matched, update value to be the content *inside* those braces
            value = match.group(
                1).strip()  # Strip any whitespace inside as well

        value = capitalize(value)

      # Format value with appropriate braces
      if key == 'title':
        # Now, `value` should be "Thinking in Space: How..." (without any outer braces)
        # We want to output `title={{...}}`, so we wrap the value with `{{` and `}}`
        self.fout.write(f'  {key}={{ {value} }}')
      else:
        # For other fields, keep the standard { {value} } wrapping
        self.fout.write(f'  {key}={{ {value} }}')

      if i < num_fields - 1:
        self.fout.write(',')
      self.fout.write('\n')

    self.fout.write('}\n\n')

  def _ensure_year(self):
    """Ensures the entry has a valid year field."""
    if 'year' not in self.cur or len(self.cur.get('year', '')) < 4:
      match = _RE_YEAR.search(self.bib)
      if match and match.groups():
        self.cur['year'] = match.groups()[0]
      else:
        print(f"Warning: No valid year for: {self.cur.get('title', '')}")
        self.cur['year'] = '2025'  # Default to current year

  def _optimize_bib_id(self):
    """Renames Bib ID to a standard format if enabled."""
    if (_OPTIMIZE_BIB_ID and 'author' in self.cur and 'title' in self.cur and
        'year' in self.cur):
      last_name = self.cur['author'].split(',', 1)[0]
      first_title_word = self.cur['title'].split(' ', 1)[0].capitalize()
      self.bib = f"{last_name}{self.cur['year']}{first_title_word}"
      self.cur['ID'] = self.bib

  def _add_defaults(self):
    """Adds default publisher and address if enabled and missing."""
    if _DEFAULT_ADDRESS and 'address' not in self.cur:
      self.cur['address'] = 'New York, NY, USA'
    if self.cur.get('type', '') == 'article' and 'address' in self.cur:
      del self.cur['address']

    if _DEFAULT_PUBLISHER and 'publisher' not in self.cur:
      if 'organization' in self.cur:
        self.cur['publisher'] = self.cur['organization']
      elif 'journal' in self.cur and self.cur['journal'].startswith('arXiv'):
        self.cur['publisher'] = 'arXiv'

  def _find_doi(self):
    """Searches online sources for a missing DOI."""
    # Create a cleaned title for searching, removing braces
    search_title = re.sub(r'\{|\}', '', self.cur.get('title', ''))

    # Use search_title for the debug message as well
    self.debug_bib(f"Missing DOI, searching for \"{search_title}\"...")

    doi = None
    # Prioritize IEEE if applicable
    if 'journal' in self.cur and 'ieee' in self.cur['journal'].lower():
      doi = ieee_xplore_lookup(search_title)

    # General lookup if still not found (Bing before arXiv)
    if not doi:
      doi = bing_lookup(search_title)

    # Now add arXiv search after Bing search
    if not doi:
      doi = arxiv_lookup(search_title)  # Pass the cleaned title for search

    if not doi:
      doi = crossref_lookup(search_title)  # Pass the cleaned title for search

    if doi:
      self.fix_doi(doi)
    else:
      self.num_missing += 1

  def debug_bib(self, s: str):
    """Prints debug information if enabled."""
    if _DEBUG_BIB_CRAWLER:
      print(s)

  def fix_doi(self, doi: str):
    """Applies a found DOI to the current entry.

        Args:
          doi: The DOI string to add.
        """
    if _DEBUG_STATISTICS:
      self.num_missing += 1
      self.num_fixed += 1

    if len(doi) > 4 and doi.startswith(('pdf', 'abs')):
      doi = doi[4:]

    self.cur['doi'] = doi
    if _DOI_TO_URL:
      self.cur['url'] = f'http://doi.org/{doi}'

  def print_statistics(self):
    """Prints final processing statistics."""
    print(f"{self.num_missing} missing doi, {self.num_fixed} fixed, "
          f"{self.num_duplicated} duplicated")

  def shutdown(self):
    """Closes files and saves the DOI dictionary if configured."""
    self.fout.close()
    if _USE_OFFLINE_DOI:
      with open(_DOI_JSON_FILE, 'w', encoding='utf-8') as outfile:
        json.dump(self.doi_dict, outfile, indent=2)
      print(f"DuBibTeX has saved known DOIs to {_DOI_JSON_FILE}.")
    self.print_statistics()


def crossref_lookup(title: str) -> Optional[str]:
  """Looks up a title on CrossRef for a DOI.

    Args:
      title: The title of the publication.

    Returns:
      The found DOI string, or None.
    """
  content = request_url(
      f'https://api.crossref.org/works?rows=5&query.title={title}')
  match = _RE_DOI_JSON.search(content)
  if match and match.groups():
    res = match.groups()[0].replace('\\', '')
    if 'policy' not in res:
      if _DEBUG_BIB_CRAWLER:
        print(f"DOI from CrossRef Lookup: {res}\n")
      return res
  return None


def ieee_xplore_lookup(title: str) -> Optional[str]:
  """Looks up a title directly via the IEEE Xplore API.

    Args:
      title: The title of the publication.

    Returns:
      The found DOI string, or None.
    """
  payload = {
      "newsearch": "true",
      "queryText": title,
      "returnFacets": ["ALL"],
      "returnType": "SEARCH",
  }
  try:
    response = requests.post('https://ieeexplore.ieee.org/rest/search',
                             json=payload,
                             headers=_HEADER)
    response.raise_for_status()
    result = response.json()
    if result.get("records"):
      return result["records"][0].get("doi")
  except requests.exceptions.RequestException as e:
    print(f"IEEE Xplore request failed: {e}")
  return None


def arxiv_lookup(title: str) -> Optional[str]:
  """Looks up a title on arXiv for a DOI (arXiv ID can act as a DOI).

    Args:
      title: The title of the publication.

    Returns:
      The found arXiv ID (as a DOI string), or None.
    """
  search_url = f'https://arxiv.org/search/?query={requests.utils.quote(title)}&searchtype=all&abstracts=show&size=50'
  try:
    html = request_url(search_url)
    # arXiv IDs are typically in the format YYMM.NNNNN
    # We'll look for common patterns that might include the DOI from arXiv if available
    match = _RE_ARXIV_DOI.search(html)
    if match:
      arxiv_id = match.group(1)
      # Add the correct DOI prefix for arXiv
      formatted_doi = f"10.48550/arXiv.{arxiv_id}"
      if _DEBUG_BIB_CRAWLER:
        print(f"DOI from arXiv Lookup: {formatted_doi}\n")
      return formatted_doi
  except requests.exceptions.RequestException as e:
    print(f"arXiv search request failed: {e}")
  return None


def bing_lookup(title: str) -> Optional[str]:
  """Performs a Bing search to find a DOI from various publisher sites.

    Args:
      title: The title of the publication.

    Returns:
      The found DOI string, or None.
    """
  search_url = f'https://www.bing.com/search?q={requests.utils.quote(title)}'

  try:
    html = request_url(search_url)

    # with open('debug.out', 'w', encoding='utf-8') as f:
    #   f.write(html)

    # Check for direct DOI links first
    m = _RE_DOI_URL.search(html)
    if m:
      return m.groups()[0]

    # Check for publisher-specific URLs
    m = _RE_DOI_ACM_URL.search(html) or _RE_DOI_SPRINGER.search(
        html) or _RE_DOI_WILEY.search(html)
    if m:
      return m.groups()[0].replace('\\', '')

    # Check for IEEE documents
    m = _RE_IEEE.search(html)
    if m and m.groups():
      html_ieee = request_url(
          f'https://ieeexplore.ieee.org/document/{m.groups()[0]}')
      m_doi = _RE_DOI_JAVASCRIPT.search(html_ieee)
      if m_doi and m_doi.groups():
        return m_doi.groups()[0].replace('\\', '')

  except requests.exceptions.RequestException as e:
    print(f"Bing search request failed: {e}")

  print("* Nothing found via Bing lookup.\n")
  return None


def fix_underscore(s: str) -> str:
  """Escapes underscores in a string for BibTeX compatibility."""
  return re.sub(r'(?<!\\)_', r'\\_', s)


def capitalize(s: str, splitter: str = ' ') -> str:
  """Capitalizes a string (e.g., a title) according to academic rules.

    Args:
      s: The string to capitalize.
      splitter: The delimiter for splitting words.

    Returns:
      The capitalized string.
    """
  SPECIAL_WORDS = {
      'a', 'an', 'the', 'to', 'on', 'in', 'of', 'at', 'by', 'for', 'or', 'and',
      'vs.', 'via'
  }

  # Words to always keep in a specific case
  EXACT_CASE_WORDS = {'iOS', '2D', '3D', '4D', '6DoF'}

  s = s.strip(',.- ')
  words = s.split(splitter)
  capitalized_words = []

  for i, word in enumerate(words):
    if not word:
      continue

    lowered_word = word.lower()
    if i > 0 and lowered_word in SPECIAL_WORDS and ':' not in words[i - 1]:
      capitalized_words.append(lowered_word)
    elif word in EXACT_CASE_WORDS:
      capitalized_words.append(word)
    else:
      capitalized_words.append(word[0].upper() + word[1:])

  s = splitter.join(capitalized_words)
  return s if splitter == '-' else capitalize(s, '-')


def main():
  """Main execution function."""
  manager = BibTexManager(output_file=_OUTPUT_FILE,
                          use_offline_doi=_USE_OFFLINE_DOI)

  for filename in _INPUT_FILE_LIST:
    print(f"--- Processing {filename} ---")
    try:
      # The bibtexparser v2.0 way to load a file
      # Explicitly set encoding to 'utf-8' for input files
      with open(filename, 'r', encoding='utf-8') as bibfile:
        library = bibtexparser.load(bibfile)

      for entry in library.entries:
        manager.process_entry(entry)

    except FileNotFoundError:
      print(f"Error: Input file not found at {filename}")
    except Exception as e:
      print(f"An error occurred while processing {filename}: {e}")

  manager.shutdown()


if __name__ == "__main__":
  main()
