# -*- coding: utf-8 -*-
"""
Merges duplicate BibTeX entries, resolves missing DOIs, and cleans formatting.

This script enhances BibTeX files by identifying and merging duplicate entries,
fetching missing DOIs from various online sources, and standardizing the
formatting of fields like titles and author names.

Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)
"""
import configparser
import json
import re
import requests
from typing import Any, Dict, List, Optional, TextIO

import bibtexparser

# --- Configuration ---
# Loads settings from config.ini. Assumes the file exists in the same directory.
_CONFIG = configparser.ConfigParser()
_CONFIG.read("config.ini")

_SECTION = "DuBibtex"
_SEARCH_DOI: bool = _CONFIG.getboolean(_SECTION, "searchDOI")
_INPUT_FILE_LIST: List[str] = _CONFIG.get(_SECTION,
                                          "inputFileList").strip().split(",")
_OUTPUT_FILE: str = _CONFIG.get(_SECTION, "outputFile").strip()
_USE_OFFLINE_DOI: bool = _CONFIG.getboolean(_SECTION, "useOfflineDOI")
_PRINT_SELF_INFO: bool = _CONFIG.getboolean(_SECTION, "printSelfInfo")
_KEEP_COMMENTS: bool = _CONFIG.getboolean(_SECTION, "keepComments")
_DEBUG_BIB_CRAWLER: bool = _CONFIG.getboolean(_SECTION, "debugBibCrawler")
_DEBUG_STATISTICS: bool = _CONFIG.getboolean(_SECTION, "debugStatistics")
_DEFAULT_ADDRESS: bool = _CONFIG.getboolean(_SECTION, "defaultAddress")
_DEFAULT_PUBLISHER: bool = _CONFIG.getboolean(_SECTION, "defaultPublisher")
_DOI_JSON_FILE: str = _CONFIG.get(_SECTION, "doiJsonFile").strip()
_MIN_YEAR: int = _CONFIG.getint(_SECTION, "minYear")
_TIME_OUT: int = _CONFIG.getint(_SECTION, "timeOut")
_HEADER: Dict[str, str] = {
    "User-Agent": _CONFIG.get(_SECTION, "header").strip()
}
_DOI_TO_URL: bool = _CONFIG.getboolean(_SECTION, "DOI2URL")
_REMOVE_URL: bool = _CONFIG.getboolean(_SECTION, "removeUrl")
_OPTIMIZE_BIB_ID: bool = _CONFIG.getboolean(_SECTION, "optimizeBibId")
_FIELD_REMOVAL_LIST: List[str] = _CONFIG.get(
    _SECTION, "fieldRemovalList").strip().split(",")

# --- Script Information ---
_AUTO_COMMENT_CREDIT: str = '% Automatically generated by DuBibTeX.\n'
_AUTO_COMMENT_URL: str = '% https://github.com/ruofeidu/DuBibtex\n'

# --- Regular Expressions ---
# For parsing and extracting information from BibTeX entries and web pages.
_RE_BIB: re.Pattern = re.compile(r'\s*\@(\w+)\s*\{\s*([^\{\,\}]+),')
_RE_ITEM: re.Pattern = re.compile(r'\s*(\w+)\s*=\s*[\{"]\s*(.*)\s*[\}"]')
_RE_ITEM2: re.Pattern = re.compile(r'\s*(\w+)\s*=\s*[\{"]\{\s*(.*)\s*[\}"]\}')
_RE_ENDL: re.Pattern = re.compile(r'\s*}\s*')
_RE_ABBR: re.Pattern = re.compile(r'@string', flags=re.IGNORECASE)
_RE_DOI_URL: re.Pattern = re.compile(r'doi\.org\/([\w\d\.\-\\\/]+)',
                                     flags=re.MULTILINE)
_RE_DOI_ACM_URL: re.Pattern = re.compile(
    r'https:\/\/dl\.acm\.org\/doi\/(?:\w+\/)?([\w\d\.\-\\\/]+)',
    flags=re.MULTILINE)
_RE_DOI_JAVASCRIPT: re.Pattern = re.compile(
    r'doi\\?\"\:?\\?\"([\w\d\.\-\\\/]+)\\?\"', flags=re.MULTILINE)
_RE_DOI_SPRINGER: re.Pattern = re.compile(r'chapter\/([\w\.\\\/\_\-]+)',
                                          flags=re.MULTILINE)
_RE_DOI_WILEY: re.Pattern = re.compile(r'doi\/abs\/([\w\.\\\/\_\-]+)',
                                       flags=re.MULTILINE)
_RE_ARXIV_DOI: re.Pattern = re.compile(r'arXiv:(\d{4}\.\d{5})')
_RE_IEEE_METADATA: re.Pattern = re.compile(
    r'xplGlobal\.document\.metadata\s*=\s*(\{.+\});', flags=re.MULTILINE)
_RE_YEAR: re.Pattern = re.compile(r'\w+(\d+)')


def request_url(url: str) -> str:
  """Sends a GET request to a URL.

    Args:
        url: The URL to request.

    Returns:
        The text content of the response.
    """
  return requests.get(url, headers=_HEADER, timeout=_TIME_OUT).text


class BibTexManager:
  """Manages the processing, cleaning, and writing of BibTeX entries."""

  def __init__(self, output_file: str, use_offline_doi: bool):
    """Initializes the BibTexManager.

        Args:
            output_file: Path to the output .bib file.
            use_offline_doi: If True, uses a local JSON file for DOI lookups.
        """
    self.fout: TextIO = open(output_file, 'w', encoding='utf-8')
    if _PRINT_SELF_INFO:
      self.fout.write(f'{_AUTO_COMMENT_CREDIT}{_AUTO_COMMENT_URL}')

    self.bib_dict: Dict[str, Dict[str, Any]] = {}
    self.doi_dict: Dict[str, str] = {}
    if use_offline_doi:
      with open(_DOI_JSON_FILE, encoding='utf-8') as f:
        self.doi_dict = json.load(f)

    self.num_missing: int = 0
    self.num_duplicated: int = 0
    self.num_fixed: int = 0

  def process_entry(self, entry: Dict[str, Any]):
    """Processes a single parsed BibTeX entry.

        This method checks for duplicates, cleans fields, searches for missing
        DOIs, and writes the final entry to the output file.

        Args:
            entry: A dictionary representing a single BibTeX entry.
        """
    bib_id = entry.get('ID', '')
    bib_type = entry.get('ENTRYTYPE', 'misc')

    if not bib_id or bib_id.lower() in self.bib_dict:
      print(f"* Duplicated or empty ID found: {bib_id}")
      self.num_duplicated += 1
      return

    self.bib_dict[bib_id.lower()] = entry
    self.cur = entry
    self.bib = bib_id
    self.cur['type'] = bib_type  # Preserve original type for processing.

    self.write_current_item()

  def write_current_item(self):
    """Validates, cleans, and writes the current BibTeX item to a file."""
    self._ensure_year()
    self._optimize_bib_id()
    self._add_defaults()

    # Attempt to find missing DOI for relevant entry types.
    if (_SEARCH_DOI and int(self.cur.get('year', 0)) > _MIN_YEAR and
        'doi' not in self.cur and
        self.cur.get('type', '').lower() not in ['misc', 'book', 'techreport']):
      self._find_doi()

    if 'doi' in self.cur:
      self.cur['doi'] = fix_underscore(self.cur['doi'])
      self.doi_dict[self.bib] = self.cur['doi']

    self._write_entry_to_file()

  def _write_entry_to_file(self):
    """Formats and writes the final BibTeX entry to the output file."""
    entry_type = self.cur.get('type', 'misc').lower()
    self.fout.write(f'@{entry_type}{{{self.bib},\n')

    fields_to_write = self.cur.copy()
    del fields_to_write['ID']
    del fields_to_write['ENTRYTYPE']
    del fields_to_write['type']

    if _REMOVE_URL and 'url' in fields_to_write:
      del fields_to_write['url']

    num_fields = len(fields_to_write)
    for i, (key, value) in enumerate(fields_to_write.items()):
      if key in _FIELD_REMOVAL_LIST:
        continue

      # Ensure titles and other fields are properly capitalized and braced.
      if key in ['booktitle', 'journal', 'title']:
        # bibtexparser can leave an extra set of braces; remove them.
        stripped_value = value.strip()
        match = re.match(r'^{(.*)}$', stripped_value, re.DOTALL)
        if match:
          value = match.group(1).strip()

        value = capitalize(value)

      # Titles require double braces to preserve capitalization.
      if key == 'title':
        self.fout.write(f'  {key} = {{{{{value}}}}}')
      else:
        self.fout.write(f'  {key} = {{{value}}}')

      if i < num_fields - 1:
        self.fout.write(',')
      self.fout.write('\n')

    self.fout.write('}\n\n')

  def _ensure_year(self):
    """Ensures the entry has a valid year, extracting it from the bib_id if necessary."""
    if 'year' not in self.cur or len(self.cur.get('year', '')) < 4:
      match = _RE_YEAR.search(self.bib)
      if match and match.groups():
        self.cur['year'] = match.groups()[0]
      else:
        print(f"Warning: No valid year for: {self.cur.get('title', '')}")
        self.cur['year'] = '2025'  # Default to current year

  def _optimize_bib_id(self):
    """Renames the BibTeX ID to a standard 'LastNameYearFirstWord' format."""
    if (_OPTIMIZE_BIB_ID and 'author' in self.cur and 'title' in self.cur and
        'year' in self.cur):
      last_name = self.cur['author'].split(',', 1)[0]
      first_title_word = self.cur['title'].split(' ', 1)[0].capitalize()
      self.bib = f"{last_name}{self.cur['year']}{first_title_word}"
      self.cur['ID'] = self.bib

  def _add_defaults(self):
    """Adds default publisher and address to entries if they are missing."""
    if _DEFAULT_ADDRESS and 'address' not in self.cur:
      self.cur['address'] = 'New York, NY, USA'
    if self.cur.get('type', '') == 'article' and 'address' in self.cur:
      del self.cur['address']

    if _DEFAULT_PUBLISHER and 'publisher' not in self.cur:
      if 'organization' in self.cur:
        self.cur['publisher'] = self.cur['organization']
      elif 'journal' in self.cur and self.cur['journal'].startswith('arXiv'):
        self.cur['publisher'] = 'arXiv'

  def _find_doi(self):
    """Searches online sources for a missing DOI in a prioritized order."""
    search_title = re.sub(r'\{|\}', '', self.cur.get('title', ''))
    self.debug_bib(f"Missing DOI, searching for \"{search_title}\"...")

    # Lookup order is prioritized by API stability and relevance.
    lookup_functions = [
        # acm_lookup, # need virtual browser now
        ieee_xplore_lookup,
        arxiv_lookup,
        crossref_lookup,
        bing_lookup,
    ]

    doi = None
    for lookup in lookup_functions:
      try:
        print("looking for " + search_title)
        doi = lookup(search_title)
        if doi:
          break
      except Exception as e:
        print(f"Error during {lookup.__name__}: {e}")

    if doi:
      self.fix_doi(doi)
    else:
      self.num_missing += 1

  def debug_bib(self, s: str):
    """Prints debug information if enabled in the configuration."""
    if _DEBUG_BIB_CRAWLER:
      print(s)

  def fix_doi(self, doi: str):
    """Applies a found DOI to the current entry and updates statistics.

        Args:
            doi: The DOI string to add.
        """
    if _DEBUG_STATISTICS:
      self.num_missing += 1
      self.num_fixed += 1

    if len(doi) > 4 and doi.startswith(('pdf', 'abs')):
      doi = doi[4:]

    self.cur['doi'] = doi
    if _DOI_TO_URL:
      self.cur['url'] = f'http://doi.org/{doi}'

  def print_statistics(self):
    """Prints final processing statistics."""
    print(f"{self.num_missing} missing doi, {self.num_fixed} fixed, "
          f"{self.num_duplicated} duplicated")

  def shutdown(self):
    """Closes files and saves the DOI dictionary if configured."""
    self.fout.close()
    if _USE_OFFLINE_DOI:
      with open(_DOI_JSON_FILE, 'w', encoding='utf-8') as outfile:
        json.dump(self.doi_dict, outfile, indent=2)
      print(f"DuBibTeX has saved known DOIs to {_DOI_JSON_FILE}.")
    self.print_statistics()


def crossref_lookup(title: str) -> Optional[str]:
  """Looks up a title on CrossRef for a DOI.

    Args:
        title: The title of the publication.

    Returns:
        The found DOI string, or None.
    """
  content = request_url(
      f'https://api.crossref.org/works?rows=5&query.title={title}')
  match = _RE_DOI_URL.search(content)

  # with open("debug.html", "w", encoding="utf8") as f:
  #   f.write(content)

  if match and match.groups():
    res = match.groups()[0].replace('\\', '')
    if 'policy' not in res:
      if _DEBUG_BIB_CRAWLER:
        print(f"DOI from CrossRef Lookup: {res}\n")
      return res
  return None


def acm_lookup(title: str) -> Optional[str]:
  """Looks up a title on ACM Digital Library for a DOI.

    Args:
        title: The title of the publication.

    Returns:
        The found DOI string, or None.
    """
  search_url = f'https://dl.acm.org/action/doSearch?AllField={requests.utils.quote(title)}'
  html = request_url(search_url)

  match = _RE_DOI_ACM_URL.search(html)
  if match and match.groups():
    doi = match.groups()[0].replace('\\', '')
    if _DEBUG_BIB_CRAWLER:
      print(f"DOI from ACM Lookup: {doi}\n")
    return doi

  return None


def ieee_xplore_lookup(title: str) -> Optional[str]:
  """Looks up a title on IEEE Xplore.

    This function searches the public IEEE search engine and parses the
    resulting HTML for embedded JSON metadata to find the DOI.

    Args:
        title: The title of the publication.

    Returns:
        The found DOI string, or None.
    """
  search_url = f'https://ieeexplore.ieee.org/search/searchresult.jsp?queryText={requests.utils.quote(title)}'
  html = request_url(search_url)

  # IEEE embeds document metadata in a script tag.
  match = _RE_IEEE_METADATA.search(html)
  if not match:
    return None

  try:
    # The matched group is a JavaScript object, which is valid JSON.
    metadata_str = match.group(1)
    metadata = json.loads(metadata_str)
    # The DOI can be in 'doi' or sometimes 'DOI' key.
    doi = metadata.get("doi") or metadata.get("DOI")
    if doi:
      if _DEBUG_BIB_CRAWLER:
        print(f"DOI from IEEE Xplore Lookup: {doi}\n")
      return doi
  except (json.JSONDecodeError, KeyError) as e:
    print(f"Could not parse IEEE Xplore metadata: {e}")

  return None


def arxiv_lookup(title: str) -> Optional[str]:
  """Looks up a title on arXiv.

    Note: arXiv uses its own identifier which can be formatted as a DOI.

    Args:
        title: The title of the publication.

    Returns:
        The found arXiv ID formatted as a DOI string, or None.
    """
  search_url = f'https://arxiv.org/search/?query={requests.utils.quote(title)}&searchtype=all&abstracts=show&size=50'
  html = request_url(search_url)
  match = _RE_ARXIV_DOI.search(html)
  if match:
    arxiv_id = match.group(1)
    formatted_doi = f"10.48550/arXiv.{arxiv_id}"
    if _DEBUG_BIB_CRAWLER:
      print(f"DOI from arXiv Lookup: {formatted_doi}\n")
    return formatted_doi
  return None


def bing_lookup(title: str) -> Optional[str]:
  """Performs a Bing search as a fallback to find a DOI from publisher sites.

    Args:
        title: The title of the publication.

    Returns:
        The found DOI string, or None.
    """
  search_url = f'https://www.bing.com/search?q={requests.utils.quote(title)}'
  html = request_url(search_url)

  # Check for direct DOI links or publisher-specific URLs in order of reliability.
  m = (_RE_DOI_URL.search(html) or _RE_DOI_ACM_URL.search(html) or
       _RE_DOI_SPRINGER.search(html) or _RE_DOI_WILEY.search(html))

  if m and m.groups():
    return m.groups()[0].replace('\\', '')

  return None


def fix_underscore(s: str) -> str:
  """Escapes underscores in a string for BibTeX compatibility."""
  return re.sub(r'(?<!\\)_', r'\\_', s)


def capitalize(s: str, splitter: str = ' ') -> str:
  """Capitalizes a string (e.g., a title) according to academic rules.

    Args:
        s: The string to capitalize.
        splitter: The delimiter for splitting words.

    Returns:
        The capitalized string.
    """
  SPECIAL_WORDS = {
      'a', 'an', 'the', 'to', 'on', 'in', 'of', 'at', 'by', 'for', 'or', 'and',
      'vs.', 'via'
  }
  EXACT_CASE_WORDS = {'iOS', '2D', '3D', '4D', '6DoF'}

  s = s.strip(',.- ')
  words = s.split(splitter)
  capitalized_words = []

  for i, word in enumerate(words):
    if not word:
      continue

    lowered_word = word.lower()
    if i > 0 and lowered_word in SPECIAL_WORDS and ':' not in words[i - 1]:
      capitalized_words.append(lowered_word)
    elif word in EXACT_CASE_WORDS:
      capitalized_words.append(word)
    else:
      capitalized_words.append(word[0].upper() + word[1:])

  s = splitter.join(capitalized_words)
  # Recurse for hyphenated words.
  return s if splitter == '-' else capitalize(s, '-')


def main():
  """Main execution function."""
  manager = BibTexManager(output_file=_OUTPUT_FILE,
                          use_offline_doi=_USE_OFFLINE_DOI)

  for filename in _INPUT_FILE_LIST:
    print(f"--- Processing {filename} ---")
    try:
      with open(filename, 'r', encoding='utf-8') as bibfile:
        library = bibtexparser.load(bibfile)

      for entry in library.entries:
        manager.process_entry(entry)

    except FileNotFoundError:
      print(f"Error: Input file not found at {filename}")
    except Exception as e:
      print(f"An error occurred while processing {filename}: {e}")

  manager.shutdown()


if __name__ == "__main__":
  main()
